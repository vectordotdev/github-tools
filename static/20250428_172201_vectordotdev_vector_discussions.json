[
    {
        "number": 22953,
        "title": "One log, multiple outputs, how to collect vector and store it in clickhouse",
        "bodyText": "gateway.log\n\u666e\u901a\u65e5\u5fd7\n[timestamp] [thread] [level] class - message\n\u5f02\u5e38\u65e5\u5fd7\n[timestamp] [thread] [level] class - message: xxxxxx xx null\njava.lang.RuntimeException: xxxxxx xx null\nat com.xxxx\nat com.xxxx\nat com.xxxx\nat com.xxxx\n\u5f02\u5e38\u65e5\u5fd7\n[timestamp] [thread] [level] class - message: xxxxxx xx null\njava.lang.RuntimeException: xxxxxx xx xxx: Message:\nHTTP response code: 404\nHTTP response body: {\"xxxx[xxx]\",\"xxxx[xxxxx]\"}\nHTTP response headers: {\"xxxx\",\nxxxx[xxxx]}\nat com.xxxx\nat com.xxxx\nat com.xxxx\nat com.xxxx\n\u5176\u5b83\u65e5\u5fd7\n[timestamp] [thread] [level] class - message : {\n\"xxxxx\" : \"xxxxx\",\n\"xxxxx\" : \"xxxxx\",\n\"xxxxx\" : \"xxxxx\",\n\"xxxxx\" : \"xxxxx\",\n\"xxxxx\" : \"xxxxx\"\n}",
        "url": "https://github.com/vectordotdev/vector/discussions/22953",
        "createdAt": "2025-04-27T03:09:46Z",
        "updatedAt": "2025-04-28T18:23:41Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "initampk"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22828,
        "title": "datadog_agent Traces forwarding",
        "bodyText": "Hi, we observe several Kubernetes clusters using the Datadog agent (traces & metrics) and Vector Agent for log forwarding to a Vector Aggregation service and on to Datadog.\nIn our DatadogAgent -> Vector Aggregator -> Datadog for traces we are seeing no data in the Vector Aggregation Service (yip have multiple_outputs: true set). The bewildering bit is even with the Datadog agent configured for alternate data destination the data still arrives at Datadog but never our Vector Aggregation service. Metrics are working perfectly fine.\nWe use the Datadog operator to deploy the Datadog agents with the following envars configured.\n    DD_OBSERVABILITY_PIPELINES_WORKER_TRACES_ENABLED: \"true\"\n    DD_OBSERVABILITY_PIPELINES_WORKER_TRACES_URL: \"https://amazing.aggregator.com:8282\"\n    DD_OBSERVABILITY_PIPELINES_WORKER_METRICS_ENABLED: \"true\"\n    DD_OBSERVABILITY_PIPELINES_WORKER_METRICS_URL: \"https://amazing.aggregator.com:8282\"\n    DD_OBSERVABILITY_PIPELINES_WORKER_LOGS_ENABLED: \"true\"\n    DD_OBSERVABILITY_PIPELINES_WORKER_LOGS_URL: \"https://amazing.aggregator.com:8282\"\n\nhttps://github.com/DataDog/datadog-agent/blob/main/pkg/config/config_template.yaml#L676\nWe are totally stumped. Appreciate this is a blend of Datadog and Vector questions, but we are all the same family here \ud83d\ude04 so hoping the the gurus have an insight as to what might be happening. Thank you.",
        "url": "https://github.com/vectordotdev/vector/discussions/22828",
        "createdAt": "2025-04-08T04:11:55Z",
        "updatedAt": "2025-04-28T17:55:41Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tbenade"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 1
    },
    {
        "number": 22951,
        "title": "Vector Source/Sink Question using NLB",
        "bodyText": "I have 5 nodes sending traffic to 2 nodes using the Vector sink and Vector source.  The 2 nodes are behind an AWS NLB which uses a hash flow hash algorithm based on the protocol, source IP address, source port, destination IP address, destination port, and TCP sequence number.  The TCP connections from a client have different source ports and sequence numbers, and can be routed to different targets. Each individual TCP connection is routed to a single target for the life of the connection.\nMy question:\nWith this setup why would my traffic always be routing to 1 of the two nodes behind the NLB.  I can stop vector and traffic immediately swaps to the other node so I know all is working and healthy.  Based on the NLB hash flow hash algorithm is the vector source / sink doing something to keep the connection alive?",
        "url": "https://github.com/vectordotdev/vector/discussions/22951",
        "createdAt": "2025-04-25T21:00:26Z",
        "updatedAt": "2025-04-25T21:00:26Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "satellite-no"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22913,
        "title": "Question about the prometheus Sink",
        "bodyText": "I want to migrate our statsd exporter to Vector with Prometheus exporter.\nWe set the sidecar vector to receive statsd data and send it to the central vector.\nI have a few questions to ask.\n\nIs there a way to use a single port for the Prometheus exporter sink and apply different bucket configurations per metric?\n\n\nStatsd-exporter can configure this, but I cannot find the option in Vector.\n\n\nIf I set the expire_metrics_per_metric_set, does this apply to the final metrics name after using the transform process and default namespace, or will it apply to the original metric name?\n\nI appreciate any help you can provide.",
        "url": "https://github.com/vectordotdev/vector/discussions/22913",
        "createdAt": "2025-04-21T06:27:48Z",
        "updatedAt": "2025-04-25T19:51:32Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "daniel-lee-sb"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22915,
        "title": "How Do I Modify the Vector Sink Configuration to Reduce the Error Reported in ES 429?",
        "bodyText": "vector.yaml:\nsinks:\n  k8s_log:\n    type: elasticsearch\n    endpoints:\n      - https://elk-elasticsearch-client-headless.elk.svc.cluster.local:9200\n    inputs:\n      - k8s_log\n    auth:\n      strategy: basic\n      user: \"xxxxx\"\n      password: \"xxxxx\"\n    batch:\n      timeout_secs: 3600\n      max_events: 100000\n    buffer:\n      max_size: 8388608000\n      type: disk\n      when_full: block\n    compression: gzip\n    request:\n      rate_limit_duration_secs: 1\n      rate_limit_num: 10000\n    bulk:\n      index: \"dev-{{ .log_source }}-%Y.%m.%d\"\n    tls:\n      ca_file: \"/usr/share/vector/certs/ca.crt\"\n\nWhen the amount of logs increases, I get a lot of errors:\nWARN sink{component_kind=\"sink\" component_id=k8s_log component_type=elasticsearch}:request{request_id=388}: vector::sinks::util::retries: Retrying after response. reason=too many requests internal_log_rate_limit=true\nERROR sink{component_kind=\"sink\" component_id=k8s_log component_type=elasticsearch}:request{request_id=389}: vector::sinks::elasticsearch::service: Response contained errors. error_code=\"http_response_429\" response=Response { status: 429, version: HTTP/1.1, headers: {\"x-elastic-product\": \"Elasticsearch\", \"content-type\": \"application/json; charset=UTF-8\", \"content-length\": \"565\"}, body: b\"{\\\"error\\\":{\\\"root_cause\\\":[{\\\"type\\\":\\\"es_rejected_execution_exception\\\",\\\"reason\\\":\\\"rejected execution of coordinating operation [coordinating_and_primary_bytes=318706156, replica_bytes=0, all_bytes=318706156, coordinating_operation_bytes=4010923, max_coordinating_and_primary_bytes=322122547]\\\"}],\\\"type\\\":\\\"es_rejected_execution_exception\\\",\\\"reason\\\":\\\"rejected execution of coordinating operation [coordinating_and_primary_bytes=318706156, replica_bytes=0, all_bytes=318706156, coordinating_operation_bytes=4010923, max_coordinating_and_primary_bytes=322122547]\\\"},\\\"status\\\":429}\" }\nSome of the options I've configured don't seem to solve this error, if there are better vector configuration parameters, or modify the configuration of ES.",
        "url": "https://github.com/vectordotdev/vector/discussions/22915",
        "createdAt": "2025-04-21T09:21:36Z",
        "updatedAt": "2025-04-25T19:41:09Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "sunyoona"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22919,
        "title": "can I  use both auto_extract_timestamp and timestamp_key in splunk_hec_logs",
        "bodyText": "I have a sceanio where some events have timestamp and some are not. while forwarding to splunk. it takes the time of ingestion.\ncan I use both auto_extract_timestamp and timestamp_key and keep auto_extract_timestamp as true for the logs that has timestamp. and use timestamp_key for logs that don't have timestamp in it.",
        "url": "https://github.com/vectordotdev/vector/discussions/22919",
        "createdAt": "2025-04-21T22:34:01Z",
        "updatedAt": "2025-04-25T19:36:46Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "shamj"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22890,
        "title": "File source not reading all the files",
        "bodyText": "I am using file source as below:\nsources:\nfile_logs:\ntype: file\ninclude: [\"/var/log/vector/*\"]\nThere are 101 files in /var/log/vector . But it's reading only 32 files and skipping others. I don't see any errors or warning .\nPlease help.",
        "url": "https://github.com/vectordotdev/vector/discussions/22890",
        "createdAt": "2025-04-16T00:26:54Z",
        "updatedAt": "2025-04-25T17:43:38Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "shamj"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 22937,
        "title": "Can I use  transformation type sample for dynamic sampling",
        "bodyText": "I have csv file that has schema for service and sampled_rate.\nWhile using transformation of type sample (below config) : the output shows sampled_rate as 1 , not the one in csv file. Can I use key as rate ?\ndynamic_sample:\ntype: sample\ninputs:\n- add_sample_rate\ngroup_by: \"{{ .service }}\"\nrate: 1\nsample_rate_key: .sample_rate",
        "url": "https://github.com/vectordotdev/vector/discussions/22937",
        "createdAt": "2025-04-23T16:35:20Z",
        "updatedAt": "2025-04-25T17:24:06Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "shamj"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22943,
        "title": "Can't get dedup to work",
        "bodyText": "Hi all, I'm new to Vector, only started experimenting two weeks ago. I have an issue where I would like to dedup some events.\nMy config looks like this at the moment:\n  parse_saml_assertions_logs:\n    type: \"remap\"\n    inputs: [saml_assertions_logs]\n    source: |\n      parts = split(string!(.message), \"|\")\n        .forwarded_for = parts[0]\n        .user = parts[1]\n        .timestamp = parse_timestamp!(parts[2], \"%+\")\n        .host = parts[3]\n        .user_agent = parts[4]\n        .assertion_id = parts[5]\n        .name_id = parts[6]\n      del(.message)\n\n  dedupe_saml_assertions_logs:\n      type: \"dedupe\"\n      inputs: [parse_saml_assertions_logs]\n      fields:\n        ignore: [\".timestamp\"]\n\nSo \"sinking\" parse_saml_assertions_logs works fine, however sinking dedupe_saml_assertions_logs does result in zero output. I tried with fields.match as well. What am I missing?",
        "url": "https://github.com/vectordotdev/vector/discussions/22943",
        "createdAt": "2025-04-24T13:31:02Z",
        "updatedAt": "2025-04-24T19:01:53Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "securitymonster"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22924,
        "title": "Performance Tuning Issue",
        "bodyText": "Hi,\nI have sort of an odd setup, 1 node group of 3 nodes that is setup to receive data largely from aws_s3.  This group then send its via vector sink to vector source on another vector node group of 2, all nodes are c6a.2xlarge.  The node group that is receiving data is avg around 30K events/s each and CPU is pegged 100% all the time and disk buffer is constantly at around 12GB on each node.  BUT the nodes recieiving and doing most of the processing are barely breathing hard like 0-20% CPU load.\nThe bottle neck seems to be on sending data fast enough but Im not certain.  Any thoughts on how to better set this up or changes to configs?",
        "url": "https://github.com/vectordotdev/vector/discussions/22924",
        "createdAt": "2025-04-22T14:10:18Z",
        "updatedAt": "2025-04-24T17:40:35Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "satellite-no"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22939,
        "title": "Is HoneyComb log sink alright?",
        "bodyText": "Hi,\nI\u2019m trying to ship logs from Fly.io via their Vector template.\nTo that end I set the dataset to the same dataset where we ship traces to and then generated a new API ingestion key (hcaik_). I gave it permission to manage datasets just in case.\nYet I don't see any logs sinking in. Vector topology checks are passing just fine.\n2025-04-23T19:21:50.152 app[5683d923a064e8] lhr [info] Configured sinks:\n\n2025-04-23T19:21:50.167 app[5683d923a064e8] lhr [info] hyperdx\n\n2025-04-23T19:21:50.168 app[5683d923a064e8] lhr [info] honeycomb\n\n2025-04-23T19:21:50.260 app[5683d923a064e8] lhr [info] 2025-04-23T19:21:50.260061Z INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=trace,rdkafka=info,buffers=info,lapin=info,kube=info\"\n\n2025-04-23T19:21:50.271 app[5683d923a064e8] lhr [info] 2025-04-23T19:21:50.271121Z INFO vector::app: Loading configs. paths=[\"/etc/vector/vector.toml\", \"/etc/vector/sinks\"]\n\n2025-04-23T19:21:50.419 app[5683d923a064e8] lhr [info] 2025/04/23 19:21:50 INFO SSH listening listen_address=[fdaa:0:6924:a7b:13d:6114:4744:2]:22\n\n2025-04-23T19:21:52.042 app[5683d923a064e8] lhr [info] 2025-04-23T19:21:52.041207Z INFO vector::topology::running: Running healthchecks.\n\n2025-04-23T19:21:52.042 app[5683d923a064e8] lhr [info] 2025-04-23T19:21:52.041270Z INFO vector::topology::builder: Healthcheck passed.\n\n2025-04-23T19:21:52.042 app[5683d923a064e8] lhr [info] 2025-04-23T19:21:52.041287Z INFO vector::topology::builder: Healthcheck passed.\n\n2025-04-23T19:21:52.042 app[5683d923a064e8] lhr [info] 2025-04-23T19:21:52.041292Z INFO vector::topology::builder: Healthcheck passed.\n\n2025-04-23T19:21:52.042 app[5683d923a064e8] lhr [info] 2025-04-23T19:21:52.042650Z INFO vector: Vector has started. debug=\"false\" version=\"0.29.1\" arch=\"x86_64\" revision=\"74ae15e 2023-04-20 14:50:42.739094536\"\n\n2025-04-23T19:21:52.050 app[5683d923a064e8] lhr [info] 2025-04-23T19:21:52.049028Z INFO vector::sinks::prometheus::exporter: Building HTTP server. address=[::]:9598\n\n2025-04-23T19:21:52.055 app[5683d923a064e8] lhr [info] 2025-04-23T19:21:52.055092Z INFO vector::sinks::blackhole::sink: Collected events. events=0 raw_bytes_collected=0\n\n2025-04-23T19:21:52.058 app[5683d923a064e8] lhr [info] 2025-04-23T19:21:52.057526Z INFO vector::internal_events::api: API server running. address=[::]:8686 playground=http://:::8686/playground\n\n2025-04-23T19:21:52.377 app[5683d923a064e8] lhr [info] 2025-04-23T19:21:52.376821Z INFO vector::topology::builder: Healthcheck passed.\n\nRegards,\nYevhenii",
        "url": "https://github.com/vectordotdev/vector/discussions/22939",
        "createdAt": "2025-04-23T19:23:23Z",
        "updatedAt": "2025-04-23T19:23:24Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "lessless"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22932,
        "title": "Merging two sources and perform an action",
        "bodyText": "I have two sources:\nOne is a number of S3 buckets allowed in a datacenter.\nThe second source is the number of buckets used.\nI have reformatted the log lines so that they both have a common key which is \"datacenter\".\nI want to merge these two sources and subtract the number of buckets used from the number of buckets entitled.\n{\"account\":\"123456\",\"bucket_limit\":\"1000\",\"byte_limit\":\"5\",\"datacenter\":\"us-east-1\",\"objects_limit\":\"50\"}\n{\"count\":2,\"datacenter\":\"us-east-1\"}\n\nSo all I want is to subtract 2 from 1000 then I will transform that into a metric and send it to a sink.\nIs this possible?",
        "url": "https://github.com/vectordotdev/vector/discussions/22932",
        "createdAt": "2025-04-23T13:04:52Z",
        "updatedAt": "2025-04-23T18:14:49Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "nathanle"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22905,
        "title": "FIltering and Tests for Filters",
        "bodyText": "Trying to write a simple filter and I think i'm over complicating things.\nGoal: drop \"unsupported\" file types from s3.  type examples are .snappy or .parquet.\ntranform:\ntype: filter\ninputs:\n  - my-s3-bucket\ncondition:\n  type: vrl\n  source: |\n    !ends_with(string!(.object), \"snappy\", case_sensitive: false) ||\n    !ends_with(string!(.object), \"parquet\", case_sensitive: false)\ntest:\nname: Drop unsupported S3 objects\ninputs:\n  - type: vrl\n    insert_at: drop_my-s3-bucket\n    source: |-\n      . = {\n        \"bucket\":\"my-s3-bucket\",\n        \"message\":\"gibberesh\",\n        \"object\":\"raw/win/log_date=2025-04-14/3d52fdb4-d5d5-4f3f-b944-c7fa0820e51d.snappy\",\n        \"region\":\"us-east-1\",\n        \"source_type\":\"aws_s3\",\n        \"timestamp\":\"2025-04-14T10:13:29Z\"\n      }\n  - type: vrl\n    insert_at: drop_my-s3-bucket\n    source: |-\n      . = {\n        \"bucket\":\"my-s3-bucket\",\n        \"message\":\"gibberesh\",\n        \"object\":\"raw/win/log_date=2025-04-14/3d52fdb4-d5d5-4f3f-b944-c7fa0820e51d.parquet\",\n        \"region\":\"us-east-1\",\n        \"source_type\":\"aws_s3\",\n        \"timestamp\":\"2025-04-14T10:13:29Z\"\n      }\n  - type: vrl\n    insert_at: drop_my-s3-bucket\n    source: |-\n      . = {\n        \"bucket\":\"my-s3-bucket\",\n        \"message\":\"gibberesh\",\n        \"object\":\"raw/win/log_date=2025-04-14/3d52fdb4-d5d5-4f3f-b944-c7fa0820e51d.gz\",\n        \"region\":\"us-east-1\",\n        \"source_type\":\"aws_s3\",\n        \"timestamp\":\"2025-04-14T10:13:29Z\"\n      }\n\noutputs:\n  - extract_from: core_s3\n    conditions:\n      - type: vrl\n        source: |-\n          # Validate .ch_database attributes.\n          assert!(exists(.object))\n          assert!(is_string(.object))\n          assert!(!ends_with(string!(.object), \"snappy\", case_sensitive: false) ||\n                      !ends_with(string!(.object), \"parquet\", case_sensitive: false),\n                  message: \"Unsupported S3 object format\")\nMy hope would be 1 message makes it through to core_s3 and that event doesnt include the .snappy or .parquet file extenstions.",
        "url": "https://github.com/vectordotdev/vector/discussions/22905",
        "createdAt": "2025-04-17T21:44:08Z",
        "updatedAt": "2025-04-22T19:37:25Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "satellite-no"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22891,
        "title": "Is there any way to access source config attribute in VRL",
        "bodyText": "for example:\nsources:\n  my_source:\n    type: http_client\n    endpoint: https://google.com\n\ntransforms:\n  my_transform:\n    type: remap\n    inputs:\n      - my_source\n    source: |\n      # how can I get the source config attributes like:\n      my_source.endpoint\n      my_source.type",
        "url": "https://github.com/vectordotdev/vector/discussions/22891",
        "createdAt": "2025-04-16T03:14:57Z",
        "updatedAt": "2025-04-21T16:28:20Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "liuyangc3"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 3
    },
    {
        "number": 22916,
        "title": "`vector validate` emits error `TCP bind failed: Address already in use (os error 98)` when doing systemd reload",
        "bodyText": "Hi Vector maintainers,\nI see that the command vector validate has problem with the case of reloading.\nI'm using Vector version 0.44.0.\nThis is the systemd default configurations for vector.service on Ubuntu 22.04.\n# /lib/systemd/system/vector.service\n[Unit]\nDescription=Vector\nDocumentation=https://vector.dev\nAfter=network-online.target\nRequires=network-online.target\n\n[Service]\nUser=vector\nGroup=vector\nExecStartPre=/usr/bin/vector validate\nExecStart=/usr/bin/vector\nExecReload=/usr/bin/vector validate\nExecReload=/bin/kill -HUP $MAINPID\nRestart=always\nAmbientCapabilities=CAP_NET_BIND_SERVICE\nEnvironmentFile=-/etc/default/vector\n# Since systemd 229, should be in [Unit] but in order to support systemd <229,\n# it is also supported to have it here.\nStartLimitInterval=10\nStartLimitBurst=5\n[Install]\nWantedBy=multi-user.target\n\nHere is the simple vector config file with datadog agents\nsources:\n  datadog_agents:\n    type: datadog_agent\n    address: 0.0.0.0:9000\n    disable_logs: true\n    disable_traces: true\n    store_api_key: true\n\nsinks:\n  datadog_platform:\n    inputs:\n      - datadog_agents\n    type: datadog_metrics\nWith that config, the running process of vector already takes the port 9000. So when running vector validate, it would always result in this error because obviously the port is still being used by the current vector process:\n$ vector validate\n2025-04-21T10:57:33.627587Z  INFO vector::app: Log level is enabled. level=\"INFO\"\n\u221a Loaded [\"/etc/vector/vector.yaml\"]\n\nComponent errors\n----------------\nx Source \"datadog_agents\": TCP bind failed: Address already in use (os error 98)\n\nI can totally just run the kill -HUP <pid> but I think it would be better if the vector validate is able to handle such above case.",
        "url": "https://github.com/vectordotdev/vector/discussions/22916",
        "createdAt": "2025-04-21T11:00:56Z",
        "updatedAt": "2025-04-21T15:45:41Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "techministrator"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22910,
        "title": "Use dosubot to label issues",
        "bodyText": "Hi, I recently discovered dosubot https://dosu.dev/ and a lot of repositories are using this (for example, opendal). It allows to automatic issue labeling, and I think it works pretty well and that it would reduce maintenance burden in Vector.\nWhat are your thoughts about this?",
        "url": "https://github.com/vectordotdev/vector/discussions/22910",
        "createdAt": "2025-04-20T12:47:35Z",
        "updatedAt": "2025-04-20T12:47:41Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "jorgehermo9"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 21919,
        "title": "Cannot find logs in Datadog.",
        "bodyText": "Hi everyone,\nI'm having trouble viewing my logs in Datadog. I've set up Vector to send logs to Datadog, but I can't see them in the Datadog portal. Below is my current Vector configuration. Could someone help me identify what might be wrong or missing in my setup?\ndata_dir: /var/lib/vector\n\napi:\n  enabled: true\n  address: 127.0.0.1:8686\n\nsources:\n  my_source:\n    type: socket\n    address: 0.0.0.0:5170\n    mode: tcp\n    max_length: 128\n    buffer_size: 356\n    encoding:\n      codec: json\n\ntransforms:\n  extract_message:\n    type: remap\n    inputs:\n      - my_source\n    source: |\n      .nxlog_eikon_message, err = parse_json(.message)\n      if err == null {\n        . = { \"nxlog_eikon_message\": .nxlog_eikon_message, \"nxlog_eikon_type\": \"json\" }\n      } else {\n        del(.)\n      }\n\n  add_tags:\n    type: remap\n    inputs:\n      - extract_message\n    source: |\n      .ddtags = {\n        \"mnd-applicationid\": \"app-xxxx\",\n        \"mnd-applicationname\": \"data_api\",\n        \"mnd-owner\": \"dataclouddevelopment@xxx.com\",\n        \"mnd-supportgroup\": \"APP-ANALYTICS-xxx-CLOUD\",\n        \"mnd-projectcode\": \"xxx-xx\",\n        \"mnd-costcentre\": \"xxxxxxxxx\",\n        \"mnd-dataclassification\": \"restricted\",\n        \"mnd-baseimagename\": \"notapplicable\",\n        \"mnd-envtype\": \"dev\",\n        \"mnd-envsubtype\": \"sandbox\",\n        \"mnd-lifecycle\": \"pre-live\",\n        \"opt-componentname\": \"xxx_api\",\n        \"opt-environment\": \"beta_ibc\",\n        \"opt-region\": \"AMERS\"\n      }\n      .service = \"xxx_api\"\n      .ddsource = \"usage\"\n      .host = \"XXX-XXXXXXX\"\n      .hostname = \"XXX-XXXXXX\"\n      .vector_tag = \"yes\"\n      .timestamp = \"2024-11-29T00:00:00.000000000Z\"\n      .env = \"dev\"\n\nsinks:\n  my_console_sink:\n    type: console\n    inputs:\n      - add_tags\n    encoding:\n      codec: json\n\n  my_datadog_sink1:\n    type: datadog_logs\n    inputs:\n      - add_tags\n    api_key: \"your_datadog_api_key_1\"\n    compression: gzip\n    site: \"datadoghq.eu\"\n    tags: [\"mnd-applicationid:app-xxxx\", \"mnd-applicationname:dataxxx_api\", \"mnd-owner:dataclouddevelopment@XXX.com\", \"mnd-supportgroup:APP-ANALYTICS-XXXX-CLOUD\", \"mnd-projectcode:1267-10\", \"mnd-costcentre:XXXXX\", \"mnd-dataclassification:restricted\", \"mnd-baseimagename:notapplicable\", \"mnd-envtype:dev\", \"mnd-envsubtype:sandbox\", \"mnd-lifecycle:pre-live\", \"opt-componentname:xxx_api\", \"opt-environment:beta_ibcxx\", \"opt-region:AMERS\"]\n    skip_ssl_verification: true\n\n  my_datadog_sink2:\n    type: datadog_logs\n    inputs:\n      - add_tags\n    api_key: \"your_datadog_api_key_2\"\n    compression: gzip\n    site: \"datadoghq.eu\"\n    tags: [\"mnd-applicationid:app-xxxxxx\", \"mnd-applicationname:dataxxx_api\", \"mnd-owner:dataclouddevelopment@XXX.com\", \"mnd-supportgroup:APP-ANALYTICS-XXX-CLOUD\", \"mnd-projectcode:1267-10\", \"mnd-costcentre:XXXXXXX\", \"mnd-dataclassification:restricted\", \"mnd-baseimagename:notapplicable\", \"mnd-envtype:dev\", \"mnd-envsubtype:sandbox\", \"mnd-lifecycle:pre-live\", \"opt-componentname:xxx_api\", \"opt-environment:beta_ibcxx\", \"opt-region:AMERS\"]\n    skip_ssl_verification: true\n\n  my_tcp_sink:\n    type: socket\n    inputs:\n      - extract_message\n    address: \"xxxxxxxxxxx.com:12201\"\n    mode: tcp\n    encoding:\n      codec: json\n\nAny help or suggestions would be greatly appreciated!\nvector.txt",
        "url": "https://github.com/vectordotdev/vector/discussions/21919",
        "createdAt": "2024-11-29T09:32:06Z",
        "updatedAt": "2025-04-18T14:10:48Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "bandisudhir"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22901,
        "title": "Use alternative sink if primary is failing?",
        "bodyText": "We've got a legacy log processing system using dockers Splunk log driver with containers and a long chain of Splunk Heavy Forwarders ultimately pushing to an on-prem Splunk instance. Due to a lot of complexity and legacy we're considering switching to Vector as from prior experience it seems to perform well and be way more flexible.\nWe've some very strict compliance needs in particular that certainly logs can't be lost so we need to have some sort of way to handle the situation that the final Splunk endpoint is down. Having read up on the memory and disk buffers and the recovery/back pressure handling it seems good but ultimately if Splunk is down for an extended period, we need to fall back to something else.\nWith AWS Kinesis Firehose you can configure undeliverable events to be dumped to S3 which is helpful. However it doesn't seem that theres any sort of logic in Vector for this? i.e. \"if cant deliver to X, send events to Y\". Nearest solution I could come up with is two sinks - one Splunk and the other S3 and use a lifecycle rule to purge logs from S3 after X days unless we need to grab them to re-ingest. Not ideal and adds cost.\nIs there any kind of solution to a fall-back sink in case of prolonged issues? Or is this something that would be a viable feature? I.e a third option for buffers to send to an alternate sync or an alternate target if a given sink is having issues?\nWould love any input on how we might achieve this.",
        "url": "https://github.com/vectordotdev/vector/discussions/22901",
        "createdAt": "2025-04-17T13:49:42Z",
        "updatedAt": "2025-04-25T20:52:41Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "NeilJed"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22904,
        "title": "Changing metric emitted by datadog agent to a metric that is otlp proto format compatible using remap transform",
        "bodyText": "Given the restrictions mentioned in this doc https://vector.dev/docs/reference/configuration/transforms/remap/#event-data-model regarding metric event data model, I was wondering if it is possible to change a metric emitted by datadog agent to a metric that is ingestible by a otlp compatible observability endpoint using remap (vrl language) transform?\nI am trying to do the same thing using remap transform, but all the new fields that I am trying to add to the metric event are ignored.",
        "url": "https://github.com/vectordotdev/vector/discussions/22904",
        "createdAt": "2025-04-17T17:19:52Z",
        "updatedAt": "2025-04-17T18:37:27Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "hamidp555"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22902,
        "title": "Vector Agent Log Push Issue and Metrics Logging",
        "bodyText": "Hello everyone,\nI have a setup where the Vector agent is installed on multiple machines. I've noticed that on some nodes, the Vector agent retains logs in memory and does not push them to Datadog as expected. Interestingly, when I restart the Vector service (installed as a Windows service), the logs start appearing in Datadog.\nCould anyone help diagnose what might be causing this issue?\nAdditionally, I'm looking for a way to monitor dropped logs, logs in the queue, and processed logs. Is there a method to log this information to a specific destination? Can the internal_metrics or internal_logs features assist with this? if yes can you please help with the configuration.\nThank you in advance for your help!",
        "url": "https://github.com/vectordotdev/vector/discussions/22902",
        "createdAt": "2025-04-17T14:02:11Z",
        "updatedAt": "2025-04-17T14:02:13Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bandisudhir"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22892,
        "title": "Vector's memory usage is erratic",
        "bodyText": "Today I conducted two runs (each lasting around an hour) to ingest logs into a deployment of vector running on AWS. The rate at which the logs were generated for both the runs was the same.\nVector was reading the SQS event notifications coming from the input S3 bucket to perform some transformations on the logs and output them to a output S3 bucket.\nI noticed that Vector was consuming around 3 GB of memory for the first 40 minutes of the first run and then the memory used shot up to around 25 GB. The entire duration of the second run used around 25 GB.\nBetween the two runs, the docker container of vector was restarted.\ntimberio/vector:0.46.1-debian is the docker image I was using.\nFew days ago I had used timberio/vector:0.46.0.custom.fba8185-debian docker image and conducted multiple runs (with the same log generation rate and duration) where I observed that vector would be producing the output appropriately (there was no significant lag) while consuming 3 GB of memory for quite some time and then the memory requirment would suddenly shoot up to 20-30 GB.\nAny idea why the memory usage of vector might be so erratic?",
        "url": "https://github.com/vectordotdev/vector/discussions/22892",
        "createdAt": "2025-04-16T09:41:05Z",
        "updatedAt": "2025-04-17T04:24:28Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "abhisgup"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22898,
        "title": "file sink use logrotate copytruncate does not truncate successfully",
        "bodyText": "Use helm to deploy vector-aggregator in k8s, and its configuration is as follows:\ndata_dir: /logfs/vector\nacknowledgements:\n  enabled: true\n\napi:\n  address: 0.0.0.0:8686\n  enabled: true\n  playground: false\n\nsources:\n  kafka_source:\n    type: \"kafka\"\n    topics:\n      - \"test1\"\n    group_id: test1\n    bootstrap_servers: xxx\n    librdkafka_options:\n      \"security.protocol\": xxx\n      \"sasl.mechanism\": xxx\n      \"sasl.username\": \"xxx\"\n      \"sasl.password\": \"xxx\"\n\n  internal_metrics:\n    type: internal_metrics\n    scrape_interval_secs: 2\n    tags:\n      host_key: \"\"\n\ntransforms:\n  kafka_message_unpack:\n    inputs:\n      - \"kafka_source\"\n    type: \"remap\"\n    source: |\n      . = parse_json!(.message)\n      parsed_log, err = parse_json(.msg)\n      if err == null {\n        . = merge!(., parsed_log)\n      }\n\n  reduce_message:\n    inputs:\n      - \"kafka_message_unpack\"\n    type: reduce\n    max_events: 50\n    merge_strategies:\n      message: concat_newline\n\nsinks:\n  test_log_file:\n    type: file\n    inputs:\n      - \"reduce_message\"\n    path: /logfs/vector/{{ print \"{{ category }}\" }}/${POD_NAME}/{{ print \"{{ category }}\" }}-%F.log\n    encoding:\n      codec: raw_message\n\n  prom-exporter:\n    type: prometheus_exporter\n    inputs:\n      - internal_metrics\n    address: 0.0.0.0:9598\n\nI want to limit the file size. I searched and found that vector has no relevant function and suggested using logrotate to implement it. Therefore, logrotate is used to rotate according to the file size, and the configuration is as follows:\n# test the specified file\n/fuse/logfs/vector/ban.ban_read_flink_log/vector-aggregator-0/ban.ban_read_flink_log-2025-04-17.log {\n        daily\n        dateext\n        dateformat -%Y%m%d%H%M%S\n        copytruncate\n        maxsize 500M\n        rotate -1\n        missingok\n        notifempty\n}\n\nAfter testing, it was found that Logrotate Successful, original file goes back to original size\nVerbose Output of Logrotate\ncopying /fuse/logfs/vector/ban.ban_read_flink_log/vector-aggregator-0/ban.ban_read_flink_log-2025-04-17.log to /fuse/logfs/vector/ban.ban_read_flink_log/vector-aggregator-0/ban.ban_read_flink_log-2025-04-17.log-20250417102809\ntruncating /fuse/logfs/vector/ban.ban_read_flink_log/vector-aggregator-0/ban.ban_read_flink_log-2025-04-17.log\n\nLog file after truncate happens\n-rw-r--r-- 1 root root    0 Apr 17 10:28 ban.ban_read_flink_log-2025-04-17.log\n-rw-r--r-- 1 root root 632M Apr 17 10:28 ban.ban_read_flink_log-2025-04-17.log-20250417102809\n\nLiterally Seconds Later\n-rw-r--r-- 1 root root 634M Apr 17 10:28 ban.ban_read_flink_log-2025-04-17.log\n-rw-r--r-- 1 root root 632M Apr 17 10:28 ban.ban_read_flink_log-2025-04-17.log-20250417102809\n\nod -c ban.ban_read_flink_log-2025-04-17.log | head -n 10\n0000000  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0\n*\n4736315260  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0  \\0   S   U   C\n4736315300   C   E   S   S   (   S   c   a   n   B   y   I   P   U   A   N\n4736315320   U   R   L       D   o   S   (   2   2   2   /   6   0   0   .\n4736315340   0   )   )   ,       p   a   y   l   o   a   d   :       {   \"\n4736315360   r   u   l   e   _   t   y   p   e   \"   :       \"   s   c   a\n4736315400   n   _   c   o   u   n   t   \"   ,       \"   i   p   \"   :\n4736315420   \"   1   1   0   .   1   6   6   .   2   1   3   .   1   4   4\n4736315440   \"   ,       \"   u   s   e   r   _   a   g   e   n   t   \"   :\n\nBut strangely, when starting local vector -c vector.yaml, a file of about 1G size is created through dd, and then logs are generated through demo_logs and continuously written to the file. After using the same logrotate, no abnormality is reproduced\nIt is suspected that the written oftest has not been reset.ref The file system uses juicefs, which is mounted in the vector running in K8S through juicefs-csi-driver. Logrotate runs in a machine that also mounts the same file system. Is it related to this configuration? How should I make it work properly, or can the file sink provide a corresponding file size limit processing method ?\nI will continue to try to see if there is any easy way to reproduce it.",
        "url": "https://github.com/vectordotdev/vector/discussions/22898",
        "createdAt": "2025-04-17T03:21:09Z",
        "updatedAt": "2025-04-17T03:23:28Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "QSummerY"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22876,
        "title": "Open Telemetry Source GRPC Max Message Size Configuration",
        "bodyText": "Hi Team,\nWe use vector to send logs to storage. I recently saw some errors from our client showing:\nError, message length too large: found 4327818 bytes, the limit is: 4194304 bytes\nAfter doing some research, it seems that 4194304 bytes is the default value. The recommended workarounds were either to reduce the batch size on the client (however there are still chances we could get batches with larger values than default) or to increase the max message size on the server.\nI went through the Vector Opentelemetry Source documentation and see nothing about configuring a max message size in the grpc source config. Am I missing something or is this currently not possible to set from Vector's side.\nThanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/22876",
        "createdAt": "2025-04-14T23:34:17Z",
        "updatedAt": "2025-04-16T20:32:18Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mjesperson"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22740,
        "title": "Proper use of reduce transform to target go panics",
        "bodyText": "I'm trying to use reduce to combine go panics into a single log line with newline delimiters. In my testing, this results in a large drop in events received by the sink:\n\nA cleaned up version my config looks like this:\nsources:\n  kubernetes:\n    type: \"kubernetes_logs\"\ntransforms:\n  combine_go_panics:\n    type: reduce\n    inputs:\n      - kubernetes\n    starts_when:\n      type: vrl\n      source: |\n        starts_with(string!(.message), \"panic: runtime error:\")\n    group_by: [\"file\"]\n    merge_strategies:\n      message: concat_newline\n    expire_after_ms: 1000\n    max_events: 1000\n  go_panic_level:\n    type: remap\n    inputs:\n      - combine_go_panics\n    source: |\n      if starts_with(string!(.message), \"panic: runtime error:\") {\n        .level = \"fatal\"\n      }\nsinks:\n  http:\n    type: \"http\"\n    uri: http://vector-aggregator:${PORT_AGGREGATOR:-8000}\n    encoding:\n      codec: \"json\"\n    inputs:\n    - \"go_panic_level\"\nI was under the impression starts_with would mean an aggregation begins when panic: runtime error is seen and that for any other log a reduce would not begin and it would flow through to the next component as normal. Is that accurate? Any idea on what might be causing what I'm seeing with components received dropping so low on the sink?",
        "url": "https://github.com/vectordotdev/vector/discussions/22740",
        "createdAt": "2025-03-28T16:20:14Z",
        "updatedAt": "2025-04-16T19:04:41Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "davidcpell"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 1
    },
    {
        "number": 22884,
        "title": "Pod logs to vector using Opentelemetry Source",
        "bodyText": "We are currently using the kubernetes_logs source in Vector to collect pod logs, as all logs are being written to the filesystem. This setup works fine because it aligns well with the kubernetes_logs source.\nHowever, due to certain requirements, we are planning to switch to using the opentelemetry source in Vector. We have configured our application to route logs to the OpenTelemetry gRPC endpoint as follows:\n\nOTEL_EXPORTER_OTLP_ENDPOINT: \"http://<vector_svc_ep>:4317\"  # Vector service endpoint\nOTEL_EXPORTER_OTLP_PROTOCOL: \"grpc\"\n\nWe updated the Vector configuration to use the opentelemetry source like this:\n\n[sources.logs]\ntype = \"opentelemetry\"\ngrpc.address = \"0.0.0.0:4317\"\n[transforms.filterlogs]\ntype = \"filter\"\ninputs = [\"logs\"]\n\nHowever, when we run vector validate test.toml, we get the following error:\n\nx Input \"logs\" for filter \"stdout\" doesn't match any components.\n\nThe source ID (logs) matches the inputs field in the filter, so I\u2019m not sure why this error is occurring.\nIs it possible to directly send pod logs to the OpenTelemetry gRPC endpoint and process them in Vector without using an OpenTelemetry Collector? If so, how can we configure Vector to achieve this?",
        "url": "https://github.com/vectordotdev/vector/discussions/22884",
        "createdAt": "2025-04-15T11:56:27Z",
        "updatedAt": "2025-04-16T18:40:26Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Padmashankari"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 22894,
        "title": "Vector Internal Metrics",
        "bodyText": "I was just looking at some differences in metrics dashboards being reported and wanted to know the difference between these types component_received_bytes_total and component_received_event_bytes_total.  The reported metrics is quite different when I'm looking at a per component dashboard using component_received_event_bytes_total and a per host total using component_received_bytes_total.  The description here doesnt clarify it for me either :(.\nExample Dashboards Queries.\nsum by (Ci) (rate(vector_component_received_event_bytes_total{component_kind=\"source\", job=\"$job\" ,instance=~\"$instance\"}[1m]))\n\nsum by (Ci) (rate(vector_component_received_bytes_total{component_kind=\"source\", instance=~\"$instance\"}[1m]))",
        "url": "https://github.com/vectordotdev/vector/discussions/22894",
        "createdAt": "2025-04-16T14:15:05Z",
        "updatedAt": "2025-04-16T17:14:27Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "satellite-no"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21617,
        "title": "Disable context keys in a Kafka source.",
        "bodyText": "Hi,\nI am using a Kafka source to read data from a topic. I would like to know if there is a way to disable the context fields from being added to the record. The fields offset, partition, source_type and topic. I found at the bottom of the docs page it says that: \"By default, the kafka source augments events with helpful context keys.\", but I found no way of turning it off. I tried to change the \"_key\" fields to empty strings in the config but that did not work either.",
        "url": "https://github.com/vectordotdev/vector/discussions/21617",
        "createdAt": "2024-10-25T09:34:45Z",
        "updatedAt": "2025-04-16T18:50:43Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "nuekaze"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22839,
        "title": "When pushing to AWS S3 sink, files are never bigger than 256Mb",
        "bodyText": "Hello,\nWe're setting up a new vector sink towards AWS S3, after reading data from Kafka.\nUnfortunately, eventhough there are Gb of data read from Kafka, the output files are always capped at 256Mb.\nWe've setup disk buffer, with a large max buffer size, without success.\nHere is our config :\n  type = \"aws_s3\"\n  inputs = [\"kafka_topic\"]\n  bucket = \"{{ outputBucketName }}\"\n  endpoint = \"{{ s3Endpoint }}\"\n  key_prefix = \"file_{{ dateFormat }}\"\n  filename_extension = \"{{ outputFileFormat }}\"\n  filename_time_format = \"\"\n  filename_append_uuid = false\n  compression = \"none\"\n\n  # Encoding\n  encoding.codec = \"{{ outputFileFormat }}\"\n  encoding.csv.delimiter = \"{{ delimiter }}\"\n  encoding.csv.quote_style = \"{{ quoteStyle }}\"\n  encoding.csv.fields = [\"md_version\", \"created_on\", \"topic\", \"id\", \"payload\"]\n  # Maximum size of internal buffer for writing CSV - 1GB in bytes\n  encoding.csv.capacity = 1073741824\n\n  # Healthcheck\n  healthcheck.enabled = true\n  # Bufferring events on the disk\n  buffer.type = \"disk\"\n  # Maximum 5Gb in the buffer\n  buffer.max_size = 5368709120\n  # If buffer is full wait for free space\n  buffer.when_full = \"block\"\n\n  # Every hour (in seconds)\n  batch.timeout_secs = 3600\n  # Maximum size of single batch - 1GB in bytes\n  batch.max_bytes = 1073741824```\n  \n\n  Any help would be much appreciated.",
        "url": "https://github.com/vectordotdev/vector/discussions/22839",
        "createdAt": "2025-04-09T07:52:54Z",
        "updatedAt": "2025-04-16T13:57:55Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "cduverne"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 4
    },
    {
        "number": 22885,
        "title": "Issue with OTLP Source and Sink - Receiving 500 Internal Server Error",
        "bodyText": "Hi,\nI'm new to using Vector and OpenTelemetry, and I'm encountering an issue when trying to send logs from a client Vector instance to a server Vector instance using the opentelemetry source and sink.\nHere's my client Vector configuration:\notlp_sink:\n    type: opentelemetry\n    inputs:\n      - demo_logs\n    protocol:\n      encoding:\n        codec: json\n      headers:\n        X-Scope-OrgID: \"demo-org\"\n        User-Agent: \"vector-client\"\n        X-Consumer-Username: \"demo-user\"\n      uri: \"http://127.0.0.1:4318\"\n      type: http\n\nAnd here's the relevant part of my server Vector configuration:\nsources:\n  internal_metrics:\n    type: internal_metrics\n  otlp_input:\n    type: opentelemetry\n    grpc:\n      address: \"0.0.0.0:4317\"\n    http:\n      address: \"0.0.0.0:4318\"\n      headers:\n        - X-Scope-OrgID\n        - User-Agent\n        - X-Consumer-Username\n\nI'm seeing the following error message in my client Vector logs:\n2025-04-15T16:32:04.389932Z  WARN sink{component_kind=\"sink\" component_id=otlp_sink component_type=opentelemetry}:request{request_id=1}: vector::sinks::util::retries: Retrying after response. reason=Http Status: 500 Internal Server Error internal_log_rate_limit=true\n\nFrom what I understand, this minimal configuration should work\u2014especially since both instances are using 4318 otlphttp. Am I missing something in the setup? Any guidance or insight would be greatly appreciated.\nThank you for your time and assistance!\nversion: vector 0.45.0",
        "url": "https://github.com/vectordotdev/vector/discussions/22885",
        "createdAt": "2025-04-15T16:40:13Z",
        "updatedAt": "2025-04-15T16:41:48Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22880,
        "title": "Events dropped Vector 0.44.0 hemlchart 0.40.0",
        "bodyText": "I'm running into a connection error that's affecting event processing, and I wanted to share some context for troubleshooting and discussion.\nWhen the issue hits, I see this debug log:\n\"level\": \"DEBUG\",\n\"message\": \"connection error: connection closed before message completed\",\n\"target\": \"hyper::server::server::new_svc\"\n\nShortly after, we also get this:\n\"message\": \"Events dropped\",\n\"internal_log_rate_limit\": true,\n\"target\": \"vector_common::internal_event::component_events_dropped\",\n\"intentional\": false,\n\"level\": \"ERROR\",\n\"timestamp\": \"2025-04-14T14:06:24.760730Z\",\n\"count\": 1446,\n\nOne thing that stands out is a mismatch between the component_discarded_events_total internal metric and the count reported in the error log.\nMy concern here is that the dropped events don\u2019t appear to be intentional, and this is especially worrying because we\u2019ve seen noticeable metrics loss when onboarding high-volume projects\u2014without any clear root cause or telemetry from Vector that helps us pinpoint what\u2019s going wrong.\nSomewhat related: we\u2019ve also observed symptoms of cardinality loss\u2014tags going missing from metrics even though no VRL was modifying them. This, along with the event loss, eventually led us to offboard one of our larger projects. We did manage to correct the cardinality issue by upping CPU limits and simplifying our pipeline. Originally, the flow was source -> filter -> remap -> remap -> sink, and we simplified it to source -> remap -> sink, dropping redundant VRL blocks in the process. That helped stabilize things, but we\u2019re still in the dark about the root cause of the metric loss and dropped events.\nFor additional context on our setup:\nWe're terminating TLS directly at the Vector pod. Traffic comes in via passthrough Network Load Balancers (Layer 4), which don\u2019t do SSL termination but just forward encrypted packets to the pods. The TLS handshake and decryption happen inside the Vector container. This lets us keep end-to-end encryption and preserve client connection info, while still balancing traffic inside the VPC. We are using datadog_agent as soource\nHappy to hear thoughts or if anyone\u2019s seen similar behavior.",
        "url": "https://github.com/vectordotdev/vector/discussions/22880",
        "createdAt": "2025-04-15T12:43:30Z",
        "updatedAt": "2025-04-15T13:43:21Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "roymartinez-mollie"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 22868,
        "title": "Logs to s3",
        "bodyText": "I am using awss3 sink to send the logs using vector. However I am trying to figure how to read the logs from s3 for a specific time period, is there a way I can do it. I tried ClickHouse , but it says invalid format. Looks like it didn't like the format the way vector writes to s3.\nPlease help.",
        "url": "https://github.com/vectordotdev/vector/discussions/22868",
        "createdAt": "2025-04-11T22:51:27Z",
        "updatedAt": "2025-04-14T21:20:37Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "shamj"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22869,
        "title": "How to get the `sample` transform type to work?",
        "bodyText": "I have the following sample:\n      dd_log_tags:\n        type: sample\n        inputs:\n          - dd_log_tags_pre\n        group_by: \"{{ service }}\"\n        ratio: 0.1\n        exclude: '.aggregator == \"datadog\"'\nBut this does not actually apply a sample ratio, I am still getting 100% of tags from the dd_log_tags_pre transform (which works fine). Every single event has an aggregator key, and some of those keys are datadog. I just want to take everything that isn't set to datadog and decrease it by 90% per service. As far as I can tell from the docs, this is correct, but it does nothing.",
        "url": "https://github.com/vectordotdev/vector/discussions/22869",
        "createdAt": "2025-04-14T15:16:41Z",
        "updatedAt": "2025-04-14T15:16:42Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "V3XATI0N"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22867,
        "title": "using apt repo with sonatype nexus proxy repo",
        "bodyText": "Hello..\nI am trying to setup so that vector can be installed with apt and also setting up the vector repo as a proxy repo in sonatype nexus.\nHas anyone done that earlier? i seem to get stuck with issues with the key and i dont know where to get the public key for the repo.",
        "url": "https://github.com/vectordotdev/vector/discussions/22867",
        "createdAt": "2025-04-14T12:05:44Z",
        "updatedAt": "2025-04-14T12:05:45Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "rikardjerner"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22709,
        "title": "Vector Config Server",
        "bodyText": "Hi,\nWe run vector in on-prem cloud solution, in order to control vector configuration we've implemented approach when vector configuration is pulled from git repo, config validation performed and in case of success - configuration applied to vector instances.\nThing that makes me consider using other approach, made me come to some extended approach, looked up in Spring Boot(framework for Java based applications)\nSpring Boot has special service called \"Spring Boot Cloud Config\", purpose of this service is to allow services to connect to special endpoint in order to retrieve configuration, in other words services have central place for configuration.\nIn my case, I need to make many changes in vector configs and apply new configuration to all instances(thankfully it happens automatically), but wrapper is not stable as it should be, due to many external factors.\nFor that reason, I would like to start discussion reg implementing new feature \"Vector Config Server\". vector instances would have configuration that provides URL to config server instances(maybe some extra options, like vector_group), and config server should expose via REST API configuration for vector which requests for configuration.",
        "url": "https://github.com/vectordotdev/vector/discussions/22709",
        "createdAt": "2025-03-23T17:49:23Z",
        "updatedAt": "2025-04-13T17:55:29Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "nazarovkv"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 3
    },
    {
        "number": 22863,
        "title": "Batch Gauge Metrics from Prometheus Remote Write to InfluxDB v2 Without Aggregation",
        "bodyText": "I\u2019m using Vector with Prometheus Remote Write (beta) as a source and InfluxDB v2 as a sink. Prometheus sends gauge metrics (e.g., cpu_usage) every 15 seconds, but I want to batch these data points over 2 minutes before writing them to InfluxDB.\nCurrent Behavior:\n\nWithout Buffer: Data points are written immediately to InfluxDB (desired).\nWith Buffer: Only the last data point is sent (e.g., 8% instead of all values like 30%, 32%, 40%, 8%).  (not desired).\nWith Aggregation: Only the last data point is sent (e.g., 8% instead of all values like 30%, 32%, 40%, 8%).  (desired). (https://vector.dev/docs/reference/configuration/transforms/aggregate/)\n\nAttempted Solutions:\n\nBuffer Configuration: Tried memory/disk buffers with max_events and max_size, but only the latest point is retained.\nBatch Settings: Added max_size to the InfluxDB sink, but no improvement.\nTimestamp Preservation: Verified Prometheus timestamps are present in the events, but InfluxDB still overwrites values.\n\nConfiguration Example:\nsources:\n  prometheus_source:\n    type: prometheus_remote_write\n    address: 127.0.0.1:9999\n    auth:\n      username: user\n      password: pass\n\nsinks:\n  influxdb2:\n    type: influxdb_metrics\n    inputs: \n      - prometheus_source\n    endpoint: https://my.url.dev/\n    org: myorg\n    bucket: mybucket\n    token: mytoken\n    tags:\n      environment: 'prod'\n    #buffer:\n    #  type: disk\n    #  max_size: 268435488\n    #  when_full: block\n\nQuestion:\nHow can I configure Vector to preserve and batch all gauge data points (with distinct timestamps) over a 2-minute window, rather than aggregating or overwriting them?",
        "url": "https://github.com/vectordotdev/vector/discussions/22863",
        "createdAt": "2025-04-13T17:01:15Z",
        "updatedAt": "2025-04-13T17:01:15Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "adiwab"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22843,
        "title": "Sending a specific http response body with the http server source",
        "bodyText": "Hello, is it possible to send a specific JSON body in responses using the http server source? I see it is possible to send specific headers; however, the docs don't mention a body.\nI have an API endpoint sending POST requests that expects specific response bodies to signify successful transmission.\nI see #14758 for this from 2023, but it has been stale for several years now.\nThank you!",
        "url": "https://github.com/vectordotdev/vector/discussions/22843",
        "createdAt": "2025-04-09T18:49:03Z",
        "updatedAt": "2025-04-13T03:15:35Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tot19"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22852,
        "title": "Don't understand blocking disk buffer memory usage behavior",
        "bodyText": "Hello, I've been investigating Vector for a use-case that I'm surprised a dedicated solution didn't exist (or at least, none that I found): a persistent disk-buffered HTTP POST proxy with all the retry and backoff bells and whistles.\nIn my configuration I have http_server sources that accept arbitrary json and protobuf payloads without decoding. I set the event body to the raw bytes in a remap, then I send the raw bytes to an http sink with appropriate content-type headers with a blocking disk buffer. I also have acknowledgements enabled.\nThis is pretty-much working, but I was confused by a memory-usage behavior that occurs when the buffer fills and begins to block. Vector's memory usage climbs as though it's spilling over to a secondary in-memory buffer, but I don't have multiple buffers configured.\nWhile the memory usage grows, the http_server source is responding with 5XX to my clients. So despite responding with an error, it actually seems like it's accepting the payload and storing in memory.\nThe http sink does receive and attempt to post the events that are in memory. Once the backend services stop back-pressuring, memory usage does drop as events are sent and accepted. So it is working exactly like a buffer.\nI enabled the --allocation-tracing command line argument to understand which component was consuming memory, as I understand there are memory buffers between components in the pipeline. What I found is that the memory usage is allocated to component_id=\"root\", not any user-defined component:\n\nBy comparing timestamps it's visible that that memory usage began increasing when the buffer growth plateaued due to reaching its 2GB size:\n\nThis is the complete config for one HTTP source-sink pipeline I've been testing (configured via helm):\n  acknowledgements:\n    enabled: true\n\n  sources:\n    src_tempo_nonprod:\n      type: http_server\n      address: 0.0.0.0:4418\n      decoding:\n        codec: bytes\n      framing:\n        method: bytes\n      keepalive:\n        max_connection_age_jitter_factor: 0.1\n        max_connection_age_secs: 300\n\n  transforms:\n    remap_tempo_nonprod: &remap_body\n      type: remap\n      inputs: [src_tempo_nonprod]\n      source: |\n        . = .message\n\n  sinks:\n    sink_tempo_nonprod: &dst_tempo_raw\n      inputs: [remap_tempo_nonprod]\n      type: http\n      request:\n        headers:\n          X-Scope-OrgID: \"{{ .Values.tenant_id_nonprod }}\"\n          Content-Type: application/x-protobuf\n        rate_limit_num: 5\n      uri: \"http://tempo-distributor.tempo.svc.cluster.local:4318/v1/traces\"\n      compression: none\n      batch:\n        # max_events must be 1 because data is already batched by the time it reaches Vector,\n        #  also, since we're handling raw bytes, we can't split or concatenate events without\n        #  corrupting the byte stream\n        max_events: 1\n      encoding:\n        codec: raw_message\n      framing:\n        method: bytes\n      buffer:\n        type: disk\n        max_size: 2_147_483_648 # 2GB\n        when_full: block\n      acknowledgements:\n        enabled: true\nI suppose my expectation would have been that with a full buffer, blocking, and acknowledgments on, the http_server sources should start rejecting client payloads without any kind of memory buffering.\nI'm not sure if this is working as designed, or if I've tripped and stumbled on something unexpected.",
        "url": "https://github.com/vectordotdev/vector/discussions/22852",
        "createdAt": "2025-04-11T01:59:36Z",
        "updatedAt": "2025-04-11T02:36:39Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "a-abella"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22848,
        "title": "Difference between sent_bytes and sent_event_bytes in component telemetry",
        "bodyText": "Hi all,\nWhat is the difference between the component_sent_event_bytes_total metric and the component_sent_bytes_total? The docs indicate that the distinction is between \"event bytes\" and \"raw bytes\", but nothing more. In our use case where we are using a kafka sink, the component_sent_bytes_total matches what our broker is reporting as receiving, but component_sent_event_bytes_total is twice as large.\nThanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/22848",
        "createdAt": "2025-04-10T13:56:54Z",
        "updatedAt": "2025-04-16T14:38:58Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mditsworth"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18305,
        "title": "File source warning: Found line that exceeds max_line_bytes; discarding. internal_log_rate_limit=true",
        "bodyText": "Hello,\nI have an instance of vector where in  I am using \"File\" as a source.\n[sources.fileSource]\ntype = \"file\"\ninclude = [\"/var/log/*\",\"/tmp/logs/**init*\"]\nread_from = \"beginning\"\n\nWith this configuration I notice the following lines keep coming.\n2023-08-18T09:29:34.670059Z  WARN source{component_kind=\"source\" component_id=file_source component_type=file component_name=file_source}:file_server: file_source::buffer: Found line that exceeds max_line_bytes; discarding. internal_log_rate_limit=true\n\nIs there a way to find out which file is generating this error? I tried enabling internal_logs, but that also did not give me more information.",
        "url": "https://github.com/vectordotdev/vector/discussions/18305",
        "createdAt": "2023-08-18T09:42:48Z",
        "updatedAt": "2025-04-10T17:25:27Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bmallya"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22841,
        "title": "cpu high",
        "bodyText": "How much file collection speed can Vector handle with 1 CPU at maximum? I noticed that when the speed reaches 6MB/s, my CPU usage hits 100%. The agent is deployed as a DaemonSet, and this performance is somewhat surprising to me. Below is my configuration\u2014are there any optimization suggestions?\n[transforms.add_tag]\ntype = \"remap\"\ninputs = [\"file_*\"]\nsource = '''\nrow = get_enrichment_table_record!(\"collect_csv\",\n{\n\"realFileName\": .file\n},\ncase_sensitive: false)\nconfig = parse_json!(row.config)\n.rinse_rule = config.rinse_rule\n.log = .message\n.collect_host_name = config.host_name\n.collect_host_ip = config.node_ip\n.pod_ip = config.pod_ip\n.pod_name = config.pod_name\n.pod_labels = config.pod_labels\n.tenant_id = config.tenant_id\n.workspace_id = config.workspace_id\n.log_filename = config.file_name\n.app_name = config.app_name\n.collect_time = format_timestamp!(now(), \"%Y-%m-%d %H:%M:%S\", timezone: \"Asia/Shanghai\")\n.task_name = config.task_name\n'''\n[sinks.aggregator]\ntype = \"vector\"\ninputs = [\"add_tag\"]\naddress = \"rinse_address:6000\"\ndata_dir = '/etc/vtrhub/data_dir/'\napi.enabled = true\napi.address = '127.0.0.1:8686'\napi.playground = false\nacknowledgements = true\nenrichment_tables.collect_csv.type = 'file'\nenrichment_tables.collect_csv.file.path = '/etc/vtrhub/csv/collect.csv'\nenrichment_tables.collect_csv.file.encoding.type = 'csv'\ntimezone='Asia/Shanghai'\ntype=\"file\"\ninclude=[\"/vtrhub-root/run/containerd/io.containerd.runtime.v2.task/k8s.io/60f44274863752b24ad2dee72e538a5aa0bd7883f02d6c09c4d4686818ff8835/rootfs/home/admin/logs/rmsdemo/pressure-log.log\"]\nmax_line_bytes = 307200\nread_from = \"beginning\"\nrotate_wait_secs = 1200\noffset_key=\"offset\"\n[encoding]\ncharset = \"utf8\"\n[fingerprint]\nstrategy = 'device_and_inode'",
        "url": "https://github.com/vectordotdev/vector/discussions/22841",
        "createdAt": "2025-04-09T15:11:06Z",
        "updatedAt": "2025-04-09T15:19:35Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Rentu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22773,
        "title": "Configure DataDog agent to send HTTP metric data to Vector",
        "bodyText": "Hello,\nI have a Datadog agent to send HTTP metric data to vector, however I am not able to see any HTTP metric data if reading through the <component_id>.metrics  as suggested in the Vector documentation. configuring-the-datadog-agent\nThis is the datadog configuration (yaml)\nvector:\n metrics.enabled: true\n metrics.url: http://0.0.0.0:6901\n\nAnd this is the vector configuration (yaml)\nsources:\n  sample_agent:\n    type: \"datadog_agent\"\n    multiple_outputs: true\n    address: \"0.0.0.0:6901\"\n\nsinks:\n  sample_metrics:\n    type: \"console\"\n    inputs:\n      - \"sample_agent.metrics\"\n    encoding:\n      codec: \"json\"\n\nThe only way I am able to see any HTTP metric data is if reading through <component_id>.traces.\nThis is the datadog configuration (yaml)\nvector:\n traces.enabled: true\n traces.url: http://0.0.0.0:6901\n\nAnd this is the vector configuration (yaml)\nsources:\n  sample_agent:\n    type: \"datadog_agent\"\n    multiple_outputs: true\n    address: \"0.0.0.0:6901\"\n\nsinks:\n  sample_traces:\n    type: \"console\"\n    inputs:\n      - \"sample_agent.traces\"\n    encoding:\n      codec: \"json\"\n\nBut for this to show 100% accurate data we have to set the -Ddd.trace.sample.rate=1 which is not sustainable.\nThe question I have and would like assistance on is to understand if there is a better way to read through the HTTP metric data instead of doing it through traces. Is there anything else I need to do in order to get Datadog to output HTTP metric data to vector?\nThanks in advance for any help!",
        "url": "https://github.com/vectordotdev/vector/discussions/22773",
        "createdAt": "2025-04-01T21:40:14Z",
        "updatedAt": "2025-04-09T15:04:36Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "fenill1"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22655,
        "title": "Metrics via exec on Linux",
        "bodyText": "Hello, I am trying to bring some linux host metrics via vector's exec command in source.\nIn windows, I have achieved this by:\n  cpu_usage:\n    type: exec\n    mode: scheduled\n    scheduled:\n      exec_interval_secs: 5\n    command: \n      - \"C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\powershell.exe\"\n      - \"-Command\"\n      - \"Write-Output (@{cpu_usage=(Get-Counter '\\\\Processor(_Total)\\\\% Processor Time').CounterSamples.CookedValue} | ConvertTo-Json -Compress)\"\n    decoding:\n      codec: json \n\nI have avoided using $ symbols as it escapes in vector as an environment variable. and used Write-Output to dump it into a variable.\nHowever for linux, I find it impossible, as a simple command for pulling cpu utilization needs $, something like this:\ncpu_util=$(top -bn2 | grep '%Cpu' | tail -1 | grep -P '(....|...) id,' | awk '{print 100-$8}')\necho \"{\\\"cpu_usage\\\": $cpu_util}\"\n\nIs there way to run exec command without variable names or not let $var names escape? I have gone through host_metrics and dont find any viable way to get direct cpu information. I would also be using these to fetch memory and disk information, so any help is appreciated.\nThanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/22655",
        "createdAt": "2025-03-14T07:57:26Z",
        "updatedAt": "2025-04-09T05:33:19Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "abubakr-cgs"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 19336,
        "title": "Proposal: SMTP source",
        "bodyText": "Overview\nI would like to like to propose an SMTP source: that is, one which accepts log messages over SMTP.\nRationale\nMany systems send system status reports over E-mail: from automated package updates, from cron jobs, from monitoring daemons like smartmond and mdadm, and so forth. In fact, I would argue that most systems have no user E-mail handling at all, and the only E-mails they generate are these system messages.\nE-mail is a terrible medium for these messages. Many people don't want them or filter them out, and even when you want them, delivery is unreliable. Messages often get classified as spam, given that they have default sender addresses like root@server1.example.com, and the content doesn't look like it's human written (because it isn't).\nI want to get these messages directly into an observability pipeline, so they can be stored and analyzed centrally and automatically.  The ideal way to do this is to configure all systems (which don't handle E-mail for humans) with a \"smarthost\" setting so that all outbound SMTP connections go directly to the logging system.\nThis is completely transparent to the sending system, and requires nothing more than what you'd need for E-mail delivery anyway: either a standard MTA with its own queue (e.g. postfix), or a stateless SMTP forwarder (e.g. ssmtp, msmtp).\nProposal\nVector.dev gains an SMTP listener on port 25, 465 and/or 587.\nEach incoming message is treated as a log.\nThe envelope (MAIL FROM and RCPT TO) are taken as-is and put into message metadata (aside: note that RCPT TO can be multi-valued). No further processing is done on these fields - there is no email routing! The source_ip and HELO/EHLO hostname would also be recorded, much like the syslog source.\nThe DATA section is split into headers and body; the body is taken as the \"message\", and the headers are additional metadata.\nA future option might be to parse MIME multipart messages, but for now, keeping the entire E-mail body as-is works for me.\nIn principle, message bodies whose charset is not UTF-8 ought to be transcoded to UTF-8. Without this, there's a risk of invalid UTF-8 codepoints in the body. Similarly, binary attachments are normally base64-encoded, but it's possible that binary messages are sent as-is using the 8BITMIME extension. (AFAICS, Vector requires logs to be UTF-8)\nFor safety, maybe there should be a max_length setting, which can be advertised using the SIZE SMTP extension.\nSecurity and authentication\nThis is much like syslog: in the first instance, the receiver can be firewalled to accepted SMTP connections from trusted sender IPs only.\nHowever, SMTP does have various standard security mechanisms which can be used:\n\nClient authentication using SASL or LOGIN (normally used on port 587)\nTLS, using either STARTTLS on port 25/587 or immediately on connection on port 465. The certificate identity can be used for authorization, in conjunction with SASL EXTERNAL\n\nAlternatives I've considered\nThe MTA on the sending system could be replaced with a custom /usr/bin/sendmail which talks another protocol that Vector already has a source for, e.g. http_server, syslog, or the vector protobuf protocol. I think this is messier, since it involves greater changes at the sending system than just pointing the smarthost at a Vector instance.\nIt would be possible to run a central MTA (e.g. postfix) and write a plugin, for example which gets exec()d on each new message, and submits it to Vector. That's not unreasonable, and not a bad way to make a prototype, but it's an extra moving part in the observability pipeline, and has performance impact as MTAs typically write and sync each message to disk.\nIt should be possible to get an MTA to write to a mbox file, and for Vector to read it using the file source with a suitable multiline delimiter pattern (^From ...). That also has an extra moving part, plus a mailbox that needs to be periodically trimmed.",
        "url": "https://github.com/vectordotdev/vector/discussions/19336",
        "createdAt": "2023-12-07T20:25:26Z",
        "updatedAt": "2025-04-08T14:28:40Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "candlerb"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 4
    },
    {
        "number": 22806,
        "title": "Does enrichment_table configuration documentation need be elevated",
        "bodyText": "Hi, having a go at enrichment of events using the file enrichment and noticed enrichment_tables have no consolidated documentation that is easily found and digested like a sink or transform.\nUnless I have not found it, currently there is nothing in the documentation about how to configure an enrichment table other than the guide which makes mention of a few properties but no explanation of their impact...for example schema\nWould it be worth elevating enrichment_tables configuration into the documentation as a first class citizen?\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/22806",
        "createdAt": "2025-04-07T03:18:38Z",
        "updatedAt": "2025-04-08T13:33:00Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tbenade"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22682,
        "title": "Source file being truncated results in duplicate events being emitted",
        "bodyText": "I was testing a specific setup with the Vector 0.41.1 and observed duplicate events in my sink. The setup is the following.\nI have the single log file, which Vector reads as a File Source. The file is the only included path without any wildcards. My application doesn't use any log rotation, it just opens the same log file, truncates it and starts logging from the beginning of the file.\nIt seems that Vector doesn't like it. Let me try to explain this step by step:\n\nMy app is started, logged something to the file. Let's say it logged 10 lines.\nVector started and consumed the file till the end, then continues watching for new data.\nMy app is restarted, truncated the file, and this time it quickly logs 100 lines to it.\nVector detects (using fingerprinting) that the file was replaced, and starts a new reader for it.\nVector reads all 100 lines. But afterwards the problem is happening.\nThe old reader of the file, which was kept at the offset of 10 lines, is forced to read the file up to EOF (i.e. lines 11-100). However, this is not the same file anymore!\nThis results lines 1-100 followed by 11-100 being sent to the sink.\n\nThe old reader was not shut down immediately. There are some seconds of timeout, after which that reader reads the file up to EOF and only then it is terminated. It is useful for the classic log rotation strategies, but breaks the case of one log file being actively overwritten.\nHere is how it looks in the Vector's log:\nINFO source{component_kind=\"source\" component_id=file-BOTH-session-lb3mc2f6xk component_type=file}:file_server: vector::internal_events::file::source: Found new file to watch. file=/home/michael/file.log\nINFO source{component_kind=\"source\" component_id=file-BOTH-session-lb3mc2f6xk component_type=file}:file_server: vector::internal_events::file::source: Found new file to watch. file=/home/michael/file.log\nINFO source{component_kind=\"source\" component_id=file-BOTH-session-lb3mc2f6xk component_type=file}:file_server: vector::internal_events::file::source: Stopped watching file. file=/home/michael/file.log reached_eof=\"true\"\n\n\n1st reader picks the file\n2nd reader picks the overwritten file\n1st reader reads the file and stops\n\nI reproduced it on Windows 10 and Ubuntu (in WSL).\nThank you. Any thoughts welcome!",
        "url": "https://github.com/vectordotdev/vector/discussions/22682",
        "createdAt": "2025-03-18T13:36:39Z",
        "updatedAt": "2025-04-07T18:09:46Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "mkudukin"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22781,
        "title": "Adaptive concurreny",
        "bodyText": "I have 3 sinks : sending metric to datadog datadog_metrics, sending logs to splunk: splunk_hec and now adding aws_s3 sink.\nHowever when I send logs from vector sink to vector source and then aws_s3 sink , I noticed the Total log rate dropping on vector sink that's back pressuring the other transforms and sources behind it.\nThere is no memory , CPU max , network thruput max out. I also don't one or two errors occasionally. I also added adaptive concurrency to all three sinks. But the behavior didn't change. I am testing other keys related to adaptive concurrency , but unable to understand which one is the right one in this scenario.\nCouple of questions:\n\nWhat is adaptive concurrency based on : is it cpu, memory, worker threads or something else?\nWhat does it do : [request.adaptive_concurrency.decrease_ratio] , [ewma_alpha] , [initial_concurrency],[max_concurrency_limit], [rtt_deviation_scale] ? (https://vector.dev/docs/reference/configuration/sinks/aws_s3/#request.adaptive_concurrency.rtt_deviation_scale)(https://vector.dev/docs/reference/configuration/sinks/aws_s3/#request.adaptive_concurrency.max_concurrency_limit)(https://vector.dev/docs/reference/configuration/sinks/aws_s3/#request.adaptive_concurrency.initial_concurrency), (https://vector.dev/docs/reference/configuration/sinks/aws_s3/#request.adaptive_concurrency.ewma_alpha)](https://vector.dev/docs/reference/configuration/sinks/aws_s3/#request.adaptive_concurrency.decrease_ratio)\nPlease advise",
        "url": "https://github.com/vectordotdev/vector/discussions/22781",
        "createdAt": "2025-04-02T16:40:32Z",
        "updatedAt": "2025-04-03T16:27:34Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "shamj"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22729,
        "title": "Configuring the Datadog Agent to send metrics to Vector with latest datadog-agent versions",
        "bodyText": "Hi,\nI have datadog-agent daemonset which sends metrics to vector statefulset.\nThe vector then processes/filters these metrics before sending them to Datadog backend.\nThe vector doc for configuring datadog-agent to send metrics & other telemetry data to Vector, redirects to the page: (LEGACY) Set Up Observability Pipelines in Datadog.\nThe info provided in the latter isn't matching or up-to-date with that mentioned in configuring-the-datadog-agent section.\nI am wondering what's the recommended way for datadog-agent to send metrics to vector.\nFollowing the vector document for configuring the datadog-agent, if I do:\nvector:\n\tmetrics.enabled: true\n\tmetrics.url: http://\"<VECTOR_HOST>:<SOURCE_PORT>\"\n\nit works with datadog-agent version 7.50.3.\nBut as soon as I upgrade datadog-agent to the later versions, say 7.60.3 or above, I run into the issue:\nagent=core file=comp/forwarder/defaultforwarder/worker.go func=process host=<host_ip> level=ERROR line=222 msg=\"Error while processing transaction: error \\\"415 Unsupported Media Type\\\" while sending transaction to \\\"http://<vector_endpoint_url>:8282/api/v1/series\\\", rescheduling it: \\\"{\\\\\\\"code\\\\\\\":415,\\\\\\\"message\\\\\\\":\\\\\\\"Unsupported encoding zstd\\\\\\\"}\\\"\" pod=datadog-agent-72sw9 time=2025-02-27T12:12:48Z\n\nNote the below configuration I have in vector to receive metrics from datadog-agent:\n    sources:\n      datadog_agent:\n        address: 0.0.0.0:8282\n        type: datadog_agent\n        store_api_key: true\n        disable_logs: true\n        disable_traces: true\n        multiple_outputs: false\n\nPlease suggest the recommended way forward.",
        "url": "https://github.com/vectordotdev/vector/discussions/22729",
        "createdAt": "2025-03-26T08:05:38Z",
        "updatedAt": "2025-04-03T04:43:19Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ABHINAV-SUREKA"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22774,
        "title": "How to perform vector tap via GraphQL",
        "bodyText": "I'm looking for example of GraphQL query in order to perform vector tap my-component-id via GraphQL",
        "url": "https://github.com/vectordotdev/vector/discussions/22774",
        "createdAt": "2025-04-01T22:18:23Z",
        "updatedAt": "2025-04-02T20:16:30Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "nazarovkv"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22137,
        "title": "Clickhouse Sink \"upstream connect error or disconnect/reset before headers\" after Clickhouse update",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nWe are running Clickhouse and Vector in Kubernetes. Vector is receiving events from Kafka and is sinking those to Clickhouse. When updating Clickhouse, thereby rolling out a new Pod, data is incoming again without dropped data, however Vector is complaining with WARN messages. This doesn't go away on its own. A 503 seems to me, like Vector is trying to reach the old Clickhouse instances? Could this be possible? Failed requests are retried and written successfully. Requests seem to time out after 10 or 30s, as can be seen in the istio-proxy logs below. A restart of Vector helps, but this can't be the right solution.\n2025-01-03T07:48:00.264670Z  WARN sink{component_kind=\"sink\" component_id=clickhouse_sink component_type=clickhouse}:request{request_id=15212892}: vector::sinks::util::retries: Internal log [Retrying after response.] has been suppressed 22 times.\n2025-01-03T07:48:00.264683Z  WARN sink{component_kind=\"sink\" component_id=clickhouse_sink component_type=clickhouse}:request{request_id=15212892}: vector::sinks::util::retries: Retrying after response. reason=503 Service Unavailable: upstream connect error or disconnect/reset before headers. retried and the latest reset reason: connection timeout internal_log_rate_limit=true\n2025-01-03T07:48:00.264715Z  WARN sink{component_kind=\"sink\" component_id=clickhouse_sink component_type=clickhouse}:request{request_id=15212891}: vector::sinks::util::retries: Internal log [Retrying after response.] is being suppressed to avoid flooding.\n2025-01-03T07:48:11.196311Z  WARN sink{component_kind=\"sink\" component_id=clickhouse_sink component_type=clickhouse}:request{request_id=15213093}: vector::sinks::util::retries: Internal log [Retrying after response.] has been suppressed 24 times.\n2025-01-03T07:48:11.196327Z  WARN sink{component_kind=\"sink\" component_id=clickhouse_sink component_type=clickhouse}:request{request_id=15213093}: vector::sinks::util::retries: Retrying after response. reason=503 Service Unavailable: upstream connect error or disconnect/reset before headers. retried and the latest reset reason: connection timeout internal_log_rate_limit=true\n2025-01-03T07:48:11.196372Z  WARN sink{component_kind=\"sink\" component_id=clickhouse_sink component_type=clickhouse}:request{request_id=15213091}: vector::sinks::util::retries: Internal log [Retrying after response.] is being suppressed to avoid flooding.\n2025-01-03T07:48:22.164840Z  WARN sink{component_kind=\"sink\" component_id=clickhouse_sink component_type=clickhouse}:request{request_id=15213341}: vector::sinks::util::retries: Internal log [Retrying after response.] has been suppressed 29 times.\n\n### Configuration\n\n```text\nclickhouse_sink:\n        type: clickhouse\n        compression: zstd\n        healthcheck:\n          enabled: true\n        acknowledgements:\n          enabled: true\n        inputs:\n          - clickhouse_tables\n        endpoint: http://service-clickhouse-prod.clickhouse-prod.svc.cluster.local:8123\n        format: json_each_row\n        database: ep_sdc_edge\n        table: '{{ \"{{kafkaSrcTopicToDbName}}\" }}'\n        skip_unknown_fields: true\n        auth:\n          strategy: basic\n          user: ${DEST_CLICKHOUSE_USERNAME}\n          password: ${DEST_CLICKHOUSE_PASSWORD}\n        date_time_best_effort: true\n        encoding:\n          timestamp_format: rfc3339\n        batch:\n          max_events: 2500\n          timeout_secs: 2\nVersion\n0.42.0-distroless-libc\nDebug Output\nDebug Logs of one request, that fails after rolling the Clickhouse Pods\nk logs vector pod -f | grep \"34123\"\n2025-01-03T09:53:57.063453Z DEBUG sink{component_kind=\"sink\" component_id=clickhouse_sink component_type=clickhouse}:request{request_id=34123}:http: vector::internal_events::http_client: Sending HTTP request. uri=http://clickhouse-clickhouse.clickhouse.svc.cluster.local:8123/?input_format_import_nested_json=1&input_format_skip_unknown_fields=1&date_time_input_format=best_effort&query=INSERT+INTO+%22ep_sdc_edge%22.%22some_table%22+FORMAT+JSONEachRow method=POST version=HTTP/1.1 headers={\"content-type\": \"application/x-ndjson\", \"content-length\": \"804\", \"content-encoding\": \"zstd\", \"authorization\": Sensitive, \"accept-encoding\": \"zstd,gzip,deflate,br\", \"user-agent\": \"Vector/0.43.1 (x86_64-unknown-linux-gnu e30bf1f 2024-12-10 16:14:47.175528383)\"} body=[804 bytes]\n2025-01-03T09:53:57.063469Z DEBUG sink{component_kind=\"sink\" component_id=clickhouse_sink component_type=clickhouse}:request{request_id=34123}:http: hyper::client::pool: reuse idle connection for (\"http\", clickhouse-clickhouse.clickhouse.svc.cluster.local:8123)\n2025-01-03T09:54:14.385982Z DEBUG sink{component_kind=\"sink\" component_id=clickhouse_sink component_type=clickhouse}:request{request_id=34123}:http: hyper::client::pool: pooling idle connection for (\"http\", clickhouse-clickhouse.clickhouse.svc.cluster.local:8123)\n2025-01-03T09:54:14.386006Z DEBUG sink{component_kind=\"sink\" component_id=clickhouse_sink component_type=clickhouse}:request{request_id=34123}:http: vector::internal_events::http_client: HTTP response. status=503 Service Unavailable version=HTTP/1.1 headers={\"content-length\": \"114\", \"content-type\": \"text/plain\", \"date\": \"Fri, 03 Jan 2025 09:54:13 GMT\", \"server\": \"envoy\"} body=[114 bytes]\n2025-01-03T09:54:14.386056Z  WARN sink{component_kind=\"sink\" component_id=clickhouse_sink component_type=clickhouse}:request{request_id=34123}: vector::sinks::util::retries: Internal log [Retrying after response.] has been suppressed 44 times.\n2025-01-03T09:54:14.386062Z  WARN sink{component_kind=\"sink\" component_id=clickhouse_sink component_type=clickhouse}:request{request_id=34123}: vector::sinks::util::retries: Retrying after response. reason=503 Service Unavailable: upstream connect error or disconnect/reset before headers. retried and the latest reset reason: connection timeout internal_log_rate_limit=true\n2025-01-03T09:54:14.386067Z DEBUG sink{component_kind=\"sink\" component_id=clickhouse_sink component_type=clickhouse}:request{request_id=34123}: vector::sinks::util::retries: Retrying request. delay_ms=720\n2025-01-03T09:54:15.107150Z DEBUG sink{component_kind=\"sink\" component_id=clickhouse_sink component_type=clickhouse}:request{request_id=34123}:http: vector::internal_events::http_client: Sending HTTP request. uri=http://clickhouse-clickhouse.clickhouse.svc.cluster.local:8123/?input_format_import_nested_json=1&input_format_skip_unknown_fields=1&date_time_input_format=best_effort&query=INSERT+INTO+%22ep_sdc_edge%22.%22some_table%22+FORMAT+JSONEachRow method=POST version=HTTP/1.1 headers={\"content-type\": \"application/x-ndjson\", \"content-length\": \"804\", \"content-encoding\": \"zstd\", \"authorization\": Sensitive, \"accept-encoding\": \"zstd,gzip,deflate,br\", \"user-agent\": \"Vector/0.43.1 (x86_64-unknown-linux-gnu e30bf1f 2024-12-10 16:14:47.175528383)\"} body=[804 bytes]\n2025-01-03T09:54:15.107195Z DEBUG sink{component_kind=\"sink\" component_id=clickhouse_sink component_type=clickhouse}:request{request_id=34123}:http: hyper::client::pool: reuse idle connection for (\"http\", clickhouse-clickhouse.clickhouse.svc.cluster.local:8123)\n2025-01-03T09:54:15.123728Z DEBUG sink{component_kind=\"sink\" component_id=clickhouse_sink component_type=clickhouse}:request{request_id=34123}:http: hyper::client::pool: pooling idle connection for (\"http\", clickhouse-clickhouse.clickhouse.svc.cluster.local:8123)\n2025-01-03T09:54:15.123748Z DEBUG sink{component_kind=\"sink\" component_id=clickhouse_sink component_type=clickhouse}:request{request_id=34123}:http: vector::internal_events::http_client: HTTP response. status=200 OK version=HTTP/1.1 headers={\"date\": \"Fri, 03 Jan 2025 09:54:15 GMT\", \"content-type\": \"text/plain; charset=UTF-8\", \"x-clickhouse-server-display-name\": \"chi-clickhouse-clickhouse-pv-0-2-0.chi-clickhouse-clickhouse-pv-0-2.clickhouse.svc.cluster.local\", \"x-clickhouse-query-id\": \"c143d4bf-f960-4197-aab8-5a78971f614d\", \"x-clickhouse-timezone\": \"UTC\", \"x-clickhouse-summary\": \"{\\\"read_rows\\\":\\\"2\\\",\\\"read_bytes\\\":\\\"537\\\",\\\"written_rows\\\":\\\"2\\\",\\\"written_bytes\\\":\\\"537\\\",\\\"total_rows_to_read\\\":\\\"0\\\",\\\"result_rows\\\":\\\"2\\\",\\\"result_bytes\\\":\\\"537\\\",\\\"elapsed_ns\\\":\\\"14808528\\\"}\", \"x-envoy-upstream-service-time\": \"15\", \"server\": \"envoy\", \"transfer-encoding\": \"chunked\"} body=[unknown]\nistio-proxy logs outgoing from vector pod:\n[2025-01-03T12:05:15.541Z] \"POST /?input_format_import_nested_json=1&input_format_skip_unknown_fields=1&date_time_input_format=best_effort&query=INSERT+INTO+%22SOME_TABLE%22+FORMAT+JSONEachRow HTTP/1.1\" 503 URX,UF upstream_reset_before_response_started{connection_timeout} - \"-\" 892 114 10004 - \"-\" \"Vector/0.43.1 (x86_64-unknown-linux-gnu e30bf1f 2024-12-10 16:14:47.175528383)\" \"3a1172a2-b1ea-43c2-a80f-110995853121\" \"clickhouse-clickhouse.clickhouse.svc.cluster.local:8123\" \"100.111.67.8:8123\" outbound|8123||clickhouse-clickhouse.clickhouse.svc.cluster.local - 100.111.67.8:8123 100.96.105.200:52044 - default\n[2025-01-03T12:04:59.054Z] \"POST /?input_format_import_nested_json=1&input_format_skip_unknown_fields=1&date_time_input_format=best_effort&query=INSERT+INTO+%22SOME_TABLE%22+FORMAT+JSONEachRow HTTP/1.1\" 503 URX,UF upstream_reset_before_response_started{connection_timeout} - \"-\" 857 114 30075 - \"-\" \"Vector/0.43.1 (x86_64-unknown-linux-gnu e30bf1f 2024-12-10 16:14:47.175528383)\" \"b0fa3e20-29b9-4f6d-a684-66f14c2a67b0\" \"clickhouse-clickhouse.clickhouse.svc.cluster.local:8123\" \"100.102.120.112:8123\" outbound|8123||clickhouse-clickhouse.clickhouse.svc.cluster.local - 100.102.120.112:8123 100.96.105.200:33804 - default\n[2025-01-03T12:05:23.567Z] \"POST /?input_format_import_nested_json=1&input_format_skip_unknown_fields=1&date_time_input_format=best_effort&query=INSERT+INTO+%22SOME_TABLE%22+FORMAT+JSONEachRow HTTP/1.1\" 503 URX,UF upstream_reset_before_response_started{connection_timeout} - \"-\" 7895 114 10465 - \"-\" \"Vector/0.43.1 (x86_64-unknown-linux-gnu e30bf1f 2024-12-10 16:14:47.175528383)\" \"4ce19de7-c1b0-49d0-adfe-6989b348582a\" \"clickhouse-clickhouse.clickhouse.svc.cluster.local:8123\" \"100.111.67.8:8123\" outbound|8123||clickhouse-clickhouse.clickhouse.svc.cluster.local - 100.111.67.8:8123 100.96.105.200:38118 - default\n[2025-01-03T12:05:24.031Z] \"POST /?input_format_import_nested_json=1&input_format_skip_unknown_fields=1&date_time_input_format=best_effort&query=INSERT+INTO+%22SOME_TABLE%22+FORMAT+JSONEachRow HTTP/1.1\" 503 URX,UF upstream_reset_before_response_started{connection_timeout} - \"-\" 816 114 10001 - \"-\" \"Vector/0.43.1 (x86_64-unknown-linux-gnu e30bf1f 2024-12-10 16:14:47.175528383)\" \"08e6db41-1ff7-410e-bd84-afd074aea060\" \"clickhouse-clickhouse.clickhouse.svc.cluster.local:8123\" \"100.111.67.8:8123\" outbound|8123||clickhouse-clickhouse.clickhouse.svc.cluster.local - 100.111.67.8:8123 100.96.105.200:39238 - default\n[2025-01-03T12:05:26.056Z] \"POST /?input_format_import_nested_json=1&input_format_skip_unknown_fields=1&date_time_input_format=best_effort&query=INSERT+INTO+%22SOME_TABLE%22+FORMAT+JSONEachRow HTTP/1.1\" 503 URX,UF upstream_reset_before_response_started{connection_timeout} - \"-\" 853 114 10000 - \"-\" \"Vector/0.43.1 (x86_64-unknown-linux-gnu e30bf1f 2024-12-10 16:14:47.175528383)\" \"90a611f4-26f3-404b-a36e-9a6b49841b5b\" \"clickhouse-clickhouse.clickhouse.svc.cluster.local:8123\" \"100.111.67.8:8123\" outbound|8123||clickhouse-clickhouse.clickhouse.svc.cluster.local - 100.111.67.8:8123 100.96.105.200:52044 - default\n[2025-01-03T12:05:34.045Z] \"POST /?input_format_import_nested_json=1&input_format_skip_unknown_fields=1&date_time_input_format=best_effort&query=INSERT+INTO+%22SOME_TABLE%22+FORMAT+JSONEachRow HTTP/1.1\" 503 URX,UF upstream_reset_before_response_started{connection_timeout} - \"-\" 875 114 10031 - \"-\" \"Vector/0.43.1 (x86_64-unknown-linux-gnu e30bf1f 2024-12-10 16:14:47.175528383)\" \"24d96855-d8fa-4b2e-9aa9-594e8e304da3\" \"clickhouse-clickhouse.clickhouse.svc.cluster.local:8123\" \"100.111.67.8:8123\" outbound|8123||clickhouse-clickhouse.clickhouse.svc.cluster.local - 100.111.67.8:8123 100.96.105.200:38118 - default\n[2025-01-03T12:05:34.045Z] \"POST /?input_format_import_nested_json=1&input_format_skip_unknown_fields=1&date_time_input_format=best_effort&query=INSERT+INTO+%22SOME_TABLE%22+FORMAT+JSONEachRow HTTP/1.1\" 503 URX,UF upstream_reset_before_response_started{connection_timeout} - \"-\" 872 114 10031 - \"-\" \"Vector/0.43.1 (x86_64-unknown-linux-gnu e30bf1f 2024-12-10 16:14:47.175528383)\" \"b268b8b4-418b-4635-b4c7-e83d7dfa0021\" \"clickhouse-clickhouse.clickhouse.svc.cluster.local:8123\" \"100.111.67.8:8123\" outbound|8123||clickhouse-clickhouse.clickhouse.svc.cluster.local - 100.111.67.8:8123 100.96.105.200:39238 - default\n[2025-01-03T12:05:36.062Z] \"POST /?input_format_import_nested_json=1&input_format_skip_unknown_fields=1&date_time_input_format=best_effort&query=INSERT+INTO+%22SOME_TABLE%22+FORMAT+JSONEachRow HTTP/1.1\" 503 URX,UF upstream_reset_before_response_started{connection_timeout} - \"-\" 825 114 10006 - \"-\" \"Vector/0.43.1 (x86_64-unknown-linux-gnu e30bf1f 2024-12-10 16:14:47.175528383)\" \"0f92425f-2d25-4bd5-9519-6f798acea71e\" \"clickhouse-clickhouse.clickhouse.svc.cluster.local:8123\" \"100.111.67.8:8123\" outbound|8123||clickhouse-clickhouse.clickhouse.svc.cluster.local - 100.111.67.8:8123 100.96.105.200:52044 - default\n[2025-01-03T12:05:22.482Z] \"POST /?input_format_import_nested_json=1&input_format_skip_unknown_fields=1&date_time_input_format=best_effort&query=INSERT+INTO+%22SOME_TABLE%22+FORMAT+JSONEachRow HTTP/1.1\" 503 URX,UF upstream_reset_before_response_started{connection_timeout} - \"-\" 809 114 30083 - \"-\" \"Vector/0.43.1 (x86_64-unknown-linux-gnu e30bf1f 2024-12-10 16:14:47.175528383)\" \"8ef3b254-e2b9-43c8-9b0b-9dbeae769cce\" \"clickhouse-clickhouse.clickhouse.svc.cluster.local:8123\" \"100.105.236.38:8123\" outbound|8123||clickhouse-clickhouse.clickhouse.svc.cluster.local - 100.105.236.38:8123 100.96.105.200:60324 - default\n[2025-01-03T12:05:44.486Z] \"POST /?input_format_import_nested_json=1&input_format_skip_unknown_fields=1&date_time_input_format=best_effort&query=INSERT+INTO+%22SOME_TABLE%22+FORMAT+JSONEachRow HTTP/1.1\" 503 URX,UF upstream_reset_before_response_started{connection_timeout} - \"-\" 866 114 10002 - \"-\" \"Vector/0.43.1 (x86_64-unknown-linux-gnu e30bf1f 2024-12-10 16:14:47.175528383)\" \"5b4a6614-7298-43b3-aa82-b04110734ee6\" \"clickhouse-clickhouse.clickhouse.svc.cluster.local:8123\" \"100.111.67.8:8123\" outbound|8123||clickhouse-clickhouse.clickhouse.svc.cluster.local - 100.111.67.8:8123 100.96.105.200:38118 - default\n[2025-01-03T12:05:44.370Z] \"POST /?input_format_import_nested_json=1&input_format_skip_unknown_fields=1&date_time_input_format=best_effort&query=INSERT+INTO+%22SOME_TABLE%22+FORMAT+JSONEachRow HTTP/1.1\" 503 URX,UF upstream_reset_before_response_started{connection_timeout} - \"-\" 8295 114 10119 - \"-\" \"Vector/0.43.1 (x86_64-unknown-linux-gnu e30bf1f 2024-12-10 16:14:47.175528383)\" \"7bd0441c-6898-4d25-8788-e0765589b115\" \"clickhouse-clickhouse.clickhouse.svc.cluster.local:8123\" \"100.111.67.8:8123\" outbound|8123||clickhouse-clickhouse.clickhouse.svc.cluster.local - 100.111.67.8:8123 100.96.105.200:39238 - default\nThis doesn't seem to be a DNS issue, IPs, are correct:\n;clickhouse-clickhouse.clickhouse.svc.cluster.local. IN A\n\n;; ANSWER SECTION:\nclickhouse-clickhouse.clickhouse.svc.cluster.local. 30 IN A 100.102.120.105\nclickhouse-clickhouse.clickhouse.svc.cluster.local. 30 IN A 100.105.236.61\nclickhouse-clickhouse.clickhouse.svc.cluster.local. 30 IN A 100.111.67.13\n k get pod -o wide\nNAME                                        READY   STATUS    RESTARTS   AGE    IP                NODE                  NOMINATED NODE   READINESS GATES\nchi-clickhouse-clickhouse-pv-0-0-0          2/2     Running   0          154m   100.102.120.105   i-094734b915a844584   <none>           <none>\nchi-clickhouse-clickhouse-pv-0-1-0          2/2     Running   0          152m   100.111.67.13     i-08e176d047c1fefdc   <none>           <none>\nchi-clickhouse-clickhouse-pv-0-2-0          2/2     Running   0          150m   100.105.236.61    i-02cbff00ea6dc3a56   <none>           <none>\nExample Data\nNo response\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/22137",
        "createdAt": "2025-01-03T08:06:36Z",
        "updatedAt": "2025-04-01T05:52:57Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "seilerre"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 22751,
        "title": "InfluxDB Logs - Output is split into multiple measurements (1 for each key-value pair)",
        "bodyText": "I have a custom remap input that returns key-value pairs (dict, json ?, not sure about the Vector terminology). I would like to push this to my InfluxDB instance. My problem is that instead of creating one measurement per log entry, it creates an entry for each key that exists in the parsed json log entry.\nThis is the config:\n  influx:\n    type: influxdb_logs\n    inputs: [\"parse_message\"]\n    bucket: bucketname\n    endpoint: https://influx.my-website.com\n    measurement: log\n    org: my-org\nAnd for debugging, I also added a console sink with this config:\n  console:\n    type: \"console\"\n    inputs: [\"parse_message\"]\n    encoding:\n      codec: \"json\"\nWhich results in results like this:\n{\n    \"context\": \"core\",\n    \"event\": \"Server is running on port 8080\",\n    \"level\": \"info\",\n    \"timestamp\": 1743355879\n}\nAnd in influx these appear as seperate entries, something like this:\nlog,host=your_host _field=context   _value=\"core\"          1743355879\nlog,host=your_host _field=event     _value=\"Server is running on port 8080\" 1743355879\nlog,host=your_host _field=level     _value=\"info\"          1743355879\nlog,host=your_host _field=timestamp _value=1743355879      1743355879\n\nAnd I want it to look like this:\nlog,host=your_host context=\"core\",event=\"Server is running on port 8080\",level=\"info\",timestamp=1743355879 1743355879\n\nIs this possible with Vector?",
        "url": "https://github.com/vectordotdev/vector/discussions/22751",
        "createdAt": "2025-03-30T20:01:30Z",
        "updatedAt": "2025-04-01T00:40:47Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "harcipulyka"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22708,
        "title": "Reading log files from the beginning just once",
        "bodyText": "I want to use vector to replace filebeat but I haven't been able to find a way to make vector replicate filebeat's ability to read through log files starting at the beginning and then exit when reaching the EOF.\nI have a static set of logs won't have anymore logs added so I don't need to tail or continuously monitor them I just need to read through them all once. I wanted to try using vector because of it's transformation abilities so I was wondering if this was possible?",
        "url": "https://github.com/vectordotdev/vector/discussions/22708",
        "createdAt": "2025-03-23T14:34:37Z",
        "updatedAt": "2025-03-31T16:42:50Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "gingerbreadtrev"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22697,
        "title": "Socket sink support for TTL?",
        "bodyText": "Hi,\nI'm using vector to deliver logs to logstash with using a socket sink :)\n    sinks:\n      logstash:\n        type: socket\n        inputs: [\"parse_json\"]\n        mode: \"tcp\"\n        address: \"xxxxxxx:5045\"\n        encoding:\n          codec: json\n\nBut on logstash we observed that traffic is not distributed evenly. It's looks like one ip is resolved from a host and all is sent to this one endpoint. Rigth now we observed that one node of our logstash is receiving a traffic and the rest is on zero.  There is any option to set a TTL in socket sink?\nWe're using right now vector version: 0.42.0",
        "url": "https://github.com/vectordotdev/vector/discussions/22697",
        "createdAt": "2025-03-20T11:28:47Z",
        "updatedAt": "2025-03-31T13:46:30Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "pkruk"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22715,
        "title": "Stop reading from source (kafka) when sink (opensearch) is down  in vector aggregator.",
        "bodyText": "I have a source as a kafka and sink as a opensearch my requirement is that vector aggregator should stop reading from kafka whenever sink (opensearch) is down and should resume reading from source (kafka) once the opensearch is up. Below is my vector aggregator configuration.\nsource:\n  kafka_logs:\n     type: kafka\n     bootstrap_servers: <kafka-endpoint>:9092\n     topics: \n      - eks-logs\n     group_id: vector-aggregator-group\n     decoding:\n         codec: \"json\"\n        \n sink:\n    opensearch:\n          type: elasticsearch\n          inputs:\n            - kafka_logs\n          batch:\n            timeout_secs: 5\n          buffer:\n            type: memory\n            when_full: block\n          bulk:\n            action: index\n            index: |-\n              {{ print \"{{ _meta.target.index }}\" }}-%Y.%m.%d\n          mode: bulk\n          pipeline: primary\n          compression: gzip\n          id_key: \"_id\"\n          endpoints:\n            - ${VECTOR_OPENSEARCH_ENDPOINT}\n          auth:\n            strategy: basic\n            user: $${VECTOR_OPENSEARCH_USERNAME}\n            password: $${VECTOR_OPENSEARCH_PASSWORD}\n          healthcheck:\n            enabled: true\n\nHow can I configure my vector aggregator to stop reading from the source if the sink is down ? As per the above configuration i guess the events will be stored in buffer.memory and it is used along withwhen_full:block option  so does that mean vector aggregator will stop reading from kafka when buffer.max_events is full?\nTo stop vector aggregator reading from kafka should I add acknowledgements.enabled: true in the opensearch sink?\nDoes it mean that if for some reason if vector aggregator was not able to index the data to opensearch will it stop reading from source as acknowledgements.enabled: true?",
        "url": "https://github.com/vectordotdev/vector/discussions/22715",
        "createdAt": "2025-03-24T11:11:38Z",
        "updatedAt": "2025-03-28T06:39:31Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "shreyasarani23"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22730,
        "title": "Vector 0.44 version : vector to vector traffic error",
        "bodyText": "Vector source and Vector destination are on vector 0.44 version. While sending the traffic from vector sink to vector source , seeing the below error: Please advise.\nERROR source{component_kind=\"source\" component_id=vector_vector_data component_type=vector}:grpc-request{grpc_service=\"vector.Vector\" grpc_method=\"PushEvents\"}: vector_common::internal_event::component_events_dropped: Events dropped intentional=false count=1000 reason=\"Source send cancelled.\" internal_log_rate_limit=true",
        "url": "https://github.com/vectordotdev/vector/discussions/22730",
        "createdAt": "2025-03-26T17:30:34Z",
        "updatedAt": "2025-03-27T19:11:15Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "shamj"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22516,
        "title": "Configuring per-sink acknowledgements with a reduce transform",
        "bodyText": "Hi,  I'm running vector with an http_server source. We have two pipelines:\n\nhttp_server -> remap -> aws_s3 sink\nThe same http_server -> reduce transform to combine logs -> aws_kinesis_streams sink\n\nWe have enabled E2E acknowledgements because we don't want to lose logs if the aws_s3 sink fails. However, based off the documentation, we would also wait on the aws_kinesis_streams sink. This takes longer since the reduce transform waits to aggregate logs, and our upstream HTTP client is actually timing out with this.\nWe decided that we're ok without the ack from aws_kinesis_streams, and we set acknowledgements.enabled to false on the sink, but it seems like it's still waiting on the logs to make it past the reduce before sending.\nAm I doing something wrong with setting acknowledgements? And if not, is there a way to work around this so we can have E2E acknowledgements without needing to wait on the reduce transform (if we don't care about ack's in the downstream sink)?\nRepro\nTo make it simple / easily reproducible, I made a similar toy case\nConfig\n{\n\t\"acknowledgements\": {\n\t\t\"enabled\": true\n\t},\n\t\"sources\": {\n                \"in\": {\n                        \"type\": \"http_server\",\n                        \"address\": \"0.0.0.0:80\"\n                }\n        },\n\t\"transforms\": {\n\t\t\"hold\": {\n\t\t\t\"type\": \"reduce\",\n\t\t\t\"inputs\": [\"in\"],\n\t\t\t\"end_every_period_ms\": 10000\n\t\t}\n\t},\n\t\"sinks\": {\n\t\t\"out\": {\n\t\t\t\"acknowledgements\": {\n\t\t\t\t\"enabled\": false\n\t\t\t},\n\t\t\t\"type\": \"console\",\n\t\t\t\"inputs\": [\"hold\"],\n\t\t\t\"encoding\": {\n\t\t\t\t\"codec\": \"json\"\n\t\t\t}\n\t\t},\n\t\t\"print\": {\n\t\t\t\"type\": \"console\",\n\t\t\t\"inputs\": [\"in\"],\n\t\t\t\"encoding\": {\n\t\t\t\t\"codec\": \"json\"\n\t\t\t}\n\t\t}\n\t}\n}\n\nThis config is a simple HTTP server and two sinks. One sink is connected to the source and immediately prints, whereas another goes to a reduce transform and waits 10s before getting printed..\nCommand\ncurl 0.0.0.0:80 -X POST -d '{}'\nResult\nEven though ack is off for out, we wait 10s before we get a response on the curl command. In fact, even with acknowledgements.enabled on both sinks, we still wait 10s. It's only when acknowledgements.enabled is turned off globally that we get an immediate response.",
        "url": "https://github.com/vectordotdev/vector/discussions/22516",
        "createdAt": "2025-02-26T01:11:02Z",
        "updatedAt": "2025-03-27T18:47:04Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "andrewma2"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22496,
        "title": "Handing a large amount of sampling / filtering rules based on string matches",
        "bodyText": "Coming from promtail/alloy world, I have the following use case,\nGiven logs from an incoming source containing logs from multiple services, I want to do two things,\n\nMatch a pattern with the incoming stream and sample all logs that match the pattern by a given %.\nMatch a pattern with the incoming stream and filter those logs (they may be sampled later).\n\nWhat's the best way to do this in vector? For use case 1, I had to do multiple things,\n\nOne filter transform that filters out logs that match my pattern. This very well could be a remap transform if simple boolean expressions don't fit the use case. And then a sample transform for this new input.\nOne remap transform that removes logs of this pattern from the primary stream to avoid duplication.\nThe output of sample transform and the remap transform both go into the primary sink.\n\nIs this the best approach? Seems very inconvenient when I have >100 such sampling rules. In promtail/alloy, I can do match and then run a set of pipeline stages on the filtered stream. The main stream and this processed stream is then merged later. Sharing a sample config below,\nloki.process \"foo\" {\n  forward_to = <RECEIVER_LIST>\n\n  stage.foo {\n    # something that applies to all the logs\n  }\n  \n  # sample logs from `bar` service selectively\n  stage.match {\n    selector = \"{service=\\\"bar\\\"}\"\n\n    stage.sampling {\n        rate = 0.25\n    }\n  }\n}\n\nI'm looking at all these transforms and seeing that they only support one liner boolean expressions. Are there any plans to add support for more verbose VRL functions that can return true/false or other values according to the transform?\n\n\nOne filter transform that filters out logs that match my pattern. This very well could be a remap transform if simple boolean expressions don't fit the use case. And then a sample transform for this new input.\nOne remap transform that removes logs of this pattern from the primary stream to avoid duplication.\n\n\nAfter a second look at the docs, it seems like it makes more sense to use remap transforms to filter logs since they support the reroute_dropped property. I won't need to handle duplicate logs.",
        "url": "https://github.com/vectordotdev/vector/discussions/22496",
        "createdAt": "2025-02-24T04:07:44Z",
        "updatedAt": "2025-03-27T18:41:27Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "PrayagS"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22494,
        "title": "Kafka source consumer group rebalance",
        "bodyText": "Hi guys. I have a general question about the kafka source and I don't see a great answer for it in the documentation so I'm wondering if anybody here has some insight. Suppose I have multiple vector aggregators with a kafka source in the same consumer group, configured to listen to multiple topics/partitions, with e2e acknowledgements enabled. Downstream, i have aws_s3 sink with a large batching window (30 minutes). If I scale up the number of aggregators in the same consumer group, this will trigger a partition rebalance amongst consumers. If a partition is revoked from an aggregator that has a batch stored in buffer, will this batch be discarded, or will the batch be written, resulting in duplicate messages?",
        "url": "https://github.com/vectordotdev/vector/discussions/22494",
        "createdAt": "2025-02-23T20:03:09Z",
        "updatedAt": "2025-03-27T18:31:57Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "dimohammed328"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22478,
        "title": "Impact of Loki 429 Errors on All Tenants in a Shared Vector Pipeline",
        "bodyText": "We are experiencing issues where a 429 (Too Many Requests) error from Grafana Loki is impacting all tenants in a shared Vector pipeline. This happens when one of our tenants generates a high volume of logs, which leads to throttling or rate-limiting on Loki's ingester. The problem is that all tenants in the same Vector pipeline are affected, even though logs are isolated by tenant via the X-Scope-OrgID header.\nI thought that adding the X-Scope-OrgID header in the request would isolate the batch request for each tenant. Is that correct?\nWhen one of the tenants gets a 429 error, how does Vector.dev behave for other tenants in the shared Vector pipeline?\nAssuming the impact on all tenants is correct behavior in one vector pipeline, and I know that one of the tenants in my Vector pipeline will get a 429 error, what is the best configuration for this scenario? should i seperate the sinks? or should i seperate the vectors or etc\nI would appreciate it ifyou could help me figure this out.",
        "url": "https://github.com/vectordotdev/vector/discussions/22478",
        "createdAt": "2025-02-20T12:42:16Z",
        "updatedAt": "2025-03-27T18:13:28Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22728,
        "title": "Vector 0.44 version : vector to vector traffic error",
        "bodyText": "source and destination vectors are on 0.44 version. While sending the traffic from vector sink to vector source , seeing the below error-\nERROR source{component_kind=\"source\" component_id=vector_vector_data component_type=vector}:grpc-request{grpc_service=\"vector.Vector\" grpc_method=\"PushEvents\"}: vector_common::internal_event::component_events_dropped: Events dropped intentional=false count=1000 reason=\"Source send cancelled.\" internal_log_rate_limit=true\nPlease advise",
        "url": "https://github.com/vectordotdev/vector/discussions/22728",
        "createdAt": "2025-03-25T21:59:17Z",
        "updatedAt": "2025-03-26T18:03:22Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "shamj"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22705,
        "title": "Reduce transform ends_with based on dynamic number of messages",
        "bodyText": "Vector version: 0.45.0\nI am working on some stuff with vector to combine log messages into a single event, so I am using the reduce transform.  The issue I have is that the logs don't have an easy \"end\" state.  Being syslog they could be out of order, etc.  The information that I do have is how many messages there should be, and the order of the specific event.  I have meta information pulled out before they reach vector and it looks like\n{\n  \"_extra\": {\"order\": 1, \"size\": 9},\n  \"message\": \"...\",\n}\nI was attempting to do an ends with by doing a check to see if all of the messages have been received, and then also a timeout\n  group_by_key:\n    type: reduce\n    inputs:\n      - log_input\n    group_by:\n      - message_key\n    merge_strategies:\n      message: array\n    end_every_period_ms: 60000\n    ends_when:\n      type: vrl\n      source: (is_array(.message) && (to_int!(._extra.size)) == length!(.message)) || (is_string(.message) && to_int!(._extra.size) == 1)\nbut after looking at the code yesterday. realized that the ends_with condition only gets the new event, not the combined data.  Any other ideas on how I might achieve this?\nI attempted to do a shorter expire time, and then to a route to put things back in the queue to further group things unless complete or expired... but it does not like cyclical references...\nI have been able to get around this by doing an http sink and putting back into the queue if time limit not reached or doesn't have all of the messages\nsources:\n  wireless-kafka-file:\n    type: file\n    include:\n      - \"/data/sample.json\"\n    data_dir: \"/data/data_dir\"\n    ignore_checkpoints: true\n  http-server-requeue:\n    type: http_server\n    address: 127.0.0.1:8000\n    path: \"/requeue\"\n    decoding:\n      codec: json\n\ntransforms:\n  http-server-remap:\n    type: remap\n    inputs:\n      - http-server-requeue\n    source: |-\n      del(.path)\n\n  wireless:\n    type: remap\n    inputs:\n      - wireless-kafka-file\n    source: |-\n      . = parse_json!(.message)\n      data = parse_json!(.value)\n      del(.value)\n      . = merge!(., data)\n      .last_timestamp = to_unix_timestamp(now(), \"seconds\")\n\n  group_by_key:\n    type: reduce\n    inputs:\n      - wireless\n    group_by:\n      - message_key\n    merge_strategies:\n      message: array\n      last_timestamp: max\n    end_every_period_ms: 10000\n\n  check_all_messages_or_expired:\n    type: remap\n    inputs:\n      - group_by_key\n    source: |-\n      if is_array(.message) {\n        .message = flatten(array!(.message))\n      } else {\n        .message = [.message]\n      }\n\n      .has_all_or_expired = false\n\n      current_timestamp = to_unix_timestamp(now(), \"seconds\")\n      .last_timestamp = int!(.last_timestamp)\n\n      message_count = to_int!(._extra.size)\n\n      # if have all messages, mark complete\n      if message_count == length(.message) {\n        log(\"HAS ALL MESSAGES!\", level: \"error\", rate_limit_secs: 0)\n        .has_all_or_expired = true\n      }\n\n      since_last_timestamp = current_timestamp - .last_timestamp\n\n      # if it's been 120 seconds since last message, set as expired.\n      if since_last_timestamp > 120 {\n        .has_all_or_expired = true\n      }\n\n  # if expired or complete, continue or route back into group_by_key using http sink and source\n  route_expired_or_complete:\n    type: route\n    inputs:\n      - check_all_messages_or_expired\n    reroute_unmatched: true\n    route:\n      expired-or-complete:\n        type: vrl\n        source: \"bool(.has_all_or_expired) ?? false\"\n\n  parse_messages:\n    type: remap\n    inputs:\n      - route_expired_or_complete.expired-or-complete\n    source: \"....\"\n\nsinks:\n  stdout:\n     type: console\n     inputs:\n       - parse_messages\n     encoding:\n       codec: json\n  http-client-requeue:\n    type: http\n    inputs:\n      - route_expired_or_complete._unmatched\n    uri: http://localhost:8000/requeue\n    encoding:\n      codec: json\n    batch:\n      max_events: 1000\nthis works, however it breaks end to end acknowledgements.\nWould be nice to be able to use some sort of meta data or doing some merging before the ends_with is merged: https://github.com/vectordotdev/vector/blob/master/src/transforms/reduce/transform.rs#L264-L266",
        "url": "https://github.com/vectordotdev/vector/discussions/22705",
        "createdAt": "2025-03-21T14:47:22Z",
        "updatedAt": "2025-03-25T20:45:25Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Goggin"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22723,
        "title": "How to enable client authentication for Elasticsearch/Vector/HTTP sinks?",
        "bodyText": "Hello.\nRules in my company declare mandatory TLS client certificate authentication.\nHow can I configure Vector sinks/sources to check said certificate?\nFor example: in Elasticsearch sink I can see parameters: tls.crt_file, tls.key_file.\nAre they meant for server-side only use?\nDocumentation is unclear about that so I'd appreciate any help :)",
        "url": "https://github.com/vectordotdev/vector/discussions/22723",
        "createdAt": "2025-03-25T11:31:24Z",
        "updatedAt": "2025-03-25T20:11:12Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "0x25CBFC4F"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22726,
        "title": "Are logs from separate files combined into the same pipeline? re: \"reduce\" transform",
        "bodyText": "Hopefully this question makes sense. I want to use \"reduce\" to combine Go panic logs in our vector agents, but wondering if I can assume whether logs from separate files are kept separate within the vector process with their own copies of the component pipeline. Does each file tailed in kubernetes_logs get its own pipeline in memory or are all the logs from the various files mixed together going through the same instances of each component?",
        "url": "https://github.com/vectordotdev/vector/discussions/22726",
        "createdAt": "2025-03-25T19:10:04Z",
        "updatedAt": "2025-03-26T20:58:38Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "davidcpell"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22717,
        "title": "debug on Vector on k8s",
        "bodyText": "I am planning to replace fluent-bit in my current k8s cluster with Vector, and already read doc to generated a vector config yaml, just want to know what is the better exprience to debug the config on k8s? thanks",
        "url": "https://github.com/vectordotdev/vector/discussions/22717",
        "createdAt": "2025-03-24T14:21:14Z",
        "updatedAt": "2025-03-26T18:58:35Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "9to1url"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22722,
        "title": "Metric to Log Timestamp gets overwritten? How to preserve log timestamp when converting to metric and sending using prometheus remote write?",
        "bodyText": "Hi, I'm trying to use log_to_metric transformation. These are logs have timestamps that aren't current such as 2025-03-24T18:53:01.419106546Z  These are metrics that were converted to logs to be stored in an s3 bucket, I am then ingesting these logs from s3 and I converting them to metrics and then sending them to Amazon Prometheus using prometheus_remote_write.\nThis is how I'm converting the log to metrics:\nconvert_s3:\n    type: log_to_metric\n    inputs:\n      - s3_metrics_as_logs\n    metrics: []\n    all_metrics: true\n\nI notice when I notice that log_to_metric transform sets the metric timestamps to the current timestamp the metric is converted. I tried adding a remap after this to change the timestamp back using\n.timestamp = parse_timestamp!(.tags.log_timestamp, \"%Y-%m-%dT%H:%M:%S%.9fZ\") \nbut I notice that when I use prometheus_remote_write to send to prometheus, the timestamp shows as the current timestamp.\nDoes both log_to_metric and prometheus_remote_write alter the timestamp?",
        "url": "https://github.com/vectordotdev/vector/discussions/22722",
        "createdAt": "2025-03-24T21:25:20Z",
        "updatedAt": "2025-03-24T21:26:21Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "generate-me12"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22718,
        "title": "AMQP sink / RabbitMQ support for headers exchange ?",
        "bodyText": "Greetings.\nDidnt find any mention thereof but maybe its just not documented...\nIs there support for setting message headers in the AMQP sink to populate for header exchanges ?\nThanks for offering Vector and best regards.",
        "url": "https://github.com/vectordotdev/vector/discussions/22718",
        "createdAt": "2025-03-24T15:09:29Z",
        "updatedAt": "2025-03-24T20:50:17Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ccmsi"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22599,
        "title": "Per-sink acknowledgements don't override global option",
        "bodyText": "Hi, I had recently raised a discussion involving acknowledgements. While I was tinkering around with ack's around that question, I noticed some odd behavior\nWith the toy config below, I would've expected to get an immediate response back from a curl request to the source endpoint. This is since in has one sink, where I've explicitly disabled ack's. And even though it's enabled globally, per the documentation:\nEnabling or disabling acknowledgements at the sink level takes precedence over any global acknowledgements configuration.\nHowever, I do see that my request hangs for 10s and only responds once the batch timeout hits and vector attempts to send out from the sink*. Similarly, if I flip it so globally ack's are off and the per-sink ack is on, I get an instant response when I would expect it to wait until vector attempts a send. This is on vector v0.45, is this expected behavior?\nThanks!\n*Note: The sink send will fail with the config on its own as there's no downstream receiver, but I don't think that changes the specific behavior I'm investigating\n{\n\t\"acknowledgements\": {\n\t\t\"enabled\": true\n\t},\n\t\"sources\": {\n                \"in\": {\n                        \"type\": \"http_server\",\n                        \"address\": \"0.0.0.0:80\"\n                }\n        },\n\t\"sinks\": {\n\t\t\"out\": {\n\t\t\t\"acknowledgements\": {\n\t\t\t\t\"enabled\": false\n\t\t\t},\n\t\t\t\"type\": \"http\",\n\t\t\t\"inputs\": [\"in\"],\n\t\t\t\"uri\": \"0.0.0.0:10\",\n\t\t\t\"encoding\": {\n\t\t\t\t\"codec\": \"json\"\n\t\t\t},\n\t\t\t\"batch\": {\n\t\t\t\t\"timeout_secs\": 10,\n\t\t\t\t\"max_events\": 1000\n\t\t\t},\n\t\t\t\"request\": {\n\t\t\t\t\"retry_attempts\": 1\n\t\t\t},\n\t\t\t\"buffer\": {\n\t\t\t\t\"max_events\": 100\n\t\t\t}\n\t\t}\n\t}\n}\n\n\nI'm hitting the source endpoint with curl 0.0.0.0:80 -X POST -d '{}'",
        "url": "https://github.com/vectordotdev/vector/discussions/22599",
        "createdAt": "2025-03-05T19:59:51Z",
        "updatedAt": "2025-03-24T20:19:21Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "andrewma2"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22656,
        "title": "Clarification on StatsD Timing Unit Conversion (ms to s)",
        "bodyText": "Hi everyone,\nI hope you're all doing well. I have a question regarding the design decision where Vector converts StatsD timing values from milliseconds (ms) to seconds (s). According to the official documentation, \"StatsD timings are emitted as distributions. Timings in milliseconds (ms) are converted to seconds (s).\" While I understand that this conversion might have been implemented with a specific rationale in mind, I\u2019m curious about the background and reasoning behind it.\nPoints for Clarification\n\n\nDesign Rationale:\nCould someone explain the main reasons for converting the timings from ms to s? Was this decision driven by consistency, compatibility with certain sinks, or some other technical factor?\n\n\nImpact on Downstream Systems:\nOur current setup uses platforms like Datadog and Grafana, and this conversion has required adjustments in our pre-existing dashboard queries. Understanding the rationale might help us better manage these changes or plan for future updates.\n\n\nAlternative Approaches:\nHas there been any discussion on allowing users to opt out of this conversion or to maintain the original units? If not, what are the challenges or potential issues that prevented such flexibility?\n\n\nI appreciate any insights or context the community can provide regarding this implementation detail. Thanks for taking the time to help clarify this matter!\nBest regards,\nJinsoo Heo",
        "url": "https://github.com/vectordotdev/vector/discussions/22656",
        "createdAt": "2025-03-14T09:09:17Z",
        "updatedAt": "2025-03-24T06:26:33Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "devkoriel"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 22703,
        "title": "rsyslog sending logs to vector. Able to receive system logs but not custom application logs",
        "bodyText": "Here is my rsyslog configuration\n#################\nMODULES\n#################\nmodule(load=\"imuxsock\")  # Provides support for local system logging\nUDP syslog reception (uncomment if using UDP for Vector)\nmodule(load=\"imudp\")\ninput(type=\"imudp\" port=\"514\")\nTCP syslog reception (uncomment if using TCP for Vector)\n#module(load=\"imtcp\")\n#input(type=\"imtcp\" port=\"514\")\nKernel logging support\nmodule(load=\"imklog\" permitnonkernelfacility=\"on\")\n###########################\nGLOBAL DIRECTIVES\n###########################\n$RepeatedMsgReduction on\n#module(load=\"imfile\")\nInput from custom log file\n#input(type=\"imfile\" File=\"/var/log/custom_app.log\" Tag=\"myapp\" Facility=\"local0\")\naction(type=\"omfile\" file=\"/var/log/custom_app.log\")\nSet the default permissions for all log files\n$FileOwner syslog\n$FileGroup adm\n$FileCreateMode 0640\n$DirCreateMode 0755\n$Umask 0022\n$PrivDropToUser syslog\n$PrivDropToGroup syslog\nWhere to place spool and state files\n$WorkDirectory /var/spool/rsyslog\nForward all logs to Vector server (UDP)\n. @192.168.1.12:514  # Forwarding all logs to Vector server\n#action(type=\"omfwd\" target=\"192.168.1.12\" port=\"514\" protocol=\"udp\" queue.type=\"linkedlist\" queue.saveOnShutdown=\"on\"queue.resumeRetryCount=\"-1\")\nOptionally, forward only logs from the 'myapp' tag (custom log file)\nlocal0.* @192.168.1.12:514  # Forward logs tagged 'myapp' to Vector server\nInclude all config files in /etc/rsyslog.d/\n$IncludeConfig /etc/rsyslog.d/*.conf\n=========================================\nI tried sending below log using logger able to recive by vector\n\"java API is working\"\nAnd able to receive system logs like below\n{\"appname\":\"sudo\",\"facility\":\"authpriv\",\"host\":\"\",\"hostname\":\"\",\"message\":\"oracle : TTY=pts/0 ; PWD=/var/log ; USER=root ; COMMAND=/usr/bin/grep -i java custom_app.log\",\"severity\":\"notice\",\"source_ip\":\"***\",\"source_type\":\"syslog\",\"timestamp\":\"2025-03-20T19:58:45Z\"}\nbut not able to receive my application logs\n{\"appname\":\"sudo\",\"facility\":\"authpriv\",\"host\":\"DESKTOP-2B46BBL\",\"hostname\":\"DESKTOP-2B46BBL\",\"message\":\"oracle : TTY=pts/0 ; PWD=/var/log ; USER=root ; COMMAND=/usr/bin/grep -i java custom_app.log\",\"severity\":\"notice\",\"source_ip\":\"192.168.1.8\",\"source_type\":\"syslog\",\"timestamp\":\"2025-03-20T19:58:45Z\"}\nHere is my vector configuration\nsources:\nsyslog:\ntype: syslog\naddress: \"0.0.0.0:514\"\nmode: udp\nencoding:\ncodec: \"plain\"\nsinks:\nfile_sink:\ntype: \"file\"\npath: \"/var/log/vector/syslog_output.log\"\ninputs:\n- syslog\nencoding:\ncodec: \"json\"\nPlease help i am new to vector. NO prior experience with syslog",
        "url": "https://github.com/vectordotdev/vector/discussions/22703",
        "createdAt": "2025-03-21T04:19:58Z",
        "updatedAt": "2025-03-21T04:19:59Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "manoharBattala"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22608,
        "title": "Filter for topk metrics",
        "bodyText": "Hi! I'm trying to extract the top client IP addresses from our request logs and export them as a metric to prometheus. Filtering for the top clients only is important to avoid overwhelming prometheus with labels.\nI can convert our logs to metrics and aggregate counts by IP easily enough, but I'm really struggling to find a way to filter for only the top K values in each aggregated batch (or even with a second layer of aggregation).\nAm I missing a trick here, or is this a missing feature?\nIt seems like this would be efficient to implement:\n\nKeep a list of the current top-K events sorted by value.\nWhen a new event comes in, if it's value is larger than the smallest current event, perform a sorted insert and drop the lowest value.\nFlush every N milliseconds\n\nThis could be an extension of tag_cardinality_limit, but it's probably better as a different thing since the tags can change every interval.\nIt probably makes most sense as an extension to aggregate so that duplicates can be properly summed/max-ed/mean-ed etc. rather than always implicitly doing max.",
        "url": "https://github.com/vectordotdev/vector/discussions/22608",
        "createdAt": "2025-03-07T05:02:41Z",
        "updatedAt": "2025-03-20T19:47:45Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "djrodgerspryor"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 22642,
        "title": "Splunk HEC source requires channel with ack disabled",
        "bodyText": "When testing a basic Splunk HEC source I've noticed that when a payload is sent to the /services/collector/raw endpoint without a X-Splunk-Request-Channel you get the below response back.\n{ \"text\": \"Data channel is missing\", \"code\": 10 }\nThe below is my configuration for Vector that I have setup which includes acknowledgment being disabled.\nsources:\n  hec_8088:\n    type: splunk_hec\n    address: 0:0:0:0:8088\n    acknowledgements:\n      enabled: false\n    valid_tokens:\n      - abc123\n\nReading the docs from Splunk found below they mention channel is only required when acknowledgement is enabled, but in Vector it looks like this is required for any payloads sent to the \"raw\" endpoint regardless if acknowledgement is enabled or not. Is this the intended behavior?\nhttps://docs.splunk.com/Documentation/Splunk/9.4.1/Data/AboutHECIDXAck#About_channels_and_sending_data",
        "url": "https://github.com/vectordotdev/vector/discussions/22642",
        "createdAt": "2025-03-12T18:03:49Z",
        "updatedAt": "2025-03-20T19:39:25Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "objectbased"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22677,
        "title": "How to organize Vector unit tests in a separate folder?",
        "bodyText": "Hi,\nI'm using Vector to process logs and send them to Graylog. I have multiple transform files written in YAML, and I want to write unit tests for each of them.\nI initially added the tests directly inside each transform file, but since I have many files, I would prefer to move all tests into a separate folder (vector/tests/).\nHowever, when I do this and run vector -C vector, I get the following error:\nvector -C vector\n2025-03-18T01:13:36.990371Z  INFO vector::app: Log level is enabled. level=\"info\"\n2025-03-18T01:13:36.991235Z  INFO vector::app: Loading configs. paths=[\"vector\"]\n2025-03-18T01:13:36.996554Z ERROR vector::cli: Configuration error. error=unknown field `tests`, expected one of `name`, `input`, `inputs`, `outputs`, `no_outputs_from`\nin `apache`\nHere the structure of my /etc/vector:\nsinks/\nsources/\ntests/\ntransforms/\nvector.yaml\nIs there a recommended way to keep tests in a separate folder without causing conflicts with the main configuration?\nI can maybe add them in my transforms but I don't found the right syntax. Here an example of transforms file that I use (/etc/vector/transforms/10_apache_combined.yaml):\ninputs:\n  - \"apache_combined\"\ntype: \"remap\"\nsource: |-\n  . |= parse_apache_log!(.message, \"combined\")\n  .tag = \"apache.combined\"\nThanks in advance!",
        "url": "https://github.com/vectordotdev/vector/discussions/22677",
        "createdAt": "2025-03-18T01:19:38Z",
        "updatedAt": "2025-03-20T17:56:07Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Oyabi"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22690,
        "title": "log level of instance vector",
        "bodyText": "Good day\nIs there a way to change the log level of a running instance of vector?\nits now show all logs i think it use log type info\nI might want to start off with logging only WARN and above\ni used this with values.yaml in k8s but it not works\n# Set log level (off, error, warn, info, debug, trace)\nlog_level: \"error\"\n\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/22690",
        "createdAt": "2025-03-19T13:39:25Z",
        "updatedAt": "2025-03-19T17:28:34Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "homelab8330"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22588,
        "title": "Auto-incrementing a counter per log line",
        "bodyText": "Hi,\nCan anyone suggest ways that vector can increment a counter each time it receives an event from a file source (i.e. a new line is written), and this counter be available to be used in transforms/sinks and also be persisted, so that when vector is restarted, the counter doesn't reset to 0.\nBonus question, a specific counter value will be needed for associated events (logs written to a different file) that might happen minutes/hours after the initial event that caused the counter to increment, so the same counter value for the initial event needs to be shared for these subsequent log lines.\nHope it makes sense, and thanks in advance for any ideas.",
        "url": "https://github.com/vectordotdev/vector/discussions/22588",
        "createdAt": "2025-03-04T15:10:37Z",
        "updatedAt": "2025-03-19T17:34:03Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "biatwc"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22595,
        "title": "Unable to extract labels from loki sink in vector",
        "bodyText": "Trying to collect kafka logs using vector latest version and sending to loki. But unable to extract any labels from loglines. when given under labels i.e.,\nlabels:\nenv: \"{{ env }}\"\nIt's stating function env not found error though it's present within logline. Please help asap",
        "url": "https://github.com/vectordotdev/vector/discussions/22595",
        "createdAt": "2025-03-05T11:59:57Z",
        "updatedAt": "2025-03-20T19:38:16Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "surekha3"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22602,
        "title": "Vector scaling challenges with Prometheus Alertmanager to Kafka integration",
        "bodyText": "Our team is developing a proof of concept to forward alerts from Prometheus Alertmanager to a Kafka topic. We selected Vector as the data pipeline solution based on initial research and compatibility requirements.\nEnvironment Configuration on Local Machine:\nSource: Prometheus Alertmanager instance\nPipeline: Vector with appropriate source configuration in vector.toml\nDestination: Kafka topic configured as sink\nTest Harness: Custom Java application designed to generate high volumes of test alerts\nIssue Description\nWe are encountering significant performance limitations during load testing of our Vector implementation. When our test harness sends 1-2 million alerts to Alertmanager, only approximately 40,000-45,000 alerts (4% of the total volume) successfully reach our Kafka topic.\nDespite enabling verbose logging mode in Vector, we observe no visibility into the incoming messages being processed, which complicates our troubleshooting efforts.\nQuestions\n\nAre there known throughput limitations in Vector when processing high volumes from Alertmanager?\nWhat configuration parameters should we examine to improve processing capacity?\nAre there specific buffer settings, batch configurations, or resource allocations we should adjust?\nCould there be a communication limitation between Alertmanager and Vector that we need to address?\n\nAdditional Context:\nvector.toml config:\n[sources.alertmanager_source]\ntype = \"http_server\"\naddress = \"0.0.0.0:8686\"\nbuffer.max_event = 10000\nbuffer.type = \"disk\"\nconcurrency = 10\nencoding.codec = \"json\"\n[transforms.split_alerts]\ntype = \"remap\"\ninputs = [\"alertmanager_source\"]\nsource = '''\nparsed =  parse_json!(.message) #ensure json parsing\nalerts = parsed.alerts\nif !is_array(alerts) { alerts = [alerts] } #convert array to non array\n. = alerts\n'''\n[sinks.kafka_out]\ntype = \"kafka\"\ninputs = [\"split_alerts\"]\nbootstrap_servers = \"localhost:9092\"\ntopic = \"vector-test\"\nencoding.codec = \"json\"\nbatch.max_events = 5000\nbatch.timeout_ms = 100000",
        "url": "https://github.com/vectordotdev/vector/discussions/22602",
        "createdAt": "2025-03-06T09:46:32Z",
        "updatedAt": "2025-03-18T16:26:58Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "niksarangi"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22683,
        "title": "arm or intel performance",
        "bodyText": "The performance of vector on ARM architecture machines seems to be not as good as that of Intel. What is the reason? Version 0.29.1",
        "url": "https://github.com/vectordotdev/vector/discussions/22683",
        "createdAt": "2025-03-18T14:07:02Z",
        "updatedAt": "2025-03-18T14:07:03Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "lhh528634141"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22131,
        "title": "Vector source through GKE Gateways",
        "bodyText": "I can't get the vector source to work through the GKE's Gateway.\nThe aggregator configuration is:\napi:\n  enabled: true\n  address: 0.0.0.0:8686\n\nsources:\n  in:\n    type: vector\n    address: 0.0.0.0:8080\n    tls:\n      enabled: false\n\nsinks:\n  out:\n    type: blackhole\n    inputs:\n    - in\n\nand is exposed via a service and a httproute (the API is enabled for the healthcheck policy).\nTrying to send dummy logs to it with the following configuration :\nsources:\n  in:\n    type: \"stdin\"\n\nsinks:\n  out:\n    type: vector\n    inputs:\n      - \"in\"\n    address: https://hostname\n\nresults in the following log line:\n2025-01-07T08:03:49.027306Z ERROR vector::topology::builder: msg=\"Healthcheck failed.\" error=Request failed: status: Unknown, message: \"h2 protocol error: http2 error: connection error detected: frame with invalid size\", details: [], metadata: MetadataMap { headers: {} } component_kind=\"sink\" component_type=\"vector\" component_id=out\n\n(Relevant logs with debug enabled: vector-log.txt)\nI get that I probably misconfigured something, but I can't find it out. I've tried port-forwarding the service locally to rule out missconfiguration of the source/sink and it's working properly, so it's probably the Gateway shitting the bed.\nMy current gateway is defined as follows:\napiVersion: gateway.networking.k8s.io/v1beta1\nkind: Gateway\nmetadata:\n  name: gateway\n  namespace: default\n  annotations:\n    networking.gke.io/certmap: \"${certmap}\"\nspec:\n  gatewayClassName: gke-l7-global-external-managed\n  listeners:\n  - name: https\n    protocol: HTTPS\n    port: 443\n\nThe one listener does TLS termination for me.\nI ended up switching to the http_server source to bypass the issue for now, I'll come back to it later. In the meantime, I would welcome any pointer or example if someone got it working.",
        "url": "https://github.com/vectordotdev/vector/discussions/22131",
        "createdAt": "2025-01-07T08:20:00Z",
        "updatedAt": "2025-03-18T08:48:58Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "elwinar"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22657,
        "title": "is there an easier way to do basename/dirname when using file source",
        "bodyText": "I have the following minimal config file (experimented on the VRL playground)\nI want to extract the \"only file name\" bit (first part of basename) from the full file name of the log file.\nI have the following so far, but was wondering if there is there a more \"proper\" way of doing this?\n(In the longer run I want to use the extracted name as a log label when forwarding to Loki.\ndata_dir: /tmp/experiment/\n\nsources:\n  logs:\n    type: file\n    include:\n      - /var/log/*.log\n\ntransforms:\n  modify:\n    type: remap\n    inputs:\n      - logs\n    source: |\n      parts = split!(.file, \"/\")\n      basename = split!(parts[-1], \".\")\n      .service_name = basename[0]\n\nsinks:\n  my_sink_id:\n    type: console\n    inputs:\n      - modify\n    encoding:\n      codec: json",
        "url": "https://github.com/vectordotdev/vector/discussions/22657",
        "createdAt": "2025-03-14T15:58:04Z",
        "updatedAt": "2025-03-18T05:32:09Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "shantanugadgil"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22670,
        "title": "monitoring file completion status in file-to-Kafka pipelines",
        "bodyText": "When using Vector for a file-to-Kafka pipeline, is there a way to configure a dashboard or check metrics to confirm that file uploads are complete/done?\nDue to the complexity of various rotation/rolling environments and real-time tailing issues, the pairing of filename:offset may not match the metrics:offset of transmitted logs.\nBased on the current \"file EOF reached\" metrics content, it's difficult to make an accurate determination.\nIs there any good method to precisely determine when a file is done - monitoring stopped - and no longer being written to?",
        "url": "https://github.com/vectordotdev/vector/discussions/22670",
        "createdAt": "2025-03-17T10:12:20Z",
        "updatedAt": "2025-03-17T22:50:22Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "whyjp"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 0
    },
    {
        "number": 22524,
        "title": "Concurrency/fault-tolerance for aggregated data",
        "bodyText": "Hi!\nWe would like to use Vector to collect logs from Kafka topics and generate aggregations with some different time windows (e.g. counts/sums for 10 min, 1 hour, 1 day, etc). We were planning to store the results on an external storage like Redis. Some questions that we have:\n\nIs it possible to run parallel Vector instances to do these aggregations? Do I need something like multiple agents + central aggregator?\nRegarding fault-tolerance, if a Vector instance crashes, do we lose current track of aggregated data? Would reducing the sink intervals (like every 10 seconds) be a possible solution for this or it would probably impact a lot the performance?",
        "url": "https://github.com/vectordotdev/vector/discussions/22524",
        "createdAt": "2025-02-26T14:48:03Z",
        "updatedAt": "2025-03-17T20:55:44Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "danielhoshi"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22538,
        "title": "Vector configuration optimisation for memory usage reduction in metric ingestion via Kafka",
        "bodyText": "Hello to all,\nI'm working on a metrics ingestion pipeline with Vector, where the source is a Kafka cluster and the approximate throughput is 15 MB/s. The messages we receive have an average size between 300 and 2000 bytes.\nTo control the memory usage, I have made some adjustments in the librdkafka_options section and in the sinks configuration. Below, I share an extract of my configuration:\n[api]\nenabled = true\n\n[sources.kafka_in]\ntype = \"kafka\"\nbootstrap_servers = \"kafka1.example.com:9093,kafka2.example.com:9093,kafka3.example.com:9093\"\ngroup_id = \"vector2kafka-XXXXXXXX.metrics\"\ntopics = [\"metrics_topic\"]\ntls.enabled = true\ntls.verify_certificate = false\ntls.ca_file = \"/certs/ca.crt\"\ntls.crt_file = \"/certs/tls.crt\"\ntls.key_file = \"/certs/tls.key\"\ndecoding.codec = \"native\"\nauto_offset_reset = \"earliest\"\ncommit_interval_ms = 2000\nfetch_wait_max_ms = 10\nsession_timeout_ms = 10000\n\nlibrdkafka_options = {\n  \"fetch.message.max.bytes\"     = \"8192\",  \n  \"queued.min.messages\"         = \"100\",\n  \"queued.max.messages.kbytes\"  = \"8192\",\n  \"socket.receive.buffer.bytes\" = \"65536\" \n}\n\nacknowledgements.enabled = true \n\n[sources.internal_metrics]\ntype = \"internal_metrics\"\n\n[transforms.filter_metrics]\ntype = \"filter\"\ninputs = [\"kafka_in\"]\ncondition = 'exists(.tags.leanix) && .tags.leanix == \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\" && exists(.tags.environment) && .tags.environment != \"production\"'\n\n[transforms.route_metrics]\ntype = \"route\"\ninputs = [\"filter_metrics\"]\nroute.private = 'exists(.tags.private) && .tags.private == \"true\"'\nroute.default = '!(exists(.tags.private) && .tags.private == \"true\")'\n\n[sinks.vmbucket1_public]\ntype = \"prometheus_remote_write\"\ninputs = [\"route_metrics.default\"]\nendpoint = \"http://vm-bucket-1.example.internal:8480/insert/0/prometheus/api/v1/write\"\ncompression = \"snappy\"\nhealthcheck = false\nbatch.timeout_secs = 1\nbatch.max_events = 300\nbuffer.max_events = 500\n\n[sinks.vmbucket1_private]\ntype = \"prometheus_remote_write\"\ninputs = [\"route_metrics.private\"]\nendpoint = \"http://vm-bucket-1.example.internal:8480/insert/103/prometheus/api/v1/write\"\ncompression = \"snappy\"\nhealthcheck = false\nbatch.timeout_secs = 1\nbatch.max_events = 300\nbuffer.max_events = 500\n\n[sinks.prometheus_exporter]\ntype = \"prometheus_exporter\"\ninputs = [\"internal_metrics\"]\naddress = \"0.0.0.0:9201\"\n\nMy question is the following:\nAre there any additional settings or parameters I can adjust in Vector (or librdkafka_options) to further optimize memory usage, without compromising throughput, in a 15 MB/s metric environment? Also, what methods do you recommend for verifying and measuring the actual size of messages in my Kafka topic?\nTo give you more context, I am currently using a customised solution that consumes 50% less resources than Vector....\nI welcome any suggestions or experiences you can share, thank you in advance!",
        "url": "https://github.com/vectordotdev/vector/discussions/22538",
        "createdAt": "2025-02-28T06:11:45Z",
        "updatedAt": "2025-03-17T20:37:29Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "JoiKKo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22567,
        "title": "Convert Journald array of numbers to string",
        "bodyText": "I have a docker container which outputs logs with ANSI colors. When forwarding via journald the body_message which looks something like this:\nuptime-kuma       | 2025-03-03T08:25:10Z [AUTH] INFO: WebSocket with no origin is allowed\nshows up as an array of numbers:\n \"body_message\": \"[\\\"27\\\",\\\"91\\\",\\\"51\\\",\\\"54\\\",\\\"109\\\",\\\"50\\\",\\\"48\\\",\\\"50\\\",\\\"53\\\",\\\"45\\\",\\\"48\\\",\\\"51\\\",\\\"45\\\",\\\"48\\\",\\\"51\\\",\\\"84\\\",\\\"48\\\",\\\"56\\\",\\\"58\\\",\\\"48\\\",\\\"53\\\",\\\"58\\\",\\\"52\\\",\\\"52\\\",\\\"90\\\",\\\"27\\\",\\\"91\\\",\\\"48\\\",\\\"109\\\",\\\"32\\\",\\\"91\\\",\\\"27\\\",\\\"91\\\",\\\"51\\\",\\\"56\\\",\\\"59\\\",\\\"53\\\",\\\"59\\\",\\\"49\\\",\\\"49\\\",\\\"57\\\",\\\"109\\\",\\\"77\\\",\\\"79\\\",\\\"78\\\",\\\"73\\\",\\\"84\\\",\\\"79\\\",\\\"82\\\",\\\"27\\\",\\\"91\\\",\\\"48\\\",\\\"109\\\",\\\"93\\\",\\\"32\\\",\\\"27\\\",\\\"91\\\",\\\"51\\\",\\\"51\\\",\\\"109\\\",\\\"87\\\",\\\"65\\\",\\\"82\\\",\\\"78\\\",\\\"58\\\",\\\"27\\\",\\\"91\\\",\\\"48\\\",\\\"109\\\",\\\"32\\\",\\\"77\\\",\\\"111\\\",\\\"110\\\",\\\"105\\\",\\\"116\\\",\\\"111\\\",\\\"114\\\",\\\"32\\\",\\\"35\\\",\\\"49\\\",\\\"56\\\",\\\"55\\\",\\\"32\\\",\\\"39\\\",\\\"76\\\",\\\"105\\\",\\\"118\\\",\\\"105\\\",\\\"110\\\",\\\"103\\\",\\\"32\\\",\\\"82\\\",\\\"111\\\",\\\"111\\\",\\\"109\\\",\\\"39\\\",\\\"58\\\",\\\"32\\\",\\\"70\\\",\\\"97\\\",\\\"105\\\",\\\"108\\\",\\\"105\\\",\\\"110\\\",\\\"103\\\",\\\"58\\\",\\\"32\\\",\\\"67\\\",\\\"104\\\",\\\"105\\\",\\\"108\\\",\\\"100\\\",\\\"32\\\",\\\"105\\\",\\\"110\\\",\\\"97\\\",\\\"99\\\",\\\"99\\\",\\\"101\\\",\\\"115\\\",\\\"115\\\",\\\"105\\\",\\\"98\\\",\\\"108\\\",\\\"101\\\",\\\"32\\\",\\\"124\\\",\\\"32\\\",\\\"73\\\",\\\"110\\\",\\\"116\\\",\\\"101\\\",\\\"114\\\",\\\"118\\\",\\\"97\\\",\\\"108\\\",\\\"58\\\",\\\"32\\\",\\\"54\\\",\\\"48\\\",\\\"32\\\",\\\"115\\\",\\\"101\\\",\\\"99\\\",\\\"111\\\",\\\"110\\\",\\\"100\\\",\\\"115\\\",\\\"32\\\",\\\"124\\\",\\\"32\\\",\\\"84\\\",\\\"121\\\",\\\"112\\\",\\\"101\\\",\\\"58\\\",\\\"32\\\",\\\"103\\\",\\\"114\\\",\\\"111\\\",\\\"117\\\",\\\"112\\\",\\\"32\\\",\\\"124\\\",\\\"32\\\",\\\"68\\\",\\\"111\\\",\\\"119\\\",\\\"110\\\",\\\"32\\\",\\\"67\\\",\\\"111\\\",\\\"117\\\",\\\"110\\\",\\\"116\\\",\\\"58\\\",\\\"32\\\",\\\"48\\\",\\\"32\\\",\\\"124\\\",\\\"32\\\",\\\"82\\\",\\\"101\\\",\\\"115\\\",\\\"101\\\",\\\"110\\\",\\\"100\\\",\\\"32\\\",\\\"73\\\",\\\"110\\\",\\\"116\\\",\\\"101\\\",\\\"114\\\",\\\"118\\\",\\\"97\\\",\\\"108\\\",\\\"58\\\",\\\"32\\\",\\\"48\\\"]\"\nIs there a way in a pipeline/function to convert this to a legible string? I can't figure it out.",
        "url": "https://github.com/vectordotdev/vector/discussions/22567",
        "createdAt": "2025-03-03T08:27:19Z",
        "updatedAt": "2025-03-17T20:28:59Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "barcar"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22570,
        "title": "Internal metrics not showed CPU and RAM used in ubuntu. i found Utilization!",
        "bodyText": "Hello, Friends\ni got dump question, how to check whether vector dont get enough resource on ubuntu?\nthe internal metrics shows Utilization. and i need CPU and Memory Usage.",
        "url": "https://github.com/vectordotdev/vector/discussions/22570",
        "createdAt": "2025-03-03T10:04:01Z",
        "updatedAt": "2025-03-17T20:17:29Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "homelab8330"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22605,
        "title": "Azure App Service Datadog Extension Error",
        "bodyText": "Azure App Service Datadog Extension version: 3.3.100\nVector version: 0.44.0\nI'm trying to send Datadog logs from Azure App Service to Datadog via the Datadog extension. I see this error in my app service.\n2025-03-05 21:19:36.902 +00:00 [ERR] Failed to submit logs with status code 400 and message: {\"code\":400,\"message\":\"Error parsing JSON: Error(\\\"unknown field `@t`, expected one of `message`, `status`, `timestamp`, `hostname`, `service`, `ddsource`, `ddtags`\\\", line: 1, column: 6)\"}  { MachineName: \".\", Process: \"[11480 w3wp]\", AppDomain: \"[1 <redacted>]\", AssemblyLoadContext: \"\\\"\\\" Datadog.Trace.ClrProfiler.Managed.Loader.ManagedProfilerAssemblyLoadContext #2\", TracerVersion: \"3.11.1.0\" }\n2025-03-05 21:19:36.908 +00:00 [ERR] An error occurred while sending 12 traces to the intake at https://<redacted>/api/v2/logs  { MachineName: \".\", Process: \"[11480 w3wp]\", AppDomain: \"[1 <redacted>]\", AssemblyLoadContext: \"\\\"\\\" Datadog.Trace.ClrProfiler.Managed.Loader.ManagedProfilerAssemblyLoadContext #2\", TracerVersion: \"3.11.1.0\" }\n\nI noticed that all of my Azure App services using this Datadog extension point directly to Datadog instead of Vector have logs like this for example in datadog. Is the issue that the Datadog extension doesn't support sending logs correctly to Vector or a misconfiguration on my part when using Vector instead of Datadog directly?\n{\n  \"@i\": \"12807b18\",\n  \"@l\": \"Error\",\n  \"@t\": \"2025-03-06T19:42:29.6317746Z\",\n  \"Application\": \"<redacted>\",\n  \"dd_service\": \"<redacted>\",\n  \"dd_span_id\": \"<redacted>\",\n  \"host\": \"<redacted>\",\n  \"MachineName\": \"<redacted>\",\n  \"RequestId\": \"<redacted>\",\n  \"RequestPath\": \"/health\",\n  \"SourceContext\": \"<redacted>\"\n}",
        "url": "https://github.com/vectordotdev/vector/discussions/22605",
        "createdAt": "2025-03-06T20:25:18Z",
        "updatedAt": "2025-03-17T18:41:44Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "blakesherry"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22654,
        "title": "Are the docs truncating information?",
        "bodyText": "I keep seeing VRL functions when I use the search bar on the vector website referenced, but when I click them, I never see them on the actual document page.  Example would be https://vector.dev/docs/reference/vrl/functions/#map_keys\nI see that the document is listed in alphabetical order, but it doesn't seem to get very far through the alphabet, seems like it is truncating information?",
        "url": "https://github.com/vectordotdev/vector/discussions/22654",
        "createdAt": "2025-03-13T19:21:14Z",
        "updatedAt": "2025-03-17T17:16:51Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bcronrath"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 1
    },
    {
        "number": 22660,
        "title": "Lack of truly guaranteed log delivery with Kafka Sink",
        "bodyText": "Hi, everyone!\nI am facing a case where guaranteed delivery of logs from files to Kafka is required (not a single line can be lost).\nDuring testing, I encountered the issue that with the default rdkafka options and disk buffering enabled, logs older than 5 minutes are lost when Kafka is down (this is the default timeout for rdkafka), and the buffer and acknowledgements do not facilitate resending since the logs have already been placed into rdkafka\u2019s internal queue, as far as I understand.\nOf course, one could disable timeouts at the rdkafka level and force it to always keep retrying, but then another problem arises \u2014 upon restart, logs that have already been queued in rdkafka will be lost.\nI can see an issue #8468 for that, but maybe there are some workarounds? It is a blocker for us and I don't think this case is particularly rare :)",
        "url": "https://github.com/vectordotdev/vector/discussions/22660",
        "createdAt": "2025-03-16T10:56:45Z",
        "updatedAt": "2025-03-16T10:56:46Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "evevseev"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22589,
        "title": "log_to_metric tags converted to lowercase",
        "bodyText": "Hi!\nI'm not sure if this is a bug, working as expected or simply bad configuration by my side.\nI'm going through the log_to_metric transform which allows to set the metric labels/tags. According to documentation you can use \"*\" to add a log field map with tags. I've used it but for some reason the tag keys are converted to lower case. Only for those added with \"*\".\nIn the example below, I added the field tags which has a key named LogGroupName. On the metric tags that key is converted to lower case loggroupname. But, if I add a tag directly with upper case, Aws_account_id, it is left without changes.\ntype: \"log_to_metric\"\n    inputs:\n    - \"inputstep\"\n    metrics:\n    - type: \"counter\"\n      ............\n      tags:\n        \"Aws_account_id\": \"xxxxxxxxxxx\"\n        \"environment\": \"development\"\n        \"*\": \"{{ tags }}\"\n\nLog input:\n{\"count\":16.0,\"dimensions\":{\"LogGroupName\":\"example-log-group\"},\"max\":3.0,\"metric_name\":\"IncomingLogEvents\", \"min\":1.0,\"namespace\":\"AWS/Logs\",\"region\":\"eu-west-1\",\"sum\":20.0,\"tags\":{\"LogGroupName\":\"example-log-group\"},\"timestamp\":\"2025-02-27T15:54:00Z\",\"unit\":\"None\"}\nMetric output:\n{\"name\":\"incominglogevents_sum\",\"tags\":{\"Aws_account_id\":\"xxxxxxxxxxx\",\"environment\":\"development\",\"loggroupname\":\"example-log-group\"},\"timestamp\":\"2025-02-27T15:54:00Z\",\"kind\":\"absolute\",\"counter\":{\"value\":20.0}}\nThanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/22589",
        "createdAt": "2025-03-04T15:55:21Z",
        "updatedAt": "2025-03-13T17:36:53Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "GGonzalezGomez"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22649,
        "title": "Missing cpu, memory metrics on upgrade to 0.44 version",
        "bodyText": "Trying upgrade to 0.44 version, but the Datadog dashboard Kubernetes metrics for cpu and memory shows no data after upgrade.\nPlease advise.\nThese are the metrics that were working before upgrade:\nkubernetes.cpu.user.total\nkubernetes.memory.usage_pct",
        "url": "https://github.com/vectordotdev/vector/discussions/22649",
        "createdAt": "2025-03-12T21:19:43Z",
        "updatedAt": "2025-03-13T01:57:51Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "shamj"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22640,
        "title": "Events dropped with error : \"Error, message length too large: found 4276576 bytes, the limit is: 4194304 bytes\"",
        "bodyText": "While upgrading to 0.30.0 version , the events dropped in prod environment, On investigating more , noticed below error:\n{component_kind=\"sink\"  component_type=vector }: vector::sinks::util::retries: Non-retriable error; dropping the request. error=Request failed: status: OutOfRange, message: \"Error, message length too large: found 4276576 bytes, the limit is: 4194304 bytes\"\nPlease advise.\nThe events are going from sink vector -> aws lb ->  source vector\nNote** We have another usecase that's lead us to upgrade to 0.30.0 version.",
        "url": "https://github.com/vectordotdev/vector/discussions/22640",
        "createdAt": "2025-03-12T15:59:31Z",
        "updatedAt": "2025-03-12T23:10:26Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "shamj"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22534,
        "title": "Meaning of `alpha` and `beta`?",
        "bodyText": "Hi team,\nThe datadog_agent's multiple_outputs configuration option states:\n\nIf this is set to true, logs, metrics (beta), and traces (alpha) are sent to different outputs.\n\nSome components like websocket_server also have a status: beta label.\nWhat exactly do beta and alpha mean in these contexts? We'd like to better understand so that we can be better informed about which vector features to use when building our vector pipeline. For what its worth, we've been collecting metrics from datadog agents for over 2 years now and haven't realized that it was still in beta.\nThanks in advance!",
        "url": "https://github.com/vectordotdev/vector/discussions/22534",
        "createdAt": "2025-02-27T18:43:13Z",
        "updatedAt": "2025-03-12T23:02:06Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "nzxwang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22189,
        "title": "vector-agent with multiple vector_sinks",
        "bodyText": "Hello,\nI have a vector agent, I want it to have N vector_sinks.\nThe vector port on my vector aggregators is 6000,\nwhen I have a config like (in my agent customConfig):\nsinks:\n  vector_sink_1:\n    type: vector\n    inputs: [k8_logs]\n    address: vector.ns1.svc.cluster.local:6000\n  vector_sink_2:\n    type: vector\n    inputs: [k8_logs]\n    address: vector.ns2.svc.cluster.local:6000\n\nI get a fatal kubernetes error , saying that I cannot have duplicate port numbers?\nSeems the way the helm-chart is configured is wrong? I can maybe just ignore the error, but it worries me.\nIt still deploys and seems to work, but how do I get around it? I cannot see ANY examples, showing this usecase am I doing something wrong?",
        "url": "https://github.com/vectordotdev/vector/discussions/22189",
        "createdAt": "2025-01-13T13:38:45Z",
        "updatedAt": "2025-03-12T18:12:57Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "jsbloo"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 22636,
        "title": "How to use vector to filter log keywords, for example, trigger telegram alarm if more than 3 times within 5 minutes, thank you very much",
        "bodyText": "vector.toml\ndata_dir = \"/opt/vector/data\"\ntimezone = \"Asia/Hong_Kong\"\n\n[api]\nenabled = true\naddress = \"127.0.0.1:8686\"\n\n[sources.log_source]\ntype = \"file\"\ninclude = [\"/data/logs/api/api.log\"]\nignore_older_secs = 600\n\n[transforms.info_filter]\ntype = \"filter\"\ninputs = [\"log_source\"]\ncondition = 'contains(string(.message) ?? \"\", \"INFO\")'\n\n[transforms.json_encoder]\ntype = \"remap\"\ninputs = [\"info_filter\"]\nsource = '''\n. = { \"text\": \"[LOG ALERT] \\n\" + string(.message) }\n'''\n\n[sinks.telegram_sink]\ntype = \"http\"\ninputs = [\"json_encoder\"]\nuri = \"https://api.telegram.org/botxxxxxxxx:xxxxxxxxxxxxxx/sendMessage\"\nmethod = \"post\"\nencoding.codec = \"json\"\n\n[ sinks.telegram_sink.headers ]\nContent-Type = \"application/json\"\n\n[ sinks.telegram_sink.request ]\nbody = '''\n{\n  \"chat_id\": \"-xxxxxxxx\",\n  \"text\": \"{{ text }}\"\n}\n'''\n\nerror.log\nMar 12 10:58:41 test vector[1514]: \u221a Loaded [\"/opt/vector/config\"]\nMar 12 10:58:41 test vector[1514]: Component errors\nMar 12 10:58:41 test vector[1514]: ----------------\nMar 12 10:58:41 test vector[1514]: x Transform \"json_encoder\":\nMar 12 10:58:41 test vector[1514]: error[E103]: unhandled fallible assignment\nMar 12 10:58:41 test vector[1514]: \u250c\u2500 :1:5\nMar 12 10:58:41 test vector[1514]: \u2502\nMar 12 10:58:41 test vector[1514]: 1 \u2502 . = { \"text\": \"[LOG ALERT] \\n\" + string(.message) }\nMar 12 10:58:41 test vector[1514]: \u2502 --- ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ this expression is fallible because at least one argument's type cannot be verified to be valid\nMar 12 10:58:41 test vector[1514]: \u2502 \u2502\nMar 12 10:58:41 test vector[1514]: \u2502 or change this to an infallible assignment:\nMar 12 10:58:41 test vector[1514]: \u2502 ., err = { \"text\": \"[LOG ALERT]\nMar 12 10:58:41 test vector[1514]: \" + string(.message) }\nMar 12 10:58:41 test vector[1514]: \u2502\nMar 12 10:58:41 test vector[1514]: = see documentation about error handling at https://errors.vrl.dev/#handling\nMar 12 10:58:41 test vector[1514]: = see functions characteristics documentation at https://vrl.dev/expressions/#function-call-characteristics\nMar 12 10:58:41 test vector[1514]: = learn more about error code 103 at https://errors.vrl.dev/103\nMar 12 10:58:41 test vector[1514]: = see language documentation at https://vrl.dev\nMar 12 10:58:41 test vector[1514]: = try your code in the VRL REPL, learn more at https://vrl.dev/examples\nMar 12 10:58:41 test systemd[1]: vector.service: control process exited, code=exited status=78\nMar 12 10:58:41 test systemd[1]: Failed to start Vector.",
        "url": "https://github.com/vectordotdev/vector/discussions/22636",
        "createdAt": "2025-03-12T03:09:35Z",
        "updatedAt": "2025-03-12T03:09:36Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "qooke"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22074,
        "title": "Azure Blob Storage sink issues with compression",
        "bodyText": "When using Azure Blob Storage sink  with gzip compression, there are issues with Azure - when downloading the file to my local machine, it seems like the file is corrupted.\nThere are many issues with Azure around it from what I see online, however when removing CONTENT-ENCODING all seems OK, any change I can override the headers added by vector.dev although keep using  gzip compression? any other suggestion will gladly accepted...",
        "url": "https://github.com/vectordotdev/vector/discussions/22074",
        "createdAt": "2024-12-21T19:54:15Z",
        "updatedAt": "2025-03-11T19:48:55Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "danelkotev"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22612,
        "title": "Use transform to add key values to windows host metric events",
        "bodyText": "Hello, I am currently fetching host metrics to ingest windows performance logs\nsources:\n  win_hostmetrics:\n    type: \"host_metrics\"\n    scrape_interval_secs: 10\n    filesystem:\n      devices:\n        excludes: [\"C:\\\\pagefile.sys\"]\n    disk:\n      devices:\n        excludes: [\"C:\\\\pagefile.sys\"]\n\nI am trying to use transform to add 2 new keys explicitly \"dt\" and \"message\" to every event through vector, like below:\ntransforms:\n  parse_win_hostmetrics:\n    type: \"remap\"\n    inputs: [\"win_hostmetrics\"]\n    source: |\n      .dt = .timestamp\n      .message = {\"Windows Host Metric Event\"}\n\nI have tried alternate ways such as encoding to json or merging as well, but it does not work.\nBut the transform is never applied, although if i give del(.timestamp), the timestamp variable gets removed: sample output below.\n...}\n{\n  \"name\": \"memory_swap_total_bytes\",\n  \"tags\": {\n    \"host\": \"DESKTOP-F1VAVE8\"\n  },\n  \"kind\": \"absolute\",\n  \"gauge\": {\n    \"value\": 13455073280.0\n  }\n}\n{...\n\nIs this an issue or am I doing something wrong? Any help would be appreciated, thanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/22612",
        "createdAt": "2025-03-07T05:20:28Z",
        "updatedAt": "2025-03-10T05:23:52Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "abubakr-cgs"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22376,
        "title": "host_metrics on Windows",
        "bodyText": "Dear all,\nWhen I installed Vector on the windows, and set the host_metrics on sources block, I just collect the metrics of CPU&Memory&Disk, why can I not collect metrics of network?\nWho can help me to find out the cause, thank you so much!\nSun",
        "url": "https://github.com/vectordotdev/vector/discussions/22376",
        "createdAt": "2025-02-06T12:54:31Z",
        "updatedAt": "2025-03-07T17:22:54Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "SunT123"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 21880,
        "title": "Kafka source low throughput",
        "bodyText": "Hello, I have created a simple Vector pipeline that looks like this: Kafka -> Remap -> Kafka.\nI am noticing a low amount of messages coming from the Kafka source no matter what configuration I set or the amount of partitions I have. The amount of events in bytes emitted looks to be ~1MB per second.\nThe utilization of my remap is low as well. Around  0.1. The rate is the same if I have lag or not (Kafka Broker has more messages in its queue)\nNow my Pipeline can keep up with the Kafka source queue if there are no hiccups in the system. If I restart Vector and let lag build up, Vector will not be able to clear it.\nI have also tried to verify the bottleneck is in the Kafka Source and just tried a pipeline in the form of:\n1 )Kafka -> Black Hole\n2) Kafka -> Remap -> Black Hole\nBoth of those designs have the same symptoms. Kafka source returning emitting a low number of events to the next step.\nAcknowledgements are turned on and setting or removing the encoding: json in the source doesnt make a difference.\nA simple example of my pipeline would be:\nsources:\n      kafka:\n         type: kafka\n         topics: \"my-topic\"\n         bootstrap_servers: \"my.server:port\"\n         group_id: \"my-group\"\n     transforms:\n        extract:\n            type: remap\n            inputs:\n               - kafka\n            source: |\n                . = parse_json!(.)\n        sinks:\n             emit_to_kafka:\n             inputs: \n                 - extract\n             ....\n\nI have tried setting the kafka options in the source: fetch.max.bytes, max.poll.records and broker options as well.\nThe only setting to have a difference was fetch.wait.max.ms but that just increased/decreased overall request rate with the amount of data returned over a period staying the same.\nIs there anything that I am missing or need to verify as to why my kafka source is not saturating my pipeline?",
        "url": "https://github.com/vectordotdev/vector/discussions/21880",
        "createdAt": "2024-11-25T18:04:13Z",
        "updatedAt": "2025-03-06T14:57:23Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Arty-Maly"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22600,
        "title": "Error with redact() function - `regex parse error: look-around, including look-ahead and look-behind, is not supported: ^^^:`",
        "bodyText": "I am currently on vector version v0.34.0. Has this feature been added between current release and v0.34.0? Thanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/22600",
        "createdAt": "2025-03-05T22:58:05Z",
        "updatedAt": "2025-03-05T23:07:29Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "JustinJKelly"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19459,
        "title": "Logs are not sent to loki without labels",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nHey guys!\nI found this some old threads on loki and labels and I still don't get the labels.\nModern loki sink asks labels as required field, and I suppose without them, vector just doesn't send logs to the loki right.\nWhat should I fill into labels? Can I just put\nlabels:\n  \"*\": \"*\"\n\ninto a loki sink config to make it send all labels as they are?\nConfiguration\nloki:\n      type: loki\n      endpoint: http://loki-gateway.loki.svc.cluster.local:80\n      encoding:\n        codec: json\n      labels:\n        \"*\": \"\"\n      inputs:\n        - container_json\n\n\n\n### Version\n\n0.32.1\n\n### Debug Output\n\n_No response_\n\n### Example Data\n\n_No response_\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_",
        "url": "https://github.com/vectordotdev/vector/discussions/19459",
        "createdAt": "2023-12-22T23:26:14Z",
        "updatedAt": "2025-03-05T13:27:16Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "dbazhal"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22590,
        "title": "Chronicle Unstructured Sink - Templated Labels",
        "bodyText": "Hey All,\nI'm looking to add dynamic content to some of my ingestion labels in the GCP Chronicle Unstructured sink.\nI believe, and from testing I've confirmed, that the key/value pairs of labels are currently string literals rather than templated strings, so I was wondering if there was a way I could create a map during a transform and then set the full value of the labels field in the sink to that map? Or would this require a PR to enable the values of the labels map support template.\nThanks in advance!",
        "url": "https://github.com/vectordotdev/vector/discussions/22590",
        "createdAt": "2025-03-04T21:04:08Z",
        "updatedAt": "2025-03-04T21:04:09Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Matt-Smart"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22579,
        "title": "What is the benefit of exclusive_route over route?",
        "bodyText": "We're currently use the route transform in a few places to route between a handful of destinations. Since our conditions are all mutually exclusive, they only ever route to 1 destination, and so I believe these transforms would be good candidates for replacing with exclusive_route. However, I'm just curious what the benefits are? Is the short-circuiting going to be that much quicker or less resource intensive?",
        "url": "https://github.com/vectordotdev/vector/discussions/22579",
        "createdAt": "2025-03-03T19:35:26Z",
        "updatedAt": "2025-03-04T15:51:26Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tronboto"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 22333,
        "title": "Help with Prometheus Scrape query",
        "bodyText": "Im trying to setup a prometheus scrape with a query parameter but having troubles.  I have a successful source setup with the below where I pull back many many metrics and then just drop the ones I dont want.  However my preference would be to only pull back the metrics that I want (filter fast and filter early).\nsources:\n  redis: \n    type: prometheus_scrape\n    endpoints:\n      - \"https://redis_server/metrics\"\n    scrape_interval_secs: 60\n    tls:\n      verify_certificate: false\nThe below is an example of one of the metrics im trying ot capture but am still pulling ALL metrics. Tried both items in list seperately which is based on the documentation from the scrape source.\nsources:\n  redis: \n    type: prometheus_scrape\n    endpoints:\n      - \"https://redis_server/metrics\"\n    query:\n      \"match[]\":\n        - '{name=\"bdb_avg_latency\"}'\n        - '{__name__=~\"bdb_avg_latency\"}'\n    scrape_interval_secs: 60\n    tls:\n      verify_certificate: false\nAny insight would be helpful even if im just interpriting what the query param does.",
        "url": "https://github.com/vectordotdev/vector/discussions/22333",
        "createdAt": "2025-01-30T20:08:44Z",
        "updatedAt": "2025-03-03T20:20:22Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "satellite-no"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22571,
        "title": "Architecture failsafe question",
        "bodyText": "I had an event where my main sink went down and I lost logging to it and the buffer started writting to disk as expected.  The unexpected thing however, was I lost my 'out-of-band' Prometheous Exporter data as well that had all of the Vector Internal node metrics.  Why would I lose all of my internal metrics that go out a different sink?\nAnother qustion is we had one bad table definition that took down all logging for everything, is there a way to architect around this so we dont lose all logging?  This seems very dangoues that 1 bad sinkcould take down TB of logging?",
        "url": "https://github.com/vectordotdev/vector/discussions/22571",
        "createdAt": "2025-03-03T14:38:06Z",
        "updatedAt": "2025-03-03T14:38:07Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "satellite-no"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22472,
        "title": "Sink victorialogs as HTTP (&_time_field)",
        "bodyText": "i found this sink in documentation about victoria logs\nsinks:\n  vlogs:\n    inputs:\n      - your_input\n    type: http\n    uri: http://localhost:9428/insert/jsonline?_stream_fields=host,container_name&_msg_field=message&_time_field=timestamp&debug=1\n    compression: gzip\n    encoding:\n      codec: json\n    framing:\n      method: newline_delimited\n    healthcheck:\n      enabled: false\n\nmy qustion is about uri and\n&_time_field=timestamp\n\nis it possible to write multi timestamps in here like\n_time_field=timestamp,ts,@timestamp,time,json.ts\n\nor it will be conflict and  i must write only one?",
        "url": "https://github.com/vectordotdev/vector/discussions/22472",
        "createdAt": "2025-02-19T10:30:04Z",
        "updatedAt": "2025-03-03T10:10:55Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "homelab8330"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22529,
        "title": "How do I ship Vector's internal metrics to Honeycomb or an OpenTelemetry collector?",
        "bodyText": "Problem\nHoneycomb as a sink does not support the metrics data type, rendering the shipment of vector's internal metrics to honeycomb incompatible out of the box. However, this telemetry is extremely valuable and I'd love to still make use of it.\nHoneycomb expects to receive telemetry shipped in OTLP format, which vector supports via the OpenTelemetry sink but does not provide a pre-built way to translate a metric event into the OTLP proto.\nVector's documentation recommends using VRL to transform data into OTLP which if I'm not mistaken could be a somewhat cumbersome transform. It would be far preferred to have the ability to either send a metric event directly to the honeycomb or OTLP sink (having the transformation occur in the background) or provide a simple means of mapping a metric to OTLP via vector such as a pre-built function.\nExample\nx Data type mismatch between vector_internal_metrics ([\"Metric\"]) and honeycomb ([\"Log\"])\n\nReferences\nhttps://vector.dev/docs/reference/configuration/sinks/opentelemetry/\nhttps://vector.dev/docs/reference/configuration/sinks/honeycomb/\nhttps://vector.dev/docs/about/under-the-hood/architecture/data-model/metric/",
        "url": "https://github.com/vectordotdev/vector/discussions/22529",
        "createdAt": "2025-02-26T22:09:47Z",
        "updatedAt": "2025-03-02T21:25:45Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "britton-from-notion"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 2
    },
    {
        "number": 15649,
        "title": "Journald as source and kafka as sink",
        "bodyText": "i try to deploy vector with this config in kubernets :\ndata_dir: /vector-data-dir\napi:\nenabled: false\nsources:\njournald:\ntype: journald\nsinks:\n  kafka:\n    type: kafka\n    inputs:\n      - journald\n    bootstrap_servers: wwwwssssasas\n    topic: \"logs\"\n    encoding:\n      codec: json\n    sasl:\n      enabled: true  \n      mechanism: \"SCRAM-SHA-512\"\n      username: \"sssasr\"\n\nand i got this error in logs of vector. it cannot collect logs from journald\n2022-12-20T07:28:49.817678Z INFO source{component_kind=\"source\" component_id=journald component_type=journald component_name=journald}: vector::sources::journald: Starting journalctl.\n2022-12-20T07:28:49.818131Z ERROR source{component_kind=\"source\" component_id=journald component_type=journald component_name=journald}: vector::sources::journald: Error starting journalctl process. error=journalctl failed to execute: No such file or directory (os error 2)\nwhat i miss here. anyone can help\"?",
        "url": "https://github.com/vectordotdev/vector/discussions/15649",
        "createdAt": "2022-12-20T08:12:10Z",
        "updatedAt": "2025-03-02T18:45:07Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "therus000"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22545,
        "title": "Is there a bug in internal metrics on 0.45.0",
        "bodyText": "Im running vector 0.45.0 (x86_64-unknown-linux-gnu 063cabb 2025-02-24 14:52:02.810034614) on three seperate Vector instances and when I tap the outputs of the internal_metrics source i do not get any data.  Also all of my data that has feed my promtheus dashboard from this source has stopped work.\nexample configs:\nSource\ntype: \"internal_metrics\"\nscrape_interval_secs: 10\nTransform:\ntype: \"remap\"\ninputs:\n  - vector_internal_host_metrics\n  - vector_internal_metrics\nsource: |\n  .tags.vector_source = \"Vector Internal Metrics\"\n  .tags.App         = \"ABC123\"\n  .tags.Sources       = \"Vector Internal\"\n  .tags.Ci            = del(.tags.host)\nSink:\ntype: prometheus_exporter\ninputs:\n  - core_metric_vector_tags\naddress: 0.0.0.0:9598\nflush_period_secs: 60\nauth:\n  token: <token>\n  strategy: bearer\nbuffer:\n  type: \"disk\"\n  max_size: 53687091200\n  when_full: \"block\"",
        "url": "https://github.com/vectordotdev/vector/discussions/22545",
        "createdAt": "2025-02-28T21:29:25Z",
        "updatedAt": "2025-02-28T22:11:46Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "satellite-no"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22495,
        "title": "Assistance with Decoding Multi-Line Logs Using Vector",
        "bodyText": "Hi everyone,\n\nI've been working on decoding structured logs and encountered difficulties due to their multi-line format. Below is an example of the log format I'm dealing with:\n\n{\n    \"parameters\": \"ERR (!!): Src:GOTREQ-CreateCharge Msg:{\\\"header\\\":{\\\"timestamp\\\":\\\"2025-01-24T09:16:11Z\\\"},\\\"body\\\":{\\\"orderid\\\":\\\"1234567890\\\",\\\"customername\\\":\\\"JOHN DOE\\\",\\\"orderstatus\\\":\\\"2\\\",\\\"orderstatusdesc\\\":\\\"Pending Inventory\\\",\\\"email\\\":\\\"example@email.com\\\",\\\"callbackinforar\\\":\\\"0\\\",\\\"callbackinforoms\\\":\\\"1\\\",\\\"faid\\\":\\\"9876543210\\\",\\\"baid\\\":\\\"123456789\\\",\\\"dealercode\\\":\\\"A1234.56789\\\",\\\"submissiondate\\\":\\\"20250124171404\\\",\\\"paymentItems\\\":[{\\\"serviceid\\\":\\\"60123456789\\\",\\\"price\\\":\\\"0.0\\\",\\\"articleid\\\":\\\"100016933001\\\",\\\"uomid\\\":\\\"2AD\\\",\\\"discountid\\\":\\\"\\\",\\\"discountamount\\\":\\\"\\\",\\\"prepostind\\\":\\\"POSTPAID\\\",\\\"accountcatdesc\\\":\\\"C\\\",\\\"marketcodedesc\\\":\\\"H\\\",\\\"itemno\\\":\\\"3491715875\\\",\\\"itemtaxgroupid\\\":\\\"0\\\",\\\"taxamount\\\":\\\"0\\\",\\\"taxpercentage\\\":\\\"0\\\",\\\"billinginvoiceno\\\":\\\"\\\",\\\"custSegment\\\":null},{\\\"serviceid\\\":\\\"60123456789\\\",\\\"price\\\":\\\"0.0\\\",\\\"articleid\\\":\\\"80000826\\\",\\\"uomid\\\":\\\"EA\\\",\\\"discountid\\\":\\\"\\\",\\\"discountamount\\\":\\\"\\\",\\\"prepostind\\\":\\\"POSTPAID\\\",\\\"accountcatdesc\\\":\\\"C\\\",\\\"marketcodedesc\\\":\\\"H\\\",\\\"itemno\\\":\\\"3491715874\\\",\\\"itemtaxgroupid\\\":\\\"0\\\",\\\"taxamount\\\":\\\"0\\\",\\\"taxpercentage\\\":\\\"0\\\",\\\"billinginvoiceno\\\":\\\"\\\",\\\"custSegment\\\":null}],\\\"misc01\\\":null,\\\"misc02\\\":null,\\\"misc03\\\":null,\\\"misc04\\\":null,\\\"misc05\\\":null}} Ref:\"\n}\n\n\nWhat I Have Tried So Far Using the VRL Playground: I implemented the following Vector configuration for parsing:\n\nparsed = parse_regex!(.parameters, r'^(?P<severity>\\w+ \\(\\!\\!\\)):\\sSrc:(?P<source>[\\w-]+)\\sMsg:(?P<json_payload>\\{.*\\})\\sRef:$', true)\n\n.severity = get(parsed, [\"severity\"]) ?? \"\"\n.source = get(parsed, [\"source\"]) ?? \"\"\n.parameters_json = get(parsed, [\"json_payload\"]) ?? \"\"\n\nif exists(.parameters_json) {\n    .structured_data = parse_json!(.parameters_json)  # Removed `?? {}` since parse_json!() never fails\n    . = merge!(., .structured_data)\n}\n\ndel(.parameters_json)\ndel(.message)\ndel(.structured_data)\n\n\nExpected Output: I expect the parsed output to be structured as follows:\n\n{ \"severity\": \"ERR (!!)\", \"source\": \"GOTREQ-CreateCharge\", \"header\": { \"timestamp\": \"2025-01-24T09:16:11Z\" }, \"body\": { \"orderid\": \"1234567890\", \"customername\": \"JOHN DOE\", \"orderstatus\": \"2\", \"orderstatusdesc\": \"Pending Inventory\", \"email\": \"example@email.com\", \"callbackinforar\": \"0\", \"callbackinforoms\": \"1\", \"faid\": \"9876543210\", \"baid\": \"123456789\", \"dealercode\": \"A1234.56789\", \"submissiondate\": \"20250124171404\", \"paymentItems\": [ { \"serviceid\": \"60123456789\", \"price\": \"0.0\", \"articleid\": \"100016933001\", \"uomid\": \"2AD\", \"discountid\": \"\", \"discountamount\": \"\", \"prepostind\": \"POSTPAID\", \"accountcatdesc\": \"C\", \"marketcodedesc\": \"H\", \"itemno\": \"3491715875\", \"itemtaxgroupid\": \"0\", \"taxamount\": \"0\", \"taxpercentage\": \"0\", \"billinginvoiceno\": \"\", \"custSegment\": null }, { \"serviceid\": \"60123456789\", \"price\": \"0.0\", \"articleid\": \"80000826\", \"uomid\": \"EA\", \"discountid\": \"\", \"discountamount\": \"\", \"prepostind\": \"POSTPAID\", \"accountcatdesc\": \"C\", \"marketcodedesc\": \"H\", \"itemno\": \"3491715874\", \"itemtaxgroupid\": \"0\", \"taxamount\": \"0\", \"taxpercentage\": \"0\", \"billinginvoiceno\": \"\", \"custSegment\": null } ] } } \n\nIssue Encountered: The configuration works in the VRL Playground but fails when applied in Vector.toml for actual log processing.\nThe actual logs contain multiple entries, making it hard to extract and structure the data properly.\nBelow is an example of how the logs appear in the system:\n\n[ { \"_index\": \"createcharge_log_info-2025.01.24\", \"_type\": \"createcharge_log_info_type\", \"_id\": \"createcharge_log_info-2025.01.24+2+1234567\", \"_score\": 2.0, \"_source\": { \"environment\": \"Production\", \"messagesource\": \"CreateCharge\", \"logdatetime\": \"2025-01-24T09:16:11.9655377Z\", \"message\": \"Log Message\", \"parameters\": \"ERR (!!): Src:GOTREQ-CreateCharge Msg:{...} Ref:\" } }, { \"_index\": \"createcharge_log_info-2025.01.24\", \"_type\": \"createcharge_log_info_type\", \"_id\": \"createcharge_log_info-2025.01.24+0+9876543\", \"_score\": 2.0, \"_source\": { \"environment\": \"Production\", \"messagesource\": \"CreateCharge\", \"logdatetime\": \"2025-01-24T09:21:20.379127Z\", \"message\": \"Log Message\", \"parameters\": \"ERR (!!): Src:ProcessingService>>ProcessHandler Msg:Start proceed create charge handle for order: 9876543210 Ref:\" } } ]\n\nWhat I Need Help With Handling Multi-Line Logs:\n\na. How can I modify my Vector.toml config to correctly process multi-line logs?\nThe logs appear as nested JSON inside the parameters field, making it tricky to extract and structure properly.\nEfficient Parsing for Multiple Entries:\nb. How can I ensure that each log entry is processed individually without losing its structure?\nBest Approach for Structured Fields:\nc. Are there any alternative methods or regex optimizations that could improve field extraction in Vector?\nAny help or insights would be greatly appreciated! Thanks in advance.",
        "url": "https://github.com/vectordotdev/vector/discussions/22495",
        "createdAt": "2025-02-24T01:18:22Z",
        "updatedAt": "2025-02-28T21:27:19Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Giampearo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22498,
        "title": "Vector crashes on k8s if the sink goes down",
        "bodyText": "I'm in a bit of a pickle. I'm running vector as an aggregator on k8s, configured to consume data from a http server source and push to an Elasticsearch sink.\nAfter running a few tests, it looks like if the sink goes down, but data is still flowing in, Vector pods will eventually crash and show the following error:\nThe container last terminated with exit code 137 because of OOMKilled.\n\n\nI've found the following related issues:\n\n#11942\n#17942\n#18578 - similar; points to a bug in vector that was resolved, but for a different sink\n#19815 - sounds similar to what we're experiencing\n\nBut I'm a bit confused about it. Mainly because as soon as the sink goes down, the sources will start seeing non-200 http status codes, which to me feels like the the buffering mechanism does not work or it's not intended to work in this case. If it would, I should see 200 since the request made it through to vector and should only start returning non-200 after the buffer is full and can no longer take anymore data.\nI'm just looking for some clarification on what exactly is the expected behaviour:\n\nDoes vector buffer events send to the http endpoint?\nIf requests return non-200, does this mean that the events were still buffered? The non-200 just means that we didn't get an ack from vector/sink that the events made it through?\nIs there some documentation on the status codes returned from vector when using the http source? At first, it looked like the status codes are the same as Elasticsearch returned, but that cannot be true since there may be multiple sinks returning different status codes\n\nI use the vector helm chart with the following config:\n\nConfig\nvector:\n  role: \"Aggregator\"\n\n  image:\n    repository: timberio/vector\n    pullPolicy: IfNotPresent\n    tag: \"0.42.0-alpine\"\n\n  replicas: 2\n\n  secrets:\n    generic: {}\n\n  env:\n    - name: ES_USERNAME\n      value: elastic\n    - name: ES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: elastic-user\n          key: elastic\n\n  resources: {}\n\n  persistence:\n    enabled: true\n\n  # NOTE: When we set this, we need to configure a bunch of other stuff (e.g. HAProxy, etc.)\n  # https://github.com/vectordotdev/helm-charts/blob/develop/charts/vector/templates/configmap.yaml\n  customConfig:\n    data_dir: /vector-data-dir\n    api:\n      enabled: true\n      address: 127.0.0.1:8686\n      playground: false\n    acknowledgements:\n      enabled: true\n    # https://vector.dev/blog/log-namespacing/\n    schema:\n      log_namespace: true\n    sources:\n      # https://vector.dev/docs/reference/configuration/sources/internal_metrics/\n      internal_metrics:\n        type: internal_metrics\n      # https://vector.dev/docs/reference/configuration/sources/host_metrics/\n      host_metrics:\n        type: host_metrics\n      # https://vector.dev/docs/reference/configuration/sources/vector/\n      vector:\n        address: 0.0.0.0:6000\n        type: vector\n        version: \"2\"\n      vector_http:\n        type: http_server\n        address: 0.0.0.0:8888\n        encoding: ndjson\n        method: POST\n        response_code: 200\n    transforms:\n      logs:\n        type: filter\n        inputs:\n          - vector_http\n        condition: '.kind == \"Log\"'\n      remapped_logs:\n        type: remap\n        inputs:\n          - logs\n        source: |-\n          .@timestamp = del(.timestamp)\n          .metadata = {}\n          .data_stream = {}\n          .data_stream.type = \"logs\"\n          .data_stream.dataset = \"blackhole\"\n          .data_stream.namespace = \"device\"\n      resource_usage:\n        type: filter\n        inputs:\n          - vector_http\n        condition: '.kind == \"ResourceUsage\"'\n      remapped_resource_usage:\n        type: remap\n        inputs:\n          - resource_usage\n        source: |-\n          .@timestamp = del(.timestamp)\n          .metadata = {}\n          .data_stream = {}\n          .data_stream.type = \"metrics\"\n          .data_stream.dataset = \"resource_usage\"\n          .data_stream.namespace = \"device\"\n    sinks:\n      prom_exporter:\n        type: prometheus_exporter\n        inputs: [internal_metrics, host_metrics]\n        address: 0.0.0.0:9090\n      stdout:\n        type: console\n        inputs: [vector, vector_http]\n        encoding:\n          codec: json\n      es_data_stream:\n        type: elasticsearch\n        inputs: [remapped_logs, remapped_resource_usage]\n        # NOTE: We do not need to remap `timestamp` -> `@timestamp`; Vector does this for us when using `data_stream` mode.\n        # https://github.com/vectordotdev/vector/blob/d49cf33748ad82292198888687b6b50a01d8500f/src/sinks/elasticsearch/mod.rs#L73\n        # TODO: It seems that the above assumption is incorrect! Because w/o adding the `@timestamp`, we see:\n        # 2025-02-12T05:53:22.008701Z ERROR sink{component_kind=\"sink\" component_id=es_data_stream component_type=elasticsearch}:request{request_id=2769}: vector::sinks::util::retries: Not retriable; dropping the request. reason=\"error type: document_parsing_exception, reason: [1:381] failed to parse: data stream timestamp field [@timestamp] is missing\" internal_log_rate_limit=true\n        mode: data_stream\n        data_stream:\n          auto_routing: true\n        auth:\n          user: \"${ES_USERNAME}\"\n          password: \"${ES_PASSWORD}\"\n          strategy: \"basic\"\n        endpoints:\n          - https://es-endpoint.svc.cluster.local:9200\n        tls:\n          verify_certificate: false\n\n  service:\n    enabled: true\n\n    # https://github.com/vectordotdev/helm-charts/blob/develop/charts/vector/templates/service.yaml\n    # https://github.com/vectordotdev/helm-charts/blob/develop/charts/vector/templates/service-headless.yaml\n    ports:\n      - name: vector\n        port: 6000\n        protocol: TCP\n      - name: vector-http\n        port: 8888\n        protocol: TCP\n      - name: prom-exporter\n        port: 9090\n        protocol: TCP\n\n  haproxy:\n    enabled: true\n\n    image:\n      repository: haproxytech/haproxy-alpine\n      pullPolicy: IfNotPresent\n      tag: \"2.6.12\"\n\n    replicas: 1\n\n    # https://github.com/vectordotdev/helm-charts/blob/develop/charts/vector/templates/haproxy/deployment.yaml\n    containerPorts:\n      - name: vector\n        containerPort: 6000\n        protocol: TCP\n      - name: vector-http\n        containerPort: 8888\n        protocol: TCP\n      - name: prom-exporter\n        containerPort: 9090\n        protocol: TCP\n\n    # https://github.com/vectordotdev/helm-charts/blob/develop/charts/vector/templates/haproxy/service.yaml\n    service:\n      annotations:\n      ports:\n        - name: vector\n          port: 6000\n          protocol: TCP\n        - name: vector-http\n          port: 8888\n          protocol: TCP\n        - name: prom-exporter\n          port: 9090\n          protocol: TCP\n\n    # https://github.com/vectordotdev/helm-charts/blob/develop/charts/vector/templates/haproxy/configmap.yaml\n    customConfig: |\n      global\n        log stdout format raw local0 debug\n        maxconn 4096\n        stats socket /tmp/haproxy\n        hard-stop-after {{ .Values.haproxy.terminationGracePeriodSeconds }}s\n\n      defaults\n        log     global\n        option  dontlognull\n        retries 3\n        option  redispatch\n        option  allbackups\n        timeout client 15s\n        timeout server 15s\n        timeout connect 15s\n\n      resolvers coredns\n        nameserver dns1 kube-dns.kube-system.svc.cluster.local:53\n        resolve_retries 3\n        timeout resolve 2s\n        timeout retry 1s\n        accepted_payload_size 8192\n        hold valid 10s\n        hold obsolete 60s\n\n      frontend stats\n        mode http\n        bind :::1024\n        option httplog\n        http-request use-service prometheus-exporter if { path /metrics }\n\n      frontend vector\n        mode http\n        bind :::6000 proto h2\n        option httplog\n        default_backend vector\n\n      frontend vector_http\n        mode http\n        bind :::8888 proto h1\n        option httplog\n        default_backend vector_http\n\n      backend vector\n        mode http\n        balance roundrobin\n        option tcp-check\n        server-template srv 10 _vector._tcp.{{ include \"vector.fullname\" $ }}-headless.{{ $.Release.Namespace }}.svc.cluster.local resolvers coredns proto h2 check\n\n      backend vector_http\n        mode http\n        balance roundrobin\n        option tcp-check\n        server-template srv 10 _vector-http._tcp.{{ include \"vector.fullname\" $ }}-headless.{{ $.Release.Namespace }}.svc.cluster.local resolvers coredns proto h1 check\n\n  # Direct trafic to Traefik's vector entrypoint\n  ingress:\n    enabled: true\n    annotations:\n      traefik.ingress.kubernetes.io/router.entrypoints: vectorhttp\n    hosts:\n      - paths:\n          - path: /\n            pathType: Prefix\n            port:\n              name: vector-http\n\nNOTE: On the source side, we have agents pushing to the http endpoint. As soon as a non-200 occurs, they will stop sending new events and keep retrying (at the first failure) until it receives 200, then it continues pushing. There's also a buffering mechanism on the agent side.",
        "url": "https://github.com/vectordotdev/vector/discussions/22498",
        "createdAt": "2025-02-24T05:49:18Z",
        "updatedAt": "2025-02-28T06:14:25Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "rolandjitsu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22515,
        "title": "View debugging log output during test run",
        "bodyText": "Hi, wanted to know if there is any way to see the output of log() statements during test runs?",
        "url": "https://github.com/vectordotdev/vector/discussions/22515",
        "createdAt": "2025-02-26T00:38:43Z",
        "updatedAt": "2025-02-26T22:11:01Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tbenade"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22419,
        "title": "How do I add Kubernetes metadata to Prometheus metrics scraped by Vector agent ?",
        "bodyText": "Hi Team,\nI am using Vector to scrape Prometheus metrics from my Kubernetes pods. Each pod exposes its metrics via an HTTP endpoint at /metrics. While Vector can enrich logs with Kubernetes metadata using the kubernetes_logs source, I couldn\u2019t find a similar built-in functionality for attaching Kubernetes metadata (such as pod_name, namespace, node.\"labels\", etc.) to Prometheus metrics.\nSince this feature does not exist out of the box, I am looking for possible ways to achieve this. Specifically:\n\nIs there a way to inject Kubernetes metadata into metrics before they are sent to Prometheus remote write?\nAre there any known workarounds or best practices for this use case?\n\nAny insights or recommendations would be greatly appreciated!\nThanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/22419",
        "createdAt": "2025-02-11T03:06:43Z",
        "updatedAt": "2025-03-27T18:06:44Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "yeshwanth1312"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22509,
        "title": "glibc support",
        "bodyText": "hello,everyone!I am a student from cn.\nI am doing a course job. But I ran into a problem when I used Vector to collect logs from my virtual machines and lab machines.\nThe problem is\n\"(base) [root@247 bin]# cat/var/log/logs/vector/vector.log\n/root/vector/bin/vector:/11664/11bc.so.6: version\"GLlBC 2.18\" not found (required by/root/vector/bin/vector) (base)\"\nIt seems to mean that vector requires my virtual machine and lab machine to have a GLIBC version of 2.18, but my machine can only support 2.17 and can't be changed, is there any version of Vector here that supports GLIBC2.17?",
        "url": "https://github.com/vectordotdev/vector/discussions/22509",
        "createdAt": "2025-02-25T06:02:14Z",
        "updatedAt": "2025-02-25T15:12:03Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "LeDyChen"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20689,
        "title": "Any example on using Vector with Sentry?",
        "bodyText": "I am using Sentry with a self-hosted instance. I couldn't find examples of how to integrate it as a Sink. In the issues, I found users describing a situation similar to mine, for example #575. Does anyone have any suggestions on this? Thank you all!",
        "url": "https://github.com/vectordotdev/vector/discussions/20689",
        "createdAt": "2024-06-18T11:09:48Z",
        "updatedAt": "2025-02-24T16:14:54Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "LucianoVandi"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22382,
        "title": "How to use vector as a lib to push to another vector instance",
        "bodyText": "I was wondering if it is possible to use vector as a lib within my own rust program to push data to another vector endpoint.\nTypically, I would just use vector as an agent and have a vector 2 vector setup, but due to some restrictions, I cannot run additional services/programs.\nRight now, I've enabled the http_server source and I simple POST the data, but it would be great to use the vector source and port instead of opening up an additional port:\n[my program -> vector lib] -> vector aggregator :6000",
        "url": "https://github.com/vectordotdev/vector/discussions/22382",
        "createdAt": "2025-02-07T05:20:19Z",
        "updatedAt": "2025-02-24T15:51:08Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "rolandjitsu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22488,
        "title": "Implement additional tags or a separate sink for process non-numerical data.",
        "bodyText": "My team and I want to start collecting additional information around the processes for better troubleshooting. Info includes:\n\nuser\nSpecific environment variables\ncwd\nparent PID\n\nWe can accomplish this via the sysinfo library similar to the current host_metrics process. The problem we have (especially for the environment variables) is that the data is bulky. We are unsure if it would be advisable to add these as tags or make a new source specific for the non-numerical data around process.\nAny guidance would be appreciated.",
        "url": "https://github.com/vectordotdev/vector/discussions/22488",
        "createdAt": "2025-02-20T23:35:44Z",
        "updatedAt": "2025-02-21T21:31:09Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "zapdos26"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22492,
        "title": "Vector to delete log files only",
        "bodyText": "HI I am trying to use vector client to just delete files after two days and  the files wont be deleted at all. Could someone help and see if this config is good or any changes need to be added/updated. I am using windows server and vector is setup to run as a service. The log files are picked up but they wont be deleted. I know this is a use case where vector is not what you guys wanted but trying to see if this is possible without shipping the logs anywhere\n[api]\nenabled = true\naddress = \"127.0.0.1:8686\"\n\n[sources.myserver]\ntype = \"file\"\nfingerprint.strategy = \"device_and_inode\"\ninclude = [ 'D:\\logs\\**\\*.log*']\nignore_not_found = true\nmax_line_bytes = 1\nread_from = \"end\"\nignore_checkpoints = false\nremove_after_secs = 172800\n\n[sources.my_internal_logs]\ntype = \"internal_logs\"\n\n[sinks.my_log_file]\ntype = \"file\"\ninputs = [\"my_internal_logs\"]\npath = \"D:\\\\logs\\\\vector_internal.log\"\nencoding.codec = \"json\"",
        "url": "https://github.com/vectordotdev/vector/discussions/22492",
        "createdAt": "2025-02-21T19:13:42Z",
        "updatedAt": "2025-02-21T20:36:27Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "makkekal-flutterint"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16101,
        "title": "Elasticsearch Data stream naming",
        "bodyText": "Hi,\nwe are currently evaluating vector as replacement for Fluentd, but I'm failing to get the data stream naming equal to the naming in Fluentd.\nBased on our old Fluentd config, the name of the data stream must look like \"applicationname.environment\". Currently with my vector-configuration I always get a name like \"logs-applicationname.environment-default\". My understanding is that \"logs\" and default are defined by data_stream.type & data_stream.namespace. We are using Elasicsearch 8.3.\n[sinks.elk-shared-dev]\ntype = \"elasticsearch\"\ninputs = [ \"parseMessageJson\" ]\nendpoints = [\"https://elasticsearch:9200\"]\nmode = \"data_stream\"\napi_version = \"v8\"\ncompression = \"none\"\n[sinks.elk-shared-dev.auth]\nuser = \"${ELASTICSEARCH_USERNAME}\"\npassword = \"${ELASTICSEARCH_PASSWORD}\"\nstrategy = \"basic\"\n[sinks.elk-shared-dev.bulk]\naction = \"create\"\n[sinks.elk-shared-dev.data_stream]\ndataset = \"applicationname.environment\"\nIs there an option to suppress the creation of data_stream.type & data_stream.namespace or why is this enforced?\nMany Thanks for you support",
        "url": "https://github.com/vectordotdev/vector/discussions/16101",
        "createdAt": "2023-01-24T16:02:40Z",
        "updatedAt": "2025-02-21T15:55:00Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "minimax75"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 3
    },
    {
        "number": 22444,
        "title": "Does `datadog_events` sink have a Privatelink Service?",
        "bodyText": "Hi team,\nSimilar to how you've helped us set up our connections to Datadog over AWS Privatelink in #21325 and #21867, I was wondering if the datadog_events sink has an appropriate Privatelink Service? It doesn't seem like the Datadog documentation on using AWS Privatelink has an appropriate service. Any insight would be appreciated.\nThank you",
        "url": "https://github.com/vectordotdev/vector/discussions/22444",
        "createdAt": "2025-02-14T20:18:24Z",
        "updatedAt": "2025-02-19T18:22:41Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "nzxwang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22438,
        "title": "Request Tag Cardinality Limit with tags exclusion option OR Best way to handle unexpected high cardinality tag",
        "bodyText": "Hi Vector maintainers and contributors,\nImpressed of the solution so far as I'm exploring to use Vector in our system mostly for processing datadog agents metrics coming in. I'm finding a best way to solve the problem of unexpectedly high cardinality tag happened with our apps custom metrics.\nI found that the Tag Cardinality Limit feature is exactly what we would want or get inspired to implement in our system. Though checking with you whether is there any plan to have \"Tag Exclusion\" option for this feature? The reason I ask is because there are some of the required tags that we don't want this limit feature to apply, like with dd-agents, the host tag, and some other tags that are important in our system. So I'm sitting in between whether accepting raising higher limits to avoid important tag values being dropped and also letting unexpected high volume tag a chance to be increased. If having an extra option to exclude a certain tags to be in this limit, it would be super helpful. May I know if this feature is being considered or any plan about this so far?\nIf there's no plan then do you think there's any best workaround that I can check out based on the current available options?\nThank you so much for taking the time.",
        "url": "https://github.com/vectordotdev/vector/discussions/22438",
        "createdAt": "2025-02-14T07:29:05Z",
        "updatedAt": "2025-02-19T10:02:35Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "techministrator"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 22231,
        "title": "How to use OpenTelemetry sink ?",
        "bodyText": "Notice that Vector already has OpenTelemetry Sink and try the following config:\nsources:\n  otel_traces:\n    type: \"opentelemetry\"\n    http:\n      address: \"0.0.0.0:4318\"\n    grpc:\n      address: \"0.0.0.0:4317\"\n\nsinks:\n  store_traces:\n    type: opentelemetry\n    inputs:\n      - otel_traces.traces\n    # My OpenTelemetry traces the backend.\n    # It will accept the protobuf encode traces data through HTTP.\n    protocol:\n      type: \"http\"\n      uri: \"http://localhost:4000/v1/otlp/v1/traces\"\n      method: post\n      encoding:\n        codec: \"native\"\nUnfortunately, my trace backend can't parse the data. It seems the data have been transformed to another format in Vector.\nMy vector version is:\n./vector --version       \nvector 0.44.0 (aarch64-apple-darwin 3cdc7c3 2025-01-13 21:26:04.735691656)\n\nAlso, I can't find any documents that describe the example of opentelemetry. How can I use OpenTelemetry sink?",
        "url": "https://github.com/vectordotdev/vector/discussions/22231",
        "createdAt": "2025-01-17T07:47:32Z",
        "updatedAt": "2025-03-26T11:16:44Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "zyy17"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 22430,
        "title": "Vector using S3 Source: Failed to parse SQS queue",
        "bodyText": "Hi, I'm deploying a vector on an ec2 instance. I have a standard SQS Queue setup to receive S3 bucket notifications. I confirmed the queue can receive messages whenever an item is created in S3. When running vector using an s3 source, I'm receiving the following error message:\naws_sqs::s3: Failed to process SQS message. message_id=62fa4f96-c072-4f0d-8245-e15f147d3cce error=Could not parse SQS message with id 12da4f96-c072-4f0d-1234-e15f147d3cce as S3 notification: data did not match any variant of untagged enum SqsEvent error_code=\"failed_processing_sqs_message\" error_type=\"parser_failed\" stage=\"processing\" internal_log_rate_limit=true\n\nThis is what an incoming message looks like:\n{\"EventId\": \"ac884700-100e-50d1-a42a-c7b93ef41242\", \"EventVersion\": \"1.1\", \"EventSource\": \"aws:s3\", \"EventType\": \"Storage\", \"EventName\": \"ObjectAvailable\", \"EventSubjectId\": \"offloads/data/2025/02/12/abcoffload.tgz\", \"EventRegion\": \"us-west-2\", \"EventTime\": \"2025-02-12T02:12:06.936Z\", \"Storage\": {\"Bucket\": \"my-s3-bucket\", \"Key\": \"offloads/data/2025/02/12/abcoffload.tgz\", \"Size\": 268293}}\n\nThis is my vector configuration:\nsources:  \n  s3_source:    \n    type: aws_s3    \n    compression: gzip\n    sqs:\n      queue_url: https://sqs.us-west-2.amazonaws.com/account-id/queue-name\n    region: us-west-2\n    decoding:\n      codec:\n        json\n\nsinks:\n  hostMetricsFile:\n    type: file\n    inputs:\n      - s3_source\n    path: file-%Y-%m-%d.log\n    encoding:\n      codec: json\n\nI'm not too familiar with SQS but is the issue with my message or is there another underlying issue?",
        "url": "https://github.com/vectordotdev/vector/discussions/22430",
        "createdAt": "2025-02-12T20:04:59Z",
        "updatedAt": "2025-02-18T21:46:10Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "generate-me12"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22461,
        "title": "spent whole day try to figure out how to parse this log",
        "bodyText": "good day\nim new here. i got this log example of ha proxy\nFeb  3 17:00:17 localhost hapee-lb[2027037]: {\"status\":\"200\",\"bytes_read\":\"381\",\"bytes_uploaded\":\"418\",\"hostname\":\"33-012145\",\"method\":\"POST\",\"request_uri\":\"/ssw/products\",\"handshake_time\":\"0\",\"request_idle_time\":\"314\",\"request_active_time\":\"1\",\"request_time\":\"0\",\"response_time\":\"1\",\"timestamp\":\"1738591219\",\"client_ip\":\"10.66.77.88\",\"client_port\":\"190\",\"real_ip\":\"10.32.69.56\",\"frontend_port\":\"443\",\"http_request\":\"POST /ssw/products HTTP/1.1\",\"ssl_ciphers\":\"TLS_AES_256_GCM_SHA384\",\"ssl_version\":\"TLSv1.3\",\"date_time\":\"03/Feb/2025:17:00:16.778\",\"http_host\":\"aos\",\"http_referer\":\"-\",\"http_user_agent\":\"Apache-HttpClient/5.3.1 (Java/21.0.2)\",\"http_req_rate\":\"0\",\"conn_rate\":\"0\",\"verdict\":\"-\",\"X-User-Type\":\"-\",\"X-Retry-Data\":\"-\",\"X-Application-Platform\":\"-\",\"X-Application-Version\":\"-\",\"X-User-ID\":\"-\",\"X-SP-GEOIP\":\"-\"} \n\nand i spent whole day try to figure how to parse this json\nmay be someone help?\ni used this but it not work. i dont under understand\ntransforms:\n  decode_json:\n    type: remap\n    inputs:\n      - file\n    source: |\n      if is_string(.message) {\n        . = merge(., parse_json(.message) ?? {})\n      }",
        "url": "https://github.com/vectordotdev/vector/discussions/22461",
        "createdAt": "2025-02-17T16:31:25Z",
        "updatedAt": "2025-02-18T17:01:15Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "homelab8330"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22311,
        "title": "Components with multiple roles (component that is both a sink and a source)",
        "bodyText": "Recently, my PR has been merged (#21348), which added a new type of enrichment table, which can also act as a sink. That has caused some changes in the topology, because enrichment tables were not meant to act as sinks and were supposed to be accessible only from VRL remap transform.\nWe have had a brief discussion on how to handle this in other ways in the PR (#21348 (comment)) - more granular approach to building the topology, giving each component parts that might make it a sink, source, transform, or other things that support each of these roles, turning all of them into generic nodes, which can then be combined into a topology in more ways, depending on what each of the components support.\nRight now, I am maintaining a branch which also enables this memory enrichment table to act as a source. You can see the changes here: master...esensar:vector:feat/vrl-cache-as-source (it is not very clean, since I assumed that something like this would not be accepted as a contribution, at least not in the near future, but I might be wrong). This enables some interesting new ways to use this table and I assume it might make sense for many more components.\nMy question is, does this sound like a direction it makes sense for Vector to move in, or does it introduce too much complexity? Does the current state of that memory enrichment table as a source branch make any sense? Would it be acceptable to merge with some changes? If not, do you have a suggestion on how to provide similar functionality in an acceptable way (dumping data from the cache)?",
        "url": "https://github.com/vectordotdev/vector/discussions/22311",
        "createdAt": "2025-01-28T09:58:45Z",
        "updatedAt": "2025-02-18T15:10:48Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "esensar"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22276,
        "title": "Issue with Loki (version < 3) Sink After Upgrading to Vector v0.43.1",
        "bodyText": "Hi Vector team,\nAfter upgrading from Vector v0.37.x to v0.43.1, I encountered an issue where logs are not being sent to Loki (v2.8.4) when the compression option is set to \"gzip\" or \"none\". However, it works fine with the latest Loki version.\nHere is debug output from Vector:\n2025-01-22T12:13:25.922616Z DEBUG sink{component_kind=\"sink\" component_id=out component_type=loki}:request{request_id=1}:http: hyper::client::pool: pooling idle connection for (\"http\", localhost:3100)\n2025-01-22T12:13:25.922662Z DEBUG sink{component_kind=\"sink\" component_id=out component_type=loki}:request{request_id=1}:http: vector::internal_events::http_client: HTTP response. status=400 Bad Request version=HTTP/1.1 headers={\"content-type\": \"text/plain; charset=utf-8\", \"x-content-type-options\": \"nosniff\", \"date\": \"Wed, 22 Jan 2025 12:13:25 GMT\", \"content-length\": \"245\"} body=[245 bytes]\n2025-01-22T12:13:25.922848Z ERROR sink{component_kind=\"sink\" component_id=out component_type=loki}:request{request_id=1}: vector::sinks::util::retries: Non-retriable error; dropping the request. error=Server responded with an error: 400 Bad Request internal_log_rate_limit=true\n2025-01-22T12:13:25.922935Z ERROR sink{component_kind=\"sink\" component_id=out component_type=loki}:request{request_id=1}: vector_common::internal_event::service: Service call failed. No retries or retries exhausted. error=Some(ServerError { code: 400 }) request_id=1 error_type=\"request_failed\" stage=\"sending\" internal_log_rate_limit=true\n2025-01-22T12:13:25.922990Z ERROR sink{component_kind=\"sink\" component_id=out component_type=loki}:request{request_id=1}: vector_common::internal_event::component_events_dropped: Events dropped intentional=false count=1 reason=\"Service call failed. No retries or retries exhausted.\" internal_log_rate_limit=true\n\nSteps to reproduce:\n\nRun Loki:\ndocker run -d --name=loki -p 3100:3100 grafana/loki:2.8.4\nUse the following vector.toml\n\ndata_dir = \"./data_dir/\"\n\n[sources.source0]\ntype = \"file\"\ninclude = [ \"logs.log\" ]\nread_from = \"beginning\"\n\n[sinks.out]\ntype = \"loki\"\ninputs = [ \"source0\" ]\nendpoint = \"http://localhost:3100\"\nout_of_order_action = \"accept\"\ncompression = \"none\"\n\n  [sinks.out.encoding]\n  codec = \"json\"\n\n  [sinks.out.labels]\n  host = \"hostname123\"\n\n\nWrite logs to logs.log and observe the behavior.\n\nObservations:\n\nThe same configuration worked in Vector v0.37.x.\nThe issue only occurs when using \"gzip\" or \"none\" as the compression option. Using \"snappy\" resolves the issue.\nUpgrading to the latest Loki version resolves the issue, but this might not be suitable for all users.\n\nQuestions and Request for Assistance\n\nIs this a known issue with changes in the Loki sink or the HTTP client in Vector v0.43.1?\nAre there any workarounds (other than upgrading Loki) to ensure compatibility with Loki v2.8.4?\nCould this be related to the way Vector formats or sends the payload with specific compression settings?\n\nThanks for your help!",
        "url": "https://github.com/vectordotdev/vector/discussions/22276",
        "createdAt": "2025-01-22T12:35:25Z",
        "updatedAt": "2025-02-19T20:17:45Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "vparfonov"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22457,
        "title": "How to exclude writing of index_name variable data to file ?",
        "bodyText": "I have the following code I want to exclude data from index_name variable in output file but creating file requires index_name variable value as file name how can I do this ?\n`sources:\nswarm:\ntype: docker_logs\nauto_partial_merge: true\ntransforms:\nfilter_swarm:\ntype: filter\ninputs:\n- \"swarm\"\ncondition:\ntype: vrl\nsource: match(to_string!(.label.\"com.docker.stack.namespace\"), r'^(dev-${PROJECT}|uat-${PROJECT}|prod-${PROJECT}|preprod-${PROJECT})')\nremap_swarm:\ntype: remap\ninputs:\n- \"filter_swarm\"\nsource: |\n.stack_namespace = .label.\"com.docker.stack.namespace\"\nif match(to_string!(.stack_namespace), r'^(dev-${PROJECT}|uat-${PROJECT}|prod-${PROJECT}|preprod-${PROJECT})') {\nindex = split!(.stack_namespace,\"-\")\nproject = slice!(\"${PROJECT}\", start: 0, end: 3)\nenv = index[0]\n.index_name = project+\"-\"+env\n}\ndel(.label)\ndel(.stream)\ndel(.index_name)\nsinks:\nfile:\ninputs:\n- \"remap_swarm\"\ntype: file\npath: \"/out/{{ .index_name }}\"\nencoding:\ncodec: \"json\"\n`\nRemoving index_name in transform Result in error ERROR sink{component_kind=\"sink\" component_id=file component_type=file}: vector::internal_events::template: Failed to render template for \"path\". error=Missing fields on event: [\".index_name\"] error_type=\"template_failed\" stage=\"processing\" internal_log_rate_limit=true",
        "url": "https://github.com/vectordotdev/vector/discussions/22457",
        "createdAt": "2025-02-17T11:51:23Z",
        "updatedAt": "2025-02-17T11:51:55Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Ilyin-V-V"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22422,
        "title": "AWS Managed Opensearch Sink IAM Auth from ECS trying to auth to dashboards uri",
        "bodyText": "We have an aws-managed Opensearch cluster which is setup to auth via IAM and SSO (Okta) for humans.  When setting up the elasticsearch sink, it receives an auth error from opensearch that makes it seem like it is trying to auth to the dashboards endpoint.\nSink config:\nsinks:\n  es_cluster:\n    inputs:\n      - alb_xform\n    type: elasticsearch\n    endpoints:\n      - https://SUB_OS_URL:443\n    bulk:\n      index: \"SUB_INDEX-%Y-%m-%d\" # daily indices\n    aws:\n      region: SUB_REGION\n    api_version: auto\n    opensearch_service_type: managed\n    healthcheck:\n      enabled: false\n\nError:\n2025-02-11T17:24:44.166755Z ERROR sink{component_kind=\"sink\" component_id=es_cluster component_type=elasticsearch}:request{request_id=2}: vector::sinks::elasticsearch::service: Response contained errors. error_code=\"http_response_401\" response=Response { status: 401, version: HTTP/1.1, headers: {\"date\": \"Tue, 11 Feb 2025 17:24:44 GMT\", \"content-type\": \"text/plain; charset=UTF-8\", \"content-length\": \"0\", \"connection\": \"keep-alive\", \"access-control-allow-origin\": \"*\", \"www-authenticate\": \"X-Security-IdP realm=\\\"OpenSearch Security\\\" location=\\\"https://OKTA_URL\\\" requestId=\\\"REDACTED\\\"\"}, body: b\"\" }\n\nThe Endpoint does not have the /_dashboards uri appended to it, and is used successfully by other IAM-based tools to interface with opensearch.",
        "url": "https://github.com/vectordotdev/vector/discussions/22422",
        "createdAt": "2025-02-11T17:40:49Z",
        "updatedAt": "2025-02-14T21:38:55Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tfmm"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 22439,
        "title": "High CPU Usage with Vector's Route Component in Large K8s Cluster",
        "bodyText": "I am using Vector in a large Kubernetes cluster where a single node can run up to 400 pods, generating up to 10k logs per second. In my Vector configuration, I am utilizing the transform component with the route function to create up to 400 different streams (split by namespace). Each of these streams is then passed through the throttle component with individual parameters before being merged back into a single stream.\nWith this configuration, the Vector pod can consume 5-6 CPU cores, which seems excessively high. I am wondering if the route operation is inherently resource-intensive or if there are any optimization strategies I can employ to reduce CPU usage.\nQuestions:\nIs the route function known to be CPU-intensive, especially when handling a large number of streams?\nAre there any best practices or alternative configurations that could help optimize CPU usage in this scenario?\nCould there be other factors in my setup contributing to high CPU usage that I should investigate?\nAdditional Information:\nVector version: 0.44.0\nKubernetes version: 1.30.1\nThank you for your assistance!",
        "url": "https://github.com/vectordotdev/vector/discussions/22439",
        "createdAt": "2025-02-14T13:11:02Z",
        "updatedAt": "2025-02-14T13:13:06Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "P0lskay"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22436,
        "title": "How to guarantee events with same `trace_id` are sampled together when vector is distributed?",
        "bodyText": "Hi team,\nWe would like to configure our distributed vector pipeline to batch all logs with the same trace id before sampling at some sample rate. The sample component's key_field does exactly this but since sample is a completely stateless component, I assume that logs with the same trace id flowing through different vector pipeline hosts are not guaranteed to be sampled together. Furthermore, in an ideal world, we would like to also batch traces too. I'd appreciate any insight on how we might achieve this.\nThank you",
        "url": "https://github.com/vectordotdev/vector/discussions/22436",
        "createdAt": "2025-02-13T23:09:24Z",
        "updatedAt": "2025-02-13T23:09:25Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "nzxwang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22432,
        "title": "How do I get the value of a variable from transforms to sinks",
        "bodyText": "How do I get the value of a variable stack_namespace from transforms to skins ?\nsources:\n  swarm:\n    type: docker_logs\n    auto_partial_merge: true\n    include_labels: [\"com.docker.stack.namespace=dev\"]\n\ntransforms:\n  remap_swarm:\n    type: remap\n    inputs:\n      - \"swarm\"\n    source: |\n      .stack_namespace = .label.\"com.docker.stack.namespace\"\n      del(.label)\n      del(.stream)\n\nsinks:\n  file:\n    inputs:\n      - \"remap_swarm\"\n    type: file\n    path: /out/{{ transforms.remap_swarm.source.stack_namespace }}\n    encoding:\n      codec: \"json\"",
        "url": "https://github.com/vectordotdev/vector/discussions/22432",
        "createdAt": "2025-02-13T09:55:54Z",
        "updatedAt": "2025-02-13T17:02:28Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Ilyin-V-V"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22367,
        "title": "Shared state across instances",
        "bodyText": "\ud83d\udc4b Migrating from Discord here:\nhttps://discord.com/channels/742820443487993987/865294205655973928/1336449652177764402\nI was wondering if the concept of shared state has ever come up & if Vector has a stance on it.\nRationale: I would love to do rate-limiting on noisy services, however I can't reliably do that as I don't know which container is going to pickup that service's logs. I was thinking it would be lovely to utilize something like redis to share ratelimiting state across instances, however I wasn't sure if that had ever been discussed (if it has happy to move there instead as well!)\nIt would be really ideal if we could make VRL calls that had the ability to utilize external services like KV stores to share state across instances. This would allow for much more impactful transforms, sampling, and rate limiting.\n[From Discord]\n\nPavlos Rontidis \u2014 Today at 4:58 PM\nIt's a challenging topic since we don't maintain any servers. We have been focusing on exclusively local components. But I can imagine designing some interfaces and allow the users to maintain their own infra. E.g. for a global quota you would need to maintain a key value store.",
        "url": "https://github.com/vectordotdev/vector/discussions/22367",
        "createdAt": "2025-02-04T22:13:41Z",
        "updatedAt": "2025-02-12T22:04:09Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "deangalvin-cb"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22425,
        "title": "2 sinks in one",
        "bodyText": "good day\ni got 2 same sinks on http (send logs to victorialog) the difference  only in uri?\ni didnt found in documentaion way to combine those two sinks in one!\nis it possible to do that ?\n    vlogs-01:\n      inputs:\n        - kafka\n      type: http\n      uri: \"http://os:9428/insert/jsonline?_stream_fields=k8s.namespace,k8s.pod_name&_msg_field=message,json.msg,log4j.message,json.err,json.message&_time_field=timestamp,ts,@timestamp,time\"\n      compression: gzip\n      encoding:\n        codec: json\n      framing:\n        method: newline_delimited\n      healthcheck:\n        enabled: false\n    vlogs-02:\n      inputs:\n        - kafka\n      type: http\n      uri: \"http://os:9428/insert/jsonline?_stream_fields=k8s.namespace,k8s.pod_name&_msg_field=message,json.msg,log4j.message,json.err,json.message&_time_field=timestamp,ts,@timestamp,time\"\n      compression: gzip\n      encoding:\n        codec: json\n      framing:\n        method: newline_delimited\n      healthcheck:\n        enabled: false",
        "url": "https://github.com/vectordotdev/vector/discussions/22425",
        "createdAt": "2025-02-12T11:38:23Z",
        "updatedAt": "2025-02-12T15:29:23Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "therus000"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22309,
        "title": "Significant metric loss when using Vector's log_to_metric vs Datadog for nginx logs",
        "bodyText": "I'm experiencing a significant discrepancy between metrics generated via Vector's log_to_metric transform and Datadog when processing the same nginx logs. The Vector pipeline consistently shows approximately 1/10th of the traffic volume compared to Datadog's measurements.\nSetup details:\n\nProcessing custom-formatted nginx logs\nVector pipeline: nginx logs \u2192 log_to_metric transform \u2192 Mimir\nMetrics viewed through Grafana\nSame time periods and filtering criteria used in both Grafana and Datadog for comparison\n\nCurrent observations:\n\nMimir distributor logs show no errors\nVector logs are mostly clean, with occasional parsing failures (few per hour) for specific cases like \"client request-body too large\" messages\nAll components appear to be functioning, just with unexpectedly low data volume\n\nConfigurations:\nMimir config: https://pastebin.com/ixUGPwNP\nVector config: https://pastebin.com/2RfCGUK8\nQuestions:\n\nWhat debugging approaches would you recommend to trace where metrics might be getting dropped?\nAre there specific Vector or Mimir metrics I should monitor to identify potential bottlenecks?\nWould logs from any other components be helpful for diagnosis?\n\nAny guidance on troubleshooting techniques or configuration issues would be greatly appreciated.",
        "url": "https://github.com/vectordotdev/vector/discussions/22309",
        "createdAt": "2025-01-27T23:35:59Z",
        "updatedAt": "2025-02-12T14:41:16Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "law"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 3
    },
    {
        "number": 22423,
        "title": "Dyanamic Routing with route or exclusive_route",
        "bodyText": "Is there any way to do dynamic routing?\nexample of what im looking for.\ntype: exclusive_route\ninputs:\n  - vector\nroute:\n  - name: \"{{ .source }}\"\n  \tcondition:\n  \t\ttype: vrl\n  \t\tsource: exists(.source)\nBackground:\nI have a \"recieving\" vector cluster sending to a backend \"transforms\" cluster with the vector sink/source.  But since the source for inputs is now just 'vector' it makes routing a little more challenging so I was hoping for a dynamic way of routing these for later inputs.",
        "url": "https://github.com/vectordotdev/vector/discussions/22423",
        "createdAt": "2025-02-11T20:33:09Z",
        "updatedAt": "2025-02-12T14:41:09Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "satellite-no"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22426,
        "title": "Vector fails to write data to S3 without any explicit error",
        "bodyText": "I am deploying a Vector instance (version 0.42.0-1.x86_64) at a client (K8S setup, writing to S3 bucket). The source is Kafka and the data undergoes simple transformation. The data is pulled as expected and transformation steps works correctly - I added sinking to console for debugging and I can see that the output is correctly delivered to console.\nHowever, when writing to S3, I encounter an issue - no error but also no output written to S3. My Vector instance does not indicate any error nor logs show it, but when running \"vector top\" I can clearly see that no output is written to S3 (Bytes Out equal to zero). Simultaneously, Events In and Events Out indicate expected number of events. I double-checked my S3 configuration, but all looks good. Furthermore, had there been any connection issue, I expected explicit error in logs - and there is none.\nThe only reason why I suspect an implicit error occurring is that I see in logs information that:\n\n> 2025-02-12T12:14:03.998036Z DEBUG sink{component_kind=\"sink\" component_id=aws_s3 component_type=aws_s3}:request{request_id=1}:invoke{service=s3 operation=PutObject sdk_invocation_id=1191016}:try_op:try_attempt: aws_smithy_runtime_api::client::interceptors::context: entering 'after deserialization' phase\n> \n> 5882025-02-12T12:14:03.998050Z DEBUG sink{component_kind=\"sink\" component_id=aws_s3 component_type=aws_s3}:request{request_id=1}:invoke{service=s3 operation=PutObject sdk_invocation_id=1191016}:try_op: aws_smithy_runtime::client::retries::strategy::standard: not retrying because we are out of attempts attempts=1 max_attempts=1\n> \n> 5892025-02-12T12:14:03.998054Z DEBUG sink{component_kind=\"sink\" component_id=aws_s3 component_type=aws_s3}:request{request_id=1}:invoke{service=s3 operation=PutObject sdk_invocation_id=1191016}:try_op: aws_smithy_runtime::client::orchestrator: a retry is either unnecessary or not possible, exiting attempt loop\n\n\nThus, this indeed indicates that the attempt was unsuccessful. However, aside from that information, there is no indication of any issues. Logs indicate only \"DEBUG\" level information, and the sinking to console returns events in expected format.\nWhat could be the problem here? For the context, I use batch set to 300 seconds and buffer set to memory and 200.000 events. Thanks so much for your help!",
        "url": "https://github.com/vectordotdev/vector/discussions/22426",
        "createdAt": "2025-02-12T12:52:35Z",
        "updatedAt": "2025-02-12T17:52:06Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ignacyklimont"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22353,
        "title": "Collect journald / systemd logs from Kubernetes nodes",
        "bodyText": "Hello,\nWe are trying to configure Kubernets nodes logs collecting using JournalD source.\nThere are almost no full/working examples around and just brief discussions in the Discord and GitHub.\n\nEnvironment - Kubernetes\nNodes - openSUSE MicroOS\njournalctl_path - /usr/bin/journalctl\njournal_directory - /var/log/journal\nDocker images - 0.44.0-distroless-libc, 0.44.0-distroless-static, 0.44.0-debian, 0.44.0-alpine\n\nvalues.yaml\n# Image\nimage:\n  repository: timberio/vector\n  tag: 0.44.0-debian\n\n# Volumes\nextraVolumes:\n  - name: log-journalctl\n    hostPath:\n      path: /var/log/journal\n\nextraVolumeMounts:\n  - name: log-journalctl\n    mountPath: /var/log/journal\n    readOnly: true\n\n# Config\ncustomConfig:\n  data_dir: /var/lib/vector\n\n  sources:\n    journald-source:\n      type: journald\n      journal_directory: /var/log/journal\n\n  transforms:\n    journald-transform-elasticsearch:\n      inputs:\n      - journald-source\n      source: |\n        .index_name = \"journald\"\n      type: remap\n\n  sinks:\n    journald-sink-elasticsearch:\n      type: elasticsearch\n      inputs:\n        - journald-transform-elasticsearch\n      encoding:\n        except_fields: [\"index_name\"]\n      endpoints:\n        - https://elastic-es-http:9200\n      tls:\n        verify_certificate: false\n        verify_hostname: false\n      api_version: auto\n      compression: gzip\n      batch:\n        max_bytes: 1000000\n        timeout_secs: 5\n      mode: bulk\n      bulk:\n        action: index\n        index: '{{ printf \"{{ index_name }}-%%Y.%%m.%%d\" }}'\nIssues\n\n\nWe need more working examples to not check different sources, trying to understand how to configure it properly and all possible drawbacks.\n\n\nA single way which was working is to use 0.44.0-debian image, without additional volumes mounting.\n\n# Volumes\nextraVolumes:\n  - name: log-pods\n    hostPath:\n      path: /var/log/pods\n  - name: log-containers\n    hostPath:\n      path: /var/log/containers\n  - name: log-journalctl\n    hostPath:\n      path: /var/log/journal\n  # - name: journalctl-binary\n  #   hostPath:\n  #     path: /usr/bin/journalctl\n  # - name: journalctl-libs\n  #   hostPath:\n  #     path: /usr/lib64/systemd\n  # - name: journalctl-extralibs\n  #   hostPath:\n  #     path: /usr/lib64/libacl.so.1s\n\n  extraVolumeMounts:\n    - name: log-pods\n      mountPath: /var/log/pods\n      readOnly: true\n    - name: log-containers\n      mountPath: /var/log/containers\n      readOnly: true\n    - name: log-journalctl\n      mountPath: /var/log/journal\n      readOnly: true\n    # - name: journalctl-binary\n    #   mountPath: /usr/bin/journalctl\n    #   readOnly: true\n    # - name: journalctl-libs\n    #   mountPath: /usr/lib64/systemd\n    #   readOnly: true\n    # - name: journalctl-extralibs\n    #   mountPath: /usr/lib64/libacl.so.1s\n    #   readOnly: true\n\n\nWhen we are using default image, it is required to mount to Pod additionally journalctl binary and if required set journalctl_path.\n\n\nThat doesn't work because we miss libraries\n/usr/lib64/systemd/libsystemd-core-257.so\n/usr/lib64/systemd/libsystemd-shared-257.so\nWe can mount /usr/lib64/systemd --> /usr/lib64/systemd, but we miss more libraries - the next one was /usr/lib64/libacl.so.1s\nldd /usr/bin/journalctl \n\n    linux-vdso.so.1 (0x00007fa1a08ce000)\n    libsystemd-shared-257.so => /usr/lib64/systemd/libsystemd-shared-257.so (0x00007fa1a0400000)\n    libc.so.6 => /lib64/libc.so.6 (0x00007fa1a0000000)\n    libacl.so.1 => /lib64/libacl.so.1 (0x00007fa1a08a1000)\n    libblkid.so.1 => /lib64/libblkid.so.1 (0x00007fa1a0863000)\n    libcap.so.2 => /lib64/libcap.so.2 (0x00007fa1a0857000)\n    libcrypt.so.1 => /lib64/libcrypt.so.1 (0x00007fa1a03c5000)\n    libmount.so.1 => /lib64/libmount.so.1 (0x00007fa1a0378000)\n    libcrypto.so.3 => /lib64/libcrypto.so.3 (0x00007fa19fa00000)\n    libpam.so.0 => /lib64/libpam.so.0 (0x00007fa1a0841000)\n    libseccomp.so.2 => /lib64/libseccomp.so.2 (0x00007fa1a0349000)\n    libselinux.so.1 => /lib64/libselinux.so.1 (0x00007fa1a031a000)\n    libm.so.6 => /lib64/libm.so.6 (0x00007fa1a022f000)\n    /lib64/ld-linux-x86-64.so.2 (0x00007fa1a08d0000)\n    libeconf.so.0 => /lib64/libeconf.so.0 (0x00007fa1a0221000)\n    libz.so.1 => /lib64/libz.so.1 (0x00007fa1a0207000)\n    libjitterentropy.so.3 => /lib64/libjitterentropy.so.3 (0x00007fa19fff4000)\n    libaudit.so.1 => /lib64/libaudit.so.1 (0x00007fa19ffc9000)\n    libpcre2-8.so.0 => /lib64/libpcre2-8.so.0 (0x00007fa19f950000)\n\n\n\nWe can try to mount whole /usr/lib64 --> /usr/lib64 but got another error related to the gclibc or so.\n\n\nSetting LD_LIBRARY_PATH is a wrong way?",
        "url": "https://github.com/vectordotdev/vector/discussions/22353",
        "createdAt": "2025-02-03T17:01:19Z",
        "updatedAt": "2025-02-11T13:13:38Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "air3ijai"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22391,
        "title": "Parse legacy S3 bucket objects using Vector.",
        "bodyText": "We currently use Vector to send our logs to our log aggregator UI for searching. Our setup includes using SQS for object notifications from an AWS S3 source.\nWe now have a requirement to ingest historical logs stored in a different S3 bucket, containing data from 2019 to 2023, and send them to our log aggregator UI for indexing and searching.\nI only know how to ingest new objects as they get updated. How can we configure Vector to also ingest and parse these legacy logs from the existing S3 bucket and forward them to our log aggregation tool?",
        "url": "https://github.com/vectordotdev/vector/discussions/22391",
        "createdAt": "2025-02-09T20:48:17Z",
        "updatedAt": "2025-02-12T15:23:06Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "neeltom92"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22377,
        "title": "Logs enable and disable",
        "bodyText": "I'm not sure if vector sink as something like these will this work:\nsinks._logs\ntype = sink\ninputs = '''\n  if get_env(\"DD_EXCLUDE_LOGS\", \"true\") == \"true\" {\n    \"trn_exclude_logs\"\n  } else {\n    \"datadog.logs\"\n  }\n'''\n\ntrn_exclude_log:\n      type: filter\n      condition: '''\n         if (env!(\"DD_EXCLUDE_LOGS\", \"false\") == \"true\"){\n          true\n         } else {\n            !(.level == \"INFO\" || .status == \"info\") &&\n            !(.logger_name == \"INBOUND_REQUEST\") &&\n            !(.status == \"DEBUG\") &&\n            !(.status == \"warn\")\n            !(.status == \"info\")\n         }\n        '''\n\n@pront can you help with these",
        "url": "https://github.com/vectordotdev/vector/discussions/22377",
        "createdAt": "2025-02-06T19:31:48Z",
        "updatedAt": "2025-02-10T17:26:26Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "adedokunk"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22388,
        "title": "Question about sending Datadog metrics to Vector",
        "bodyText": "I'm trying to use the datadog agent to send metrics to Vector, but something's not working.\nHere's the vector config (toml)\n[sources.dd_agent_metrics]\ntype = \"datadog_agent\"\naddress = \"0.0.0.0:8282\"\n\nHere's the datadog config (yaml)\nvector:\n  metrics.enabled: true\n  metrics.url: \"http://localhost:8282\" # Use https if SSL is enabled in Vector source configuration\n\nThis is running the same server inside of a datacenter.\nI can see that vector is indeed listening on port 8282 using:  telnet localhost 8282 and also using netstat. However, no  metrics flow from datadog into vector and a tcpdump command  tcpdump port 8282 -v  yields no packets sent. Is there anything else I need to do in order to get Datadog to output metric data to vector?\nThanks in advance for any help!",
        "url": "https://github.com/vectordotdev/vector/discussions/22388",
        "createdAt": "2025-02-08T04:40:41Z",
        "updatedAt": "2025-02-10T17:05:53Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "mpatterson2100-ctds"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22383,
        "title": "Azure Monitor Logs Sink openssl issue",
        "bodyText": "Hi,\nI am having an issue with the Azure Monitor Logs sink, when I enable the sink instead of blackhole I receive an openssl error\n2025-02-07T12:28:28.318108Z ERROR vector::topology::builder: Configuration error. error=Sink \"azure_monitor_logs_sink\": OpenSSL error\nI noticed in prior issue that this can be related to a connection issue such as malformed connection I,  but I am struggling to debug as my resourceid, customer_id, shared_key are all set correctly. I have also the following Envs set and CA certs present.\nIs there any more debugging information available or suggestions. I am not sure if there is a tracing option for OpenSSL itself that is available.\nVECTOR_LOG:                  Trace\nSSL_CERT_DIR:                /etc/ssl/certs\nRUST_BACKTRACE:              full\n\nsinks:\n    azure_monitor_logs_sink:\n      type: azure_monitor_logs\n      inputs:\n        - azure_events_parse\n      azure_resource_id: \"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/example-rg/providers/Microsoft.OperationalInsights/workspaces/example-la\"\n      customer_id: \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n      host: \"ods.opinsights.azure.com\"\n      log_type: \"mytablesourcename\"\n      shared_key: \"redacted_for_issue\"\n      time_generated_key: \"timestamp\"",
        "url": "https://github.com/vectordotdev/vector/discussions/22383",
        "createdAt": "2025-02-07T12:33:52Z",
        "updatedAt": "2025-02-10T16:13:25Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "wiperpaul"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 22407,
        "title": "In aws_s3 sink issues when using buffer type as disk.",
        "bodyText": "I am using aws_s3 to send logs to s3. While using buffer type as disk noticed, if vector is stopped and and there are logs in the buffer, when vector comes up does not read the logs from that file and does not empty the file. New logs comes and that is read by vector and send to s3. Please advise.",
        "url": "https://github.com/vectordotdev/vector/discussions/22407",
        "createdAt": "2025-02-10T15:11:58Z",
        "updatedAt": "2025-02-10T15:11:59Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "shamj"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22390,
        "title": "Kafka Sink missing Keys",
        "bodyText": "Hi there,\nI\u2019m attempting to replicate Kafka messages from a topic in Cluster A to another topic in Cluster B. The messages are being sent without any issues, but unfortunately, they lack the Key fields (all of them are set to Null). I\u2019ve been searching for an option to force the replication of the keys as well, but I haven\u2019t been able to find one. Is there a way to achieve this in Vector?\nHere is my configuration:\n  sources:\n    topic:\n      type: kafka\n      bootstrap_servers: \"kafkaA:9092\"\n      topics: [\"topic\"]\n      group_id: \"vector-group\"\n      auto_offset_reset: \"latest\"\n\n    internal_metrics:\n      type: internal_metrics\n \n  transforms:\n    retain_key:\n      type: remap\n      inputs: [\"topic\"]\n      source: |\n        # Preserve the Kafka key by copying it explicitly\n        .key = .key\n\n  sinks:\n    vector:\n      type: kafka\n      bootstrap_servers: \"KafkaB:32040\"\n      inputs:\n        - retain_key\n      topic: \"test-vector\"\n      encoding:\n        codec: \"raw_message\"\n        include_keys: true\n      sasl:\n        enabled: true\n        mechanism: \"SCRAM-SHA-512\"\n        username: \"user\"\n        password: \"asda\"\n      librdkafka_options:\n        security.protocol: \"SASL_PLAINTEXT\"\n    prometheus_exporter:\n      type: prometheus_exporter\n      inputs: [\"internal_metrics\"]\n      address: 0.0.0.0:9090",
        "url": "https://github.com/vectordotdev/vector/discussions/22390",
        "createdAt": "2025-02-09T08:26:18Z",
        "updatedAt": "2025-02-09T08:26:50Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "themkarimi"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22385,
        "title": "Possible to test dedupe transform?",
        "bodyText": "Is it possible to test a dedupe transform? Say I have something like this:\ntransforms:\n  dedupe-tfm-1:\n    type: dedupe\n    inputs: [\"router.level_warn\"]\n    fields:\n      match:\n      - data.message.error\n\ntests:\n- name: dedupes warn logs based on error message\n  inputs:\n  - insert_at: dedupe-tfm-1\n    type: log\n    log_fields:\n      data.message.level: warn\n      data.message.error: \"message 1\"\n  - insert_at: dedupe-tfm-1\n    type: log\n    log_fields:\n      data.message.level: warn\n      data.message.error: \"message 2\"\n  - insert_at: dedupe-tfm-1\n    type: log\n    log_fields:\n      data.message.level: warn\n      data.message.error: \"message 2\"\n  outputs:\n  - extract_from: dedupe-tfm-1\n    conditions:\n\nDo these inputs work? What would the output conditions look like?",
        "url": "https://github.com/vectordotdev/vector/discussions/22385",
        "createdAt": "2025-02-07T14:57:58Z",
        "updatedAt": "2025-02-07T17:08:57Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "davidcpell"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22375,
        "title": "Geeting errors when using podman socket to get logs with docker_logs source in vector",
        "bodyText": "My product supports \"docker\" and \"podman\" both as container runtime. We are trying to replace our home grown log handler with Vector. Support for both docker and podman are critical to us.\nWe have tried capturing log using \"docker_logs\" source from docker.sock and podman.sock. But while using podman it looks like Vector is not working properly. I see below errors in vector logs when using it with podman.\n2025-02-06T06:58:08.641980Z ERROR source{component_kind=\"source\" component_id=my_source_id component_type=docker_logs}: vector::internal_events::docker_logs: Error in communication with Docker daemon. error=RequestTimeoutError error_type=\"connection_failed\" stage=\"receiving\" container_id=None internal_log_rate_limit=true\nI would like to know when this error is thrown - \"Error in communication with Docker daemon\". What are the after effects of this ? Will the \"docker_logs\" source not get any logs after this error is hit ?\nAlso on podman we have seen that as soons as all the containers stop Vector container also exits.  Is there config setting we can use to prevent this ?",
        "url": "https://github.com/vectordotdev/vector/discussions/22375",
        "createdAt": "2025-02-06T09:05:23Z",
        "updatedAt": "2025-02-06T20:05:04Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "harikeshtripathi"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22365,
        "title": "AWS s3 sink permission issue",
        "bodyText": "Hello,\nI am trying to send logs to s3 using vector sink aws_s3.\nI have s3 bucket test and correesponding role created. Role has trusted policy and sts:AssumeRole action associated.\nHowever on deploying the configuration, below errors are seen:\n{\"component_id\":\"s3_sink\",\"component_kind\":\"sink\",\"component_name\":\"s3_sink\",\"component_type\":\"aws_s3\",\"error\":\"failed to construct request: Failed to load credentials from the credentials provider: An error occurred while loading credentials: Error { code: \"AccessDenied\", message: \"User: arn:aws:sts::1:assumed-role/test/web-identity-token-1 is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::1:role/test\" }\",\nBelow is the config :\n\"s3_sink\": {\n            \"type\": \"aws_s3\",\n            \"inputs\": [\n                \"test\"\n            ],\n            \"bucket\": \"test\",\n            \"region\": \"us-west-2\",\n            \"compression\": \"gzip\",\n            \"encoding\": {\n                \"codec\": \"text\"\n            },\n            \"auth\": {\n                \"assume_role\": \"arn:aws:iam::1:role/test\"\n            }\n        }\n\nPlease advise.",
        "url": "https://github.com/vectordotdev/vector/discussions/22365",
        "createdAt": "2025-02-04T15:49:16Z",
        "updatedAt": "2025-02-04T15:54:50Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "shamj"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22355,
        "title": "Route by json field",
        "bodyText": "Hello,\nI have this config:\napi:\n  enabled: true\n  address: 0.0.0.0:8686\n\nsources:\n   mobilelog-json:\n    type: http_server\n    address: 0.0.0.0:6624\n   \n\ntransforms:\n  json-parser:\n    type: remap\n    inputs:\n      - mobilelog-json\n    source: |-\n      . = parse_json!(string!(.message))\n\n  routed_mobile_logs:\n    type: route\n    inputs:\n      - json-parser\n    route:\n      demo: '.attributes.env == \"demo\"'\n\nsinks:\n  console:\n    inputs:\n      - routed_mobile_logs.demo\n    target: stdout\n    type: console\n    encoding:\n      codec: json\n\nbut the routing is not working.\nI tried to POST this json to my vector instance:\n{ \"message\":  \"{\\\"attributes\\\":{\\\"logtype\\\":\\\"activitylog\\\",\\\"env\\\":\\\"demo\\\",\\\"app\\\":{\\\"name\\\":\\\"TESTAPP\\\"}},\\\"env\\\": \\\"demo\\\", \\\"attributes.env\\\": \\\"demo\\\"}\" }",
        "url": "https://github.com/vectordotdev/vector/discussions/22355",
        "createdAt": "2025-02-03T17:44:32Z",
        "updatedAt": "2025-02-04T15:52:30Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mohalaci"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22358,
        "title": "Subtracting timestamps in VRL",
        "bodyText": "Hi all!\nIs it possible to perform timestamp math in VRL?\nIf I've got an event that looks like { message: \"foo\", timestamp: 2025-02-03T17:42:24.630761338Z } is there a way to perform math similar to now() - .timestamp and convert the result to a float?",
        "url": "https://github.com/vectordotdev/vector/discussions/22358",
        "createdAt": "2025-02-03T18:55:58Z",
        "updatedAt": "2025-02-03T20:16:58Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Maixy"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22340,
        "title": "Separate labels from message in loki",
        "bodyText": "Hello.\nI deploy vector using helm chart with this config:\ncustomConfig:\n  data_dir: /vector-data-dir\n  sources:\n    k8s:\n      type: kubernetes_logs\n  sinks:\n    loki:\n      type: loki\n      compression: snappy\n      encoding:\n        codec: json\n      inputs: [k8s]\n      endpoint: http://loki-gateway.default.svc.dev.k8s.local\n      path: /loki/api/v1/push\n      out_of_order_action: accept\n      remove_timestamp: false\n      remove_label_fields: true\n      labels:\n        forwarder: \"vector\"\n\nAnd the problem is that all labels comes as regualar text, and message looks like that:\n\nMessage value and labels are mixed and I need to separate them.",
        "url": "https://github.com/vectordotdev/vector/discussions/22340",
        "createdAt": "2025-01-31T14:05:49Z",
        "updatedAt": "2025-02-03T15:24:56Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "freedomwarrior"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22349,
        "title": "moving from filebeat to vector",
        "bodyText": "Good day\nim try to move for file to vector and i got this config\nfilebeat_path_custom:\n  - type: log\n    paths:\n      - \"/var/log/app-1/*.log\"\n    fields:\n      haproxy: true\n      product: nlb\n    fields_under_root: true\n    processors:\n      - dissect:\n          tokenizer: \"%{prefix}[%{pid}]: %{message}\"\n          field: \"message\"\n          overwrite_keys: true\n          target_prefix: \"\"\n      - decode_json_fields:\n          fields: [\"message\"]\n          target: \"json\"\n          overwrite_keys: true\n      - drop_fields:\n          fields: [\"prefix\", \"pid\", \"log\", \"ecs\"]\n          ignore_missing: true\ni think i should use file as source for vector. but what about else. any one can help",
        "url": "https://github.com/vectordotdev/vector/discussions/22349",
        "createdAt": "2025-02-03T12:33:30Z",
        "updatedAt": "2025-02-03T15:06:24Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "therus000"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22338,
        "title": "Vector FIPS Config",
        "bodyText": "Hi All, we are following this link https://vector.dev/docs/reference/configuration/tls/ and working on FIPS provider example section, we have created a fips.so and fipsmodule.cnf file and defined in /etc/default/vector file as an environment variable.....Post restarting vector services we don't see any reference in vector internal logs that says Vector is running with FIPS config or FIPS config is active or vector is using FIPS modules etc etc.....are we missing something ? Is there a way to validate this ?\nWe have installed vector in Amazon Linux using yum install instructions.",
        "url": "https://github.com/vectordotdev/vector/discussions/22338",
        "createdAt": "2025-01-31T07:31:25Z",
        "updatedAt": "2025-02-03T14:04:33Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ankycampy"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 5
        },
        "upvoteCount": 2
    },
    {
        "number": 22343,
        "title": "Problem using OTel / OpenTelemetry as a source",
        "bodyText": "Hi All, I new to Vector and I have an error that I can't make sense of and was looking for pointers to research.\nFor this  Config\nI am getting the error below which does not make sense, as it's saying both, I see it, but I don't see it.\nI have tried to create different source, i.e. a File source which works find, but when I assign Otel as a source it fails.\n2025-02-01T02:11:55.697404Z  INFO vector::config::graph:\nAvailable components: Shows that OTel is an available source\n\"otel\":\ncomponent_kind: source\noutputs:\nport: \"logs\", types: [\"Log\"]\nport: \"traces\", types: [\"Trace\"]\n\"file_sink\":\ncomponent_kind: sink\ntypes: [\"Log\", \"Metric\", \"Trace\"]\nAnd here it can't see the component that is listed above\n2025-02-01T02:11:55.697681Z ERROR vector::cli: Configuration error.\nerror=Input \"otel\" for sink \"file_sink\" doesn't match any components.\nThank you\n-John",
        "url": "https://github.com/vectordotdev/vector/discussions/22343",
        "createdAt": "2025-02-01T04:22:16Z",
        "updatedAt": "2025-02-01T20:30:46Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "gentijo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22337,
        "title": "Slowdown in stream processing when using remap",
        "bodyText": "Hi! When I try to transfer from Kafka to Kafka, I get a fairly high performance of 180k+ messages per second, but when aggregating or processing this stream via vrl (kafka -> remap -> kafka), the value drops to 30k, while the server is loaded by no more than 20%. Can you tell me what to look for?",
        "url": "https://github.com/vectordotdev/vector/discussions/22337",
        "createdAt": "2025-01-31T07:24:02Z",
        "updatedAt": "2025-01-31T07:24:03Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Leffka1"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22325,
        "title": "Clickhouse - create table if not exists",
        "bodyText": "Hi, is there some option to create table in clickhouse if it not exists? Like self creating indexes in elastic?",
        "url": "https://github.com/vectordotdev/vector/discussions/22325",
        "createdAt": "2025-01-30T12:43:14Z",
        "updatedAt": "2025-01-30T23:10:55Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "robinpecha"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22334,
        "title": "Extra null bytes as start of protobuf message",
        "bodyText": "I'm testing http/protobuf between vector instances and ran into some 400s I do not understand. It seems like a http sink in protobuf mode adds extra null bytes at the start of the payload.\nAs a reproducer I created a single instance to act as both the log generator, and sink, and source, with 2 pipelines i.e.\nsource demo_logs -> remap to fixed format -> sink http/protobuf -->\nsource http/protobuf -> sink console as json\n\nconfig:\napi:\n  enabled: true\n\nsources:\n  generate_syslog:\n    type: \"demo_logs\"\n    format: \"syslog\"\n    count: 10\n    interval: 5\n\n  accept_http:\n    type: http_server\n    address: 127.0.0.1:5555\n    decoding:\n      # codec: protobuf\n      codec: json\n      protobuf:\n        desc_file: test.desc\n        message_type: SimpleMessage\n\ntransforms:\n  remap_custom:\n    inputs: [\"generate_syslog\"]\n    type: \"remap\"\n    source: |\n      . = {}\n      .message = \"a test message\"\n      .bliep = 2\n\nsinks:\n  emit_http:\n    inputs: ['remap_custom']\n    type: http\n    uri: http://127.0.0.1:5555\n    encoding:\n      # codec: protobuf\n      codec: json\n      protobuf:\n        desc_file: test.desc\n        message_type: SimpleMessage\n    request:\n      headers:\n        content-type: x-protobuf\n\n  emit_console:\n    type: console\n    encoding:\n      codec: json\n    inputs:\n    - accept_http\n    - remap_custom\n\nIn json mode this work and the console shows 2 lines:\n{\"bliep\":2,\"message\":\"a test message\",\"path\":\"/\",\"source_type\":\"http_server\",\"timestamp\":\"2025-01-30T18:53:03.513011Z\"}\n{\"bliep\":2,\"message\":\"a test message\"}\n\ninspection in wireguard shows:\nPOST / HTTP/1.1\ncontent-type: x-protobuf\naccept-encoding: zstd,gzip,deflate,br\nuser-agent: Vector/0.44.0 (aarch64-apple-darwin 3cdc7c3 2025-01-13 21:26:04.735691656)\nhost: 127.0.0.1:5555\ncontent-length: 40\n\n[{\"bliep\":2,\"message\":\"a test message\"}]\n\nHTTP/1.1 200 OK\ncontent-length: 0\ndate: Thu, 30 Jan 2025 20:20:11 GMT\n\nHowever when switching to protobufs using:\nsyntax = \"proto3\";\n\nmessage SimpleMessage {\n  int64 bliep = 1;\n  string message = 2;\n  int64 bla = 3;\n}\n\nPOST / HTTP/1.1\ncontent-type: x-protobuf\naccept-encoding: zstd,gzip,deflate,br\nuser-agent: Vector/0.44.0 (aarch64-apple-darwin 3cdc7c3 2025-01-13 21:26:04.735691656)\nhost: 127.0.0.1:5555\ncontent-length: 22\n\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffda test message\n\nHTTP/1.1 400 Bad Request\ncontent-type: application/json\ncontent-length: 149\ndate: Thu, 30 Jan 2025 20:22:36 GMT\n\n{\"code\":400,\"message\":\"Failed decoding body: ParsingError(Error parsing protobuf: DecodeError { description: \\\"invalid tag value: 0\\\", stack: [] })\"}\n\nNote the 3 nulls at the start of the protobuf message. The full payload:\n000000120802120e612074657374206d657373616765\nAAAAEggCEg5hIHRlc3QgbWVzc2FnZQ==\n\nIf I post the proto and base64 string in https://www.protobufpal.com/ it does seem to decode it ok.\n{\n  \"bliep\": 2,\n  \"message\": \"a test message\",\n  \"bla\": 0\n}\n\nIf I use that same tool to encode just the message:\n{\n  \"message\": \"a test message\"\n}\n\n-> Eg5hIHRlc3QgbWVzc2FnZQ==\n\nand send that it does actually work:\ncurl localhost:5555 -XPOST -d \"$(echo -n Eg5hIHRlc3QgbWVzc2FnZQ==|base64 -D)\" -v\n< HTTP/1.1 200 OK\n< content-length: 0\n< date: Thu, 30 Jan 2025 20:34:50 GMT\n\nvector console shows:\n{\"message\":\"a test message\",\"path\":\"/\",\"source_type\":\"http_server\",\"timestamp\":\"2025-01-30T20:34:50.871782Z\"}\n\nSame for adding bla: 123\ncurl localhost:5555 -XPOST -d \"$(echo -n Eg5hIHRlc3QgbWVzc2FnZRh7|base64 -D)\"\n{\"bla\":123,\"message\":\"a test message\",\"path\":\"/\",\"source_type\":\"http_server\",\"timestamp\":\"2025-01-30T20:37:39.358163Z\"}",
        "url": "https://github.com/vectordotdev/vector/discussions/22334",
        "createdAt": "2025-01-30T20:47:36Z",
        "updatedAt": "2025-01-30T21:42:12Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "fbs"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22331,
        "title": "Using all_metrics in log_to_metric transform?",
        "bodyText": "Hi, I'm trying to use the log_to_metric transform but all_metrics. Basically I just vector to try to transform my logs to metrics if possible since all the logs will have different metric type counter, gauge, and different fields and metric kind so I won't be able to define them all as they are dynamic.\nI've tried using it but all_metrics and from the documentation it says \"notably the metrics field will be ignored\" but it isn't ignored.\nI tried converting host metrics to logs and then back to metrics but it just uses the fields defined in metrics instead of ignoring it.\nThis is my test configuration.\nsources:\n  host_metrics:\n    type: host_metrics\n    scrape_interval_secs: 3\n\ntransforms:\n    internal_to_log:\n      type: metric_to_log\n      inputs: \n        - host_metrics\n    log_to_metrics:\n      type: log_to_metric\n      inputs:\n        - internal_to_log\n      metrics:\n        - type: counter\n          field: namespace  # Use an actual field name\n          name: \"converted_metric\"\n\nsinks:\n  my_sink_id:\n    type: console\n    inputs:\n      - log_to_metrics\n    encoding:\n      codec:\n        json\n\napi:\n  enabled: true\n\nThis is the output metrics:\n{\"name\":\"converted_metric\",\"timestamp\":\"2025-01-30T19:31:24.821371Z\",\"kind\":\"incremental\",\"counter\":{\"value\":1.0}}",
        "url": "https://github.com/vectordotdev/vector/discussions/22331",
        "createdAt": "2025-01-30T19:33:56Z",
        "updatedAt": "2025-01-30T20:35:32Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "generate-me12"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22330,
        "title": "Adding environment variables to logs",
        "bodyText": "Hello,\nI'm pretty sure this is possible and I just can't seem to find the docs on it. I want to add some information to each logs that gets generated like hostname, ip address, env. (most of these are in env variables).",
        "url": "https://github.com/vectordotdev/vector/discussions/22330",
        "createdAt": "2025-01-30T19:31:10Z",
        "updatedAt": "2025-01-30T19:36:11Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "umpa385"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22256,
        "title": "Vector don't send logs from kubernetes cluster",
        "bodyText": "Hi,\nI try to deploy Vector V0.44 on Tanzu Kubernetes Cluster.\nI was able to do that using helm and application work also use this config below to test log sending to my external graylog:\n[sources.demo_logs]\nformat = \"syslog\"\ninterval = 0.5\ntype = \"demo_logs\"\n\n[sinks.graylog_syslog]\naddress = \"graylog:12201\"\ninputs = [\"demo_logs\"]\nmode = \"tcp\"\ntype = \"socket\"\n\n[sinks.graylog_syslog.encoding]\ncodec = \"gelf\"\n\n\nAnd everything seems to work I can se this demo logs.\nProblem is with kubernetes logs.\nWith this config in vector.toml:\n[sources.kubernetes_logs]\ntype = \"kubernetes_logs\"\nauto_partial_merge = true\ndelay_deletion_ms = 50\n\nfingerprint_lines = 1\nglob_minimum_cooldown_ms = 60000\ningestion_timestamp_field = \".ingest_timestamp\"\nmax_line_bytes = 64768\nmax_read_bytes = 22048\noldest_first = true\nread_from = \"beginning\"\nself_node_name = \"${VECTOR_SELF_NODE_NAME}\"\ntimezone = \"local\"\n\nlogs are not sended to graylog, I think because they're not collected.\nCan anyone help me or provide correct setting to collect them and sending very long logs like complete java stack traces?\nTanzu use containerd. With other collector like fluentbit or fluentd logs work perfectly.",
        "url": "https://github.com/vectordotdev/vector/discussions/22256",
        "createdAt": "2025-01-20T13:21:42Z",
        "updatedAt": "2025-02-16T20:48:09Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ZrytyADHD"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22321,
        "title": "error[E101]: invalid regular expression",
        "bodyText": "I'm getting error the filter. i will appreciate any assistance\ndatadog_exclude_log:\ntype: filter\ncondition: |\nstructured, err = parse_regex(.message, r'^(-|(?P\\S+)) - (-|(?P<user_agent>\\S+)) [(?P.+)] \"(?P(?P\\w+) (?P\\S+) (?P\\S+))\" (?P\\d+) (?P\\d+) \"(-|(?P.+))\" \"(-|(?P.+))\" \"(-|(?P<http_x_forwarded_for>.+))\" \"(-|(?P.+))\" \"(-|(?P<session_cookie>.+))\" \"(-|(?P<endpoint.request>\\d+.\\d+))\" \"user[(-|(?P<http_method>.+))]\"$')\nif err != null {\nreturn true  # Exclude logs with parsing errors\n}\nreturn (\n.level != \"info\" ||\n.logger_name != \"INBOUND_REQUEST\" ||\n(\n.status != \"DEBUG\" &&\n.status != \"INFO\"\n) ||\n.request_time != to_float!(.request_time) ||\n.timestamp != parse_timestamp!(.timestamp, \"%d/%b/%Y:%H:%M:%S %z\")\n)",
        "url": "https://github.com/vectordotdev/vector/discussions/22321",
        "createdAt": "2025-01-29T19:43:24Z",
        "updatedAt": "2025-01-29T21:43:53Z",
        "isAnswered": false,
        "locked": true,
        "author": {
            "login": "adedokunk"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22322,
        "title": "Implement batching in the AWS SQS Sink",
        "bodyText": "Thoughts on implementing batches in the AWS SQS Sink?\nIs this being worked on or are there reasons it hasn't been implemented yet?\n#8550",
        "url": "https://github.com/vectordotdev/vector/discussions/22322",
        "createdAt": "2025-01-29T21:13:22Z",
        "updatedAt": "2025-01-29T21:19:24Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "juan-ramirez-sp"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22312,
        "title": "Unit testing for throttle",
        "bodyText": "Hi! I'm currently implementing the throttle transform to prevent huge spikes in logs ingested, and have written it like this (input is kubernetes logs):\n  throttling:\n    type: throttle\n    inputs:\n      - ...\n    key_field: \"{{.kubernetes.pod_uid}}\"\n    threshold: ${LOG_THROTTLE_THRESHOLD:-1000}\n    window_secs: 10\nIn testing I'm setting the LOG_THROTTLE_THRESHOLD to 1 to ensure the throttling kicks in (I've verified that the actual functionality works as expected).\nThe unit test looks like this:\n  - name: log throttling\n    inputs:\n      - insert_at: throttling\n        type: log\n        log_fields:\n          kubernetes.pod_uid: \"1\"\n          message: \"hello 1\"\n      - insert_at: throttling\n        type: log\n        log_fields:\n          kubernetes.pod_uid: \"1\"\n          message: \"hello 2\"\n      - insert_at: throttling\n        type: log\n        log_fields:\n          kubernetes.pod_uid: \"2\"\n          message: \"hello 3\"\n    outputs:\n      - extract_from: throttling\n        conditions:\n          - type: vrl\n            source: |\n              assert!(.message != \"hello 2\")\nBut it passes when I set the threshold to >1 as well. Is there any way to specify that I want to not observe the hello 2 message in the output?",
        "url": "https://github.com/vectordotdev/vector/discussions/22312",
        "createdAt": "2025-01-28T10:58:34Z",
        "updatedAt": "2025-01-29T09:48:28Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "FredrikAugust"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22314,
        "title": "How to parse custom log file?",
        "bodyText": "I want to use vector to read logs from the vmware-vmsvc-root.log file and convert them to json for sending to victoriametris logs.\nBut no data is being transmitted... Unfortunately, I could not find data on my problem or a similar problem in the documentation. What am I doing wrong? Please tell me how this functionality can be implemented??\nmy configuration:\n# /etc/vector/vector.yaml\n\ndata_dir: /var/lib/vector\n\napi:\n   enabled: true\n\nsources:\n\n  # Getting vmware.log logs\n  vmware_logs:\n    type: file\n    include:\n      - /var/log/vmware-vmsvc-root.log\n\n\n# Transform getting data\ntransforms:\n  vmware_logs_transform:\n    type: remap\n    inputs:\n      - vmware_logs\n    source: |\n      match = parse_regex!(.message, r'^\\[(?P<timestamp>[^\\]]+)\\] \\[ (?P<level>[^\\]]+)\\] \\[(?P<service>[^\\]]+)\\] \\[(?P<code>[^\\]]+)\\] (?P<message>.*)$')\n      if !is_null(match){\n        .timestamp = parse_timestamp!(match.timestamp, \"%Y-%m-%dT%H:%M:%S.%3fZ\")\n        .level = match.level\n        .service = match.service\n        .code = match.code\n        .message = match.message\n      }\n\n\nsinks:\n  \n  victorialogs_vmware:\n    type: http\n    inputs:\n      - vmware_logs_transform\n    uri: \"http://192.168.1.3:9428/insert/jsonline\"\n    headers:\n        AccountID: '0'\n        ProjectID: '0'\n        VL-Stream-Fields: source=vmware-vmswc-root.log,host=192.196.1.2,environment=production\n        VL-Msg-Field: message.msg\n        VL-Time-Field: timestamp\n    compression: gzip\n    encoding:\n      codec: json\n    healthcheck:\n      enabled: false\n\n  console_parse:\n    type: console\n    inputs:\n      - vmware_logs_transform\n    encoding:\n      codec: json",
        "url": "https://github.com/vectordotdev/vector/discussions/22314",
        "createdAt": "2025-01-28T15:03:32Z",
        "updatedAt": "2025-01-28T15:13:56Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "hightwoltt"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21948,
        "title": "Elastic search to send metrics",
        "bodyText": "Hi team,\nI am using sink: elasticsearch (https://vector.dev/docs/reference/configuration/sinks/elasticsearch/) to send logs to opensearch and they are working fine using below configuration:\nopensearch-dev:\n sink:\n        type: elasticsearch\n        inputs: [output_opensearch_logs]\n        api_version: v7\n        compression: none\n        doc_type: _doc\n        endpoints:\n          - https://endpooint:port\n        auth:\n          strategy: \"basic\"\n          user: \"\"\n          password: \"\"\n        tls:\n          verify_certificate: false\n        mode: bulk\n        bulk:\n            index: \"vector\"\n        batch:\n          max_bytes: 100\n          max_events: 5000\n          timeout_secs: 11\n\nI want to send metrics as well to opensearch. Will elasticsearch will be able to do?\nOr is there any other way to send metrics?",
        "url": "https://github.com/vectordotdev/vector/discussions/21948",
        "createdAt": "2024-12-04T11:38:25Z",
        "updatedAt": "2025-01-27T08:46:49Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "uni-pooja-laad"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22298,
        "title": "Unmount actions getting denied on vector file source",
        "bodyText": "Hi,\nI'm trying to collect log files using Vector's file source. However, this keeps the file open indefinitely, resulting in umount actions getting denied. This issue occurs even when the file writer process has been stopped, as Vector continues to read the file. Is there any way to follow the logs without preventing umount actions?\nrelated to issue:  #22297\nCould you please help me figure this out and let me know how I can fix it? I would really appreciate it.\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/22298",
        "createdAt": "2025-01-26T16:19:29Z",
        "updatedAt": "2025-01-26T16:19:30Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22282,
        "title": "Prometheus Remote Write to Amazon Prometheus?",
        "bodyText": "Hi,\nI have Vector running on an ec2 instance. I am collecting prometheus metrics on this instance and trying to use prometheus_remote_write sink to forward these metrics to Amazon Managed Prometheus workspace. The AMP workspace is created and uses an VPC interface endpoint. Usually we don't generate access keys, we just use instance roles or profiles. How does authentication for this sink work? I tried just supplying the assume_role in auth section but I'm running into:\nWARN sink{component_kind=\"sink\" component_id=prometheus sink component_type=prometheus_remote_write}: aws_config::imds::region: failed to load region from IMDS err=failed to load IMDS session token: dispatch failure: other: No HTTP client was available to send this request. Enable the rustlscrate feature or configure a HTTP client to fix this. (FailedToLoadToken(FailedToLoadToken { source: DispatchFailure(DispatchFailure { source: ConnectorError { kind: Other(None), source: \"No HTTP client was available to send this request. Enable therustls crate feature or configure a HTTP client to fix this.\", connection: Unknown } }) })) If I remove it then, healthcheck just says:\nsh-4.2$ vector validate \u221a Loaded [\"vector.yaml\"] \u221a Component configuration \u221a Health check \"print\" 2025-01-22T19:57:35.514440Z ERROR vector::topology::builder: msg=\"Healthcheck failed.\" error=Unexpected status: 403 Forbidden component_kind=\"sink\" component_type=\"prometheus_remote_write\" component_id=prometheus_sink x Health check for \"prometheus_sink\" failed: Unexpected status: 403 Forbidden \nIs providing the access keys the only way to authenticate ? Thank you.",
        "url": "https://github.com/vectordotdev/vector/discussions/22282",
        "createdAt": "2025-01-22T20:00:55Z",
        "updatedAt": "2025-01-24T23:08:19Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "generate-me12"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22292,
        "title": "support dynamically pipeline \uff1f",
        "bodyText": "The type of transform is fixed. Suppose there are 10 transforms, and these 10 transforms can form a pipeline, but the order might vary. If I have 10 different pipelines, I would need to write 100 transforms, which is not very flexible and might even affect performance. Is there a way to dynamically assemble the pipeline with these 10 transforms according to different sequences on a regular basis?",
        "url": "https://github.com/vectordotdev/vector/discussions/22292",
        "createdAt": "2025-01-24T05:30:35Z",
        "updatedAt": "2025-01-24T14:15:34Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Rentu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22291,
        "title": "Message processing guranties for http_client source",
        "bodyText": "Is there any build-in mechanism or recommendation I can use to ensure that if I send data from http_client source it will be either delivered to sink or placed into some flow like dead-letter queue (let's say all configured retries for datadog logs sink are exceeded).",
        "url": "https://github.com/vectordotdev/vector/discussions/22291",
        "createdAt": "2025-01-23T18:31:10Z",
        "updatedAt": "2025-01-23T19:23:44Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "DmitryLukyanov"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22289,
        "title": "Cosmos DB source or sink",
        "bodyText": "I see, there is azure blob storage sink, are there any plans to implement similar sink for cosmosdb? Any plans about sources in azure (cosmosdb, blob)?",
        "url": "https://github.com/vectordotdev/vector/discussions/22289",
        "createdAt": "2025-01-23T16:21:07Z",
        "updatedAt": "2025-01-23T17:19:21Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "DmitryLukyanov"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22084,
        "title": "a lot of sources, transforms",
        "bodyText": "I have a lot of log collection, and the simplest way to manage them is to use the filename as a source. However, this may lead to too many sources and impact performance, so we do some merging. But in the transform stage, different logs may have different conditions for multi-line matching or different rules for timestamp extraction. This could lead to a large number of transforms, possibly over a thousand. What are some good solutions for such a scenario? Our daily log collection volume is more than 30TB, and the logging scenarios are complex, with inconsistent encoding and a large number of log collections, making the cleansing rules complex.",
        "url": "https://github.com/vectordotdev/vector/discussions/22084",
        "createdAt": "2024-12-27T07:04:06Z",
        "updatedAt": "2025-01-23T14:49:34Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Rentu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22086,
        "title": "Will Enhancement CSV  files be cached? When querying, is the data retrieved from the disk or from memory? If it's from memory, how can hot reload be achieved when the CSV file is updated?",
        "bodyText": "Will Enhancement CSV  files be cached? When querying, is the data retrieved from the disk or from memory? If it's from memory, how can hot reload be achieved when the CSV file is updated?",
        "url": "https://github.com/vectordotdev/vector/discussions/22086",
        "createdAt": "2024-12-28T07:50:54Z",
        "updatedAt": "2025-01-23T02:40:28Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Rentu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22286,
        "title": "Detecting when vector drops a large log line",
        "bodyText": "Hey all, I'm working on adding some alerting for certain error conditions in vector. I have a file source writing to an s3 sink and want to detect when vector has dropped a log because the log line is larger than the source buffer.\nI've been looking at the following internal_metrics \"component_discarded_events_total\", \"buffer_discarded_events_total\" and \"events_discarded_total\" on both the sink and source, but none of these seem to capture this case (I tested by appending a 16MB log line to the source file which is larger than the default 10MB buffer) and it none of these metrics were present when I checked the internal_metrics.\nWondering if there's another metric that might capture this or if there's some other way to detect when this happens. Thank you!",
        "url": "https://github.com/vectordotdev/vector/discussions/22286",
        "createdAt": "2025-01-22T23:15:27Z",
        "updatedAt": "2025-01-22T23:15:28Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "sean-brandenburg"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 12
    },
    {
        "number": 22278,
        "title": "Convert Datadog traces to OpenTelemetry traces",
        "bodyText": "Hi! Is it possible to convert Datadog traces to OpenTelemetry traces via Vector?",
        "url": "https://github.com/vectordotdev/vector/discussions/22278",
        "createdAt": "2025-01-22T16:53:32Z",
        "updatedAt": "2025-01-22T18:43:23Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "medzin"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22265,
        "title": "Is Graviton supported for deploying Vector ?",
        "bodyText": "Please suggest if graviton supported for vector and if there are any cost savings associated with it or any risks imposed with that.\nAlso please suggest if Discord is still in use for open discussions or any where else I can post vector questions.",
        "url": "https://github.com/vectordotdev/vector/discussions/22265",
        "createdAt": "2025-01-21T17:43:23Z",
        "updatedAt": "2025-01-22T18:44:02Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "shamj"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21915,
        "title": "log data in JSON format",
        "bodyText": "Good afternoon!\nI'm working on setting up log transmission to OpenSearch using a JSON log format. My test attempts have been successful, but I need to find a way to configure Vector to send log data in JSON format without sending the entire log in one message.\nI've tried specifying the type like this:\n[sources.my_source]\ntype = \"file\"\ninclude = [\"/path/to/your/logs/log.json\"]\nfile_type = \"json\"\nI tried the encoding.codec = \"json\" option\n\nHowever, I encountered an error. What is the correct way to do this?",
        "url": "https://github.com/vectordotdev/vector/discussions/21915",
        "createdAt": "2024-11-29T06:51:02Z",
        "updatedAt": "2025-01-22T18:17:06Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "soulg24"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 1
    },
    {
        "number": 22268,
        "title": "Cannot send logs from shared drive",
        "bodyText": "Hi Team,\nTrying to send logs from shared drive but vector not sending any logs, no errors, there are many files but no files getting checkpointed , changed checkpoint strategy but no success, please help\n[sources.app_logs]\ntype = \"file\"\ndata_dir = \"/opt/vector/local/\"\ninclude = [\"/gsx/shared/app_logs/ip-10-1-1-97_125614_1.out\"]\nfile_key = \"file\"\nglob_minimum_cooldown_ms = 1000\nhost_key = \"\"\nignore_older_secs = 600\nfingerprint.strategy = \"device_and_inode\"\nline_delimiter = \"\\n\"\nignore_checkpoints = true\n\n[transforms.enrich_remap]\ntype = \"remap\"\ninputs = [ \"app_logs\" ]\ndrop_on_abort = true\nsource = \"\"\"\nfilename_parts = split!(.file, \"_\" )\n.hostname = filename_parts[0]\n.job_id = filename_parts[1]\n\"\"\"\n\n[sinks.my_sink_id]\ntype = \"axiom\"\ninputs = [ \"enrich_remap\" ]\ndataset = \"app-logs\"\ntoken = \"xxxxx\"\n\n[sinks.test-console]\ntype = \"console\"\ninputs = [ \"app_logs\" ]\ntarget = \"stdout\"\nencoding.codec = \"json\"\n\nvector Logs\n2025-01-21T21:03:12.687039Z DEBUG vector::internal_events::file::source: Files checkpointed. count=0 duration_ms=3\n2025-01-21T21:03:12.715208Z DEBUG source{component_kind=\"source\" component_id=app_logs component_type=file}:file_server: file_source::file_server: event_throughput=0.000/sec bytes_throughput=0.000/sec ratios={\"discovery\": 7.884506e-5, \"other\": 4.845607e-5, \"reading\": 0.00020746754, \"sending\": 3.4234923e-5, \"sleeping\": 0.99963105}",
        "url": "https://github.com/vectordotdev/vector/discussions/22268",
        "createdAt": "2025-01-21T21:08:12Z",
        "updatedAt": "2025-01-22T15:30:09Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "rrs-llabs"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22228,
        "title": "Does the S3 source assume role setting apply to just s3:GetObject or include the sqs:ReceiveMessage too?",
        "bodyText": "Hello \ud83d\udc4b\nWondering if when setting the auth.assume_role setting for a given S3 source, which AWS API calls leverage this assumed role.\nFor example, if an user has 2 roles:\n\nRole A - has access to s3:GetObject\nRole B - has access to sqs:ReceiveMessage\n\nWill the S3 source still successfully GetObject and ReceiveMessage if vector is configured to assume Role A?",
        "url": "https://github.com/vectordotdev/vector/discussions/22228",
        "createdAt": "2025-01-16T20:53:30Z",
        "updatedAt": "2025-02-10T17:31:51Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "britton-from-notion"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22233,
        "title": "help configuring kafka sink",
        "bodyText": "Hi Everyone,\nI tried posting same question in discord but did not get any response. Trying to repost it here in case anyone can help me out.\ni am trying to push my webserver logs to kafka(MSK) but for some reason it seems to fail continuously and it is unclear to me what needs to done to fix this. Earlier i used to push from filebeat it worked fine can somone help me out on this please\nmy setup details\nvector version: vector 0.44.0 (x86_64-unknown-linux-gnu 3cdc7c3 2025-01-13 21:26:04.735691656)\nMSK/kafka(unauthenticated) :  Apache Kafka: 3.5.1\nvector conf:\napi:\n  enabled: true\n\nsources:\n  test_source:\n    type: \"file\"\n    include:\n      - \"/var/log/access/*.ind\"\n\nsinks:\n  test_kafka_sink:\n    type: kafka\n    inputs:\n      - test_source\n    bootstrap_servers: \"b-3.dummy.amazonaws.com:9092,b-2.dummy.amazonaws.com:9092,b-1.dummy.amazonaws.com:9092\"\n    topic: \"ondemand-accesslog\"\n    encoding:\n      codec: json\n\nvector error:\n2025-01-16T18:15:36.103476Z ERROR sink{component_kind=\"sink\" component_id=test_kafka_sink component_type=kafka}:request{request_id=16}: vector_common::internal_event::service: Service call failed. No retries or retries exhausted. error=Some(KafkaError (Message production error: MessageTimedOut (Local: Message timed out))) request_id=16 error_type=\"request_failed\" stage=\"sending\" internal_log_rate_limit=true\n2025-01-16T18:15:36.103571Z ERROR sink{component_kind=\"sink\" component_id=test_kafka_sink component_type=kafka}:request{request_id=16}: vector_common::internal_event::component_events_dropped: Events dropped intentional=false count=1 reason=\"Service call failed. No retries or retries exhausted.\"\n internal_log_rate_limit=true\n2025-01-16T18:15:36.103591Z ERROR sink{component_kind=\"sink\" component_id=test_kafka_sink component_type=kafka}:request{request_id=9}: vector_common::internal_event::service: Internal log [Service call failed. No retries or retries exhausted.] is being suppressed to avoid flooding.\n2025-01-16T18:15:36.103603Z ERROR sink{component_kind=\"sink\" component_id=test_kafka_sink component_type=kafka}:request{request_id=9}: vector_common::internal_event::component_events_dropped: Internal log [Events dropped] is being suppressed to avoid flooding.",
        "url": "https://github.com/vectordotdev/vector/discussions/22233",
        "createdAt": "2025-01-17T12:21:55Z",
        "updatedAt": "2025-01-21T16:05:13Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "thisis2394"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22210,
        "title": "Address already in use (os error 98)",
        "bodyText": "When trying to configure Vector, I encounter an error.\n2025-01-15T06:32:04.648465Z  INFO vector: Vector has started. debug=\"false\" version=\"0.43.1\" arch=\"x86_64\" revision=\"e30bf1f 2024-12-10 16:14:47.175528383\" 2025-01-15T06:32:04.651295Z ERROR vector::internal_events::socket: Error binding socket. error=error creating server listener: Address already in use (os error 98) error_code=\"socket_bind\" error_type=\"io_failed\" stage=\"receiving\" mode=tcp internal_log_rate_limit=true 2025-01-15T06:32:04.651461Z ERROR vector::app: An error occurred that Vector couldn't handle: error creating server listener: Address already in use (os error 98). 2025-01-15T06:32:04.651511Z  INFO vector: Vector has stopped.\nI can only see this error if I go into the container and type the \"Vector\" command, when starting the container itself or when trying to debug using docker logs vector -f this error is not displayed.\nVector runs in a Docker container, which is running in a single instance. There are no more containers or Vector instances on the host.\nThe container was started using:\ndocker run -d \\ --name vector \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /etc/vector/vector.yaml:/etc/vector/vector.yaml:ro \\ timberio/vector:0.43.1-debian \\ --config /etc/vector/vector.yaml\nI tried reinstalling the container.\nCan you tell me how to solve this problem? how do I see what makes Vector think it's already running on the host?",
        "url": "https://github.com/vectordotdev/vector/discussions/22210",
        "createdAt": "2025-01-15T06:49:52Z",
        "updatedAt": "2025-01-21T14:51:49Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "hightwoltt"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21556,
        "title": "Metric events - what is the algorithm of summary metrics?",
        "bodyText": "Hi!\nQuote from the page Metric events:\nSimilar to a histogram, a summary samples observations (usually things like request durations and response sizes). While it also provides a total count of observations and a sum of all observed values, it calculates configurable quantiles over a sliding time window.\nWhat is the algorithm for \"sliding time window\"?\nBy a certain number of metrics? Or for a certain time period? Or some other algorithm?\nI tried to analyze the source code and found the \"entry point\" in file \\vector-master\\src\\sinks\\prometheus\\collector.rs\n                MetricValue::Distribution {\n                    samples,\n                    statistic: StatisticKind::Summary,\n                } => {\n                    if let Some(statistic) = DistributionStatistic::from_samples(samples, quantiles)\n\nwhere quantiles are calculated. But I don't have enough experience to understand how and where the \"samples\" array is formed.\nPlease tell me the complete sampling algorithm.",
        "url": "https://github.com/vectordotdev/vector/discussions/21556",
        "createdAt": "2024-10-19T09:54:48Z",
        "updatedAt": "2025-01-21T09:29:36Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "AlexSTAL1980"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 1
    },
    {
        "number": 22037,
        "title": "High memory utilization",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nWe are currently testing Vector in our k8s clusters to replace the Fluent Bit log collector. However, we are faced with high memory consumption relative to the Fluent Bit. At the same load, the Fluent Bit consumes 25 MiB of RAM, while Vector consumes up to 150 MiB. Why is there such a big difference?\nConfiguration\napi:\n  address: 0.0.0.0:8686\n  enabled: true\ndata_dir: /vector-data-dir\nsinks:\n  main-rules-prom_exporter:\n    address: 0.0.0.0:9598\n    inputs:\n      - main-rules-internal_metrics\n    type: prometheus_exporter\n  main-rules-vector_output:\n    address: http://${VECTOR_SERVICE_HOST}:6000\n    batch:\n      max_events: 3000\n      timeout_secs: 3\n    buffer:\n      max_events: 50000\n      type: memory\n      when_full: block\n    healthcheck:\n      enabled: false\n    inputs:\n      - main-rules-container_tag\n      - main-rules-api_tag\n      - main-rules-event_tag\n      - main-rules-node_tag\n    request:\n      adaptive_concurrency:\n        initial_concurrency: 50\n        max_concurrency_limit: 200\n    type: vector\n  throttle-rule-container_output:\n    address: http://${VECTOR_SERVICE_HOST}:6000\n    batch:\n      max_events: 3000\n      timeout_secs: 3\n    buffer:\n      max_events: 50000\n      type: memory\n      when_full: block\n    healthcheck:\n      enabled: false\n    inputs:\n      - throttle-rule-container_tag\n    request:\n      adaptive_concurrency:\n        initial_concurrency: 50\n        max_concurrency_limit: 200\n    type: vector\nsources:\n  main-rules-api_input:\n    file_key: ''\n    host_key: ''\n    include:\n      - /var/log/kubernetes/audit/kube-apiserver-audit.log\n    max_line_bytes: 500000\n    type: file\n  main-rules-container_input:\n    delay_deletion_ms: 60000\n    exclude_paths_glob_patterns:\n      - /var/log/pods/k8s-event-logger-*.log\n      - /var/log/containers/k8s-event-logger-*.log\n      - '**/*.gz'\n      - '**/*.tmp'\n    extra_namespace_label_selector: vector.dev/throttle-logs!=true\n    glob_minimum_cooldown_ms: 5000\n    namespace_annotation_fields:\n      namespace_labels: ''\n    node_annotation_fields:\n      node_labels: ''\n    oldest_first: true\n    pod_annotation_fields:\n      container_image_id: ''\n      pod_annotations: ''\n      pod_ip: ''\n      pod_ips: ''\n      pod_labels: .k8s.labels\n      pod_namespace: .kubernetes.namespace_name\n      pod_node_name: .hostname\n      pod_owner: ''\n      pod_uid: ''\n    type: kubernetes_logs\n  main-rules-event_input:\n    file_key: ''\n    host_key: ''\n    include:\n      - /var/log/containers/k8s-event-logger-*.log\n    max_line_bytes: 500000\n    type: file\n  main-rules-host_input:\n    file_key: ''\n    host_key: ''\n    include:\n      - /var/log/messages\n    max_line_bytes: 500000\n    type: file\n  main-rules-internal_metrics:\n    scrape_interval_secs: 30\n    type: internal_metrics\n  throttle-rule-container_input:\n    delay_deletion_ms: 60000\n    exclude_paths_glob_patterns:\n      - /var/log/pods/k8s-event-logger-*.log\n      - '**/*.gz'\n      - '**/*.tmp'\n    extra_namespace_label_selector: vector.dev/throttle-logs=true\n    glob_minimum_cooldown_ms: 5000\n    namespace_annotation_fields:\n      namespace_labels: ''\n    node_annotation_fields:\n      node_labels: ''\n    oldest_first: true\n    pod_annotation_fields:\n      container_image_id: ''\n      pod_annotations: ''\n      pod_ip: ''\n      pod_ips: ''\n      pod_labels: .k8s.labels\n      pod_namespace: .kubernetes.namespace_name\n      pod_node_name: .hostname\n      pod_owner: ''\n      pod_uid: ''\n    type: kubernetes_logs\ntransforms:\n  main-rules-api_tag:\n    inputs:\n      - main-rules-api_input\n    source: |-\n      . = parse_json!(.message)\n      del(.requestObject)\n      del(.responseObject)\n      .hostname = get_env_var!(\"VECTOR_SELF_NODE_NAME\")\n      .log_type = \"api\"\n    type: remap\n  main-rules-container_tag:\n    inputs:\n      - main-rules-container_input\n    source: .log_type = \"container\"\n    type: remap\n  main-rules-event_tag:\n    inputs:\n      - main-rules-event_input\n    source: >-\n      . = parse_regex!(.message, r'^(?P<ts>.*?) (?P<stream>.*?) (?P<logtag>.*?)\n      (?P<message>.*?)$')\n\n      .event_parsed = parse_json!(.message)\n\n      del(.event_parsed.metadata.managedFields)\n\n      del(.message)\n\n      .hostname = get_env_var!(\"VECTOR_SELF_NODE_NAME\")\n\n      .log_type = \"event\"\n    type: remap\n  main-rules-node_tag:\n    inputs:\n      - main-rules-host_input\n    source: |-\n      .log = del(.message)\n      .hostname = get_env_var!(\"VECTOR_SELF_NODE_NAME\")\n      .log_type = \"node\"\n    type: remap\n  throttle-rule-container_tag:\n    inputs:\n      - throttle-rule-throttle-*\n    source: .log_type = \"container\"\n    type: remap\n  throttle-rule-routing:\n    inputs:\n      - throttle-rule-container_input\n    reroute_unmatched: false\n    route:\n      personal-aekralin: .kubernetes.namespace_name == \"personal-aekralin\"\n      personal-akkocheshkov: .kubernetes.namespace_name == \"personal-akkocheshkov\"\n      personal-anmakarov: .kubernetes.namespace_name == \"personal-anmakarov\"\n      personal-annikulin: .kubernetes.namespace_name == \"personal-annikulin\"\n      personal-aszubkov: .kubernetes.namespace_name == \"personal-aszubkov\"\n      personal-avshevchuk: .kubernetes.namespace_name == \"personal-avshevchuk\"\n      personal-avvorobev2: .kubernetes.namespace_name == \"personal-avvorobev2\"\n      personal-birkganms: .kubernetes.namespace_name == \"personal-birkganms\"\n      personal-mskondratev: .kubernetes.namespace_name == \"personal-mskondratev\"\n      system-ingress: .kubernetes.namespace_name == \"system-ingress\"\n    type: route\n  throttle-rule-throttle-personal-aekralin:\n    inputs:\n      - throttle-rule-routing.personal-aekralin\n    threshold: 100\n    type: throttle\n    window_secs: 60\n  throttle-rule-throttle-personal-akkocheshkov:\n    inputs:\n      - throttle-rule-routing.personal-akkocheshkov\n    threshold: 100\n    type: throttle\n    window_secs: 60\n  throttle-rule-throttle-personal-anmakarov:\n    inputs:\n      - throttle-rule-routing.personal-anmakarov\n    threshold: 5000000\n    type: throttle\n    window_secs: 60\n  throttle-rule-throttle-personal-annikulin:\n    inputs:\n      - throttle-rule-routing.personal-annikulin\n    threshold: 100\n    type: throttle\n    window_secs: 60\n  throttle-rule-throttle-personal-aszubkov:\n    inputs:\n      - throttle-rule-routing.personal-aszubkov\n    threshold: 100\n    type: throttle\n    window_secs: 60\n  throttle-rule-throttle-personal-avshevchuk:\n    inputs:\n      - throttle-rule-routing.personal-avshevchuk\n    threshold: 100\n    type: throttle\n    window_secs: 60\n  throttle-rule-throttle-personal-avvorobev2:\n    inputs:\n      - throttle-rule-routing.personal-avvorobev2\n    threshold: 100\n    type: throttle\n    window_secs: 60\n  throttle-rule-throttle-personal-birkganms:\n    inputs:\n      - throttle-rule-routing.personal-birkganms\n    threshold: 100\n    type: throttle\n    window_secs: 60\n  throttle-rule-throttle-personal-mskondratev:\n    inputs:\n      - throttle-rule-routing.personal-mskondratev\n    threshold: 100\n    type: throttle\n    window_secs: 60\n  throttle-rule-throttle-system-ingress:\n    inputs:\n      - throttle-rule-routing.system-ingress\n    threshold: 10000\n    type: throttle\n    window_secs: 60\n\nVersion\n0.42.0\nDebug Output\nNo response\nExample Data\nNo response\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/22037",
        "createdAt": "2024-12-16T05:49:21Z",
        "updatedAt": "2025-01-20T12:59:16Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "P0lskay"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22240,
        "title": "Transform has no consumers",
        "bodyText": "Hi!\nThat my vector.yaml conf file\nsources:\n  docker_app_test:\n    type: docker_logs\n    auto_partial_merge: true\n    include_containers:\n      - mynginx\n\ntransforms:\n  formatting_logs:\n    type: remap\n    inputs:\n      - docker_app_test\n    source: parse_nginx_log!(\n      .message,\n      \"combined\",\n      )\n\nsinks:\n  loki:\n    type: loki\n    inputs:\n      - docker_app_test\n    endpoint: http://10.10.10.10:3100\n    path: /loki/api/v1/push\n    encoding:\n      codec: \"json\"\n      json:\n        pretty: true\n    labels:\n      mynginx: docker_app_test\n\nWhen i run vector i get this warning: WARN \"vector::config::loading: Transform \"formatting_logs\" has no consumers\"\nI look at what came to me in loki, and the log comes in the format:\n{\n  \"container_created_at\": \"2024-12-13T11:18:19.516635042Z\",\n  \"container_id\": \"aab04b98e1bd6f873d4b54839910e9092922fe6e7834f28e6c2f7cd7c7f98f59\",\n  \"container_name\": \"mynginx\",\n  \"host\": \"some.host\",\n  \"image\": \"mynginx\",\n  \"label\": {\n    \"com.docker.compose.config-hash\": \"5b605e531df47bb8647df114edc733cf4fdb809d062e569bd268f7c4fc7d5381\",\n    \"com.docker.compose.container-number\": \"1\",\n    \"com.docker.compose.depends_on\": \"mynginx:service_started\",\n    \"com.docker.compose.image\": \"sha256:42c602de2c7f9379f42bdf57dc93256cb7bc622f51aed6424c60362400d9aa4f\",\n    \"com.docker.compose.oneoff\": \"False\",\n    \"com.docker.compose.project\": \"mynginx\",\n    \"com.docker.compose.project.config_files\": \"/home/user/platform/mynginx/docker-compose_platform.yml\",\n    \"com.docker.compose.project.working_dir\": \"/home/user/platform/mynginx\",\n    \"com.docker.compose.service\": \"mynginx\",\n    \"com.docker.compose.version\": \"2.6.1\",\n    \"maintainer\": \"NGINX Docker Maintainers <docker-maint@nginx.com>\"\n  },\n  \"message\": \"10.10.10.10 - - [18/Jan/2025:12:05:39 +0000] \\\"GET /api/users/info HTTP/1.1\\\" 401 12 \\\"https://mysite-my.domain.com/\\\" \\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0\\\" \\\"-\\\"\",\n  \"source_type\": \"docker_logs\",\n  \"stream\": \"stdout\"\n}\n\nWell, i try a simpler function and want to remove labels, i don't need them\nChange vector.yaml transforms.source section\ntransforms:\n  formatting_logs:\n    type: remap\n    inputs:\n      - docker_app_test\n    source: del(.label)\n\nAnd even that doesn't work. I still see WARN \"vector::config::loading: Transform \"formatting_logs\" has no consumers\"\nI can't understand why vector doesn't see consumers, what am I doing wrong?\nThanks for any help!",
        "url": "https://github.com/vectordotdev/vector/discussions/22240",
        "createdAt": "2025-01-18T13:12:46Z",
        "updatedAt": "2025-01-18T17:30:29Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "DVVolkov"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22229,
        "title": "I think the checksum ignores duplicated data when reading the source file, but can I also receive duplicated data?",
        "bodyText": "I'm intentionally trying to upload duplicate data, is it possible?\nvector sources code\nsources:\n      container_log_source:\n        encoding:\n          charset: utf-8\n        include:\n        - log.log\n        line_delimiter: \\n\n        oldest_first: true\n        read_from: beginning\n        type: file\n\nThis is sample data.\n{\"log_type\":\"log\",\"location\":{\"x\":163.79275512695312,\"y\":-294.9390869140625,\"z\":92.150016784667969},\"account_id\":\"96ba3bfc\"}\n{\"log_type\":\"log\",\"location\":{\"x\":163.79275512695312,\"y\":-294.9390869140625,\"z\":92.150016784667969},\"account_id\":\"96ba3bfc\"}\n{\"log_type\":\"log\",\"location\":{\"x\":163.79275512695312,\"y\":-294.9390869140625,\"z\":92.150016784667969},\"account_id\":\"96ba3bfc\"}",
        "url": "https://github.com/vectordotdev/vector/discussions/22229",
        "createdAt": "2025-01-17T04:40:13Z",
        "updatedAt": "2025-01-17T04:59:47Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "dpa456"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22218,
        "title": "Can the API address be configured to use a Unix socket?",
        "bodyText": "I realize that the API address can be set to a port on 127.0.0.1, but that would still allow everyone with access to the system to connect. Is it possible to use a Unix socket as the address so that we can use file permissions to limit access to specific users?",
        "url": "https://github.com/vectordotdev/vector/discussions/22218",
        "createdAt": "2025-01-15T22:49:14Z",
        "updatedAt": "2025-01-16T17:42:54Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "paul-palmer"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22220,
        "title": "Can you have multiple remap configurations in one transform?",
        "bodyText": "I have developed the following in the vrl playground to manipulate a json event.\n. = parse_json!(string!(.message))\n. = unnest!(.records)\n. = del(.records)\nI got it to work in the configuration file by creating 3 different transforms, one calling the previous one.\nBut I don't know the syntax to make this all happen in one transform or if that is even possible?",
        "url": "https://github.com/vectordotdev/vector/discussions/22220",
        "createdAt": "2025-01-16T00:04:17Z",
        "updatedAt": "2025-01-16T14:55:00Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "pezkins"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22204,
        "title": "[Splunk][JournalD] Missing journald metadata fields",
        "bodyText": "Hi, I'm using vector to get the journald logs from splunk universal forwarder that I collect using Journald input: https://docs.splunk.com/Documentation/Splunk/9.3.2/Data/CollecteventsfromJournalD.\nHere is my Splunk output config:\n[tcpout]\ndefaultGroup = vector\n\n[tcpout:vector]\nserver = 10.12.2.22:8088\nsendCookedData = false\nformat = raw\nmetadata = true\n\nHere is my vector config:\nsources:\n  splunk_input:\n    type: \"socket\"\n    address: \"0.0.0.0:8088\"\n    mode: \"tcp\"\n    decoding:\n      codec: \"bytes\"\n\nsinks:\n  console_out:\n    type: \"console\"\n    inputs: [\"splunk_input\"]\n    encoding:\n      codec: \"json\"\n\nIts working but in the console I can see only log messages without journald metadata fields like TRANSPORT, that splunk forwarder adds to the logs. I can see these metadata fields when I send logs directly to Splunk indexer. How can I see the Splunk metadata fields in Vector?",
        "url": "https://github.com/vectordotdev/vector/discussions/22204",
        "createdAt": "2025-01-14T19:45:48Z",
        "updatedAt": "2025-01-14T19:45:49Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "MC13-cmd"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 22197,
        "title": "Disable Buffer for Kafka Source to Clickhouse Sink",
        "bodyText": "I got a Vector config where I am writing from kafka to clickhouse. As Kafka is supposed to persist events and hand them to the consumer (Vector) in a reasonable pace, I would like to disable the buffer in Vector, so that when Vector is too slow to consumer a consumer lag is build in Kafka. Is this a bad idea? In my Clickhouse sink I tried to set:\n        buffer:\n          max_events: 0\n          type: memory\nThis however produces a config error.",
        "url": "https://github.com/vectordotdev/vector/discussions/22197",
        "createdAt": "2025-01-14T12:52:08Z",
        "updatedAt": "2025-01-14T19:21:45Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "seilerre"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22142,
        "title": "Generate an array from source output with transform and use that array for making more http_client source requests",
        "bodyText": "I have an http_client source that returns a dynamic JSON object and I am using a transform to collect the items I need and store them in an array. I then need to iterate over that array and make multiple http_client source requests with each value in the array.\nI apologize, but I am not clear about the proper way to reuse results from VRL on subsequent source calls.\nCan I make new source requests after the transform again? I can't invoke a source request from inside the transform can I?\nIs it better to use a scripting language to make the first request, get the array, and then invoke vector for each item in the array?",
        "url": "https://github.com/vectordotdev/vector/discussions/22142",
        "createdAt": "2025-01-08T15:19:16Z",
        "updatedAt": "2025-01-14T14:20:36Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "nathanle"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20860,
        "title": "Vector Sidecar data-dir config",
        "bodyText": "We are planning to run vector as a sidecar for some critical application whose load is very high as compared to others. We are doing this to avoid CPU split meaning some pods consuming very large cpu as compared to others.\nIn the sidecar pattern we can have more than 1 pod running on the same node but we also want to limit that sidecar reads only its own logs so we have this in includes\n\"/var/log/pods/{{ $namespace }}_${VECTOR_SELF_POD_NAME}_*/**/*\"\nHowever we want to know how should we configure data-dir. Can we have same data-dir shared between 2 vector pods or we should separate them out.\nAlso since the include path has pod_name in it, if pod restarts, the path will change. So even if there is 1 pod on 1 node the files which are read will be different. How should we configure data-dir i.e. checkpointing in this case\nConfigs:\ndata_dir: /vector-data-dir\nexpire_metrics_secs: 300\nacknowledgements:\n  enabled: true\napi:\n  enabled: true\n  address: 0.0.0.0:8686\n  playground: false\nsources:\n  kubernetes_logs:\n    type: kubernetes_logs\n    glob_minimum_cooldown_ms: 5000\n    ingestion_timestamp_field: \"ingest_timestamp\"\n    include_paths_glob_patterns:\n    - \"/var/log/pods/{{ $namespace }}_${VECTOR_SELF_POD_NAME}_*/**/*\"\n  internal_metrics:\n    type: internal_metrics\ntransforms:\n  dedot_keys:\n    type: remap\n    inputs:\n      - kubernetes_logs\n    source: |\n      . = map_keys(., recursive: true) -> |key| { replace(key, \".\", \"_\") }\nsinks:\n  prometheus_exporter:\n    type: prometheus_exporter\n    flush_period_secs: 60\n    inputs:\n      - internal_metrics\n    address: 0.0.0.0:9090\n    buffer:\n      type: memory\n      when_full: block\n      max_events: 500\n  kafka:\n    type: kafka\n    bootstrap_servers: kafka:9092\n    topic: vector\n    batch:\n      timeout_secs: 1\n      max_bytes: 1000000\n      max_events: 4000\n    librdkafka_options:\n      client.id: \"vector\"\n      request.required.acks: \"1\"\n    message_timeout_ms: 0\n    buffer:\n      type: memory\n      when_full: block\n      max_events: 1000\n    inputs:\n      - dedot_keys\n    encoding:\n      codec: json\n    compression: zstd",
        "url": "https://github.com/vectordotdev/vector/discussions/20860",
        "createdAt": "2024-07-15T09:39:54Z",
        "updatedAt": "2025-01-14T03:43:38Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ShahroZafar"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22185,
        "title": "aws_ecs_metrics source - invalid type: null, expected struct ContainerStats",
        "bodyText": "Good day.\nTried to configure vector to gather ECS task metrics but got an error:\nERROR source{component_kind=\"source\" component_id=source_aws_ecs_metrics component_type=aws_ecs_metrics}: vector::internal_events::aws_ecs_metrics: Parsing error. endpoint=http://169.254.170.2/v3/f4275bb6-2577-45f4-8500-49c6f5fa28ad/task/stats error=Error(\"invalid type: null, expected struct ContainerStats\", line: 1, column: 72) stage=\"processing\" error_type=\"parser_failed\" internal_log_rate_limit=true\nVector configuration:\nsources: ... source_aws_ecs_metrics: type: aws_ecs_metrics scrape_interval_secs: 15 version: v4 ...\nIn a deployed vector container, I can observe environments\nECS_CONTAINER_METADATA_URI=http://169.254.170.2/v3/f4275bb6-2577-45f4-8500-49c6f5fa28ad ECS_CONTAINER_METADATA_URI_V4=http://169.254.170.2/v4/f4275bb6-2577-45f4-8500-49c6f5fa28ad\nThe versions:\nEC2: AWS amz linux 2, amzn2-ami-ecs-hvm-2.0.20250102-x86_64-ebs\nECS agent: \"Starting Amazon ECS Agent\" version=\"1.89.2\" commit=\"41d593c6\"\nVector.dev: \"timberio/vector:0.43.1-debian\"\nQuestions:\n\nIf I have specify v4 in source - and vector can see env ECS_CONTAINER_METADATA_URI_V4 - why does it try to use v3 endpoint? In the doc it is specified that v4 in that case should be used\nI can curl to http://169.254.170.2/v3/f4275bb6-2577-45f4-8500-49c6f5fa28ad/task/stats and get payload from ec2 instance itself and from any container in the ECS task. Unfortunately, I can't check if this endpoint is available from vector container. But if endpoint is available from other containers - I assume that it is available from vector container too. The question why it sees null instead of payload.",
        "url": "https://github.com/vectordotdev/vector/discussions/22185",
        "createdAt": "2025-01-12T11:52:16Z",
        "updatedAt": "2025-01-13T19:36:20Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "winadm87"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22141,
        "title": "\u91c7\u96c6socket\u6e90\u6d88\u606f,\u600e\u4e48\u6d88\u9664\u6362\u884c\u7b26",
        "bodyText": "\u4f8b\u5982\u4ee5\u4e0b\u6d88\u606f\u4f53message\u8ffd\u52a0\u7684\\n\n{\n\"host\": \"10.33.210.7\",\n\"message\": \"hello world\\n\",\n\"port\": 40322,\n\"source_type\": \"socket\",\n\"timestamp\": \"2025-01-08T03:20:17.637566208Z\"\n}",
        "url": "https://github.com/vectordotdev/vector/discussions/22141",
        "createdAt": "2025-01-08T03:29:17Z",
        "updatedAt": "2025-01-13T08:44:15Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "papapy"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22149,
        "title": "Host_metrics collection over http_server and data loss in pipelines",
        "bodyText": "I get host_metrics from local machine, parse and send successfully to file/prometheus/blackhole. Vector top displays it correctly. This works.\nI have other instances where I want to collect host_metrics and send them to a central vector where they will be collected and stored. For this I use http_server endpoint (I didn't find anything better for it, some kind of vector tunnel).\nThe problem is that I receive the data, I see it at the input, I see it at the first transformation, but then it does not get into the conversion of metrics to logs, also when writing to the database, etc. The only place it mysteriously ends up in a black hole.\nAnd in addition, in this case there is a difference between what is in a vector top and a vector tap. In some cases, vector top says there is no input, but vector tap --inputs-of makes it clear that there is an input.\nI've tried to graphically show what's going on:\nfull config vector.yaml https://pastebin.com/mRZSSVF9\n(the port on the tunnel was the same in the configuration, it's just different in this screenshot)",
        "url": "https://github.com/vectordotdev/vector/discussions/22149",
        "createdAt": "2025-01-09T13:00:42Z",
        "updatedAt": "2025-01-13T08:25:05Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "robinpecha"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 22122,
        "title": "Errors when configuring the vector buffer",
        "bodyText": "My Vector is running in a Docker container, and when I try to configure buffering, I get these errors:\nthere is only 1 vector container on the host, I've tried different options with drop_newest and block combined with different types of buffering. Vector works fine, but if I write docker exec -it vector bash and write the vector command in the container, I get the following errors.\nWhen configuring the buffer on disk:\nConf\nsinks:\n\n  intermediary_vector:\n    type: vector\n    inputs:\n      - add_host\n    address: \"192.168.1.103:9009\"\n\n    buffer:\n      - type: disk\n\tmax_size: 1073741824\n        when_full: drop_newest\n\nError:\nERROR vector::topology::builder: Configuration error. error=Sink \"intermediary_vector\": error occurred when building buffer: failed to build individual stage 0: failed to load/create ledger: failed to lock buffer.lock; is another Vector process running and using this buffer?\nThe buffer.lock file also appears in the container in the /var/lib/vector/buffer/v2/intermediary_vector directory. Deleting it doesn't help.\nWhen configuring the buffer in memory:\nConf\nsinks:\n\n  intermediary_vector:\n    type: vector\n    inputs:\n      - add_host\n    address: \"192.168.1.103:9009\"\n\n    buffer:\n      - type: memory\n        max_events: 5000\n        when_full: drop_newest\n\nError:\nERROR vector::internal_events::socket: Error binding socket. error=error creating server listener: Address already in use (os error 98) error_code=\"socket_bind\" error_type=\"io_failed\" stage=\"receiving\" mode=tcp internal_log_rate_limit=true\nERROR vector::app: An error occurred that Vector couldn't handle: error creating server listener: Address already in use (os error 98).\n\nI will be very grateful for your help in solving this problem!",
        "url": "https://github.com/vectordotdev/vector/discussions/22122",
        "createdAt": "2025-01-04T20:32:57Z",
        "updatedAt": "2025-01-10T07:43:55Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "hightwoltt"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22132,
        "title": "Vector CPU Utilisation",
        "bodyText": "Hi, im trying to understand limit of vector. I read scaling documents and i should be easily see 10MB/s for each CPU core.\nIm testing on my lab with a python script sending syslog and listening on vector end with socket source without transforms and sinking to blackhole.\nI can reach around 70MB/s without packet drops with having 16 cpu cores, after that it starts to drop packets. When i monitor each cpu core with htop, 5 cores are working (changing which core is active) with %50 utilisation.\nI'm expecting from vector to use all CPU cores to %100 (or close) before it start dropping packets. Is my expectation wrong? If not what should be the issue here?",
        "url": "https://github.com/vectordotdev/vector/discussions/22132",
        "createdAt": "2025-01-07T13:00:44Z",
        "updatedAt": "2025-01-08T23:41:27Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "prayerx"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21926,
        "title": "Splunk HEC Sink: Set Host From Field But Not Transmit in Payload?",
        "bodyText": "I'm receiving and shipping some data from a Datadog lambda extension.\nSample data:\n[\n    {\n        \"message\": {\n            \"message\": \"2024-12-01 17:54:22 UTC | DD_EXTENSION | DEBUG | Runtime done metrics: {responseLatency:0.947 responseDuration:0.091 producedBytes:35}\\n\",\n            \"lambda\": {\n                \"arn\": \"arn:aws:lambda:us-west-2:123:function:some-function\",\n                \"request_id\": \"a941db01-99f3-45c6-923f-1133a3e055a1\"\n            }\n        },\n        \"status\": \"info\",\n        \"timestamp\": 1733075662661,\n        \"hostname\": \"arn:aws:lambda:us-west-2:123:function:some-function\",\n        \"service\": \"dd-lambda-playground\",\n        \"ddsource\": \"lambda\",\n        \"ddtags\": \"functionname:some-function,aws_account:123\"\n    }\n]\n\nA http server input, simple transform and simple output is fine (a bit confusing but fine). I can set the host field just fine using the sink after transforming:\n[sources.http]\ntype = \"http_server\"\naddress = \"0.0.0.0:8282\"\nencoding = \"json\"\npath = \"/api/v2/logs\"\nmethod = \"POST\"\n\n[transforms.set_splunk_fields]\ntype = \"remap\"\ninputs = [\"http\"]\nsource = '''\n.host = .hostname\ndel(.hostname)\n\n.ddtags = parse_key_value!(.ddtags,key_value_delimiter: \":\",field_delimiter: \",\")\n.account_id = .ddtags.account_id\n'''\n\n[sinks.splunk_hec_logs]\ntype = \"splunk_hec_logs\"\ninputs = [\"set_splunk_fields\"]\nendpoint = \"https://splunk-hec\"\nencoding.codec = \"json\"\ndefault_token = \"123\"\nsourcetype = \"datadog:vector:{{ sourcetype }}\"\nindex = \"main\"\ntls.verify_certificate = false\ntimestamp_key = \"timestamp\"\n\nBut how do I remove the .host field from the Splunk payload? That field is already indexed; theres no need to pass it in the body. But if I remove it in the transform, then I cant set it in the sink...",
        "url": "https://github.com/vectordotdev/vector/discussions/21926",
        "createdAt": "2024-12-02T02:59:34Z",
        "updatedAt": "2025-01-08T21:40:40Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tommyorndorff"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22060,
        "title": "Metrics related to kubernetes_logs rotate_wait_secs",
        "bodyText": "I've recently turned on rotate_wait_secs in my Kubernetes environments because I was running into issues where Vector would be holding tons of file handles for files that have passed rotation size but hasn't finished processing them due to the amount of work it's doing. This led to node DiskPressure over time and pod evictions.\nUsing this helped, but it'd be really helpful to be able to see how often Vector is dropping file handles early and any possible metadata related to which files (container name, whatever). Is this available/something that could be reasonably added?",
        "url": "https://github.com/vectordotdev/vector/discussions/22060",
        "createdAt": "2024-12-19T19:19:53Z",
        "updatedAt": "2025-01-08T21:40:55Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "davidcpell"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 22128,
        "title": "How to handle long log lines",
        "bodyText": "Hi, I'm having issues using the file source and having my logs being truncated when they are longer log lines and subsequently having issues with parsing it. For context my message is essentially just a json string.\nBelow is my config for my source and my transform I am using.\n    sources:\n      tail_logs:\n        data_dir: /vector-data-dir\n        exclude:\n        - /var/log/containers/vector-agent*.log\n        - /var/log/containers/vector-aggregator*.log\n        fingerprint:\n          strategy: device_and_inode\n        ignore_older_secs: 86400\n        include:\n        - /var/log/containers/*.log\n        type: file\n        max_line_bytes: 1024000\n    transforms:\n      content_logging_only:\n        condition:\n          source: |-\n            remove_stdout, err = split(.message, r'stdout [a-zA-Z] ')[1]\n            parsed_json, err = parse_json(remove_stdout)\n            if err != null { log(remove_stdout, level: \"error\") }\n            parsed_json.logType == \"Special.Logger\"\n          type: vrl\n        inputs:\n        - tail_logs\n        type: filter\nIn the log I've put in I can see the log get split into 2 in the agent logs.\nAny ideas what I need to do? I couldn't find appropriate docs for this",
        "url": "https://github.com/vectordotdev/vector/discussions/22128",
        "createdAt": "2025-01-06T20:30:53Z",
        "updatedAt": "2025-01-08T18:01:32Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "trahim"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22092,
        "title": "Prometheus Remote Write Sink Timestamp",
        "bodyText": "Hello,\nI\u2019m currently testing a setup where:\n\nVector A sends metrics to a Kafka topic.\nVector B consumes those metrics from Kafka.\nVector B then writes them remotely to Prometheus.\n\nHere\u2019s the issue:\n\nWhen Prometheus is unavailable for a while, Vector doesnt acknowledges messages in kafka (which what i expect with acknowledgements set to true)\nVector acknowledges metrics in Kafka as soon as Prometheus becomes available again.\nAlthough it looks like Vector is sending the data, I see gaps in Prometheus for the period when it was down.\nI\u2019m not sure if Vector is sending the original timestamps to Prometheus or not or it is something on prometheus side.\n\nI believe Vector should handle it since i tested the same thing using prometheus agent and it works without any issue.\nCould someone please help me figure out how to preserve these timestamps so I don\u2019t have gaps?\nBelow is my Vector B configuration:\n---\nsources:\n  metrics:\n    type: kafka\n    bootstrap_servers: localhost:19092\n    topics:\n      - metrics\n    group_id: metrics\n    decoding:\n      codec: native\n    acknowledgements:\n      enabled: true\n\nsinks:\n  rw:\n    type: prometheus_remote_write\n    inputs:\n      - metrics\n    endpoint: http://localhost:9090/api/v1/write\n    batch:\n      timeout_secs: 30 ## send data every 30 seconds\n    healthcheck:\n      enabled: false\n    acknowledgements:\n      enabled: true",
        "url": "https://github.com/vectordotdev/vector/discussions/22092",
        "createdAt": "2024-12-29T09:31:10Z",
        "updatedAt": "2025-01-07T22:33:54Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "BinjaFan"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 2
    },
    {
        "number": 22133,
        "title": "Adding fields to metrics doesnt work, only updating existing one.",
        "bodyText": "Hi, I have following config where I need to add some fields to metric.\nsources:\n   hostmetrics_local:\n    type: host_metrics\n    scrape_interval_secs: 30\n\ntransforms:\n  host_metrics_local_remap:\n    inputs: [ \"hostmetrics_local\" ]\n    type: \"remap\"\n    source: |\n      .namespace = \"my_company\"\n      .maintainer = \"maintainer\"\n      .source_cat = \"source_cat\"\n\nsinks:\n  blackhole:\n    type: blackhole\n    inputs: \n      - host_metrics_local_remap\n\nAnd this is output from vector tap, no fields was added, only namespace was updated.\n{\"name\":\"load5\",\"namespace\":\"my_company\",\"tags\":{\"collector\":\"load\",\"host\":\"be7de7baa9db\"},\"timestamp\":\"2025-01-07T14:51:11.606623331Z\",\"kind\":\"absolute\",\"gauge\":{\"value\":0.2490234375}}\n\nPlease how to add fields to metrics?\nThank you.",
        "url": "https://github.com/vectordotdev/vector/discussions/22133",
        "createdAt": "2025-01-07T15:08:39Z",
        "updatedAt": "2025-01-07T19:30:41Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "robinpecha"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 22127,
        "title": "question on vector agent",
        "bodyText": "I am planning to install vector agent as a service on my OpenShift cluster nodes to send logs and metrics to backend, my scanning tool says License unknown, is there any license for this?",
        "url": "https://github.com/vectordotdev/vector/discussions/22127",
        "createdAt": "2025-01-06T19:59:39Z",
        "updatedAt": "2025-01-06T23:00:37Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ry58848"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22065,
        "title": "Prometheus scrape source with headless service as endpoint",
        "bodyText": "Hi,\nI would like to use vector prometheus scrape source to scrape metrics from various endpoints in my kubernetes clusters.  It works pretty well for kubelet and api server metrics and single pods that are behind a service. But what if I have multiple replicas of a pod? If I scrape the service of type ClusterIP I think vector will miss metrics as the service will probably do something like round-robin... What if my endpoint is a headless service? Would vector in that case scrape all the pods that are behind the headless service? My end goal is to use vector instead of prometheus to scrape metrics from multiple kubernetes clusters and ship all metrics to a centralized prometheus.",
        "url": "https://github.com/vectordotdev/vector/discussions/22065",
        "createdAt": "2024-12-20T19:08:20Z",
        "updatedAt": "2025-01-06T18:49:38Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "adelmoradian"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 21728,
        "title": "Broken auth for Elasticsearch sink after version 0.32.1",
        "bodyText": "Does anyone experienced issues with authorising Elasticsearch sink after version 0.32.1?\nTried upgrading straight from 0.30.0 to 0.42.0 and my previously fine working config just not accepting auth credentials for elasticsearch in any form. Rolling back to 0.32.1 fixes it. Creds separately working well.\nFound zero posts about it both in discord and github. It could not be just mine issue I am sure.\nTo be clear, behind opensearch sink we use AWS Opensearch 1.3 with enabled Elasticsearch 7.10 API compatibility.",
        "url": "https://github.com/vectordotdev/vector/discussions/21728",
        "createdAt": "2024-11-07T16:21:13Z",
        "updatedAt": "2025-01-06T18:23:11Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "nhlushak"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 4
    },
    {
        "number": 22087,
        "title": "Error when trying to configure buffering",
        "bodyText": "I'm trying to set up buffering, in case the intermediate Vector host is unavailable.\nBelow is my sinks configuration:\nsinks:\n\n  intermediary_vector:\n    type: vector\n    inputs:\n      - add_host\n\n   buffer:\n      type: disk\n      max_size: 1GiB\n      when_full: block\n    address: \"192.168.8.103:9009\"\n\nAfter restarting the Vector container, I get the error:\nERROR vector::cli: Configuration error. error=did not find expected key at line 46 column 4, while parsing a block mapping at line 41 column 3\nPlease tell me what could be the reason? or how to do buffering in a different way? Thanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/22087",
        "createdAt": "2024-12-28T13:09:31Z",
        "updatedAt": "2025-01-03T19:36:58Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "hightwoltt"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22117,
        "title": "File read log interval",
        "bodyText": "Hi @xufeixianggithub ,\n\nI see the issue you are describing, but I don't think we'd solve it by adding configuration to the file source. Instead, I think a more holistic way to solve this problem is for the throttle transform to support applying back-pressure. This is being tracked by #13651\nYou could also consider configuring the sink to apply back-pressure by limiting the concurrency or batch sizes.\nI'll close this issue, but let me know if you disagree with my assessment!\nOriginally posted by @jszwedko in #22095 (comment)",
        "url": "https://github.com/vectordotdev/vector/discussions/22117",
        "createdAt": "2025-01-03T01:05:06Z",
        "updatedAt": "2025-01-03T15:50:55Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "xufeixianggithub"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 1
    },
    {
        "number": 22105,
        "title": "Get compression working with aws_s3 sink (files uncompressed)",
        "bodyText": "I'm having a hard time getting compression to work with the aws_s3 sink when using batch. Batching works just fine, but the uploaded file is always uncompressed.\nThis is my configuration\nsinks:\n  amplitude_backup_s3:\n    batch:\n      max_bytes: 5242880\n      timeout_secs: 300\n    bucket: my-bucket\n    buffer:\n      max_size: 268435488\n      type: disk\n      when_full: block\n    compression: gzip\n    encoding:\n      codec: json\n    filename_append_uuid: false\n    filename_extension: json.gz\n    key_prefix: \"year=%Y/month=%m/day=%d/ts=\"\n    inputs:\n      - amplitude_backup_source\n    region: eu-central-1\n    storage_class: INTELLIGENT_TIERING\n    type: aws_s3\n\nsources:\n  amplitude_backup_source:\n    address: 0.0.0.0:8080\n    encoding: json\n    type: http\n    path: /2/httpapi\nI'm using timberio/vector:latest-alpine docker image so I don't know the exact vector version. But fairly recent I would guess.",
        "url": "https://github.com/vectordotdev/vector/discussions/22105",
        "createdAt": "2025-01-02T05:58:59Z",
        "updatedAt": "2025-01-03T08:12:16Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mblarsen"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 9899,
        "title": "Need more example",
        "bodyText": "I used vector a few weeks ago, it's great.\nBut I think it may need more examples. These  examples is more difficult  than the document example we use now. because  sometimes  I don't know how to combine component when the logic is complex. And may need some best practice example.",
        "url": "https://github.com/vectordotdev/vector/discussions/9899",
        "createdAt": "2021-11-04T12:56:19Z",
        "updatedAt": "2025-01-02T21:21:52Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "linnaname"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22100,
        "title": "`parse_grok!` expression can't fail even though it's marked fallible",
        "bodyText": "Hello, it seems like there is some confusion (maybe on my part) when trying to handle a failure in parse_grok! -  I'm getting this error when I attemp to remap using parse_grok!:\nerror[E651]: unnecessary error coalescing operation\n  \u250c\u2500 :1:17\n  \u2502\n1 \u2502   structured =  parse_grok!(.message, \"%{TIME:time}%{SPACE}%{POSINT:pid}%{SPACE}%{BASE16NUM:thread}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{WORD:category}%{SPACE}%{PROG:prog}%{SPACE}%{GREEDYDATA:message}\") ?? {\"message\":.message, \"level\": \"info\" }\n  \u2502                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ -- -------------------------------------- this expression never resolves\n  \u2502                 \u2502                                                                                                                                                                                       \u2502\n  \u2502                 \u2502                                                                                                                                                                                       remove this error coalescing operation\n  \u2502                 this expression can't fail\n  \u2502\n  = see language documentation at https://vrl.dev\n  = try your code in the VRL REPL, learn more at https://vrl.dev/examples\n\nparse_grok is marked as fallible, so I'm not sure why this expression can't fail.  We have messages that are not formatted properly and failing this grok parsing (which is currently expected as our app has a mix of 'formatted' and 'un-formatted' logging causing the failures), causing logs in vector like this:\n2024-12-31T16:33:05.610160Z ERROR transform{component_kind=\"transform\" component_id=cust_apps component_type=remap}: vector::internal_events::remap: Mapping failed with event. error=\"function call error for \\\"parse_grok\\\" at (8:191): unable to parse input with grok pattern\" error_type=\"conversion_failed\" stage=\"processing\" internal_log_rate_limit=true\n2024-12-31T16:33:07.419820Z ERROR transform{component_kind=\"transform\" component_id=cust_apps component_type=remap}: vector::internal_events::remap: Internal log [Mapping failed with event.] is being suppressed to avoid flooding.\n\nFor now I was just trying to naively transform the level to info to avoid other processing we have later on which would ship these messages with an incorrect level downstream.\nI am running vector in a docker container:\nvector 0.39.0 (aarch64-unknown-linux-gnu 73da9bb 2024-06-17 16:00:23.791735272)\n\nUsing timberio/vector:0.39.0-debian as the base image.",
        "url": "https://github.com/vectordotdev/vector/discussions/22100",
        "createdAt": "2024-12-31T16:43:08Z",
        "updatedAt": "2025-01-02T18:40:12Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "kvberge"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22103,
        "title": "Clarify \"File\" Source behavior",
        "bodyText": "I have very simple vector configuration (windows OS):\napi:\n  enabled: true\n\nacknowledgements:\n  enabled: true\n\ndata_dir: ./data\n\nsources:\n  fileIn:\n\ttype: \"file\"\n\tinclude: \n\t  - ./logs/*.log\n\tinternal_metrics:\n\t  include_file_tag: true\n\ntransforms:\n  msg_parser:\n\ttype: remap\n\tinputs: [fileIn]\n\tsource: |\n\t  # send everything to sinks\n\t  .=.\n\nsinks:\n\n  http:\n\ttype: \"http\"\n\tmethod: post\n\tinputs:\n\t  - \"msg_parser\"\n\turi: http://localhost:7000/api/MyFunction\n\ttls:\n\t  verify_certificate: false\n\tencoding:\n\t  codec: \"json\"\n\trequest:\n\t  retry_attempts: 3\n\nwhich I run with the following command:\nvector `\n    -c .\\vector.yaml `\n    --require-healthy true `\n    --color always `\n    --no-graceful-shutdown-limit `\n    --watch-config `\n    --verbose `\n\nLet's say I have a log file with 10000 lines (each line is guid\\r\\n). When I run vector command, it looks like it processes only first \"batch\" (in my case first 52 lines):\nThe last output lines in the shell is:\n    ..\n\t2025-01-01T20:42:56.006974Z DEBUG sink{component_kind=\"sink\" component_id=http component_type=http}:request{request_id=1}:http: vector::internal_events::http_client: HTTP response. status=200 OK version=HTTP/1.1 headers={\"content-type\": \"text/plain; charset=utf-8\", \"date\": \"Wed, 01 Jan 2025 20:42:55 GMT\", \"server\": \"Kestrel\", \"transfer-encoding\": \"chunked\"} body=[unknown]\n\t2025-01-01T20:42:56.059867Z DEBUG hyper::proto::h1::conn: incoming body completed\n\t2025-01-01T20:42:56.070865Z DEBUG hyper::client::pool: pooling idle connection for (\"http\", localhost:7000)\n\t2025-01-01T20:42:56.078576Z DEBUG sink{component_kind=\"sink\" component_id=http component_type=http}: vector::topology::builder: Sink finished normally.\n\t2025-01-01T20:42:56.127379Z DEBUG source{component_kind=\"source\" component_id=fileIn component_type=file}: vector::topology::builder: Source finished normally.\n\tPS C:\\VECTOR_TOOL_PATH>\n\nto process a next batch, I need to launch the vector -c .. command again (checkpoints.json each time is updated accordingly).\nSo my questions are:\n\nIs there a way to process the whole file until the end without recalling vector -c .. command multiple time?\nHow to understand how many lines will be placed into a single batch? It looks like it's completely random (sizes are like 52, 500+, 447..)",
        "url": "https://github.com/vectordotdev/vector/discussions/22103",
        "createdAt": "2025-01-01T20:49:45Z",
        "updatedAt": "2025-01-13T17:08:19Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "DmitryLukyanov"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22040,
        "title": "Preventing One Noisy Pod from Overloading kubernetes_logs Source in Vector",
        "bodyText": "Our Setup\nWe run pods from different services on the same Kubernetes node and use a Vector daemon pod on the node to collect logs using the kubernetes_logs source, apply some transformation, and eventually send to the datadog_logs sink.\nThe Problem\nIf one service was buggy and generated a large volume of logs, we observed that the kubernetes_logs source could consume excessive memory, leading to an OOM.\nWe are looking for a way to achieve \"fair sharing\" or \"resource isolation\" so that a single misbehaving service does not impact the log collection of other colocated pods.\nWhat We Have Tried\n\n\nWe have tried using the throttling filter, which can enforce rate limits per pod or per namespace. However, since the throttling filter is applied downstream of the kubernetes_logs source, it doesn't prevent the source from processing the buggy pod's logs at a high speed (and consuming a lot of memory). In fact, because the throttling filter discards messages quickly, it doesn't create backpressure for kubernetes_logs, causing it to process logs even faster compared to when backpressure is applied by the datadog_logs sink.\nIf kubernetes_logs itself provided a configuration option to limit the rate of log lines read from each k8s pod or namespace, it would satisfy our use case. However, such a configuration option doesn\u2019t seem to exist.\n\n\nWe have also tried using the request.rate_limit_num setting of the datadog_logs sink. It successfully creates backpressure to the kubernetes_logs source and causes it to slow down processing incoming logs, preventing the excessive memory consumption. But this approach does not provide fair sharing, meaning logs from the normal pods are also blocked when the rate limit is reached.\n\n\nRelated issues\n#17123 Limit memory usage for sources - Make vector 'resource aware': If this issue is resolved, it would help prevent our Vector daemon from OOMing. However, in addition to that, we also want to enforce \"fair sharing\", which remains unsolved by the solutions proposed in that issue.",
        "url": "https://github.com/vectordotdev/vector/discussions/22040",
        "createdAt": "2024-12-16T18:44:01Z",
        "updatedAt": "2025-01-02T17:10:21Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "caesarxuchao"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 22107,
        "title": "kafka to prometheus metrics",
        "bodyText": "Hi!\nTell me why the exporter metrics are not displayed in the endpoint 0.0.0.0:9598/metrics?\nBelow I attach the configuration and the collected event.\nThanks!\nConfig:\n[sources.client_verification_kafka]\ntype = \"kafka\"\ngroup_id = \"mdm_pos_client_verification_event_router\"\ntopics = [ \"mdm.client_verification\" ]\nbootstrap_servers = \"${VECTOR_KAFKA_SERVER}\"\nsasl.enabled = true\nsasl.mechanism = \"PLAIN\"\nsasl.username = \"${VECTOR_KAFKA_USERNAME}\"\nsasl.password = \"${VECTOR_KAFKA_USER_PASSWORD}\"\ndecoding.codec = \"json\"\n\n\n[transforms.passport_verifyi_event]\ntype = \"filter\"\ninputs = [ \"client_verification_kafka\" ]\ncondition = '.type == \"PassportVerificationResultEvent\"'\n\n[transforms.passport_verifyi_builder_event]\ntype = \"remap\"\ninputs = [ \"passport_verifyi_event\" ]\nsource = \"\"\"\n. = {\n   \"metric\": {\n        \"counter\": {\n         \"value\": 1\n       },\n       \"kind\": \"incremental\",\n       \"name\": \"logins\",\n       \"namespace\": \"passport\",\n       \"timestamp\": now(),\n       \"tags\": {\n         \"status\": .status,\n       }\n   }\n}\n\"\"\"\n\n[sinks.debug_log_remap]\ntype = \"console\"\ninputs = [ \"passport_verifyi_builder_event\" ]\nencoding.codec = \"json\"\n\n\n[sinks.prometheus_summary_passport_verifyi]\ntype = \"prometheus_exporter\"\ninputs = [ \"passport_verifyi_builder_event\" ]\naddress = \"0.0.0.0:9598\"\nBuilded event:\n{\n  \"metric\": {\n    \"counter\": {\n      \"value\": 1\n    },\n    \"kind\": \"incremental\",\n    \"name\": \"logins\",\n    \"namespace\": \"passport\",\n    \"tags\": {\n      \"status\": \"UNKNOWN\"\n    },\n    \"timestamp\": \"2025-01-02T16:09:09.464480881Z\"\n  }\n}",
        "url": "https://github.com/vectordotdev/vector/discussions/22107",
        "createdAt": "2025-01-02T16:13:35Z",
        "updatedAt": "2025-01-02T17:02:28Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "root-aza"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22098,
        "title": "Can metadata from the pod be added to the source file?",
        "bodyText": "Can metadata from the pod be added to the source file? I need to specify which log files to collect in the pod, using the include option, and at the same time, I need the metadata information of the pod for integration with transform. Relying solely on the filename can be very challenging in complex scenarios.",
        "url": "https://github.com/vectordotdev/vector/discussions/22098",
        "createdAt": "2024-12-31T04:53:02Z",
        "updatedAt": "2025-01-03T17:46:50Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Rentu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22104,
        "title": "How does kubernetes log collection enable multiline",
        "bodyText": "Doesn't the \"Kubernetes logs\" component currently support multi-line collection? I couldn't find the configuration in the documentation",
        "url": "https://github.com/vectordotdev/vector/discussions/22104",
        "createdAt": "2025-01-02T02:06:05Z",
        "updatedAt": "2025-01-07T17:53:51Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mumu-lab"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22096,
        "title": "Cloudfoundry Application logs",
        "bodyText": "How can I send Cloudfoundry Application logs to Elasticsearch using Vector?",
        "url": "https://github.com/vectordotdev/vector/discussions/22096",
        "createdAt": "2024-12-30T13:18:48Z",
        "updatedAt": "2025-01-07T17:53:06Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "manuvnair"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22097,
        "title": "Cloudfoundry",
        "bodyText": "Shipping logs from Cloudfoundry",
        "url": "https://github.com/vectordotdev/vector/discussions/22097",
        "createdAt": "2024-12-30T17:12:59Z",
        "updatedAt": "2024-12-30T19:32:49Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "manuvnair"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 8429,
        "title": "Cannot Parse HTTP Error Log with parse_apache_log",
        "bodyText": "Having issues with the parse_apache_log parser doesnt like our error logs, see below for example,\n$ parse_apache_log!(s'[Fri Jul 23 00:28:19.615325 2021] [:error] [pid 17705] ipa: INFO: [jsonserver_session] fred@example.local: idview_show(None, version=u\\'2.237\\'): RequirementError', timestamp_format:\"%a %b %d %H:%M:%S.%6f %Y\", format: \"error\")\nfunction call error for \"parse_apache_log\" at (0:246): failed parsing common log line\nThanks\nTasty",
        "url": "https://github.com/vectordotdev/vector/discussions/8429",
        "createdAt": "2021-07-23T02:10:28Z",
        "updatedAt": "2024-12-24T22:07:52Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tastyfrankfurt"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22034,
        "title": "vector halts after an error from the buffer",
        "bodyText": "Hi folks,\nI have a vector pipeline sourcing data from kafka and persisting data to clickhouse, with hyrid buffer setting. The pipelines halts processing after following error. Restarting vector resumes the processing.\nerror\n{\"log\":{\"error\":\"The reader detected that a data file contains a partially-written record.\",\"error_code\":\"partial_write\",\"error_type\":\"reader_failed\",\"host\":\"96e4807a14f8\",\"internal_log_rate_limit\":true,\"message\":\"Error encountered during buffer read.\",\"metadata\":{\"kind\":\"event\",\"level\":\"ERROR\",\"module_path\":\"vector_buffers::internal_events\",\"target\":\"vector_buffers::internal_events\"},\"pid\":1,\"source_type\":\"internal_logs\",\"stage\":\"processing\",\"timestamp\":\"2024-12-14T22:11:23.461122537Z\",\"vector\":{\"component_id\":\"xx_sink_clickhouse\",\"component_kind\":\"sink\",\"component_type\":\"clickhouse\"}}}\nbuffer setting\n{\n      \"buffer\": [\n        {\n          \"type\": \"memory\",\n          \"max_events\": 250000,\n          \"when_full\": \"overflow\"\n        },\n        {\n          \"type\": \"disk\",\n          \"max_size\": 1073741824\n        }\n      ]\n{",
        "url": "https://github.com/vectordotdev/vector/discussions/22034",
        "createdAt": "2024-12-15T02:33:06Z",
        "updatedAt": "2024-12-21T03:03:02Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "canopenerda"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 5
        },
        "upvoteCount": 1
    },
    {
        "number": 21978,
        "title": "Attempting to upgrade from vector 0.27.0-distroless-libc to new versions",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nI am attempting to go from vector 0.27.0-distroless-libc to 0.42.0-distroless-libc.\u2028\u2028Taking an incremental approach from 0.27.0-distroless-libc to 0.33.0-distroless-libc to 0.38.0-distroless-libc to 0.42.0-distroless-libc we notice memory increases along the way.\nOn version 0.27.0-distroless-libc we saw an average use of 60-80 MiB\u2028\u2028On version 0.33.0-distroless-libc we saw an average use of 105 MiB\nOn both versions 0.38.0-distroless-libc and 0.42.0-distroless-libc we saw an average use of 115-125 MiB.\u2028\u2028Is it expected for memory usage to increase in this manner? Are there any VRL changes that come to mind between these changes that you think I\u2019d have to make?\nVersion\n0.27.0-distroless-libc, 0.33.0-distroless-libc, 0.38.0-distroless-libc, 0.42.0-distroless-libc\nExample Data\nOn version 0.27.0-distroless-libc we saw an average use of 60-80 MiB\u2028\u2028On version 0.33.0-distroless-libc we saw an average use of 105 MiB\nAdditional Context\nVector is running on Kubernetes\nHere's what vector top shows on 0.42.0-distroless-libc",
        "url": "https://github.com/vectordotdev/vector/discussions/21978",
        "createdAt": "2024-12-06T19:24:24Z",
        "updatedAt": "2024-12-20T22:16:48Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "mohdAkibUddin"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 2
    },
    {
        "number": 22063,
        "title": "same source input many sinks problem",
        "bodyText": "same source\uff1a kafka\ndifferent many sinks\uff1adatabend\uff0celasticsearch\nsimple config.yaml:  difference sinks using same source input source-kafka\nvim sources/source-kafka.yaml\ntype: kafka\n\nvim sinks/sink-databend.yaml\ntype: databend\ninputs:\n - source-kafka\n\nvim sinks/sink-es.yaml\ntype: elasticsearch\ninputs:\n - source-kafka\n\nif elasticsearch run abnormal and blocked\uff0cwhich will result  databend sink be blocked\uff0cit is right? reference follow code\uff1a\n\n  \n    \n      vector/src/topology/builder.rs\n    \n    \n         Line 255\n      in\n      029a2ff\n    \n  \n  \n    \n\n        \n          \n           fanout \n        \n    \n  \n\n\n\n  \n    \n      vector/lib/vector-core/src/fanout.rs\n    \n    \n         Line 427\n      in\n      029a2ff\n    \n  \n  \n    \n\n        \n          \n           pending!(); \n        \n    \n  \n\n\nbecausesender_group.send function will pending, if one of sinks run abnormal?\nif not, am i missing something?\nbest wishes",
        "url": "https://github.com/vectordotdev/vector/discussions/22063",
        "createdAt": "2024-12-20T09:19:59Z",
        "updatedAt": "2024-12-23T02:40:45Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "lddlww"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22059,
        "title": "Getting account name by account ID in ELK",
        "bodyText": "Would it be possible to get aws_account_name by account_id from logs and map it to a seperate field in ELK? we are using vector agent",
        "url": "https://github.com/vectordotdev/vector/discussions/22059",
        "createdAt": "2024-12-19T06:41:35Z",
        "updatedAt": "2025-01-03T17:41:48Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Padmashankari"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22053,
        "title": "AWS EKS uses an OIDC provider to assume an IAM Role, but I am encountering a 403 error when trying to connect to OpenSearch.",
        "bodyText": "Hi, I\u2019m currently running Vector in a KES pod and have provided a service account to the pod. I\u2019m confident that the service account in the pod can assume the IAM role provided by the OIDC provider. Here\u2019s the evidence:\nroot@vector-xxx-xxxxxx-wkjvn:/# aws sts get-caller-identity\n{\n    \"UserId\": \"xxxxx:botocore-session-xxxxxx\",\n    \"Account\": \"278722xxxxx\",\n    \"Arn\": \"arn:aws:sts::2787xxxxx:assumed-role/irsa-role-xxxxxg/botocore-session-1xxxxx\"\n}\n\n\nHowever, no matter what I do, I can\u2019t get the OpenSearch permissions to work, and I keep receiving a 403 error.\n\n2024-12-18T11:14:53.351382Z INFO vector::app: Loading configs. paths=[\"/etc/vector\"]\n2024-12-18T11:14:53.371771Z INFO vector::topology::running: Running healthchecks.\n2024-12-18T11:14:53.371831Z INFO vector: Vector has started. debug=\"false\" version=\"0.43.1\" arch=\"x86_64\" revision=\"e30bf1f 2024-12-10 16:14:47.175528383\"\n2024-12-18T11:14:53.371838Z INFO vector::app: API is disabled, enable by setting `api.enabled` to `true` and use commands like `vector top`.\n2024-12-18T11:14:53.371839Z INFO vector::topology::builder: Healthcheck disabled.\n2024-12-18T11:14:59.203183Z ERROR sink{component_kind=\"sink\" component_id=sink_ilm-test-stg-log-forward component_type=elasticsearch}:request{request_id=1}: vector::sinks::elasticsearch::service: Response contained errors. error_code=\"http_response_403\" response=Response { status: 403, version: HTTP/1.1, headers: {\"date\": \"Wed, 18 Dec 2024 11:14:59 GMT\", \"content-type\": \"application/json\", \"access-control-allow-origin\": \"*\", \"connection\": \"keep-alive\"}, body: b\"{\\\"Message\\\":\\\"User: anonymous is not authorized to perform: es:ESHttpPost because no resource-based policy allows the es:ESHttpPost action\\\"}\" }\n2024-12-18T11:14:59.203223Z ERROR sink{component_kind=\"sink\" component_id=sink_ilm-test-stg-log-forward component_type=elasticsearch}:request{request_id=1}: vector::sinks::util::retries: Not retriable; dropping the request. reason=\"client-side error, 403 Forbidden: {\\\"Message\\\":\\\"User: anonymous is not authorized to perform: es:ESHttpPost because no resource-based policy allows the es:ESHttpPost action\\\"}\" internal_log_rate_limit=true\n2024-12-18T11:14:59.203236Z ERROR sink{component_kind=\"sink\" component_id=sink_ilm-test-stg-log-forward component_type=elasticsearch}:request{request_id=1}: vector_common::internal_event::service: Service call failed. No retries or retries exhausted. error=None request_id=1 error_type=\"request_failed\" stage=\"sending\" internal_log_rate_limit=true\n2024-12-18T11:14:59.203246Z ERROR sink{component_kind=\"sink\" component_id=sink_ilm-test-stg-log-forward component_type=elasticsearch}:request{request_id=1}: vector_common::internal_event::component_events_dropped: Events dropped intentional=false count=1 reason=\"Service call failed. No retries or retries exhausted.\" internal_log_rate_limit=true\n\n[sinks.sink_{{ .name }}]\n    type = \"elasticsearch\"\n    api_version= \"v8\"\n    inputs = [\"my_transform\"]\n    healthcheck = false\n    acknowledgements.enabled= true\n    bulk.index = \"vector-{{ $.Release.Name }}-{{ .name }}-%Y-%m\"\n    endpoints = [\n          xxxxxxxx.xxxx.xxxx\n    ]\n\nI tried setting the environment variable VECTOR_LOG=\"aws_config=debug\", but the pod fails to display logs properly. What should I do next to troubleshoot this issue?\nhttps://github.com/vectordotdev/vector/blob/0a2dc2bafa6e56218797a0c238118ed58fd94113/website/cue/reference/components/aws.cue\nis not work?\n\nIn Web IdentityToken credentials from the environment or container (including EKS). These credentials will automatically refresh when expired.",
        "url": "https://github.com/vectordotdev/vector/discussions/22053",
        "createdAt": "2024-12-18T11:39:29Z",
        "updatedAt": "2024-12-18T15:56:33Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ericshiu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22051,
        "title": "does http_client source support json body in post method\uff1f",
        "bodyText": "i found http_client source support method\uff0c but can't find body parameter when method is post\nfor example\uff1a\ncurl -u root: \\\n  --request POST \\\n  '127.0.0.1:8001/v1/query/' \\\n  --header 'Content-Type: application/json' \\\n  --data-raw '{\"sql\": \"SELECT avg(number) FROM numbers(100000000)\"}'",
        "url": "https://github.com/vectordotdev/vector/discussions/22051",
        "createdAt": "2024-12-18T09:34:29Z",
        "updatedAt": "2024-12-18T15:51:49Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "lddlww"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22042,
        "title": "_RESERVED_service help needed",
        "bodyText": "Regarding logs going to datadog. How do I get rid of a previously defined service and add my own .service field without it being called _RESERVED_service. when I look at the raw datastructure coming into datadog I see a tags: [ ] section that has a service in it, then I'm trying to add a field called .service. When I add my field called .service it is renamed to _RESERVED_service, because one already exists??? I guess...\nAny help appreciated.",
        "url": "https://github.com/vectordotdev/vector/discussions/22042",
        "createdAt": "2024-12-16T21:15:53Z",
        "updatedAt": "2024-12-17T23:27:29Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bsteve99"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 22045,
        "title": "Vector File Source to SEQ Forward",
        "bodyText": "I have following vector.yaml file. My target is to pickup a file contents and push them to SEQ via http. One of the sinks in the below is the console. I have taken the console JSON file and pushed it to SEQ forwarder (SEQ) via Postman. And the event seems to be happily created. But when pushed through vector i receive errors\nERRORs from VECTOR:\n2024-12-17T09:08:12.124624Z ERROR sink{component_kind=\"sink\" component_id=send_to_seq component_type=http}:request{request_id=1}: vector::sinks::util::retries: Not retriable; dropping the request. reason=\"Http status: 400 Bad Request\" internal_log_rate_limit=true\n2024-12-17T09:08:12.125611Z ERROR sink{component_kind=\"sink\" component_id=send_to_seq component_type=http}:request{request_id=1}: vector_common::internal_event::service: Service call failed. No retries or retries exhausted. error=None request_id=1 error_type=\"request_failed\" stage=\"sending\" internal_log_rate_limit=true\n2024-12-17T09:08:12.128592Z ERROR sink{component_kind=\"sink\" component_id=send_to_seq component_type=http}:request{request_id=1}: vector_common::internal_event::component_events_dropped: Events dropped intentional=false count=1 reason=\"Service call failed. No retries or retries exhausted.\" internal_log_rate_limit=true\n\nERRORS FROM SEQ\n{\"@t\":\"2024-12-17T09:08:12.1220557Z\",\"@m\":\"Rejecting payload due to invalid JSON, request body could not be parsed\",\"@i\":\"7b7ae0e2\",\"@l\":\"Debug\",\"@x\":\"Newtonsoft.Json.JsonSerializationException: Deserialized JSON type 'Newtonsoft.Json.Linq.JArray' is not compatible with expected type 'Newtonsoft.Json.Linq.JObject'. Path '', line 1, position 363.\\r\\n   at Newtonsoft.Json.Serialization.JsonSerializerInternalReader.CreateJToken(JsonReader reader, JsonContract contract)\\r\\n   at Newtonsoft.Json.Serialization.JsonSerializerInternalReader.CreateValueInternal(JsonReader reader, Type objectType, JsonContract contract, JsonProperty member, JsonContainerContract containerContract, JsonProperty containerMember, Object existingValue)\\r\\n   at Newtonsoft.Json.Serialization.JsonSerializerInternalReader.Deserialize(JsonReader reader, Type objectType, Boolean checkAdditionalContent)\\r\\n   at Newtonsoft.Json.JsonSerializer.DeserializeInternal(JsonReader reader, Type objectType)\\r\\n   at Seq.Forwarder.Web.Api.IngestionController.IngestRawFormat() in C:\\\\projects\\\\seq-forwarder\\\\src\\\\Seq.Forwarder\\\\Web\\\\Api\\\\IngestionController.cs:line 85\",\"@tr\":\"b9ce0f00bfabb750ca80e821189b2b28\",\"@sp\":\"b80d4c44681015c8\",\"ClientHostIP\":\"127.0.0.1\",\"ActionId\":\"1f7d2331-3067-4c83-add2-414b47a6c9d3\",\"ActionName\":\"Seq.Forwarder.Web.Api.IngestionController.Ingest (seqfwd)\",\"RequestId\":\"0HN8RI00BAD46:00000001\",\"RequestPath\":\"/api/events/raw\",\"ConnectionId\":\"0HN8RI00BAD46\",\"MachineName\":\"DESKTOP-RNVC4PD\",\"Application\":\"Seq Forwarder\"}\nIt seems like the JSON format is not compatible, but i took the same json and pushed it via postman it works. Let me know if someone has a solution for this\nSources\nsources:\ncommand_logs:\ntype: file\ninclude: [\"C:/test/CommandLog0.txt\"]\nvector_logs:\ntype: internal_logs\nTransforms\ntransforms:\nclean_message:\ntype: lua\ninputs: [\"command_logs\"]\nversion: \"2\"\nhooks:\nprocess: process\nsource:  |\nfunction process(event, emit)\nlocal message = event.log.message\nif message then\n-- Remove null characters from the message\nmessage = message:gsub(\"%z\", \"\")\n-- Construct the new event with explicit order\nlocal clean_event = {\nEvents = {\n{\n-- Timestamp first\nTimestamp = os.date(\"!%Y-%m-%dT%H:%M:%S\") .. \".000Z\",\nLevel = \"Information\",\nMessageTemplate = \"ABC LOG\",\nProperties = {\nmessage = message\n}\n}\n}\n}\n-- Emit the new cleaned-up event\nemit({ log = clean_event })\nend\nend\nSinks\nsinks:\nclean_message_output:\ntype: console\ninputs: [\"clean_message\"]\nencoding:\ncodec: json\njson:\npretty: true\nsend_to_seq:\ntype: http\ninputs: [\"clean_message\"]\nuri: \"http://localhost:15341/api/events/raw?apikey=ABC\" # Correct endpoint for SEQ\nmethod: \"post\"\nencoding:\ncodec: json\nbatch:\nmax_events: 1   # Send each log as an individual event\ntimeout_secs: 1 # Minimum delay for batching\nrequest:\nrate_limit_num: 1\nrate_limit_duration_secs: 1\nheaders:\nContent-Type: \"application/json\"`",
        "url": "https://github.com/vectordotdev/vector/discussions/22045",
        "createdAt": "2024-12-17T09:18:26Z",
        "updatedAt": "2025-01-20T08:03:16Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ghaffar3"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22024,
        "title": "SQS configure multiple consumers",
        "bodyText": "Consume data from S3 to Kafka. If I configure multiple vectors, will it be consumed repeatedly?",
        "url": "https://github.com/vectordotdev/vector/discussions/22024",
        "createdAt": "2024-12-12T04:47:46Z",
        "updatedAt": "2024-12-13T12:34:48Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "lhh528634141"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22028,
        "title": "parse alb logs failed",
        "bodyText": "source log\ntls 2.0 2024-12-13T07:54:19 net/data-saas-open-api/22d5cbdd8729cb7dad e90f3ddddd565d3d3 17.177.32.66:609445 10.15.16.92:443 28 - 0 0 42 - - - - - - - - - 2024-12-13T07:54:19\n\nerror logs\nfunction call error for \\\"parse_aws_alb_log\\\" at (4:41): failed to get field `client_host`: Parsing Error: (\\\"net/data-saas-open-api/22d5cb8729cb7dad e90f395b0565d3d3 18.177.32.66:60944 10.255.16.92:443 28 - 0 0 42 - - - - - - - - - 2024-12-13T07:54:19\\\", TakeWhile1)",
        "url": "https://github.com/vectordotdev/vector/discussions/22028",
        "createdAt": "2024-12-13T08:32:50Z",
        "updatedAt": "2024-12-13T10:40:52Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "lhh528634141"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18791,
        "title": "Timestamp literal in tests",
        "bodyText": "Hello,\nI got the following vector.yaml configuration:\nsources:\n  vector_internal_logs:\n    type: \"internal_logs\"\n\ntransforms:\n  vector_internal_format:\n    type: \"remap\"\n    inputs:\n      - \"vector_internal_logs\"\n    source: |-\n      . = {\n          \"time\": format_timestamp!(.timestamp, format: \"%FT%T%.3f\"),\n          \"message\": .message\n      }\n\nsinks:\n  console:\n    type: \"console\"\n    inputs:\n      - \"vector_internal_format\"\n    encoding:\n      codec: \"json\"\n\ntests:\n  - name: \"vector_internal_format\"\n    inputs:\n      - insert_at: \"vector_internal_format\"\n        type: \"log\"\n        log_fields:\n          timestamp: t'2023-10-05T12:44:54.345262416Z'\n          message: \"Some message\"\n\n    outputs:\n      - extract_from: \"vector_internal_format\"\n        conditions:\n          - type: \"vrl\"\n            source: |-\n              assert_eq!(.time, \"2023-10-05T12:44:54.345\", \"time invalid\")\n\n\nI want to test vector_internal_format transformation and I don't know how to specify timestamp input (under log_fields) to be of a type Timestamp (so that format_timestamp function would not throw an error: expected timestamp, got string).",
        "url": "https://github.com/vectordotdev/vector/discussions/18791",
        "createdAt": "2023-10-06T08:53:52Z",
        "updatedAt": "2024-12-13T08:15:20Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tadej0mins"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 3
    },
    {
        "number": 21995,
        "title": "Number do not match for log entries and log to metrics count.",
        "bodyText": "Sending 2 log entries to datadog and converting one of them to a metrics using vector (would like to stop sending log entries to datadog).\nWhen comparing the count between the two, there are 13k log enties and only 2k metrics. Trying to understand why the difference.\nWondering if vector is rate limiting anything that I do not see.\nSnippet from vector config file.\ncustomConfig:\n  sources:\n    datadog_agent:\n      type: datadog_agent\n      address: 0.0.0.0:8282\n      multiple_outputs: true\n\n  transforms: \n    filter_for_tp_stop:\n      type: filter\n      inputs:\n        - datadog_agent.logs\n      condition: 'contains!(.message, \"tp-stop\")'\n\n    tp_log_remap:\n      type: remap\n      inputs:\n        - filter_for_tp_stop\n      source: |-\n        .message = replace!(.message,\"<\",\"\")\n        .message = replace(.message,\">\",\"\")\n        .message = replace(.message,\"-\",\"_\")\n        .message = replace(.message,\"{\",\" \")\n        .message = replace(.message,\"}\",\" \")\n        .message = replace(.message,\",\",\" \")\n        .message = parse_key_value!(.message, field_delimiter: \" \", key_value_delimiter: \":\")\n        .message.message = parse_key_value!(.message.message, field_delimiter: \" \", key_value_delimiter: \":\")\n\n    filter_tp_stop:\n      type: filter\n      inputs:\n        - tp_log_remap\n      condition: 'exists(.message.message.stop)'\n\n    filter_logs_ex0:\n      type: filter\n      inputs:\n        - filter_tp_stop\n      condition: '.message.message.ex == \"0\"'\n\n    datadog_logs_to_metrics_ex0:\n      type: log_to_metric\n      inputs:\n        - filter_logs_ex0\n      metrics:\n        - type: counter\n          field: .message.message\n          name: wwex.metric.tp.ex_none\n          tags:\n            act: |-\n              {{ print \"{{.message.message.act}}\" }}\n            service: |-\n              {{ print \"{{.service}}\" }}\n            ex: |-\n              {{ print \"{{.message.message.ex}}\" }}\n\n    metric_add_tags:\n      type: remap\n      inputs:\n        - datadog_logs_to_metrics_ex0\n      source: |-\n        .tags.processed_by = \"vector\"\n\n  sinks:\n    datadog_metrics:\n      type: datadog_metrics\n      inputs:\n        - metric_add_tags\n      default_api_key: ${DATADOG_API_KEY}\n\nAttaching sample log entries.\nlogs.json",
        "url": "https://github.com/vectordotdev/vector/discussions/21995",
        "createdAt": "2024-12-09T23:17:35Z",
        "updatedAt": "2024-12-11T15:11:45Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "LewisWWEX"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 22005,
        "title": "Correct HTTP Server source settings for newline delimted JSON?",
        "bodyText": "I've set-up and HTTP Server source that I want to push JSON log events to. They're being sent in batches with one event per line with a LF newline terminator. e.g.\n{\"id\":\"00227a8ec90a4dc0820100000000009d\",\"remoteIP\":\"113.212.87.42\"}\n{\"id\":\"00227a8ec90a4dc0820100000000009e\",\"remoteIP\":\"113.212.87.42\"}\n{\"id\":\"015332cbaf5545d8917a000000000001\",\"remoteIP\":\"201.123.133.189\"}\nI'm pushing them to the HTTP endpoint using the following cURL:\ncurl -X POST -vk \"http://127.0.0.1:8002\" \\\n     -H \"Content-Type: application/json\" \\\n     -H \"x-source: signal-source\" \\\n     --data @data.json\n\nMy source is configured as:\nsources:\n  http_server_json_flattened:\n    type: http_server\n    address: 0.0.0.0:8002\n    headers:\n      - x-source\n    compression: auto\n    decoding:\n      codec: json\n    framing:\n      method: newline_delimited\nWith the above, I get the error message:\n{\"code\":400,\"message\":\"Failed decoding body: ParsingError(Error parsing JSON: Error(\\\"trailing characters\\\", line: 1, column: 3357))\"}%\nWhich I'm interpreting that it's trying to parse the entire input as JSON and because it's not well formed (being one per line) its failing. This seems to go in the face of my understanding of the config options where framing tells it how to split/find each log event and decoding is the format of each log event.\nSecond attempt I switched the decoding to bytes and try to use vrl to parse it.\nsources:\n  http_server_json_flattened:\n    type: http_server\n    address: 0.0.0.0:8002\n    headers:\n      - x-source\n    compression: auto\n    decoding:\n      codec: bytes\n    framing:\n      method: newline_delimited\n\ntransforms:\n  parse_logs:\n    type: \"remap\"\n    inputs:\n      - http_server_json_flattened\n    source: |\n      ., err = parse_json(.message)\n      if err != null {\n        log(\"Unable to parse message as JSON.\", level: \"error\", 0)\n        abort\n      }\nWhich results in:\nERROR transform{component_kind=\"transform\" component_id=parse_logs component_type=remap}: vrl::stdlib::log::implementation: Unable to parse message as JSON. internal_log_rate_secs=0 vrl_position=49\nLooking at the output of the HTTP sever, I can see that the .message field contains my entire file escaped and hasn't been split into individual events by the newline delimiter.\nCoincidentally, the AWS S3 source when set-up the same way has no problem reading AWS WAFv2 logs which are stored on S3 in the exact same format?\nAt this point I'm at a bit of loss???",
        "url": "https://github.com/vectordotdev/vector/discussions/22005",
        "createdAt": "2024-12-10T15:52:04Z",
        "updatedAt": "2024-12-10T17:26:25Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "NeilJed"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21997,
        "title": "Reduce issue with certain labels",
        "bodyText": "Hey!\nI've been trying to experiment with getting the reduce transform to work with the kubernetes_logs source since I want to parse multiline logs. I notice that when I run on the latest version v0.43.0, I get errors like\n2024-12-10T01:21:04.356278Z ERROR transform{component_kind=\"transform\" component_id=reduce component_type=reduce}: vector::internal_events::reduce: Event field could not be reduced. path=KeyString(\"kubernetes.pod_labels.kubernetes.io/metadata.name\") error=InvalidPathSyntax { path: \"kubernetes.pod_labels.kubernetes.io/metadata.name\" } error_type=\"condition_failed\" stage=\"processing\" internal_log_rate_limit=true\n2024-12-10T01:21:04.356316Z ERROR transform{component_kind=\"transform\" component_id=reduce component_type=reduce}: vector::internal_events::reduce: Internal log [Event field could\n\nI think it's because some of the labels from the kubernetes info aren't readable from the reduce perspective?\nI found previous discussions like this one where similar issues are brought up, but it seems like the tracking issues were closed a couple months ago, so the latest version should have the fix?\nWhat's interesting is that when I use v0.41.0, it does work. Did something change between these versions?\nRepro\nI was using the kubernetes source, but came up with a toy example that simplifies it down a lot and be reproduced even without kubernetes\nConfig\n{\n\t\"sources\": {\n                \"in\": {\n                        \"type\": \"file\",\n                        \"ignore_checkpoints\": true,\n\t\t\t\"data_dir\": \"{DATA_DIR}\",\n\t\t\t\"include\": [\"{PATH}/multiline_example.txt\"]\n                }\n        },\n\t\"transforms\": {\n\t\t\"add_labels\": {\n\t\t\t\"type\": \"remap\",\n\t\t\t\"inputs\": [\"in\"],\n\t\t\t\"source\": \".kubernetes = {\\\"pod_labels\\\": {\\\"kubernetes.io/metadata.name\\\": \\\"a\\\"}}\"\n\t\t},\n\t\t\"reduce\": {\n\t\t\t\"type\": \"reduce\",\n\t\t\t\"inputs\": [\"add_labels\"],\n\t\t\t\"starts_when\": {\n\t\t\t\t\"type\": \"vrl\",\n\t\t\t\t\"source\": \"match!(.message, r'^[^\\\\s]')\"\n\t\t\t},\n\t\t\t\"merge_strategies\": {\n\t\t\t\t\"message\": \"concat_newline\"\n\t\t\t}\n\t\t}\n\t},\n        \"sinks\": {\n                \"out\": {\n                        \"type\": \"console\",\n                        \"inputs\": [\"reduce\"],\n                        \"encoding\": {\n                                \"codec\": \"json\"\n                        }\n                }\n        }\n}   \n\nFile (multiline_example.txt)\nThis is line one\n\tThis is line two\nThis is line three\n\nLogs/result on v0.41\n...\n2024-12-10T01:29:01.285705Z  INFO source{component_kind=\"source\" component_id=in component_type=file}: vector::sources::file: Starting file server. include=[\"/Users/andrew.ma/vector-conf/multiline_example.txt\"] exclude=[]\n2024-12-10T01:29:01.286294Z  INFO source{component_kind=\"source\" component_id=in component_type=file}:file_server: file_source::checkpointer: Loaded checkpoint data.\n2024-12-10T01:29:01.286796Z  INFO source{component_kind=\"source\" component_id=in component_type=file}:file_server: vector::internal_events::file::source: Found new file to watch. file=/Users/andrew.ma/vector-conf/multiline_example.txt\n{\"file\":\"/Users/andrew.ma/vector-conf/multiline_example.txt\",\"host\":\"YFWLXXW2GN\",\"kubernetes\":{\"pod_labels\":{\"kubernetes.io/metadata.name\":\"a\"}},\"message\":\"This is line one\\n\\tThis is line two\",\"source_type\":\"file\",\"timestamp\":\"2024-12-10T01:29:01.287097Z\",\"timestamp_end\":\"2024-12-10T01:29:01.287124Z\"}\n...\n\nLogs/result on v0.43\n...\n2024-12-10T01:29:04.723601Z  INFO source{component_kind=\"source\" component_id=in component_type=file}: vector::sources::file: Starting file server. include=[\"/Users/andrew.ma/vector-conf/multiline_example.txt\"] exclude=[]\n2024-12-10T01:29:04.724043Z  INFO source{component_kind=\"source\" component_id=in component_type=file}:file_server: file_source::checkpointer: Loaded checkpoint data.\n2024-12-10T01:29:04.724439Z  INFO source{component_kind=\"source\" component_id=in component_type=file}:file_server: vector::internal_events::file::source: Found new file to watch. file=/Users/andrew.ma/vector-conf/multiline_example.txt\n2024-12-10T01:29:04.724798Z ERROR transform{component_kind=\"transform\" component_id=reduce component_type=reduce}: vector::internal_events::reduce: Event field could not be reduced. path=KeyString(\"kubernetes.pod_labels.kubernetes.io/metadata.name\") error=InvalidPathSyntax { path: \"kubernetes.pod_labels.kubernetes.io/metadata.name\" } error_type=\"condition_failed\" stage=\"processing\" internal_log_rate_limit=true\n2024-12-10T01:29:04.724836Z ERROR transform{component_kind=\"transform\" component_id=reduce component_type=reduce}: vector::internal_events::reduce: Internal log [Event field could not be reduced.] is being suppressed to avoid flooding.\n{\"file\":\"/Users/andrew.ma/vector-conf/multiline_example.txt\",\"host\":\"YFWLXXW2GN\",\"message\":\"This is line one\\n\\tThis is line two\",\"source_type\":\"file\",\"timestamp\":\"2024-12-10T01:29:04.724642Z\",\"timestamp_end\":\"2024-12-10T01:29:04.724662Z\"}\n...\n\nI also tried switching it from kubernetes.io/metadata.name to kubernetes_io/metadata_name but that also doesn't seem to work (as in it works for v0.41 but not v0.43",
        "url": "https://github.com/vectordotdev/vector/discussions/21997",
        "createdAt": "2024-12-10T01:35:24Z",
        "updatedAt": "2024-12-10T01:39:14Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "andrewma2"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21994,
        "title": "Internal Metrics transforms question",
        "bodyText": "Hi All!\nIm trying to utilize the internal_metrics source and send that externally but it appears it does not behave similar to other sources when using transforms.  Like I pipe internal_metrics > default_transform (adds field) > console that field is not there.  Whne I use the exact same transforms on the demo_log source it works as execpted.  Any guidance?\nTest config for replicate.\nsources:\n  demo_log:\n    type: demo_logs\n    format: apache_common\n\n  internal_metrics:\n    type: \"internal_metrics\"\n\n\ntransforms:\n  default_transform:\n    type: remap\n    inputs:\n      - \"internal_metrics\"\n    source: >-\n      .app_id = \"100000000\"\n\nsinks:\n  console:\n    type: console\n    inputs:\n      - \"default_*\"\n    target: stdout\n    encoding:\n      codec: json",
        "url": "https://github.com/vectordotdev/vector/discussions/21994",
        "createdAt": "2024-12-09T20:06:45Z",
        "updatedAt": "2024-12-09T20:41:25Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "satellite-no"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21968,
        "title": "error=service error with CloudWatch Metrics sink",
        "bodyText": "Hi, I have a Vector Aggregator deployment in ECS/Fargate, which receives logs and metrics from other Vector deployments and sends them to CloudWatch. This works well for logs, however when deploying using metrics I get the following error:\n2024-12-05T17:04:12.628092Z ERROR sink{component_kind=\"sink\" component_id=cloudwatch_metrics component_type=aws_cloudwatch_metrics}:request{request_id=1551}: vector::sinks::util::sink: Request failed. error=service error\n\nThis error starts pretty much as soon as the cluster starts up. Sometimes some metrics make it into Cloudwatch and sometimes they don't (one hypothesis is that they may have started reaching cloudwatch after I reduced the rate limit, but that hasn't stopped the error).\nMy config is as below:\n[sources.vector]\naddress = \"0.0.0.0:6000\" \ntype = \"vector\"\n\n[transforms.vector_logs_metrics]    \ninputs = [ \"vector\" ]\ntype = \"route\"\nroute.logs = { type = \"is_log\" }\nroute.metrics = { type = \"is_metric\" }\n\n[sinks.cloudwatch_metrics]\ntype = \"aws_cloudwatch_metrics\"\ndefault_namespace = \"default-namespace\"\nrequest.rate_limit_num = 50\ninputs = [\"vector_logs_metrics.metrics\"]\n\nThere isn't much to go on on this error, so I'm wondering if anyone has seen anything similar or has some steps to help debug?\nVector version: 0.42.0",
        "url": "https://github.com/vectordotdev/vector/discussions/21968",
        "createdAt": "2024-12-05T17:26:02Z",
        "updatedAt": "2024-12-09T11:52:40Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "PeteLeeds"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 2
    },
    {
        "number": 21950,
        "title": "Aggregate method for kubernetes_logs",
        "bodyText": "Hello everyone,\nI was wondering whether i could aggregate the kubernetes_logs of /var/log/pod/../../0.log in source. I want at the produced json the message part to hold more than one entry. For example, these two entries, i want them both in the message of one json, and not different. Java splits the two lines even though the\nERROR 2024-12-03T10:53:16.259Z testAPI com.controller.CustomRestExceptionHandler Application - Error\n2024-12-03T10:53:16.259Z org.springframework.security.access.AccessDeniedException: Access is denied\nBest Regards,\nGiannis",
        "url": "https://github.com/vectordotdev/vector/discussions/21950",
        "createdAt": "2024-12-04T13:55:46Z",
        "updatedAt": "2024-12-04T18:17:59Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "GiannisKosm"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21943,
        "title": "Backpressure for Redis sink",
        "bodyText": "I'm in the process of evaluating Vector as a replacement for a logstash instance I'm having. One of the things we do is dump events into a Redis list using their builtin support. Vector can do the same thing according to the documentation, but there's one piece missing to be a drop-in replacement for us.\nLogstash allows us to configure this output (\"sink\") to be limited to a number of items in the Redis list, effectively allowing us to make sure that\n\nthe Redis instance won't explode if the consumer dies\ngive us backpressure in logstash\n\nIt seems Vector's Redis sink doesn't support this. Would there be any other way for me to make this work?",
        "url": "https://github.com/vectordotdev/vector/discussions/21943",
        "createdAt": "2024-12-04T03:26:36Z",
        "updatedAt": "2024-12-04T18:15:49Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "darklajid"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21951,
        "title": "Where is the CHANGELOG for 0.43.0?",
        "bodyText": "It has been many (many) hours since the 0.43.0 release happened. The release notes' link is still broken. Any news about that?",
        "url": "https://github.com/vectordotdev/vector/discussions/21951",
        "createdAt": "2024-12-04T14:49:20Z",
        "updatedAt": "2024-12-04T16:57:13Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "shantanugadgil"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 21947,
        "title": "Get http 200 response code from sink opensearch, but contains errors",
        "bodyText": "I encountered this issue intermittently and now it results in vector top error counter, how could I solve this?\nWhile I am using vector as aggregator ingest the logs from kafka to opensearch.\nError info: vector::sinks::elasticsearch::service: Response contained errors. error_code=\"http_response_200\nAny ideas would be appreciated.\n[sinks.opensearch]\n  type = \"elasticsearch\"\n  inputs = [ \"extract_file\" ]\n  api_version = \"v7\"\n  endpoints = [ \"https://localhost:9200\" ]\n  bulk.index = \"logs-{{ index_name }}-%Y-%m\"\n  mode = \"bulk\"\n  batch.max_events = 5000\n  batch.max_bytes = 262144\n  compression = \"zstd\"\n  auth.strategy = \"basic\"\n  auth.user = \"vector\"\n  auth.password = \"***\"\n  tls.verify_certificate = false",
        "url": "https://github.com/vectordotdev/vector/discussions/21947",
        "createdAt": "2024-12-04T10:29:02Z",
        "updatedAt": "2024-12-04T16:32:51Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "7czl"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 15977,
        "title": "Performance",
        "bodyText": "How are people finding vectors performance?\nWe have been running vector for quite a while now and quite like it. However we can't help but note it's not exactly performant.\n\n\nI've created a few issues with areas noted for improvements over the past year. Is the current focus strictly moving towards new features or is there room for a performance orientated milestone release? It seems like one would go a miss.\n\n\nWould it be helpful to receive some perf reports?\n\n\n+    7.13%     7.05%  vector-worker  vector              [.] _rjem_sdallocx                                                                                                                                        `\n+    7.10%     0.00%  vector-worker  [unknown]           [.] 0x0000000000000001                                                                                                                                    a\n+    4.40%     4.29%  vector-worker  vector              [.] _rjem_malloc                                                                                                                                          a\n+    3.35%     3.31%  vector-worker  vector              [.] metrics_tracing_context::TracingContext<R,F>::enhance_key::{{closure}}::{{closure}}                                                                   a\n+    2.78%     0.00%  vector-worker  [unknown]           [k] 0000000000000000                                                                                                                                      a\n+    2.75%     0.00%  vector-worker  [unknown]           [.] 0x0000000000000008                                                                                                                                    a\n+    2.74%     0.00%  vector-worker  [unknown]           [.] 0x7669656365725f74                                                                                                                                    a\n+    2.73%     2.69%  vector-worker  vector              [.] metrics::key::generate_key_hash                                                                                                                       a\n+    2.65%     2.63%  vector-worker  vector              [.] <metrics::common::KeyHasher as core::hash::Hasher>::write                                                                                             a\n+    2.62%     0.00%  vector-worker  libc-2.31.so        [.] 0x00007fd457f35315                                                                                                                                    a\n+    2.54%     2.54%  vector-worker  libc-2.31.so        [.] 0x0000000000161315                                                                                                                                    a\n+    2.51%     2.37%  vector-worker  vector              [.] <core::hash::sip::Hasher<S> as core::hash::Hasher>::write                                                                                             a\n+    2.26%     0.00%  vector-worker  [unknown]           [.] 0x000000000000001e                                                                                                                                    a\n+    2.23%     2.23%  vector-worker  vector              [.] <vector_core::event::array::EventArray as vector_core::event::estimated_json_encoded_size_of::EstimatedJsonEncodedSizeOf>::estimated_json_encoded_sizea\n+    2.04%     1.98%  vector-worker  vector              [.] vector_core::metrics::recorder::Registry::visit_metrics                                                                                               a\n+    2.01%     1.93%  vector-worker  vector              [.] vector_core::event::vrl_target::precompute_metric_value                                                                                               a\n+    1.96%     1.94%  vector-worker  vector              [.] alloc::collections::btree::map::BTreeMap<K,V,A>::bulk_build_from_sorted_it\n\nMy first finding from that is that (if I'm reading that right)\nOn a system with vector is sitting at 41% CPU memory allocation is costing 11.5%?\nThis is a simple kubernetes_logs -> vector configuration with no real processing. promethetheus_metrics (cardinality limited, not that metric cardinality seems to be the issue here)\n[transforms.internal_metrics_remove_tags]\n    type = \"remap\"\n    inputs = [\"internal_metrics\"]\n    source = '''\n    del(.tags.file)\n    del(.tags.pod_name)\n    '''\n[transforms.internal_metrics_cardinality]\n    type = \"tag_cardinality_limit\"\n    inputs = [ \"internal_metrics_remove_tags\" ]\n    mode = \"exact\"\n    value_limit = 50\n\nA configuration sample can be provided.",
        "url": "https://github.com/vectordotdev/vector/discussions/15977",
        "createdAt": "2023-01-17T06:19:54Z",
        "updatedAt": "2024-12-04T16:07:06Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "splitice"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 11
        },
        "upvoteCount": 2
    },
    {
        "number": 21949,
        "title": "Understanding vector cknowledgements and Kafka source configuration options for reliability",
        "bodyText": "Hi Vector Community,\nI'm looking for guidance on how vectors acknowledgements work especially in relation the Kafka source and librdkafka settings.\nGoal is to maximize reliability and ensure no messages are lost, especially during restarts or scaling events with a high topic lag.\nI recently experienced a loss off events after I had to restart the vector consumer group with an updated configuration while we had a significant lag in the topic and I am not as deep in the subject yet as I would like to be.\nat this point the source configuration was pretty basic, relying mostly on the vector default:\n[sources.kafka]\ntype = \"kafka\"\nbootstrap_servers = \"kafka:9092\"\ngroup_id = \"infra-vector\"\nWith no specific librdkafka options aside SSL options.\nOne of the downward sinks encountered an issue that stopped the complete event flow and lead to a topic lag of a few million events. So reconfigured the transform/sink and restarted the consumer group. My understand was that with global acknowledgements and default consumer group topic offset settings it would reprocess the events in the topic. Even with dynamic group member ids, as we have a non static deployment scaled by a HPA.\nBut after the restart the topic lag dropped instantly and the events were not reprocessed and indexed to the target elastic.\nMy questions are:\nDoes the Kafka source wait for downstream components (e.g., sinks) to confirm processing before committing offsets, or does it commit offsets independently of sink success?\nOptimizing Kafka Source for Reliability:\nI would be over the moon to get insights about the recommended settings to gain maximum reliability, as I am not sure how especially librdkafka options interact with the acknowledgement mechanism.\nDoes commit_interval_ms interact with enable.auto.commit?\nIs it still recommended to set auto offset reset to \"earliest\" if the priority is reliability compared to possible duplicate events in elastic?\nWould increasing the interval improve reliability, or is it primarily for performance optimization?\nIf Vector crashes or a downstream sink fails (e.g., Elasticsearch or HTTP), how does Vector handle acknowledgements and ensure messages are not lost?\nWhat are the best practices for configuring Vector and librdkafka to ensure that unprocessed messages are not acknowledged prematurely?\nGeneral Best Practices:\nAre there any additional librdkafka settings or Vector configurations you recommend for achieving maximum reliability in a high-throughput Kafka-to-Vector pipeline?\nAny insights, detailed explanations, or recommendations would be greatly appreciated!\nAnd sorry for the wall of text and maybe stupid questions. I am a kind of \"advocate\" for vector in our current project (trying to get rid of as much logstash as possible) but I don't have an extensive kafka background/knowledge.\nThanks again!",
        "url": "https://github.com/vectordotdev/vector/discussions/21949",
        "createdAt": "2024-12-04T13:32:30Z",
        "updatedAt": "2024-12-04T13:35:02Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "kgorskowski"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 21921,
        "title": "Filter to drop based on condition not working",
        "bodyText": "Hello,\nI have this transformation applied in my values.yaml:\ndrop_kubelet_removestalestate:\n  type: filter\n  inputs:\n    - datadog_agent.logs\n  condition: 'contains(string!(.message), \"RemoveStaleState\") == false'\n\nWhat I want to do is drop all logs that have the \"RemoveStaleState\" string in the message.\nHere is an example event of what I want to drop:\n{\n  \"ddsource\": \"kubelet\",\n  \"ddtags\": \"\",\n  \"hostname\": \"i-0fdf4636c7521e97c\",\n  \"message\": \"{\\\"journald\\\":{\\\"PRIORITY\\\":\\\"6\\\",\\\"SYSLOG_FACILITY\\\":\\\"3\\\",\\\"SYSLOG_IDENTIFIER\\\":\\\"kubelet\\\",\\\"_BOOT_ID\\\":\\\"7f773d2c40724ce294bae234f80c82da\\\",\\\"_CAP_EFFECTIVE\\\":\\\"1ffffffffff\\\",\\\"_CMDLINE\\\":\\\"/usr/bin/kubelet --config /etc/kubernetes/kubelet/kubelet-config.json --kubeconfig /var/lib/kubelet/kubeconfig --container-runtime-endpoint unix:///run/containerd/containerd.sock --image-credential-provider-config /etc/eks/image-credential-provider/config.json --image-credential-provider-bin-dir /etc/eks/image-credential-provider --node-ip=abstractedforsecurity --pod-infra-container-image=602401143452.dkr.ecr.eu-west-1.amazonaws.com/eks/pause:3.5 --v=2 --hostname-override=ip-10-112-119-57.eu-west-1.compute.internal --cloud-provider=external --node-labels=karpenter.sh/capacity-type=on-demand,karpenter.sh/nodepool=arm64 --max-pods=17 --eviction-soft=nodefs.available\\\\u003c20% --eviction-soft-grace-period=nodefs.available=1m0s --eviction-max-pod-grace-period=120\\\",\\\"_COMM\\\":\\\"kubelet\\\",\\\"_EXE\\\":\\\"/usr/bin/kubelet\\\",\\\"_GID\\\":\\\"0\\\",\\\"_HOSTNAME\\\":\\\"abstractedforsecurity\\\",\\\"_MACHINE_ID\\\":\\\"ec2d00c9fc328b9a9a6335b682d5d75e\\\",\\\"_PID\\\":\\\"1547\\\",\\\"_STREAM_ID\\\":\\\"45b436960b6549acbf97fb1e711cfa67\\\",\\\"_SYSTEMD_CGROUP\\\":\\\"/runtime.slice/kubelet.service\\\",\\\"_SYSTEMD_SLICE\\\":\\\"runtime.slice\\\",\\\"_SYSTEMD_UNIT\\\":\\\"kubelet.service\\\",\\\"_TRANSPORT\\\":\\\"stdout\\\",\\\"_UID\\\":\\\"0\\\"},\\\"message\\\":\\\"I1129 13:37:32.121217    1547 memory_manager.go:354] \\\\\\\"RemoveStaleState removing state\\\\\\\" podUID=\\\\\\\"ded87050-9112-4567-b7df-1f5e3b2eae7e\\\\\\\" containerName=\\\\\\\"reserve-resources\\\\\\\"\\\"}\",\n  \"service\": \"kubelet\",\n  \"source_type\": \"datadog_agent\",\n  \"status\": \"info\",\n  \"timestamp\": \"2024-11-29T13:37:32.124Z\"\n}\n\nI am running vector as a helm chart deployment, inside an AWS EKS cluster. I am also running datadog agent inside that cluster.\nI am using v0.42.0 vector.\nI used vector tap to get the example event above.\nAny ideas why my transform is not working? Expected would be to drop the event but it is still kept and forwarded.",
        "url": "https://github.com/vectordotdev/vector/discussions/21921",
        "createdAt": "2024-11-29T14:25:32Z",
        "updatedAt": "2024-12-04T12:16:09Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "AnaVomvylas"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21891,
        "title": "Help need: `aws_s3` sink can't read the credential file.",
        "bodyText": "Hello, I've leveraging vectordotdev in order to syslog collecting then putting in the S3 Bucket.\nVector installed version\nvector 0.42.0 (x86_64-unknown-linux-gnu 3d16e34 2024-10-21 14:10:14.375255220)\n\nBut. I ran into problem regard of file read permission error.\nI've configuration like this:\nsinks:\n  s3-syslog:\n    healthcheck:\n      enable: false\n    inputs:\n      - \"syslog-server\"\n    type: \"aws_s3\"\n    auth:\n      credentials_file: \"/home/ubuntu/.aws/credentials\"\n      profile: \"default\"\n\n    bucket: \"syslog-bucket\"\n    region: \"ap-northeast-2\"\n    batch:\n      timeout_secs: 30\n    buffer:\n      type: \"memory\"\n      max_events: 1000\n      when_full: \"block\"\n    compression: \"gzip\"\n    content_encoding: \"gzip\"\n    encoding:\n      codec: \"raw_message\"\n    server_side_encryption: \"AES256\"\n    timezone: \"Asia/Seoul\"\n    tls:\n      verify_certificate: false\n\nIt works when I testing to this config through the CMD vector -v. But has not working when I let run with systemctl in order to want to be work in background.\n\u00d7 vector.service - Vector\n     Loaded: loaded (/lib/systemd/system/vector.service; disabled; vendor preset: enabled)\n     Active: failed (Result: exit-code) since Tue 2024-11-26 16:23:44 KST; 5s ago\n       Docs: https://vector.dev\n    Process: 10308 ExecStartPre=/usr/bin/vector validate (code=exited, status=78)\n        CPU: 110ms\n\nNov 26 16:23:44 HOSTNAME systemd[1]: vector.service: Scheduled restart job, restart counter is at 5.\nNov 26 16:23:44 HOSTNAME systemd[1]: Stopped Vector.\nNov 26 16:23:44 HOSTNAME systemd[1]: vector.service: Start request repeated too quickly.\nNov 26 16:23:44 HOSTNAME systemd[1]: vector.service: Failed with result 'exit-code'.\nNov 26 16:23:44 HOSTNAME systemd[1]: Failed to start Vector.\n\nAnd I got this system log:\nNov 26 16:25:29 HOSTNAME systemd[1]: Starting Vector...\nNov 26 16:25:29 HOSTNAME vector[10417]: \u221a Loaded [\"/etc/vector/vector.yaml\"]\nNov 26 16:25:29 HOSTNAME vector[10417]: 2024-11-26T07:25:29.487089Z  WARN sink{component_kind=\"sink\" component_id=s3-syslog component_type=aws_s3}: vector_core::tls::settings: The `verify_certificate` option is DISABLED, this may lead to security vulnerabilities.\nNov 26 16:25:29 HOSTNAME vector[10417]: 2024-11-26T07:25:29.521580Z  WARN sink{component_kind=\"sink\" component_id=s3-syslog component_type=aws_s3}: vector_core::tls::settings: The `verify_certificate` option is DISABLED, this may lead to security vulnerabilities.\nNov 26 16:25:29 HOSTNAME vector[10417]: \u221a Component configuration\nNov 26 16:25:29 HOSTNAME vector[10417]: 2024-11-26T07:25:29.557294Z  WARN aws_config::provider_config: failed to parse profile err=could not read file `/home/ubuntu/.aws/credentials`: Permission denied (os error 13) (CouldNotReadFile(CouldNotReadProfileFile { path: \"/home/ubuntu/.aws/credentials\", cause: Os { code: 13, kind: PermissionDenied, message: \"Permission denied\" } }))\nNov 26 16:25:29 HOSTNAME vector[10417]: 2024-11-26T07:25:29.557366Z ERROR vector::topology::builder: msg=\"Healthcheck failed.\" error=dispatch failure component_kind=\"sink\" component_type=\"aws_s3\" component_id=s3-syslog\nNov 26 16:25:29 HOSTNAME vector[10417]: x Health check for \"s3-syslog\" failed: dispatch failure\nNov 26 16:25:29 HOSTNAME vector[10417]: \u221a Health check \"console-out\"\nNov 26 16:25:29 HOSTNAME systemd[1]: vector.service: Control process exited, code=exited, status=78/CONFIG\nNov 26 16:25:29 HOSTNAME systemd[1]: vector.service: Failed with result 'exit-code'.\nNov 26 16:25:29 HOSTNAME systemd[1]: Failed to start Vector.\nNov 26 16:25:29 HOSTNAME systemd[1]: vector.service: Scheduled restart job, restart counter is at 1.\nNov 26 16:25:29 HOSTNAME systemd[1]: Stopped Vector.\n\nI though that just adjust permission of credential file. Thus changed permission to maximum 777. But still not working with same error.\nyes! tried that changed to the file ownership perform as the vector user. but same this occur.",
        "url": "https://github.com/vectordotdev/vector/discussions/21891",
        "createdAt": "2024-11-26T07:36:50Z",
        "updatedAt": "2024-12-04T05:48:32Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "waltzbucks"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21874,
        "title": "transform output square brackets",
        "bodyText": "Hi. Please tell me, can I wrap the output-transform in square brackets? This is the format my application requires.\nExample:\ninvalid\n{\n\"json\": \"json 1\"\n}\nThat's right, that's the only way\n[{\n\"json\": \"json 1\"\n}]\nThanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/21874",
        "createdAt": "2024-11-24T17:56:45Z",
        "updatedAt": "2024-12-04T18:16:18Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "RFskynet"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21917,
        "title": "does vrl support include  common vrl files?",
        "bodyText": "like other config software, common config in common file , and using include!(/path/to/common.vrl) for include it , so we do not need to write multiple same vrl configurations\nbest wishes",
        "url": "https://github.com/vectordotdev/vector/discussions/21917",
        "createdAt": "2024-11-29T08:41:21Z",
        "updatedAt": "2024-12-03T02:54:16Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "lddlww"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21907,
        "title": "User and group for Vector",
        "bodyText": "Hi, I'm in the process of rebuilding a server that had Vector on it. The service requires a Vector user and a Vector group to run. How do I add them? OS: Ubuntu 22.",
        "url": "https://github.com/vectordotdev/vector/discussions/21907",
        "createdAt": "2024-11-28T09:49:29Z",
        "updatedAt": "2024-12-02T21:49:01Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "n0usl3ss"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21925,
        "title": "Source:file and any sink (?) redundancy options",
        "bodyText": "Hello team!\nCan you please share your thoughts about redundancy options when I'm using file as a source. I had vector sink before, now switched to clickhouse. But issue still the same. As soon as one sink become unavailable, shortly after vector stoppin upload to second sink as well. Is it smth related to both sinks using the same checkpoint file as source? And when one sink goes to block, second sink can't transfer data as well? Currently I have a following configuration:\n[sources.L3_cdr_file]\n    type = \"file\"\n    include = [\n                \"*.log\",\n                ]\n    read_from = \"beginning\"\n    ignore_not_found = true\n    oldest_first = true\n\n[transforms.L3_cdr_remap]\n    type = \"remap\"\n    inputs = [ \"L3_cdr_file\" ]\n    source = \"\"\"\n        ., err = \"L3;\"+.message\n            \"\"\"\n\n\n\n[sinks.L3_clickhouse_site1]\ntype = \"clickhouse\"\ninputs = [\"L3_cdr_remap\"]\nendpoint = \"http://XX.XX.XXX.X:8123\"\ndatabase = \"statistics\"\ntable = \"L3_vector\"\nskip_unknown_fields = true\ncompression = \"zstd\"\nauth.strategy = \"basic\"\nauth.user = \"vector\"\nauth.password = \"vector\"\n        [sinks.L3_clickhouse_site1.buffer]\n        type = \"disk\"\n        max_size = 8192000000\n        when_full = \"block\"\n\n[sinks.L3_clickhouse_site2]\ntype = \"clickhouse\"\ninputs = [\"L3_cdr_remap\"]\nendpoint = \"http://YY.YY.YYY.Y:8123\"\ndatabase = \"statistics\"\ntable = \"L3_vector\"\nskip_unknown_fields = true\ncompression = \"zstd\"\nauth.strategy = \"basic\"\nauth.user = \"vector\"\nauth.password = \"vector\"\n        [sinks.L3_clickhouse_site2.buffer]\n        type = \"disk\"\n        max_size = 8192000000\n        when_full = \"block\"\n\nHow should i configure vector, that in case one sink is unavailable, second still works as usual.",
        "url": "https://github.com/vectordotdev/vector/discussions/21925",
        "createdAt": "2024-12-01T10:31:38Z",
        "updatedAt": "2024-12-02T21:42:46Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ADovgalyuk"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21904,
        "title": "sampling not working as expected for custom field",
        "bodyText": "Hello team,\nI am getting data on socket(TCP) and trying to sample data based on hostname. here is the example yaml\nsources:\n  tcp_514:\n    type: socket\n    address: 0.0.0.0:514\n    mode: TCP\n\ntransforms:\n  sample_based_on_hostname:\n    type: sample\n    inputs:\n      - tcp_514\n    rate: 2000\n    key_field: host\n\nsinks:\n  console:\n      type: console\n      encoding:\n        codec: json\n      inputs:\n        - sample_based_on_hostname\nm I missing anything?",
        "url": "https://github.com/vectordotdev/vector/discussions/21904",
        "createdAt": "2024-11-27T18:29:39Z",
        "updatedAt": "2024-12-02T20:45:20Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "yjagdale"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21903,
        "title": "aws_sqs: ignore message when processed",
        "bodyText": "Hey. Is it possible to ignore a message without using delete_message: true ?",
        "url": "https://github.com/vectordotdev/vector/discussions/21903",
        "createdAt": "2024-11-27T17:15:27Z",
        "updatedAt": "2024-12-02T20:44:21Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "lukasmrtvy"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21901,
        "title": "Turning off console output for http sink",
        "bodyText": "Is it possible to turn off the response from the http sink? Trying to debug things here but console is flooded by the output from this sink. Attempting to sink to Quickwit if that matters.",
        "url": "https://github.com/vectordotdev/vector/discussions/21901",
        "createdAt": "2024-11-27T16:18:52Z",
        "updatedAt": "2024-12-02T20:39:02Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "rterbush"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21890,
        "title": "Metrics filters are not working properly",
        "bodyText": "Hi team\nPlease see the last support chat:\nhttps://discord.com/channels/742820443487993987/1285180794381275157/1285180794381275157\nBy latest suggrestion I got that I can use OR condition to get multiple prefix metrics.\nMy ask here is I just need metrics starting with either cpu.* or  kong.* or mem.*\nI am trying this remap:\nfilter_metrics:\n    type: filter\n    inputs: [datadog_agent.metrics]\n    condition: \n       type: vrl\n       source: match!(to_string(.name) ?? \"default\", r'^(cpu.*|kong.*|mem.*)')\n\nError is :\n2024-11-26T07:20:11.641948Z ERROR vector::topology::builder: Configuration error. error=Transform \"filter_metrics\":\nerror[E620]: can't abort infallible function\n  \u250c\u2500 :1:1\n  \u2502\n1 \u2502 match!(to_string(.name) ?? \"default\", r'^(cpu.*|kong.*|mem.*)')\n  \u2502 ^^^^^- remove this abort-instruction\n  \u2502 \u2502\n  \u2502 this function can't fail\n  \u2502\n  = see documentation about error handling at https://errors.vrl.dev/#handling\n  = see language documentation at https://vrl.dev\n  = try your code in the VRL REPL, learn more at https://vrl.dev/examples\n\nAlso tried this (tostring() function correction):\nfilter_metrics:\n    type: filter\n    inputs: [datadog_agent.metrics]\n    condition: \n       type: vrl\n       source: match!(tostring(.name) ?? \"default\", r'^(cpu.*|kong.*|mem.*)')\n\nError is:\n2024-11-26T07:35:26.637294Z ERROR vector::topology::builder: Configuration error. error=Transform \"filter_metrics\":\nerror[E105]: call to undefined function\n  \u250c\u2500 :1:8\n  \u2502\n1 \u2502 match!(tostring(.name) ?? \"default\", r'^(cpu.*|kong.*|mem.*)')\n  \u2502        ^^^^^^^^\n  \u2502        \u2502\n  \u2502        undefined function\n  \u2502        did you mean \"to_string\"?\n  \u2502\n  = learn more about error code 105 at https://errors.vrl.dev/105\n  = see language documentation at https://vrl.dev\n  = try your code in the VRL REPL, learn more at https://vrl.dev/examples",
        "url": "https://github.com/vectordotdev/vector/discussions/21890",
        "createdAt": "2024-11-26T07:33:05Z",
        "updatedAt": "2024-12-02T16:56:51Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "uni-pooja-laad"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21916,
        "title": "Error reporting writing data to clickhouse",
        "bodyText": "JSON DATA\n{\n\t\"notificationtype\": \"TRAP\",\n\t\"receivedfrom\": \"UDP: [127.0.0.1]:56013->[127.0.0.1]:162\",\n\t\"version\": \"0\",\n\t\"errorstatus\": \"0\",\n\t\"hostname\": \"127.0.0.1\",\n\t\"messageid\": \"0\",\n\t\"community\": \"public\",\n\t\"timestamp\": \"2024-11-29 11:39:13\",\n\t\"transactionid\": \"1\",\n\t\"varbinds\": [\n\t\t{\n\t\t\t\"value\": \"Timeticks: (55) 0:00:00.55\",\n\t\t\t\"oid\": \"DISMAN-EVENT-MIB::sysUpTimeInstance\",\n\t\t\t\"type\": 67\n\t\t},\n\t\t{\n\t\t\t\"value\": \"OID: IF-MIB::linkDown.0.33\",\n\t\t\t\"oid\": \"SNMPv2-MIB::snmpTrapOID.0\",\n\t\t\t\"type\": 6\n\t\t},\n\t\t{\n\t\t\t\"value\": \"STRING: \\\"test\\\"\",\n\t\t\t\"oid\": \"IF-MIB::linkDown\",\n\t\t\t\"type\": 4\n\t\t},\n\t\t{\n\t\t\t\"value\": \"STRING: \\\"public\\\"\",\n\t\t\t\"oid\": \"SNMP-COMMUNITY-MIB::snmpTrapCommunity.0\",\n\t\t\t\"type\": 4\n\t\t},\n\t\t{\n\t\t\t\"value\": \"OID: IF-MIB::linkDown\",\n\t\t\t\"oid\": \"SNMPv2-MIB::snmpTrapEnterprise.0\",\n\t\t\t\"type\": 6\n\t\t}\n\t],\n\t\"errorindex\": \"0\",\n\t\"requestid\": \"0\"\n}\n\nCREATE TABLE SQL\n-- ncolog.trap definition\n\nCREATE TABLE ncolog.trap\n(\n\n    `ts` DateTime64(3,\n 'Asia/Shanghai'),\n\n    `community` String,\n\n    `errorindex` UInt32,\n\n    `errorstatus` UInt32,\n\n    `hostname` String,\n\n    `messageid` String,\n\n    `notificationtype` String,\n\n    `path` String,\n\n    `receivedfrom` String,\n\n    `requestid` String,\n\n    `source_type` String,\n\n    `timestamp` DateTime,\n\n    `transactionid` String,\n\n    `varbinds` String,\n\n    `version` UInt8\n)\nENGINE = MergeTree\nORDER BY ts\nSETTINGS index_granularity = 8192;\n\nI was going to write this json data to clickhouse, but I got an error while writing. I located the problem with the varbinders field type. The field type in the data table is string, but in json, the type is array. How should I write the data to the database? I hope that the varbinders will remain the original form and be a string type.\nERROR LOG\n2024-11-29T08:26:04.338561Z ERROR sink{component_kind=\"sink\" component_id=ck component_type=clickhouse component_name=ck}:request{request_id=0}: vector::sinks::util::retries: Not retriable; dropping the request. reason=\"response status: 400 Bad Request\" internal_log_rate_limit=true\n2024-11-29T08:26:04.338593Z ERROR sink{component_kind=\"sink\" component_id=ck component_type=clickhouse component_name=ck}:request{request_id=0}: vector::sinks::util::sink: Response failed. response=Response { status: 400, version: HTTP/1.1, headers: {\"date\": \"Fri, 29 Nov 2024 08:26:04 GMT\", \"connection\": \"Keep-Alive\", \"content-type\": \"text/plain; charset=UTF-8\", \"x-clickhouse-server-display-name\": \"mydev.localdomain\", \"transfer-encoding\": \"chunked\", \"x-clickhouse-query-id\": \"a2b5711f-5bd6-4d2f-9f39-4b64dce8b518\", \"x-clickhouse-timezone\": \"Asia/Shanghai\", \"x-clickhouse-exception-code\": \"26\", \"keep-alive\": \"timeout=3\", \"x-clickhouse-summary\": \"{\\\"read_rows\\\":\\\"0\\\",\\\"read_bytes\\\":\\\"0\\\",\\\"written_rows\\\":\\\"0\\\",\\\"written_bytes\\\":\\\"0\\\",\\\"total_rows_to_read\\\":\\\"0\\\",\\\"result_rows\\\":\\\"0\\\",\\\"result_bytes\\\":\\\"0\\\",\\\"peak_memory_usage\\\":\\\"0\\\"}\"}, body: b\"Code: 26. DB::ParsingException: Cannot parse JSON string: expected opening quote: (while reading the value of key varbinds): While executing ParallelParsingBlockInputFormat: (at row 1)\\n. (CANNOT_PARSE_QUOTED_STRING) (version 23.8.3.48 (official build))\\n\" }\n2024-11-29T08:26:04.338637Z ERROR sink{component_kind=\"sink\" component_id=ck component_type=clickhouse component_name=ck}:request{request_id=0}: vector_common::internal_event::service: Service call failed. No retries or retries exhausted. error=\"Response failed.\" request_id=0 error_type=\"request_failed\" stage=\"sending\" internal_log_rate_limit=true\n2024-11-29T08:26:04.338655Z ERROR sink{component_kind=\"sink\" component_id=ck component_type=clickhouse component_name=ck}:request{request_id=0}: vector_common::internal_event::component_events_dropped: Events dropped intentional=false count=1 reason=\"Service call failed. No retries or retries exhausted.\" internal_log_rate_limit=true\n\n\nVERSION\nvector 0.27.0 (x86_64-unknown-linux-musl 5623d1e 2023-01-18)\nCONFIG\n[sources.mydev]\ntype = \"http_server\"\naddress = \"0.0.0.0:8008\"\nmethod = \"POST\"\nresponse_code = 200\nauth.username=\"admin\"\nauth.password=\"password\"\n\n[transforms.json]\ntype = \"remap\"\ninputs = [ \"mydev\" ]\nsource = '''\n.ts,err=to_float(.timestamp)\n.ts=to_string(.ts)\n. |=object!(parse_json!(string!(.message)))\ndel(.message)\n'''\n[sinks.print]\ntype = \"console\"\ninputs = [\"json\"]\nencoding.codec = \"json\"\n\n\n[sinks.ck]\ntype = \"clickhouse\"\ninputs = [ \"json\" ]\nauth.strategy=\"basic\"\nauth.password=\"password\"\nauth.user=\"default\"\ndatabase = \"ncolog\"\nendpoint = \"http://192.168.159.133:8123\"\ntable = \"trap\"\ncompression = \"gzip\"\nencoding.timestamp_format=\"unix\"",
        "url": "https://github.com/vectordotdev/vector/discussions/21916",
        "createdAt": "2024-11-29T08:35:23Z",
        "updatedAt": "2024-11-29T08:37:39Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "FengZh61"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 21879,
        "title": "Question on throttling: is there a non-lossy option for stretching a stream versus dropping events",
        "bodyText": "Question on a variant of \"throttle\": We have a need to spread out a number of events transmitted over time, but we do not want to lose events.\nWhy? We have many devices at the very edge of reasonable connectivity - a few mbps, at best. We must share that capacity with our other services. If we send giant blocks of data from Vector (which is the end result of aggregation processes) then we clog the network for a few minutes as we try to send that through, leading to failures on other services. These are \"pulses\" of data which are only for a minute or so every hour or two. The goal is to flatten the large event/bandwidth spikes into a low, long duration stream of events being transmitted.\nOur goal is to stretch out the transmission, but not lose any events.  The \"throttle\" transform loses events from how I understand it. That is not desirable.\nIs adding this functionality simply a modification to be made to \"throttle\" so that it moves events that do not fit in a \"bucket\" to the next \"bucket\", instead of being dropped?  Eventually, a maximum number of buffered events to be shifted needs to be specified so it does not become an infinitely-growing queue, but it seems that a non-lossy version of the throttle function may be useful.\nOr can \"throttle\" do this already?  The documentation is not particularly clear.\nWe are using kafka, which has no \"request.concurrency\".  The receiver will accept messages as fast as they're sent - the backpressure does not exist from the remote side until we are using 100% of the bandwidth, at which point the damage is being done. There needs to be an artificial maximum applied within Vector.\nI thought maybe the sink was the right place for this, but after more consideration it seems that a generic method would make more sense as an addition to \"throttle\" since that sort-of does most of what is desired already, except for the fact that throttle discards messages instead of delaying them.  The challenge with doing this in \"throttle\" is that the batching on the sink may short-circuit this concept, and perhaps the sink is the only place it can be done while still getting the benefits of compression from larger batches (the larger the batch, the better the compression.)\nWe could consider writing a patch for this either to extend \"throttle\" or to modify the kafka sink depending on which direction it makes sense to pursue.",
        "url": "https://github.com/vectordotdev/vector/discussions/21879",
        "createdAt": "2024-11-25T17:36:05Z",
        "updatedAt": "2024-11-27T20:56:44Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "johnhtodd"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 1
    },
    {
        "number": 21875,
        "title": "kafka source topic/partition discovery in vector",
        "bodyText": "Hi folks,\nI'm aware that kafka source support regex for topics and I would like to know how new topics matching the regex are discovered, in what interval, and how vector react to rebalance when partition number is changed for topics.\nThanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/21875",
        "createdAt": "2024-11-25T00:09:39Z",
        "updatedAt": "2024-11-27T17:38:13Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "canopenerda"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21716,
        "title": "Chunked GELF: Support for Non-Chunked Messages and Mixed Compression Methods",
        "bodyText": "Hello, I'm excited about Vector's upcoming support for chunked GELF messages! This feature will simplify our log pipeline quite a lot. I know this has not been released yet, but I already have a couple of questions regarding how Vector will handle different types of GELF messages.\nIn my environment, we have multiple implementations for sending GELF messages, and logs vary quite a bit. I\u2019d like to know the following:\n\nWill I still be able to process non-chunked messages when using framing.method = \"chunked_gelf\"?\nWill this support chunks with mixed compression methods (gzip, zlib, or uncompressed)?\n\nI know the implementation is similar to the go-gelf specification (which supports these), but since I don't understand Rust I can't confirm this from the source code.\nThanks in advance for your help!",
        "url": "https://github.com/vectordotdev/vector/discussions/21716",
        "createdAt": "2024-11-06T12:47:20Z",
        "updatedAt": "2024-11-27T11:32:30Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "arch-xtof"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21863,
        "title": "vector protobuf encoding uses 3x CPU compared to JSON encoding",
        "bodyText": "Image: timberio/vector:0.42.0-debian\nI was very surprised when I switch kafka encoding to protobuf from JSON having 4.5x increase in CPU usage. Config:\n  kafka:\n    compression: gzip\n    encoding:\n      codec: protobuf\n      protobuf:\n        desc_file: proto.desc\n        message_type: <>\n    batch:\n      max_bytes: 1048576 # 1MiB\n      max_events: 100\n      timeout_secs: 1\n\nwith proto being (simple string key/value fields):\nmessage LogRecord {\n  ...\n\n  // log message. \n  string message = 3;\n  // log level (e.g. info, warn, error, debug)\n  string level = 4;\n\n  \n   ...\n\n  // timestamp in unix_ns\n    google.protobuf.Timestamp timestamp = 24;\n}\nSo yeah, I'm not sure what's happening here, or how to get performance in line with plain JSON shemaless encoding",
        "url": "https://github.com/vectordotdev/vector/discussions/21863",
        "createdAt": "2024-11-21T18:19:02Z",
        "updatedAt": "2024-11-25T17:36:33Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "nmiculinic"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21799,
        "title": "Can't get batch working with s3 sink",
        "bodyText": "I have the following configured in vector.yaml:\nsinks:\n  s3:\n    type: aws_s3\n    inputs:\n      - remap_s3_logs\n    bucket: my-bucket-name\n    batch:\n      max_events: 1000000\n      timeout_secs: 300\n    encoding:\n      codec: raw_message\n    filename_append_uuid: false\n    filename_time_format: \"%+\"\n    key_prefix: \"{{ .container_name }}/%Y-%m-%d/{{ .container_name }}-{{ .image_tag }}-\"\nEverything seems to work except the batch config. I am seeing very small files (~3000 lines, ~50kB).\nDocumentation seems to suggest that max_bytes is optional, so I didn't set that.\nAnd from what I'm reading, the buffer size should be independent of the batch size, so I didn't configure anything there either.\nAm I missing something?\nEdit: Setting max_bytes: 500000000 resolved this for me, though docs claim this is optional.",
        "url": "https://github.com/vectordotdev/vector/discussions/21799",
        "createdAt": "2024-11-14T23:46:10Z",
        "updatedAt": "2024-11-25T17:29:33Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "extremeandy"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21871,
        "title": "How to specify logline/content in loki sink?",
        "bodyText": "Hi, how to specify logline/content  in loki sink?\nCurrent config works labels are applied, but all json lines is written to logline and i only need .message. I know i could delete some fields by using del(), but i can't delete .app, .hostname, .level as they are used as lables later in the sink.\nHere's my values.yaml file.\nrole: Agent\n\nlogLevel: \"debug\"\n\ncustomConfig:\n  data_dir: /vector-data-dir\n  api:\n    enabled: true\n    address: 0.0.0.0:8686\n    playground: false\n  sources:\n    vitta_app_logs:\n      type: kubernetes_logs\n      extra_label_selector: \"developed-by=lucky\"\n  transforms:\n    vitta_app_logs_transform:\n      type: remap\n      inputs:\n        - vitta_app_logs\n      source: |\n        .app = .kubernetes.pod_labels.\"app.kubernetes.io/name\"\n        .hostname = .kubernetes.node_labels.\"kubernetes.io/hostname\"\n        .messageJson = parse_json!(.message)\n        .level = .messageJson.Level\n        .message = .messageJson.Message\n\n  sinks:\n    console:\n      type: console\n      inputs:\n        - vitta_app_logs_transform\n      encoding:\n        codec: \"json\"\n    vitta_app_loki_sink:\n      type: loki\n      inputs:\n        - vitta_app_logs_transform\n      compression: snappy\n      encoding:\n        codec: \"json\"\n      endpoint: http://redacted.redacted:9999\n      labels:\n        app: \"{{`{{.app}}`}}\"\n        hostname: \"{{`{{.hostname}}`}}\"\n        level: \"{{`{{.level}}`}}\"\n      path: /loki/loki/api/v1/push\n      remove_timestamp: true\n      tenant_id: fake\n\nresources:\n  requests:\n    cpu: 200m\n    memory: 64Mi\n  limits:\n    cpu: 200m\n    memory: 256Mi",
        "url": "https://github.com/vectordotdev/vector/discussions/21871",
        "createdAt": "2024-11-22T10:42:58Z",
        "updatedAt": "2024-11-22T15:20:24Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "9Lucky9"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21851,
        "title": "CloudWatch metric sink is not sending `http_client_rtt_seconds` (aggregated histogram) metrics to CloudWatch",
        "bodyText": "Hi Everyone,\nI'm running Vector 0.37.1 with a configuration that has:\n\nAn internal metrics source\nA HTTP sink\nA CloudWatch sink\nA filter transform step for the internal metrics source to filter out the metrics I want\nA Console sink to stdout\n\nI'm seeing an issue in which a metric that I can see being emitted for my HTTP Sink, http_client_rtt_seconds, is not being emitted to CloudWatch and there are no logs (debug, trace or error) indicating that there was an issue. From STDOUT, I can see that the metric looks like this:\n{\n    \"name\": \"http_client_rtt_seconds\",\n    \"namespace\": \"redacted\",\n    \"tags\":\n    {\n        \"component_id\": \"redacted\",\n        \"component_kind\": \"sink\",\n        \"component_type\": \"http\",\n        \"status\": \"200\"\n    },\n    \"timestamp\": \"2024-11-20T18:32:45.508483798Z\",\n    \"kind\": \"absolute\",\n    \"aggregated_histogram\":\n    {\n        \"buckets\":\n        [\n            ... (removed some for spacing)\n            {\n                \"upper_limit\": 0.25,\n                \"count\": 0\n            },\n            {\n                \"upper_limit\": 0.5,\n                \"count\": 1\n            },\n            {\n                \"upper_limit\": 1.0,\n                \"count\": 0\n            },\n            ... (removed some for spacing)\n        ],\n        \"count\": 1,\n        \"sum\": 0.326666255\n    }\n}\n\nI looked through the code for the CloudWatch Metrics Sink and saw that these http_client_rtt_seconds metric are defined as a having a MetricValue type of AggregatedHistogram, however the Sink doesn't seem to support that MetricValue type: code ref\nI have two questions around this:\n\n\nIs there any way for me to get around the current lack of support for AggregatedHistogram metrics for this sink in the short term?\n\n\nFor the long term, does it make sense to add support for AggregatedHistogram metric value types?\n\n\nThanks in advance for the help!",
        "url": "https://github.com/vectordotdev/vector/discussions/21851",
        "createdAt": "2024-11-20T21:20:22Z",
        "updatedAt": "2024-11-21T22:14:17Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mkwan-amzn"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21854,
        "title": "Input for Source via Sink",
        "bodyText": "Let's say I have 30k devices that I'd like to hit with prometheus. The list of IPs is queryable via an HTTP endpoint.\nHow can I hit the HTTP endpoint and have that list of IPs be used for the prometheus list of endpoint?",
        "url": "https://github.com/vectordotdev/vector/discussions/21854",
        "createdAt": "2024-11-20T22:41:13Z",
        "updatedAt": "2024-11-21T20:11:15Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "average-gary"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21862,
        "title": "How to have Vector only pickup older files in Source (files)",
        "bodyText": "How can I configure vector to only grab day old files or older in the source portion of conifg (file)\nI see in the docs there is an option for ignore_older_secs -> Is there a way to do the opposite?\nThanks,\nSam",
        "url": "https://github.com/vectordotdev/vector/discussions/21862",
        "createdAt": "2024-11-21T16:44:35Z",
        "updatedAt": "2024-11-21T17:02:27Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Trowa929"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21852,
        "title": "Sankey Visualizer - Event Flow",
        "bodyText": "vector graph is great for tracing the topology and internal metrics show how much volume is being processed/dropped\nbut what I really want is an easy way to inflows and outflows for each component.\nI'm imagining vector sankey <events,event_bytes> to link and instantaneous event counts/bytes flows.\nSimple link format would be ingestible by https://observablehq.com/@mbostock/flow-o-matic and would be 90% of the way to fit https://visactor.io/vchart/guide/tutorial_docs/Chart_Types/Sankey",
        "url": "https://github.com/vectordotdev/vector/discussions/21852",
        "createdAt": "2024-11-20T21:48:22Z",
        "updatedAt": "2024-11-20T22:06:26Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Aergonus"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21847,
        "title": "Does config-watched also watches enrichment table for file changes?",
        "bodyText": "I'm using https://vector.dev/highlights/2021-11-18-csv-enrichment/ and am running vector with --watch-config flag.\nIf underlying CSV file changes, will the config be reloaded or not?",
        "url": "https://github.com/vectordotdev/vector/discussions/21847",
        "createdAt": "2024-11-20T19:23:33Z",
        "updatedAt": "2024-11-20T19:24:52Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "nmiculinic"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21835,
        "title": "Help Needed: Adding account-id and instance-id to Vector's S3 Sink key_prefix",
        "bodyText": "Hello everyone,\nI'm currently working with Vector and trying to configure the S3 sink to include both the account-id and instance-id in the key_prefix. I've experimented with several approaches, including using remap, lua, and environment variables, but haven't found a solution that is both obvious and clean.\nHere's what I've tried so far:\nRemap: Attempted to use the remap transform to modify the data before it reaches the sink, but I couldn't find a straightforward way to inject these values directly into the key_prefix.\nLua: Considered using a Lua script to manipulate the data, but integrating this with the S3 sink configuration proved challenging.\nEnvironment Variables: Explored setting environment variables for these IDs and referencing them in the configuration, but this approach didn't seem to integrate well with the key_prefix.\nHas anyone successfully managed to include dynamic values like account-id and instance-id in the key_prefix for an S3 sink in Vector? If so, could you please share your approach or any tips you might have?\nAny help or guidance would be greatly appreciated!\nThank you!",
        "url": "https://github.com/vectordotdev/vector/discussions/21835",
        "createdAt": "2024-11-19T20:06:46Z",
        "updatedAt": "2024-11-20T17:29:44Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "block-erichter"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 21829,
        "title": "Guidance Needed: Converting Logstash Config with Nested Conditionals and Handling Multiple Input Files in Vector",
        "bodyText": "Hi Vector Community,\nOur team is migrating from Logstash to Vector, and I\u2019m currently focused on converting the output.conf from Logstash's pipeline-parser. However, I\u2019m facing challenges with nested conditionals and handling multiple input files during the migration.\nHere\u2019s an example of the Logstash configuration we\u2019re working with:\nlogstash pipeline-parser/output.conf\nif \"__###__\" in [tags] or\n    ( [input_type] == \"log\" ) or\n    [input_type] == \"es\" {\n    ...\n} else if {\n    ...\n    if [src] or [dst] {\n        ruby {\n            init => \"\n                @file_path = '/usr/share/user.csv'\n            \"\n        ...\n        }\n    }\n    if [src] {\n        translate {\n            source => \"[src]\"\n            target => \"[tmp]\"\n            dictionary_path => \"/usr/share/indict.csv\"\n            refresh_interval => 300\n        }\n    }\n}\n\nWe\u2019ve decided to use multiple config files and are dividing each pipeline into sources, transforms, and sinks. While we\u2019ve successfully converted the sources and sinks, we\u2019re currently stuck at the transforms phase, particularly due to file read/write operations and handling nested conditionals.\nSpecific Questions:\nNested conditionals: What\u2019s the best way to handle deeply nested if structures in Vector\u2019s configuration format?\nFile operations in transforms: In Logstash, we use filters like Ruby and Translate to read external files (e.g., /usr/share/user.csv, /usr/share/indict.csv). Vector\u2019s remap transform doesn\u2019t seem to support direct file reads. Is there a recommended workaround?\nAdditionally, we are specifying multiple files as input in sources like this:\n[sources.my_source_id]\ntype = \"file\"\ninclude = [ \"/tmp/foo.csv\", \"/tmp/bar.proto\" ]\n\nIn the transforms stage, how can we distinguish which log event originated from which file? For example:\nIs there metadata (e.g., file name or path) available that identifies the source file?\nHow can this metadata be accessed in a remap transform to process events differently based on their input file?\nI\u2019d greatly appreciate any guidance, examples, or resources to help resolve these issues.\nThank you for your support!",
        "url": "https://github.com/vectordotdev/vector/discussions/21829",
        "createdAt": "2024-11-19T09:36:56Z",
        "updatedAt": "2024-11-20T04:26:14Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "JeeIn-Shin"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21834,
        "title": "aws_s3 sink, azure to aws access issue...different account than where my role and external_id located",
        "bodyText": "I am having an issue with an aws_s3 sink. I am able to send logs to an s3 bucket in my account, but there is some issue with the profile that is in another account. is there a particular setting i need to do for this?",
        "url": "https://github.com/vectordotdev/vector/discussions/21834",
        "createdAt": "2024-11-19T18:59:46Z",
        "updatedAt": "2024-11-19T19:03:58Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "trax721"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 5472,
        "title": "Welcome to Vector Discussions!",
        "bodyText": "Welcome to the Vector community\nWelcome to Vector's Github Discussions! We are trying Discussions to help organize support for Vector. Please use the following guidelines to get the most out of the Vector community:\n\nUse discussions to ask questions and share ideas.\nUse issues for bug reports and feature requests.\nUse chat to engage with the Vector team and other community members.\n\nCommunity is very important to Vector, we value feedback and take pride in Vector's user experience. Engaging with users is an important feedback loop for the project; please feel free to share everything (good and bad). Thank you!",
        "url": "https://github.com/vectordotdev/vector/discussions/5472",
        "createdAt": "2020-12-10T02:52:48Z",
        "updatedAt": "2024-11-19T19:01:05Z",
        "isAnswered": null,
        "locked": false,
        "author": {
            "login": "binarylogic"
        },
        "category": {
            "name": "Announcements"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21824,
        "title": "By using parse_key_value the key-values are not getting into the output",
        "bodyText": "Hi team,\nin contineousion of https://discord.com/channels/742820443487993987/1288128207111065650/1295892531246268540\nI am using remap with parse_key_value. The vector configuration file attached. The log coming from datadog_agent attached in the file orch_swepper_logs.yaml.\nIn Datadog it is appearing without tags attached screenshot. (filename: after.png)\nBefore applying this remap, the tags were getting properly, that Screenshot too attached(filename: before.png)\nWhy this is happening? I just used it to parse ddtags key and fetch kube_namespace from it to match with the condition.\n\n\norch_swepper_logs.json\nvector-config-ai.txt\nAttaching vector-config-ai as txt as json format not supported here.",
        "url": "https://github.com/vectordotdev/vector/discussions/21824",
        "createdAt": "2024-11-18T14:43:59Z",
        "updatedAt": "2024-11-19T15:32:52Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "uni-pooja-laad"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21821,
        "title": "Ensuring inflight messages are not dropped when HPA kills a Vector pod",
        "bodyText": "Hi Team,\nWe are using Vector's gcp_pubsub source to pull pubsub messages. We want to add k8s hpa to auto-scale based on CPU. To that my question: How can we ensure all  inflight events (in memory or buffer) are gracefully handled before the HPA kills and removes a vector pod ? a.k.a is there a concept of \"drain\" ? If not, a hacky way to achieve the same could be to enable end to end acknowledgements, right ?",
        "url": "https://github.com/vectordotdev/vector/discussions/21821",
        "createdAt": "2024-11-18T08:53:29Z",
        "updatedAt": "2024-11-18T17:58:44Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "atibdialpad"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21811,
        "title": "does vector support containerd runtime?",
        "bodyText": "i found the parameter docker_host  can't compatible containerd runtime\uff0cin the docker_logs source; when i set docker_host=\"/var/run/containerd/containerd.sock\"\uff0c vector through follow error\uff1a\n2024-11-15T06:15:39.065760Z ERROR source{component_kind=\"source\" component_id=dummy_logs component_type=docker_logs}: vector::sources::docker_logs: Listing currently running containers failed. error=Error in the hyper legacy client: client error (SendRequest)\n\nam i missing something?",
        "url": "https://github.com/vectordotdev/vector/discussions/21811",
        "createdAt": "2024-11-15T06:20:54Z",
        "updatedAt": "2024-11-18T16:32:07Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "lddlww"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21775,
        "title": "Send metrics from vector to prometheus hosted on cloud",
        "bodyText": "Hi team,\nI am trying to send metrics from vector to Prometheus which is on cloud. Below is the configuration I am using:\ntransforms:\naggregate_metrics:\ntype: aggregate\ninputs: [datadog_agent.metrics]\ninterval_ms: 10000\nsinks:\nprom_metrics:\ntype: prometheus_remote_write\ninputs: [aggregate_metrics]\nendpoint: \"https://prometheus-prod-.grafana.net\"\nauth:\nstrategy: basic\nuser: \"\"\npassword: \nThe error here is : 2024-11-08T06:59:19.740100Z ERROR vector::topology::builder: msg=\"Healthcheck failed.\" error=Unexpected status: 405 Method Not Allowed component_kind=\"sink\" component_type=\"prometheus_remote_write\" component_id=prom_metrics\nEven I tried by adding suffix like: /api/prom/push or /api/v1/write etc. but seems its either 400 or 405 error.",
        "url": "https://github.com/vectordotdev/vector/discussions/21775",
        "createdAt": "2024-11-13T04:56:05Z",
        "updatedAt": "2024-11-15T20:42:15Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "uni-pooja-laad"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21765,
        "title": "Vector Filter Not Working on EKS Cluster",
        "bodyText": "EDIT: Adding vital information.\nWe have an EKS Cluster that we have to monitor.\nI have already installed Vector and the logs are being sent to Betterstack.\nI'm trying to learn advanced filtering and have been testing some sample Transforms, but for some reason, they're not working.\nThe Problem\nThe integration works and logs are being recorded and sent out from Vector to Better Stack. However, the transforms are not working. The logs that should NOT have matched the filter condition are still being sent, and the remap is not reflecting in the logs, also.\nEverything else works, except for the transforms.\nHere is a sample Event Data:\n{\n    \"dt\": \"2024-11-12T06:29:28.211962730Z\",\n    \"file\": \"/var/log/pods/dev_analytic-service-79f65f9b58-w5xh5_723620b6-82aa-4828-8cc6-f9408249d7ba/analytic-service/0.log\",\n    \"kubernetes\": {\n        \"container_id\": \"removed_data_for_simplicity\",\n        \"container_image\": \"removed_data_for_simplicity\",\n        \"container_image_id\": \"removed_data_for_simplicity\",\n        \"container_name\": \"analytic-service\",\n        \"namespace_labels\": {\n            \"kubernetes.io/metadata.name\": \"dev\"\n        },\n        \"node_labels\": \"removed_data_for_simplicity\",\n        \"pod_annotations\": {\n            \"fluentbit.io/exclude\": \"true\",\n            \"kubectl.kubernetes.io/restartedAt\": \"2024-08-23T11:07:04+02:00\"\n        },\n        \"pod_ip\": \"10.0.16.52\",\n        \"pod_ips\": [\n            \"10.0.16.52\"\n        ],\n        \"pod_labels\": {\n            \"app\": \"analytic-service\",\n            \"pod-template-hash\": \"79f65f9b58\"\n        },\n        \"pod_name\": \"analytic-service-79f65f9b58-w5xh5\",\n        \"pod_namespace\": \"dev\",\n        \"pod_node_name\": \"ip-10-0-21-8.us-east-2.compute.internal\",\n        \"pod_owner\": \"ReplicaSet/analytic-service-79f65f9b58\",\n        \"pod_uid\": \"723620b6-82aa-4828-8cc6-f9408249d7ba\",\n        \"stream\": \"stdout\"\n    },\n    \"message\": {\n        \"ctx\": \"rpc\",\n        \"data\": {\n            \"action\": \"start\"\n        },\n        \"debug\": {\n            \"event\": \"GET /healthz\",\n            \"protocol\": \"http\",\n            \"trace\": \"1c54a365-8f56-4524-acff-ee57904d3f91\"\n        },\n        \"level\": 30,\n        \"message\": \"GET /healthz\",\n        \"service\": {\n            \"name\": \"analytic\",\n            \"version\": \"1.3.15\"\n        },\n        \"time\": 1731392968211\n    },\n    \"platform\": \"Kubernetes\"\n}\n\nAnd here is my Sources and Transforms Blocks in YAML.\n # Sources\n    sources:\n      better_stack_kubernetes_logs:\n        type: kubernetes_logs\n        extra_field_selector: \"metadata.namespace!=monitoring,metadata.namespace!=kube-system,metadata.namespace!=prometheus\"\n        extra_label_selector: \"app!=optimove-gateway\"\n    # Transforms\n    transforms:\n      extra_filters:\n        type: filter\n        inputs:\n          - \"better_stack_kubernetes_logs\"\n        condition: .message.message != \"GET /healthz\"\n      example:\n        type: remap\n        inputs:\n          - \"*\"\n        source: |\n          .extra = .message\n\nThe extra_field_selector and extra_label_selector are working properly, but transforms, are not.\nThe filter is not working despite it having the right source id and remap is also not working despite it having a simple reassignment line.\nI even went to VRL Playground to check if that one line of code is incorrect, but it seems to be correct (as seen below).\n\nI'm pretty sure it's just another simple mistake that I'm just overlooking, but I really can't seem to find what's wrong. I've checked the documentations again and again and again, but still can't see what's wrong here.\nI hope someone can help me. It also does not help that the examples in the documentations are just very minimal. I wish there's a complete example somewhere for Kubernetes (or maybe I'm just blind)",
        "url": "https://github.com/vectordotdev/vector/discussions/21765",
        "createdAt": "2024-11-12T08:11:41Z",
        "updatedAt": "2024-11-15T20:05:36Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "justinAtBeleb"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21786,
        "title": "Duplicated env tag",
        "bodyText": "Hi there,\nI am using vector to parse log files from the SoftwareAG platform into Datadog. When the logs arrive in DD the env tag seems to be duplicated like this:\n\nI am setting this tag like so in a remap transform step\n.ddtags = \"env:${ENVIRONMENT}\" and the ENVIRONMENT env var is set to dev on the Windows host machine where the vector pipe is running. I am failing to understand why the other env:development tag is also appearing as we have no other env vars on the machine that might be setting this value. Any ideas what I may be doing wrong here?",
        "url": "https://github.com/vectordotdev/vector/discussions/21786",
        "createdAt": "2024-11-13T15:06:11Z",
        "updatedAt": "2024-11-15T15:05:35Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "einarjohnson"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21742,
        "title": "Possible Memory leak issue with kube parser?",
        "bodyText": "Has anyone had issues with the kube log parser using up a lot of memory? My config is attached because its quite big. Wanted to know if there was some type of memory leak bug? It is using a lot more memory even compared to fluent which just doesn't seem right.  I don 't have any errors on the vector config or logs for vector.\ncustomConfig:\n  data_dir: \"/vector-data-dir\"\n  sources:\n    syslog:\n      type: \"file\"\n      include:\n      - \"/var/log/syslog\"\n      - \"/var/log/auth.log\"\n      - \"/var/log/kern.log\"\n      - \"/var/log/messages\"\n      read_from: \"beginning\"\n    var_logs:\n      type: \"file\"\n      include:\n      - \"/var/log/*.log\"\n      exclude:\n      - \"/var/log/syslog\"\n      - \"/var/log/auth.log\"\n      - \"/var/log/kern.log\"\n      - \"/var/log/messages\"\n      read_from: \"beginning\"\n    containers:\n      type: \"kubernetes_logs\"\n      exclude_paths_glob_patterns:\n      - \"/var/log/pods/blend-system_fluent*/**\"\n      - \"/var/log/pods/vector_vector*/**\"\n      - \"/var/log/pods/blend_*apitest[0-9]*/**\"\n      - \"/var/log/pods/blend_*e2e[0-9]*/**\"\n      - \"/var/log/pods/blend_lending-webdriver-test*/**\"\n      - \"/var/log/pods/blend_faye*/**\"\n      - \"/var/log/pods/kube-system_datadog-[a-z0-9][a-z0-9][a-z0-9][a-z0-9][a-z0-9]_*/**\"\n      - \"/var/log/pods/blend_load-test-task-pod*/**\"\n      - \"/var/log/pods/osquery*/**\"\n      - \"/var/log/pods/*_actions-runner-system_*/**\"\n      - \"/var/log/pods/*_appsmith_*/**\"\n      - \"/var/log/pods/*_velero_*/**\"\n      read_from: \"beginning\"\n    osquery_results:\n      type: \"file\"\n      include:\n      - \"/var/log/osquery/osqueryd.results.log\"\n      read_from: \"beginning\"\n    osquery_snapshots:\n      type: \"file\"\n      include:\n      - \"/var/log/osquery/osqueryd.snapshots.log\"\n      read_from: \"beginning\"\n  transforms:\n    syslog_parser:\n      type: \"remap\"\n      inputs:\n      - syslog\n      source: |\n        parse_syslog!(.message)\n    containers_ec2_metadata:\n      type: \"aws_ec2_metadata\"\n      inputs:\n      - containers\n      fields:\n      - instance-id\n      - local-ipv4\n    containers_transform:\n      type: \"remap\"\n      drop_on_abort: false\n      reroute_dropped: true\n      inputs:\n      - containers_ec2_metadata\n      source: |\n        .k8s.namespace_name = .kubernetes.pod_namespace\n        .k8s.pod_name = .kubernetes.pod_name\n        .k8s.pod_ip = .kubernetes.pod_ip\n        .k8s.container_name = .kubernetes.container_name\n        .k8s.container_image = .kubernetes.container_image\n        del(.kubernetes)\n        .kubernetes = del(.k8s)\n        if is_json(to_string(.message) ?? \"\") {\n          .parsed = parse_json!(.message)\n        } else {\n          .parsed = .message\n        }\n        del(.message)\n        del(.source_type)\n        del(.stream)\n        .time = del(.timestamp)\n        .env = \"sandbox\"\n        .k8s_cluster_name = \"temp.k8s.centrio.com\"\n        .instance_id = del(.\"instance-id\")\n        .ip_address = del(.\"local-ipv4\")\n#         del(.file)\n    osquery_results_parser:\n      type: \"remap\"\n      inputs:\n      - osquery_results\n      source: |\n        if exists(.message) {\n          .parsed = parse_json!(.message)\n        } else {\n          log(\"JSON parse error in osquery_results\", level: \"error\")\n          .parsed = null\n        }\n    osquery_snapshots_parser:\n      type: \"remap\"\n      inputs:\n      - osquery_snapshots\n      source: |\n        if exists(.message) {\n          .parsed = parse_json!(.message)\n        } else {\n          log(\"JSON parse error in osquery_snapshots\", level: \"error\")\n          .parsed = null\n        }\n    osquery_results_filter:\n      type: \"filter\"\n      inputs:\n      - osquery_results_parser\n      condition: 'exists(.parsed)'\n    osquery_snapshots_filter:\n      type: \"filter\"\n      inputs:\n      - osquery_snapshots_parser\n      condition: 'exists(.parsed)'",
        "url": "https://github.com/vectordotdev/vector/discussions/21742",
        "createdAt": "2024-11-08T14:15:37Z",
        "updatedAt": "2024-11-14T18:24:46Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "umpa385"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21783,
        "title": "How to enable optional tags from the telemetry sent by datadog-agent source and datadog sink?",
        "bodyText": "Hi team,\nCurrently trying to enhance the monitoring for our vector setup. We run vector as an aggregator between our datadog-agent source and datadog-metrics sink. One of the limitations we are facing is on the optional tags being sent with the telemetry data of the datadog-agent source and datadog-metrics sink, namely the podname tag.\nIn the documentation (e.g. source.component_received_events_count or sink.component_received_events_count) some metrics have this optional tag available but when observing it in datadog, it lacks the tag. Currently the reported metric has the component_id, component_type, component_kind, env, version, host and service tags. Do we have to configure it? If so what would be steps necessary to enable it?",
        "url": "https://github.com/vectordotdev/vector/discussions/21783",
        "createdAt": "2024-11-13T11:31:41Z",
        "updatedAt": "2024-11-14T13:45:48Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "dcebola-mollie"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21792,
        "title": "Vector wont install in 32bit laptop running debian 12 (bookworm)",
        "bodyText": "hello and good day. Am trying to install vector in my old 32 bit laptop running debian 12 but an error shows which say 'vector: unsupported arch: i686-unknown-linux-gnu'. is there a workaround to this? appreciate any help. thanks you.\ni was pointed to this (#10223) however, i do not possess the technical expertise on how to build it myself. i was of the hope that there is a working solution which i have somehow missed.\ni believe there might be others out there looking to repurpose their old laptops and hence, support for this 32-bit project would be beneficial for others as well.\nthank you!",
        "url": "https://github.com/vectordotdev/vector/discussions/21792",
        "createdAt": "2024-11-14T05:28:52Z",
        "updatedAt": "2024-11-14T05:28:53Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ChrisGitH0b"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 21763,
        "title": "Support for CloudWatch Metrics unit specification",
        "bodyText": "CloudWatch metrics supports the specification of the unit of measurement, like Percent, Bytes, Megabytes, Count, etc, which permits the graphs to be labelled and even scaled appropriately to aid in interpretation.\nIs there (or could there be) some way to have the CloudWatch Metrics sink use a property/tag on the metric value to pass as the Unit property in its PutMetricData API call?",
        "url": "https://github.com/vectordotdev/vector/discussions/21763",
        "createdAt": "2024-11-11T20:09:17Z",
        "updatedAt": "2024-11-13T21:09:38Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "robbytx"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21665,
        "title": "Update document on elasticsearch",
        "bodyText": "Hello,\nWe are migrating from fluentd to vector and we are stuck to replace the update document with a same ID. FluentD supports upsert with Elasticsearch but this not the case of vector. Is there an another solution ?",
        "url": "https://github.com/vectordotdev/vector/discussions/21665",
        "createdAt": "2024-10-31T14:56:37Z",
        "updatedAt": "2024-11-12T17:15:55Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "blackrez"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21767,
        "title": "Getting \"fallible argument\" with handled error",
        "bodyText": "I am using Vector to move data from a Kafka broker to Clickhouse. In the transforms in vector.yaml, I've got:\n\n      transaction_time, err = parse_timestamp(.transaction_time, format: \"%Y-%m-%d %H:%M:%S.%6f\")\n\n      if err != null {\n\n        log(\"could not parse timestamp\" + .transaction_time + \" using format: \" + err)\n\n      } else {\n\n        .transaction_time = transaction_time\n\n      }\n\n\nWhen I run Vector, I get:\n2024-11-12T09:04:54.506894Z ERROR vector::topology::builder: Configuration error. error=Transform \"field_mapping\": \n\nerror[E630]: fallible argument\n\n  \u250c\u2500 :8:7\n\n  \u2502\n\n8 \u2502   log(\"could not parse timestamp\" + .transaction_time + \" using format: \" + err)\n\n  \u2502       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  \u2502       \u2502\n\n  \u2502       this expression can fail\n\n  \u2502       handle the error before passing it in as an argument\n\nWhat am I not seeing? I have been following https://vector.dev/docs/reference/vrl/errors/#handling, so this should work...\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/21767",
        "createdAt": "2024-11-12T09:13:35Z",
        "updatedAt": "2024-11-12T14:23:52Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ElToro1966"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21756,
        "title": "Does the NATS source drain messages when shutdown signal is given?",
        "bodyText": "Hi, I've been looking into the NATS source and was wondering if Vector executes a \"graceful\" shutdown when the SIGTERM signal occurs. Based on my understanding, the subscriber is killed after a certain point in time after the signal is given and continues to take messages until then.\nShould there be an additional unsubscribe() call somewhere? (or otherwise _connection.close())",
        "url": "https://github.com/vectordotdev/vector/discussions/21756",
        "createdAt": "2024-11-11T10:21:50Z",
        "updatedAt": "2024-11-12T03:16:07Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "benjamin-awd"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21747,
        "title": "Anyone ever seen oddly garbled field names before?",
        "bodyText": "Hey. So I've just noticed that some of our logs which are coming in through the logstash source (it might not be exclusive to this source but it's definitely not a majority) have garbled fields in elasticsearch. For example:\ndtn_request_ocessing_start_time\ndtn_request_processing__finish_time\ndtn_request_processing_fih_time\ndtn_request_processing_finh_time\ndtn_request_processing_finis_time\ndtn_request_processing_finish__time\ndtn_request_processing_finish_h_time\ndtn_request_processing_finish_time\ndtn_request_processing_finishish_time\ndtn_request_processing_finisish_time\ndtn_request_processing_finisnish_time\ndtn_request_processing_finistime\ndtn_request_processing_fish_time\ndtn_request_processing_start_time\n\nLooking back, this only seems to have started happening when we migrated from logstash to vector. I'm not too sure where to even start so would appreciate if anyone has any ideas.\nThere are some error messages for the source but it's not very clear to me what they mean (we don't have acknowledgements enabled on any of our sinks):",
        "url": "https://github.com/vectordotdev/vector/discussions/21747",
        "createdAt": "2024-11-08T17:47:54Z",
        "updatedAt": "2024-11-11T12:36:15Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tronboto"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21727,
        "title": "Tracking number of open file handles by vector",
        "bodyText": "Is there a metric that exposes the number of actively open file handles used?   For example, I'd like to be able to track the number of open files by the file sink.   Thank you.",
        "url": "https://github.com/vectordotdev/vector/discussions/21727",
        "createdAt": "2024-11-07T13:06:44Z",
        "updatedAt": "2024-11-08T17:32:50Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "hartfordfive"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21694,
        "title": "Why don't 429 errors in the sink cause the buffer to fill up?",
        "bodyText": "vector-repro.zip\nSee attached docker config to reproduce this but essentially I have a pipeline with the demo_logs source and the elasticsearch sink. I'm intentionally setting the write thread pool queue size to 0 in elasticsearch (opensearch) so that 429 errors are returned, or at least 429s are returned in the json response body, which is how elasticsearch does 429s.\nAt this point, given that I have set request_retry_partial: true, I'm expecting to see the vector_buffer_events metric go up but it remains at 0. However, if I then stop the opensearch container so that vector can no longer connect, vector_buffer_events does start to go up. Why is this?\nAdditionally, when our elasticsearch cluster is under load and starts returning 429 errors, we're seeing rate(vector_http_client_requests_sent_total[5m]) for the elasticsearch sink continue to rise to huge numbers (see charts below), which isn't what I would expect given that the cluster is under load. From what I can tell, this is then causing bulk indexing tasks to pile up on the cluster, which is something ARC should be helping to avoid.",
        "url": "https://github.com/vectordotdev/vector/discussions/21694",
        "createdAt": "2024-11-04T19:24:38Z",
        "updatedAt": "2024-11-08T14:11:42Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tronboto"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 21720,
        "title": "SQS Parser failed",
        "bodyText": "Every attempt to reach logs from an S3 bucket is failing\nsource{component_kind=\"source\" component_id=archival_logs component_type=aws_s3 component_name=archival_logs}: vector::internal_events::aws_sqs::s3: Failed to process SQS message. message_id=596ee12e-ad8f-442f-add5-fccf41406819 error=Failed to fetch s3://some-bucket/some_filename.log.gz: service error error_code=\"failed_processing_sqs_message\" error_type=\"parser_failed\" stage=\"processing\" internal_log_rate_limit=true\n\nWhen I download this file I can read and parse each line as valid JSON in Ruby.",
        "url": "https://github.com/vectordotdev/vector/discussions/21720",
        "createdAt": "2024-11-06T19:04:25Z",
        "updatedAt": "2024-11-07T19:11:53Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "cnwobi"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 20235,
        "title": "Parse IIS access log",
        "bodyText": "Given an IIS access log (below an excerpt, anonymized), what is the suggested way to parse this and have a message with all fields? Should I use parse_regex!, can you point me at any example?\n#Software: Microsoft Internet Information Services 10.0\n#Version: 1.0\n#Date: 2024-03-04 11:00:00\n#Fields: date time s-ip cs-method cs-uri-stem cs-uri-query s-port cs-username c-ip cs(User-Agent) cs(Referer) sc-status sc-substatus sc-win32-status time-taken\n2024-03-04 11:00:00 10.0.0.1 GET /url1 module=5378 443 - 0.0.0.0 Mozilla/5.0+(iPhone;+CPU+iPhone+OS+17_3_1+like+Mac+OS+X)+AppleWebKit/605.1.15+(KHTML,+like+Gecko)+Version/17.3.1+Mobile/15E148+Safari/604.1 https://referer1 200 0 0 3569\n2024-03-04 11:00:00 10.0.0.1 GET /url2 v=799 443 - 0.0.0.0 Mozilla/5.0+(Windows+NT+10.0;+Win64;+x64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/121.0.0.0+Safari/537.36+OPR/107.0.0.0 https://referer2 200 0 0 106",
        "url": "https://github.com/vectordotdev/vector/discussions/20235",
        "createdAt": "2024-04-04T14:23:15Z",
        "updatedAt": "2024-11-07T16:33:34Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "atomotic"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21718,
        "title": "Ability to use metadata fields that do not end up in final sink",
        "bodyText": "I'm trying to figure out if there's a way to set metadata fields that can be used within the context of the VRL remap execution and within the path of the file sink although I can't seem to determine if this is actually possible or not.    To compare to an existing similar feature, Logstash has the notion of metadata via a special @metadata field (see docs).    Anything within this object can be used within the execution context of the pipeline but it will not end up in the final event that's sent to a given output destination.\nI appreciate any help or guidance with this.   Thanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/21718",
        "createdAt": "2024-11-06T14:31:55Z",
        "updatedAt": "2024-11-07T13:05:19Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "hartfordfive"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 21666,
        "title": "Generate a new metric from the incoming metrics",
        "bodyText": "Hello,\nI have a use case where I need to count the incoming metrics and generate a new metric with the label \"service_namespace\".\nBased on the docs, I have found only 2 possible solutions:\n\nUse metric_to_log -> log_to_metric\nGenerate metric using Lua code\n\nThe problem: these 2 solutions works well, but they are slow or needs a lot more CPU.\nIs there any other alternative to generate this metric? Thanks\nHere is the code of the 2 solutions I have\nSolution 1\n      transforms:\n        metric_to_log:\n          type: metric_to_log\n          inputs:\n            - prometheusreceiver\n\n        metrics_counter_by_service_namespace:\n          type: log_to_metric\n          inputs:\n            - metric_to_log\n          metrics:\n            - type: counter\n              field: name\n              name: count_events_total\n              tags:\n                service_namespace: {{ .tags.service_namespace }}\n\nSolution 2\ntransforms:\n  metrics_counter:\n    type: lua\n    inputs:\n      - prometheusreceiver\n    version: \"2\"\n    hooks:\n      init: init\n      process: process\n      shutdown: shutdown\n    timers:\n      - interval_seconds: 30\n        handler: timer_handler\n    source: |-\n      function init()\n        count = {}\n      end\n\n      function process(event)\n        if event.metric.tags.service_namespace then\n          if not count[event.metric.tags.service_namespace] then\n            count[event.metric.tags.service_namespace] = 0\n          end\n          count[event.metric.tags.service_namespace] = count[event.metric.tags.service_namespace] + 1\n        end\n      end\n\n      function timer_handler(emit)\n        sendMetrics(emit, count)\n      end\n\n      function shutdown(emit)\n        sendMetrics(emit, count)\n      end\n\n      function sendMetrics(emit, count)\n        for k,v in pairs(count) do\n          metric = {\n            name = \"count_events_total\",\n            kind = \"incremental\",\n            timestamp = os.date(\"!*t\"),\n            counter = {\n              value = v\n            },\n            tags = {\n              service_namespace= k,\n            }\n          }\n          emit({ metric = metric })\n          count[k] = 0\n        end\n      end",
        "url": "https://github.com/vectordotdev/vector/discussions/21666",
        "createdAt": "2024-10-31T15:29:19Z",
        "updatedAt": "2024-11-07T12:56:17Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "omarghader"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21717,
        "title": "how to control amount of memory used by vector?",
        "bodyText": "I tried controlling the batch at the sink level but still the amount of memory is not getting controller,",
        "url": "https://github.com/vectordotdev/vector/discussions/21717",
        "createdAt": "2024-11-06T13:54:25Z",
        "updatedAt": "2024-11-06T15:26:51Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "yjagdale"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21713,
        "title": "Replace function by regex - invalid escape character: \\d",
        "bodyText": "Hey,\nI'm trying to run the replace function within a remap transform, here's my config:\ntransforms:\n  remove_fields:\n    type: remap\n    inputs: [\"file\"]\n    source: |-\n      if contains(.message, \"stderr F \") {\n          .message = replace(.message, \"r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{1,}Z' stderr F \", \"\")\n      }\n      if contains(.message, \"stdout F \") {\n          .message = replace(.message, \"r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{1,}Z' stdout F \", \"\")\n      }\n      if contains(.message, \"r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{1,}Z' \") {\n          .message = replace(.message, r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{1,}Z)', \"\")\n      }\n\nI've read that it is possible to run the function with regex according to the docs here:  https://vector.dev/docs/reference/vrl/functions/#replace\nBut still I get this error:\n22 \u2502 \u2502     .message = replace(.message, r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}.\\d{1,}Z)', \"\")\n23 \u2502 \u2502 }\n\u2502 \u2570\u2500^ unexpected error: invalid escape character: \\d\nAny idea what's wrong here?\nI tried to use double backslash and using parentheses instead of braces but nothing works, I get the same error of invalid escape character all the time..\nPlease help :)",
        "url": "https://github.com/vectordotdev/vector/discussions/21713",
        "createdAt": "2024-11-06T09:03:51Z",
        "updatedAt": "2024-11-06T09:57:04Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "itayvolo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 21705,
        "title": "Remove file source additional context text",
        "bodyText": "Hi,\nIm using file source, and im trying to collect log lines from a file as raw as possible.\nSeems like the file source adds some \"context keys\" to the log lines by default, how can I remove them?\nHere's an example of it:\n\noriginal log lines:\n\nTraceback (most recent call last):\nFile \"\", line 198, in _test_test\n\nlog lines after vector pipeline with file source:\n\n2024-11-05T14:20:03.873658952Z stderr F Traceback (most recent call last):\n2024-11-05T14:20:03.873688283Z stderr F\n2024-11-05T14:20:03.874434068Z stderr F File \"\", line 198, in _test_test",
        "url": "https://github.com/vectordotdev/vector/discussions/21705",
        "createdAt": "2024-11-05T14:35:55Z",
        "updatedAt": "2024-11-06T14:58:07Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "itayvolo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21703,
        "title": "Metrics for elastic batches",
        "bodyText": "Hi all, is there a way the get some metrics of how big the batches are that are send by the elastic sink to an endpoint. Are there any metrics or logs present. And maybe also which of the three settings for batches triggered the batch. That would be really helpful for debugging and optimizing the batches.",
        "url": "https://github.com/vectordotdev/vector/discussions/21703",
        "createdAt": "2024-11-05T10:30:21Z",
        "updatedAt": "2024-11-06T15:22:42Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "timfehr"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16225,
        "title": "Vector throwing 401 with basic_auth for loki sink",
        "bodyText": "I have enabled basic auth for loki. But vector client is unable to authenticate itself while sending messages. Below is my vector client config\nsinks:\n    loki_sink:\n      type: loki\n      inputs:\n        - sflow_trans\n      endpoint: http://loki-gateway:80\n      auth:\n        password: examplepassword\n        strategy: basic\n        user: lokiuser\n      labels:\n        forwader1: vector\n        type: nflow\n        tenant_id: xx-nflow\n        zone: |-\n          {{ print \"{{ dvc_zone }}\" }}\n      encoding:\n        codec: json\n      healthcheck:\n        enabled: false\n\nWhen I check the nginx logs , I can see the loki user is trying to POST something but getting 401. Below are my Nginx log\n10.0.1.6 - lokiuser [31/Jan/2023:22:37:47 +0000]  401 \"POST /loki/api/v1/push HTTP/1.1\" 10 \"-\" \"Vector/0.27.0 (x86_64-unknown-linux-gnu 5623d1e 2023-01-18)\" \"-\"\nI am switching from logstash to vector. I thought try to with logstash again to confirm if there is some other problem, But it turns out that with logstash it just works fine. This is my logstash config -\n   output {\n      loki{\n       url => \"http://loki-gateway:80/loki/api/v1/push\"\n       include_fields => [\"stream\",\"vpcid\",\"subnetid\",\"interfaceid\",\"accountid\",\"protocol\",\"flowdirection\",\"logstatus\",\"action\",\"cluster\",\"logtype\"]\n       message_field => \"formatted_message\"\n       tenant_id => \"xx-nflow1\"\n       username => \"lokiuser\"\n       password => \"examplepassword\"\n     }\n     }\n\nBelow are my nginx logs for logstash\n10.0.1.42 - new [01/Feb/2023:00:39:38 +0000]  204 \"POST /loki/api/v1/push HTTP/1.1\" 0 \"-\" \"loki-logstash\" **\"-\"**\nAny pointers? How can I get this resolved ?\nThanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/16225",
        "createdAt": "2023-02-01T00:47:59Z",
        "updatedAt": "2024-11-19T16:26:47Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Amit-Hora"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 21682,
        "title": "Best way to structure sources and sinks in an agent + aggregator architecture",
        "bodyText": "We are using an agent + aggregator architecture. The agent is ingesting data from file sources but also from kubernetes log sources. These logs are shipped into an aggregator but ultimately end up on different sinks.\nIs there a recommendation for how one should setup the sources and sinks? Currently, we have two sinks in agent (for each source) but are using a single vector source in the aggregator with a route transform on top to separate the log streams. I would like to know if that's the most appropriate or whether the aggregator should also use two separate sources and then avoid the route transform.",
        "url": "https://github.com/vectordotdev/vector/discussions/21682",
        "createdAt": "2024-11-01T22:42:51Z",
        "updatedAt": "2024-11-01T23:06:51Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "flaviofcruz"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21672,
        "title": "Aggregator transform on count metrics don't behave as expected",
        "bodyText": "Using version 0.42.0\nI'm ingesting millions of of log events that I'm transforming into +1 count metrics. These events span over 1 hour. The metric conversion part went smoothly, but then I was hoping to aggregate them into 10 second bins before pushing to the metric server.\nHowever, during testing I couldn't seem to get the metric aggregation to respect the interval_ms setting. I've expected set it to 10_000, but that seemed to have aggregated everything into a single metric. Further playing around with the metric showed that the value I enter into interval_ms seem to be treated as \"minutes\" instead of \"milliseconds\". For example, if I input a value of 2 for interval_ms, all my metrics get aggregated into 2 minute bins. Is this a bug?\nHere's the relevant part of my config\n[sources.in]\ntype = \"stdin\"\n\n[transforms.alb]\ntype = \"remap\"\ninputs = [\"in\"]\nsource = '''\n  . = parse_aws_alb_log!(.message)\n  .timestamp = parse_timestamp!(.request_creation_time, \"%+\")\n'''\n\n[transforms.alb_metric]\ntype = \"log_to_metric\"\ninputs = [ \"alb\" ]\n\n  [[transforms.alb_metric.metrics]]\n  type = \"counter\"\n  field = \"elb_status_code\"\n  name = \"load_balancer_request_response\"\n  namespace = \"test\"\n\n    [transforms.alb_metric.metrics.tags]\n    status = \"{{elb_status_code}}\"\n    target_group = \"{{target_group_arn}}\"\n\n\n[transforms.aggregated_alb_metric]\ntype = \"aggregate\"\ninputs = [ \"alb_metric\"]\ninterval_ms = 10_000",
        "url": "https://github.com/vectordotdev/vector/discussions/21672",
        "createdAt": "2024-10-31T22:03:43Z",
        "updatedAt": "2024-11-01T14:25:54Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "timtylin"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21632,
        "title": "RollWorkload with New Config",
        "bodyText": "Hello!\nWe are running vector aggregator as a helm chart with configuration through existingConfigMap. When we update the ConfigMap, it gets updated in the Vector pods, but the changes are not applied until the statefulset is restarted manually. We have rollWorkload set to the default true. I don't see the checksum annotation on the pods that should roll the workloads (if I am understanding that configuration correctly). Is there something I am missing that will allow the pods to restart automatically?",
        "url": "https://github.com/vectordotdev/vector/discussions/21632",
        "createdAt": "2024-10-28T14:45:01Z",
        "updatedAt": "2024-10-30T22:27:05Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "twcchu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21664,
        "title": "Buffering per service without Kafka",
        "bodyText": "I'm interested in whether vector might be a good fit for my use case -- I have an existing solution, but it doesn't seem to solve a specific problem I have. I have a kubernetes cluster using the prometheus operator to scrape metrics from many different workloads. Unfortunately, if a single Deployment or other workload begins to create many pods rapidly, this can produce too many series for my metrics sink.\nIn my current solution, I don't have a good way to proactively limit the number of cumulative series from a single ServiceMonitor (all pods from a Deployment, for example). And no single pod produces so many metrics that I can use prometheus scrape_config.label_limits.\nIf I used vector without kafka as described in the docs, would it be possible to have vector drop metrics from a single ServiceMonitor based on the cumulative cardinality of that service monitor?",
        "url": "https://github.com/vectordotdev/vector/discussions/21664",
        "createdAt": "2024-10-30T21:06:46Z",
        "updatedAt": "2024-10-30T21:14:24Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "brycefisher"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21628,
        "title": "How to filter collected log files by pod label in kuberneteslogs?",
        "bodyText": "For example, only collect logs from pods that have either the .kubernetes.podlabels.service or .kubernetes.podlabels.unit labels",
        "url": "https://github.com/vectordotdev/vector/discussions/21628",
        "createdAt": "2024-10-28T03:00:15Z",
        "updatedAt": "2024-10-30T03:19:00Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "guidao"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 21652,
        "title": "How long does backpressure \"stick around\" on a sink?",
        "bodyText": "We're running Agents and Aggregators in Kubernetes (http sink, http_server source). In most clusters the majority of log volume comes from a few nodes/Agents. Our Aggregators scale out with a Horizontal Pod Autoscaler as average CPU goes up but in general a handful of our Aggregators are getting blasted while the rest aren't doing all that much.\nIf an one of these hot Agents receives backpressure because it's blasting a specific Aggregator, when max_connection_age_secs is reached on the http_server source and the Agent disconnects to find a new target, will that backpressure from the previous Aggregator hang around? Or will it immediately get feedback from the next Aggregator it sends to (assuming it's one that wasn't just getting blasted by another Agent) that it can speed up sending again?",
        "url": "https://github.com/vectordotdev/vector/discussions/21652",
        "createdAt": "2024-10-29T20:08:01Z",
        "updatedAt": "2024-10-31T18:55:57Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "davidpellcb"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21638,
        "title": "Splunk HEC Metrics AggregatedSummary Support",
        "bodyText": "Good afternoon,\nI am testing out vector to see if it fulfill some of our needs. In our tests so far we are going prometheus -> vector -> splunk enterprise. So far it looks like it does what we need it to do, but I've hit one roadblock. It appears the AggregatedSummary metric type is not supported by the splunk_hec_metrics sink. Is this the case, have I not configured it correctly, or am I misreading the error message?\nFor example here is the prometheus config:\nglobal:\n  scrape_interval: 10s\n\nscrape_configs:\n  - job_name: 'prometheus_master'\n    static_configs:\n      - targets: ['localhost:9090']\n\nHere is the vector config:\napi:\n  enabled: true\n\nsources:\n  promScrape:\n    type: prometheus_scrape\n    endpoints:\n      - http://localhost:9090/metrics\n\nsinks:\n  promToSplunk:\n    type: splunk_hec_metrics\n    inputs:\n      - promScrape\n    default_token: [my token]\n    endpoint: [my endpoint]\n    index: \"promvecmetrics\"\n    tls:\n      verify_certificate: false\n\n\nWith the above I get the following error:\nERROR sink{component_kind=\"sink\" component_id=promToSplunk component_type=splunk_hec_metrics}: vector::internal_events::splunk_hec::sink: Invalid metric received. error=\"Metric kind not supported.\" error_type=\"invalid_metric\" stage=\"processing\" value=AggregatedSummary { quantiles: [Quantile { quantile: 0.0, value: 5.0525e-5 }, Quantile { quantile: 0.25, value: 8.2525e-5 }, Quantile { quantile: 0.5, value: 0.000108933 }, Quantile { quantile: 0.75, value: 0.000187652 }, Quantile { quantile: 1.0, value: 0.001971355 }], count: 2869, sum: 0.454619628 } kind=Absolute internal_log_rate_limit=true\n\nBecause of the vector::internal_events::splunk_hec::sink: Invalid metric received.  I assume this means that the splunk_hec_metrics sink does not support AggregatedSummary metrics. Is that interpretation correct. I ask because I see the open issue  Support AggregatedSummary for gcp_stackdriver sink #9530  So I am thinking this may be another case where one of the sinks does not support AggregatedSummary.\nIf it is a supported metric type could you point me in the direction of a working example?\nIf it is not supported, is there a workaround to get this metric type through to splunk? I am guessing I would have to transform it, remap it, then send it too splunk_hec_logs where I could use the built in log2metric sourcetypes.\nFinally is there a page I can go to too see all of the supported metric types for each sync for future reference.",
        "url": "https://github.com/vectordotdev/vector/discussions/21638",
        "createdAt": "2024-10-28T20:12:41Z",
        "updatedAt": "2024-10-29T14:08:48Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "clmaestas-work"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21630,
        "title": "Manticore integration",
        "bodyText": "Is vector compatible with manticore as a sink ?\nhttps://github.com/manticoresoftware/manticoresearch",
        "url": "https://github.com/vectordotdev/vector/discussions/21630",
        "createdAt": "2024-10-28T07:00:13Z",
        "updatedAt": "2024-10-28T14:29:59Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "inferno-umar"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21618,
        "title": "prometheus_remote_write sinks stuck if one sink is unavailable",
        "bodyText": "Hi!\nProblem\nI'm setting up a simple configuration where metrics from node exporter are pushed to victoriametrics. One is primary and the second one is backup. I have an issue that metrics are not pushed to backup if primary is down.\nWhen I stop vict1, metrics stop going into vict2 too.\nI expect these sinks to work separatelly and independent from each other.\nI'm using tlast_over_time function to check incoming metrics on backup vm, but last TS for backup vm is when primary was up.\nDebug log doesn't say much\nConfiguration\nsources:\n  local_node_exporter:\n    endpoints:\n    - http://127.0.0.1:9429/metrics\n    instance_tag: instance\n    scrape_interval_secs: 5\n    scrape_timeout_secs: 4\n    type: prometheus_scrape\ntransforms:\n  local_node_exporter_transform:\n    drop_on_abort: true\n    drop_on_error: true\n    inputs:\n    - local_node_exporter\n    source: 'metric_port = split!(.tags.instance, pattern: \":\")[1]\n\n      .tags.instance = \"127.0.0.1:\" + metric_port\n\n      .tags.domain = \"test_vict\"'\n    type: remap\n\nsinks:\n  vict1:\n    endpoint: http://127.0.0.1:8431/api/v1/write\n    healthcheck:\n      enabled: false\n    inputs:\n    - local_node_exporter_transform\n    type: prometheus_remote_write\n  vict2:\n    endpoint: http://127.0.0.1:8432/api/v1/write\n    healthcheck:\n      enabled: false\n    inputs:\n    - local_node_exporter_transform\n    type: prometheus_remote_write\n\nVersion\nI'm using docker image timberio/vector:0.39.0-alpine\nDebug Output\n2024-10-25T10:08:43.907375Z DEBUG sink{component_kind=\"sink\" component_id=vict2 component_type=prometheus_remote_write}: vector::utilization: utilization=0.00000004839025547411212\n2024-10-25T10:08:48.907572Z DEBUG sink{component_kind=\"sink\" component_id=vict2 component_type=prometheus_remote_write}: vector::utilization: utilization=0.00000005487706745736119\n2024-10-25T10:08:53.906891Z DEBUG sink{component_kind=\"sink\" component_id=vict2 component_type=prometheus_remote_write}: vector::utilization: utilization=0.000000039692361454338906\n2024-10-25T10:08:58.907082Z DEBUG sink{component_kind=\"sink\" component_id=vict2 component_type=prometheus_remote_write}: vector::utilization: utilization=0.00000004662761927132699\n2024-10-25T10:09:03.907504Z DEBUG sink{component_kind=\"sink\" component_id=vict2 component_type=prometheus_remote_write}: vector::utilization: utilization=0.000000037779958012585306\n2024-10-25T10:09:05.269205Z DEBUG sink{component_kind=\"sink\" component_id=vict1 component_type=prometheus_remote_write}:request{request_id=1}:http: vector::internal_events::http_client: Sending HTTP request. uri=http://127.0.0.1:8431/api/v1/write method=POST version=HTTP/1.1 headers={\"x-prometheus-remote-write-version\": \"0.1.0\", \"content-type\": \"application/x-protobuf\", \"content-encoding\": \"snappy\", \"user-agent\": \"Vector/0.39.0 (x86_64-unknown-linux-musl 73da9bb 2024-06-17 16:00:23.791735272)\", \"accept-encoding\": \"identity\"} body=[11959 bytes]\n2024-10-25T10:09:05.269297Z DEBUG sink{component_kind=\"sink\" component_id=vict1 component_type=prometheus_remote_write}:request{request_id=1}:http: hyper::client::connect::http: connecting to 127.0.0.1:8431\n2024-10-25T10:09:05.269635Z  WARN sink{component_kind=\"sink\" component_id=vict1 component_type=prometheus_remote_write}:request{request_id=1}:http: vector::internal_events::http_client: Internal log [HTTP error.] has been suppressed 1 times.\n2024-10-25T10:09:05.269654Z  WARN sink{component_kind=\"sink\" component_id=vict1 component_type=prometheus_remote_write}:request{request_id=1}:http: vector::internal_events::http_client: HTTP error. error=error trying to connect: tcp connect error: Connection refused (os error 111) error_type=\"request_failed\" stage=\"processing\" internal_log_rate_limit=true\n2024-10-25T10:09:05.269721Z  WARN sink{component_kind=\"sink\" component_id=vict1 component_type=prometheus_remote_write}:request{request_id=1}: vector::sinks::util::retries: Internal log [Retrying after error.] has been suppressed 1 times.\n2024-10-25T10:09:05.269740Z  WARN sink{component_kind=\"sink\" component_id=vict1 component_type=prometheus_remote_write}:request{request_id=1}: vector::sinks::util::retries: Retrying after error. error=Failed to make HTTP(S) request: error trying to connect: tcp connect error: Connection refused (os error 111) internal_log_rate_limit=true\n2024-10-25T10:09:05.269759Z DEBUG sink{component_kind=\"sink\" component_id=vict1 component_type=prometheus_remote_write}:request{request_id=1}: vector::sinks::util::retries: Retrying request. delay_ms=781\n2024-10-25T10:09:06.052105Z DEBUG sink{component_kind=\"sink\" component_id=vict1 component_type=prometheus_remote_write}:request{request_id=1}:http: vector::internal_events::http_client: Sending HTTP request. uri=http://127.0.0.1:8431/api/v1/write method=POST version=HTTP/1.1 headers={\"x-prometheus-remote-write-version\": \"0.1.0\", \"content-type\": \"application/x-protobuf\", \"content-encoding\": \"snappy\", \"user-agent\": \"Vector/0.39.0 (x86_64-unknown-linux-musl 73da9bb 2024-06-17 16:00:23.791735272)\", \"accept-encoding\": \"identity\"} body=[11959 bytes]\n2024-10-25T10:09:06.052185Z DEBUG sink{component_kind=\"sink\" component_id=vict1 component_type=prometheus_remote_write}:request{request_id=1}:http: hyper::client::connect::http: connecting to 127.0.0.1:8431\n2024-10-25T10:09:06.052497Z  WARN sink{component_kind=\"sink\" component_id=vict1 component_type=prometheus_remote_write}:request{request_id=1}:http: vector::internal_events::http_client: Internal log [HTTP error.] is being suppressed to avoid flooding.\n2024-10-25T10:09:06.052555Z  WARN sink{component_kind=\"sink\" component_id=vict1 component_type=prometheus_remote_write}:request{request_id=1}: vector::sinks::util::retries: Internal log [Retrying after error.] is being suppressed to avoid flooding.\n2024-10-25T10:09:06.052572Z DEBUG sink{component_kind=\"sink\" component_id=vict1 component_type=prometheus_remote_write}:request{request_id=1}: vector::sinks::util::retries: Retrying request. delay_ms=18577\n2024-10-25T10:09:08.907050Z DEBUG sink{component_kind=\"sink\" component_id=vict2 component_type=prometheus_remote_write}: vector::utilization: utilization=0.000000028800265357621418\n2024-10-25T10:09:13.907465Z DEBUG sink{component_kind=\"sink\" component_id=vict2 component_type=prometheus_remote_write}: vector::utilization: utilization=0.0000000401369387910745",
        "url": "https://github.com/vectordotdev/vector/discussions/21618",
        "createdAt": "2024-10-25T10:12:52Z",
        "updatedAt": "2024-10-28T14:19:28Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "hedard"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 3
    },
    {
        "number": 21604,
        "title": "Using the HTTP source or VRL is there any way to store the source message on a separate field?",
        "bodyText": "Hello,\nI'm using vector to build something like an ingestion layer to make it easier to onboard new applications on our SIEM, for this I'm using the http source and the kafka sink.\nAny request made the http source in the path /logs/someName will create a topic in kafka named some_prefix-someName.\nI'm using json in both the source and the sink and the main issue I'm having right now is that the http source will save the fields from the source message in the root of the event, and the timestamp field that the http source adds can conflict with a timestamp field from the source message.\nI know that if the source message has a timestamp field it will override the timestamp added by vector, but with this I lose the information of the vector timestamp.\nIs there any way to store the source message received by the http source in a different field?  Or maybe tell vector to use a different field name for its timestamp field?\nMy config is something like this:\n# sources\n\nsources:\n  logs:\n    type: http_server\n    address: 0.0.0.0:30000\n    encoding: json\n    host_key: \"\"\n    method: POST\n    path: /logs\n    path_key: vector_request_path\n    response_code: 200\n    strict_path: false\n\n# transform\n\ntransforms:\n  topic:\n    inputs: [ \"logs\" ]\n    type: remap\n    drop_on_error: true\n    source: |\n      . |= parse_groks!(\n                .vector_request_path,\n                patterns: [\n                    \"^/logs/%{DATA:log_topic}\"\n                ]\n            )\n      del(.source_type)\n      del(.vector_request_path)\n\n  event:\n    inputs: [\"*\"]\n    type: filter\n    condition:\n      type: \"vrl\"\n      source: \".log_topic != null\"\n\n# sinks\n\nsinks:\n  kafka:\n    type: kafka\n    inputs: [\"event\"]\n    bootstrap_servers: 127.0.0.1:29092\n    topic: \"some_prefix-{{log_topic}}\"\n    encoding:\n      codec: json\n\nThe sample document I'm testing is this one:\n[ {\"field\": \"value1\", \"anotherField\": \"anotherValue\"},\n{\"field\": \"value2\", \"extraField\": [\"value2.1\", \"value2.2\"]},\n{\"field\": \"value3\", \"timestamp\": \"override-vector-timestamp\"}\n]\n\nAnd this how it looks like in Kafka\n\nI'm looking for to have something like this in Kafka:\n{\n\t\"message\": {\n        \"anotherField\": \"anotherValue\",\n\t    \"field\": \"value1\"\n    },\n\t\"log_topic\": \"sample\",\n\t\"timestamp\": \"2024-10-24T04:00:58.691623361Z\"\n}\n\n{\n\t\"message\": { \n        \"extraField\": [\n\t\t    \"value2.1\",\n\t\t    \"value2.2\"\n\t    ],\n\t    \"field\": \"value2\"\n    },\n\t\"log_topic\": \"sample\",\n\t\"timestamp\": \"2024-10-24T04:00:58.691623361Z\"\n}\n\n{\n\t\"message\": { \n        \"field\": \"value3\",\n        \"timestamp\": \"override-vector-timestamp\"\n    },\n    \"log_topic\": \"sample\",\n\t\"timestamp\": \"2024-10-24T04:00:58.691623361Z\"\n}",
        "url": "https://github.com/vectordotdev/vector/discussions/21604",
        "createdAt": "2024-10-24T04:10:32Z",
        "updatedAt": "2024-10-25T04:05:17Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "leandrojmp"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18806,
        "title": "Best way to parse mysql slow query log",
        "bodyText": "Hello,\nI have this mysql slow query log content bebow\n/usr/sbin/mysqld, Version: 5.7.35-38-log (Percona Server (GPL), Release '38', Revision '3692a61'), Time: 2023-10-05T00:02:02.380498Z. started with:\nTcp port: 3308  Unix socket: /var/run/mysqld/mysqld@mysqld80.sock\nTime                 Id Command    Argument\n# Time: 2023-10-05T04:00:57.766790Z\n# User@Host: db_user1[db_user1] @  [192.168.1.100]  Id: 774102996\n# Schema: my_db_name1  Last_errno: 0  Killed: 0\n# Query_time: 56.509475  Lock_time: 0.000331  Rows_sent: 4012  Rows_examined: 2969281  Rows_affected: 0  Bytes_sent: 5562372\n# Tmp_tables: 2  Tmp_disk_tables: 1  Tmp_table_sizes: 16384\n# InnoDB_trx_id: 0\n# QC_Hit: No  Full_scan: Yes  Full_join: No  Tmp_table: Yes  Tmp_table_on_disk: Yes\n# Filesort: Yes  Filesort_on_disk: Yes  Merge_passes: 4\n#   InnoDB_IO_r_ops: 0  InnoDB_IO_r_bytes: 0  InnoDB_IO_r_wait: 0.000000\n#   InnoDB_rec_lock_wait: 0.000000  InnoDB_queue_wait: 0.000000\n#   InnoDB_pages_distinct: 8157\nuse my_db_name1;\nSET timestamp=1696478457;\nSELECT\n                                profile,\n                                id\n                        FROM event_table;\n# Time: 2023-10-05T05:00:03.564425Z\n# User@Host: db_user2[db_user2] @  [192.168.2.101]  Id: 774984395\n# Schema: my_db_name2  Last_errno: 0  Killed: 0\n# Query_time: 2.032343  Lock_time: 0.000237  Rows_sent: 8  Rows_examined: 357339  Rows_affected: 0  Bytes_sent: 960\n# Tmp_tables: 5  Tmp_disk_tables: 0  Tmp_table_sizes: 8374272\n# InnoDB_trx_id: 0\n# QC_Hit: No  Full_scan: Yes  Full_join: No  Tmp_table: Yes  Tmp_table_on_disk: No\n# Filesort: Yes  Filesort_on_disk: No  Merge_passes: 0\n#   InnoDB_IO_r_ops: 0  InnoDB_IO_r_bytes: 0  InnoDB_IO_r_wait: 0.000000\n#   InnoDB_rec_lock_wait: 0.000000  InnoDB_queue_wait: 0.000000\n#   InnoDB_pages_distinct: 4\nuse my_db_name2;\nSET timestamp=1696482003;\nSELECT *\n                        FROM event_2_table;\n\nI try to parse the log but couldn't have good result.\nHere is my vector config:\nsources:\n  mysql_slow_log:\n    type: file\n    multiline:\n      condition_pattern: \"^# Time: \"\n      mode: \"halt_before\"\n      start_pattern: \"^# Time: \"\n      timeout_ms: 20000\n    include:\n      - \"/tmp/mysql-slow-example.log\"\n\nsinks:\n  print:\n    type: \"console\"\n    inputs: [\"mysql_slow_log_parser_grok\"]\n    encoding:\n      codec: \"json\"\n\ntransforms:\n  mysql_slow_log_parser_grok:\n    type: remap\n    inputs: [\"mysql_slow_log\"]\n    drop_on_error: true\n    source: >-\n      . |= parse_grok!(\n      .message,\n      \"^# Time: %{TIMESTAMP_ISO8601:timestamp}\\n# User@Host: %{USER:user}\\\\[%{DATA:user2}\\\\] @  \\\\[(%{IP:ip}|)]  Id: %{NUMBER:thread_id}\\n# Schema: %{DATA:schema}  Last_errno: %{NUMBER:last_errno}  Killed: %{NUMBER:killed}\\n# Query_time: %{NUMBER:query_time}  Lock_time: %{NUMBER:lock_time}  Rows_sent: %{NUMBER:rows_sent}  Rows_examined: %{NUMBER:rows_examined}  Rows_affected: %{NUMBER:rows_affected}  Bytes_sent: %{NUMBER:bytes_sent}\\n# Tmp_tables: %{NUMBER:tmp_tables}  Tmp_disk_tables: %{NUMBER:tmp_disk_tables}  Tmp_table_sizes: %{NUMBER:tmp_table_sizes}\\n# InnoDB_trx_id: %{NUMBER:innoDB_trx_id}\\n# QC_Hit: %{GREEDYDATA:qc_hit}  Full_scan: %{GREEDYDATA:full_scan}  Full_join: %{GREEDYDATA:full_join}  Tmp_table: %{GREEDYDATA:tmp_table}  Tmp_table_on_disk: %{GREEDYDATA:tmp_table_on_disk}\\n# Filesort: %{GREEDYDATA:Filesort}  Filesort_on_disk: %{GREEDYDATA:filesort_on_disk}  Merge_passes: %{NUMBER:merge_passes}\\n#   InnoDB_IO_r_ops: %{NUMBER:innodb_io_r_ops}  InnoDB_IO_r_bytes: %{NUMBER:innodb_io_r_bytes}  InnoDB_IO_r_wait: %{NUMBER:innodb_io_r_wait}\\n#   InnoDB_rec_lock_wait: %{NUMBER:innodb_rec_lock_wait}  InnoDB_queue_wait: %{NUMBER:innodb_queue_wait}\\n#   InnoDB_pages_distinct: %{NUMBER:innodb_pages_distinct}\\n%{GREEDYDATA:query}\"\n\n\nHere is the result in console:\nOct 09 09:13:44 myserver vector[3286784]: 2023-10-09T09:13:44.106100Z ERROR transform{component_kind=\"transform\" component_id=mysql_slow_log_parser_grok component_type=remap component_name=mysql_slow_log_parser_grok}: vector::internal_events::remap: Mapping failed with event. error=\"function call error for \\\"parse_grok\\\" at (5:1272): unable to parse input with grok pattern\" error_type=\"conversion_failed\" stage=\"processing\" internal_log_rate_limit=true\nOct 09 09:13:44 myserver vector[3286784]: 2023-10-09T09:13:44.106258Z ERROR transform{component_kind=\"transform\" component_id=mysql_slow_log_parser_grok component_type=remap component_name=mysql_slow_log_parser_grok}: vector_common::internal_event::component_events_dropped: Events dropped intentional=false count=1 reason=\"Mapping failed with event.\" internal_log_rate_limit=true\nOct 09 09:13:44 myserver vector[3286784]: 2023-10-09T09:13:44.106393Z ERROR transform{component_kind=\"transform\" component_id=mysql_slow_log_parser_grok component_type=remap component_name=mysql_slow_log_parser_grok}: vector::internal_events::remap: Internal log [Mapping failed with event.] is being suppressed to avoid flooding.\nOct 09 09:13:44 myserver vector[3286784]: 2023-10-09T09:13:44.106440Z ERROR transform{component_kind=\"transform\" component_id=mysql_slow_log_parser_grok component_type=remap component_name=mysql_slow_log_parser_grok}: vector_common::internal_event::component_events_dropped: Internal log [Events dropped] is being suppressed to avoid flooding.\nOct 09 09:13:44 myserver vector[3286784]: {\"Filesort\":\"Yes\",\"bytes_sent\":\"5562372\",\"file\":\"/tmp/mysql-slow-example.log\",\"filesort_on_disk\":\"Yes\",\"full_join\":\"No\",\"full_scan\":\"Yes\",\"host\":\"myserver\",\"innoDB_trx_id\":\"0\",\"innodb_io_r_bytes\":\"0\",\"innodb_io_r_ops\":\"0\",\"innodb_io_r_wait\":\"0.000000\",\"innodb_pages_distinct\":\"8157\",\"innodb_queue_wait\":\"0.000000\",\"innodb_rec_lock_wait\":\"0.000000\",\"ip\":\"192.168.1.100\",\"killed\":\"0\",\"last_errno\":\"0\",\"lock_time\":\"0.000331\",\"merge_passes\":\"4\",\"message\":\"# Time: 2023-10-05T04:00:57.766790Z\\n# User@Host: db_user1[db_user1] @  [192.168.1.100]  Id: 774102996\\n# Schema: my_db_name1  Last_errno: 0  Killed: 0\\n# Query_time: 56.509475  Lock_time: 0.000331  Rows_sent: 4012  Rows_examined: 2969281  Rows_affected: 0  Bytes_sent: 5562372\\n# Tmp_tables: 2  Tmp_disk_tables: 1  Tmp_table_sizes: 16384\\n# InnoDB_trx_id: 0\\n# QC_Hit: No  Full_scan: Yes  Full_join: No  Tmp_table: Yes  Tmp_table_on_disk: Yes\\n# Filesort: Yes  Filesort_on_disk: Yes  Merge_passes: 4\\n#   InnoDB_IO_r_ops: 0  InnoDB_IO_r_bytes: 0  InnoDB_IO_r_wait: 0.000000\\n#   InnoDB_rec_lock_wait: 0.000000  InnoDB_queue_wait: 0.000000\\n#   InnoDB_pages_distinct: 8157\\nuse my_db_name1;\\nSET timestamp=1696478457;\\nSELECT\\n                                profile,\\n                                id\\n                        FROM event_table;\",\"qc_hit\":\"No\",\"query\":\"use my_db_name1;\",\"query_time\":\"56.509475\",\"rows_affected\":\"0\",\"rows_examined\":\"2969281\",\"rows_sent\":\"4012\",\"schema\":\"my_db_name1\",\"source_type\":\"file\",\"thread_id\":\"774102996\",\"timestamp\":\"2023-10-05T04:00:57.766790Z\",\"tmp_disk_tables\":\"1\",\"tmp_table\":\"Yes\",\"tmp_table_on_disk\":\"Yes\",\"tmp_table_sizes\":\"16384\",\"tmp_tables\":\"2\",\"user\":\"db_user1\",\"user2\":\"db_user1\"}\nOct 09 09:13:44 myserver vector[3286784]: {\"Filesort\":\"Yes\",\"bytes_sent\":\"960\",\"file\":\"/tmp/mysql-slow-example.log\",\"filesort_on_disk\":\"No\",\"full_join\":\"No\",\"full_scan\":\"Yes\",\"host\":\"myserver\",\"innoDB_trx_id\":\"0\",\"innodb_io_r_bytes\":\"0\",\"innodb_io_r_ops\":\"0\",\"innodb_io_r_wait\":\"0.000000\",\"innodb_pages_distinct\":\"4\",\"innodb_queue_wait\":\"0.000000\",\"innodb_rec_lock_wait\":\"0.000000\",\"ip\":\"192.168.2.101\",\"killed\":\"0\",\"last_errno\":\"0\",\"lock_time\":\"0.000237\",\"merge_passes\":\"0\",\"message\":\"# Time: 2023-10-05T05:00:03.564425Z\\n# User@Host: db_user2[db_user2] @  [192.168.2.101]  Id: 774984395\\n# Schema: my_db_name2  Last_errno: 0  Killed: 0\\n# Query_time: 2.032343  Lock_time: 0.000237  Rows_sent: 8  Rows_examined: 357339  Rows_affected: 0  Bytes_sent: 960\\n# Tmp_tables: 5  Tmp_disk_tables: 0  Tmp_table_sizes: 8374272\\n# InnoDB_trx_id: 0\\n# QC_Hit: No  Full_scan: Yes  Full_join: No  Tmp_table: Yes  Tmp_table_on_disk: No\\n# Filesort: Yes  Filesort_on_disk: No  Merge_passes: 0\\n#   InnoDB_IO_r_ops: 0  InnoDB_IO_r_bytes: 0  InnoDB_IO_r_wait: 0.000000\\n#   InnoDB_rec_lock_wait: 0.000000  InnoDB_queue_wait: 0.000000\\n#   InnoDB_pages_distinct: 4\\nuse my_db_name2;\\nSET timestamp=1696482003;\\nSELECT *\\n                        FROM event_2_table;\",\"qc_hit\":\"No\",\"query\":\"use my_db_name2;\",\"query_time\":\"2.032343\",\"rows_affected\":\"0\",\"rows_examined\":\"357339\",\"rows_sent\":\"8\",\"schema\":\"my_db_name2\",\"source_type\":\"file\",\"thread_id\":\"774984395\",\"timestamp\":\"2023-10-05T05:00:03.564425Z\",\"tmp_disk_tables\":\"0\",\"tmp_table\":\"Yes\",\"tmp_table_on_disk\":\"No\",\"tmp_table_sizes\":\"8374272\",\"tmp_tables\":\"5\",\"user\":\"db_user2\",\"user2\":\"db_user2\"}\n\n\nQuestions:\n\nThe query can contain only the first line of the query. How can we define grok to contain a full query?\nHow to remove message in the parsed log? It contains full info including the comments\n\nIf you have a better way to parse mysql slow query log, please tell me. I tried to search but could get the info I need....\nAppreciate if someone can help me.\nThank you.",
        "url": "https://github.com/vectordotdev/vector/discussions/18806",
        "createdAt": "2023-10-09T09:34:08Z",
        "updatedAt": "2024-10-24T21:40:22Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "hungpr0"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21570,
        "title": "Number of consumer threads for a kafka source",
        "bodyText": "Based on the kafka source documentation, it doesn't appear you can directly specify the number of consumer threads.   Does it end up being a single thread per defined kafka source block or does it automatically detect the number of partitions and set the number of consumers accordingly?\nI'm looking at running multiple Vector instances in Kubernetes to consumer from various topics, although I need each instance to have an optimal consumer count in order to avoid having too many Vector pods deployed, each using up an IP.    For example, if I have a topic with 10 partitions, I could choose to deploy two Vector pods, each with a kafka source, configured with the same topic and consumer group but each only having a total of 5 consumer threads.   This would ensure that the consumption of all 10 partitions is evenly distributed amongst both Vector instances.   For example, with a tool such as Logstash, I can easily specify the number of consumer threads:\ninput {\n  kafka {\n    bootstrap_servers => \"kafka01:9092,kafka02:9092,kafka03:9092\"\n    topics => [\"service-logs\"]\n    group_id => \"service-logs-consumer\"\n    consumer_threads => 5\n    codec => json\n  }\n}\n\n\nI appreciate any information/help relating to this!",
        "url": "https://github.com/vectordotdev/vector/discussions/21570",
        "createdAt": "2024-10-21T18:07:41Z",
        "updatedAt": "2024-10-22T15:56:10Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "hartfordfive"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 21553,
        "title": "Workaround for \"Missing environment variable in config\" validation",
        "bodyText": "Hi!\nI'm looking for a way to workaround or suppress Vector's \"Missing environment variable in config\" validation. The reason is I'm trying to attach the schema generated by vector generate-schema to Vector's YAML. There are two ways IDEs support doing that:\n# $schema: ./vector.schema.json\nand\n# yaml-language-server: $schema=./vector.schema.json\nUnfortunately, both options include $schema in a comment so Vector validation fails with\nx Missing environment variable in config. name = \"schema\"",
        "url": "https://github.com/vectordotdev/vector/discussions/21553",
        "createdAt": "2024-10-18T23:00:18Z",
        "updatedAt": "2024-10-21T14:55:23Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "GreyTeardrop"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21545,
        "title": "Experiences with vector as Logstash replacement in a high throughput Filebeat/Auditbeat env",
        "bodyText": "Hi everyone,\nI'm currently exploring vector as a potential drop-in replacement for logstash in a central log shipping environment. w\nThe customer has about 1000 clients running filebeat and auditbeat that are continuously sending logs to two logstash \"aggregators\".\nFor numerous reasons we would like to get rid of the logstashes and are looking at vector for help.\nI've encountered some performance and stability issues related to the high number of connections being opened and closed by the clients, and I\u2019m looking for advice on tuning the setup.\nCurrent Setup:\nLog Sources: Around 1000 Filebeat and Auditbeat clients configured on logstash output, so we were expecting about 2000 connections to the vector (+/- overhead)\nLog Format: JSON\nVector Source: logstash source receiving data using the Logstash/Lumberjack protocol\nDownstream: Events sent via HTTP over a \"one way\" gateway to a vector HTTP ingest\nThe Problem:\nThe connections between the beat clients and the vector instance seem very very noisy, unstable and flappy. A lot of connections are getting created and closed in short periods of time, up to hitting the file descriptor limits before we limited the max_connections to 4000.\nMany of these connections are stuck in the CLOSE_WAIT state for extended periods and multiple warnings and errors on the beats and the vector, from connection reset by peer, \"framing errors\", being unable to write acknowledgements. The whole monty.\nA lot of events are actually coming through but in a lot of cases we have a huge backlog in the beats cliients with thousands of events in a seemingly endless retry loop.\nThis was less of a problem when using Logstash, but it seems more pronounced after switching to Vector.\nBoth Vector and the Filebeat instances are running in the same network and can connect directly, so issues with infrastructure in between are unlikely.\nI\u2019ve experimented the connection, timeout and buffer-related parameters in the Vector config and started looking into the TCP settings on the machine (RHEL host), but the number of opening and closing connections remain a mystery to me.\nQuestion:\nHas anyone experience in scaling  a vector to handle a similar number of Beats clients? Specifically, I\u2019m looking for guidance on lumberjack specific protocol quirks or TCP settings I could look at.\nbtw vector 0.41.1 so latest release.\nI am thankful for any insights, otherwise we will probably be stuck with logstash at the moment",
        "url": "https://github.com/vectordotdev/vector/discussions/21545",
        "createdAt": "2024-10-17T22:35:25Z",
        "updatedAt": "2024-10-19T11:26:45Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "kgorskowski"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21552,
        "title": "How do I calculate coverage for my unit tested vector configurations?",
        "bodyText": "https://vector.dev/docs/reference/configuration/unit-tests/\nIs it possible to calculate coverage ? If not, is this a planned feature?\nWe're hoping we can set thresholds of coverage such as 10% or 20% so when new vector configs are added, if the coverage falls below the threshold, then the PR would be unable to be merged until the coverage is above the minimum again.",
        "url": "https://github.com/vectordotdev/vector/discussions/21552",
        "createdAt": "2024-10-18T17:46:09Z",
        "updatedAt": "2024-10-19T02:35:49Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "nitrocode"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21554,
        "title": "docker creds and vectordev",
        "bodyText": "Hi, I'm use this vector.yaml:\nsecret:\n  clickhouse:\n    type: \"exec\"\n    command: [ \"cat\", \"/run/secrets/clickhouse\" ]\n\nsources:\n  service_logs:\n    type: file\n    include:\n      - \"/app/logs/*.log\"\n\n\ntransforms:\n  prepare_service_logs:\n    inputs:\n      - \"service_logs\"\n    type: \"remap\"\n    source: |-\n      . = parse_json!(string!(.message))\n\nsinks:\n  clickhouse_sink:\n    type: clickhouse\n    auth:\n      user: \"${LOGS_CLICKHOUSE_USER}\"\n      password: \"SECRET[clickhouse.password]\"\n      strategy: \"basic\"\n    inputs:\n      - prepare_service_logs\n    endpoint: \"${LOGS_CLICKHOUSE_HOST}\"\n    database: \"${LOGS_CLICKHOUSE_DB}\"\n    table: \"${LOGS_CLICKHOUSE_TABLE}\"\n    format: \"json_as_string\"\n\nwhere /run/secrets/clickhouse is: \necho \"{\\\"password\\\": \\\"${LOGS_CLICKHOUSE_PASSWORD}\\\"}\" | docker secret create clickhouse -\n\nand i get mistake:\n\nERROR vector::cli: Configuration error. error=Error while retrieving secret from backend \"clickhouse\": invalid type: string \"MY_PASSWORD\", expected struct ExecResponse at line 1 column 46.\n\nCould you please tell me what is wrong and perhaps there are some best practices for using docker creds in vectordev?",
        "url": "https://github.com/vectordotdev/vector/discussions/21554",
        "createdAt": "2024-10-19T00:29:29Z",
        "updatedAt": "2024-10-19T11:09:27Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Andrey250419"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 21520,
        "title": "How to solve rate-limiting issue for http sink?",
        "bodyText": "Dear community,\nI faced an issue with logging via vector:\n\u2502 2024-10-16T15:51:55.535532Z ERROR sink{component_kind=\"sink\" component_id=ingress- component_type=http}:request{request_id=29}: vector_common::internal_event::component_event \u2502\n\u2502 s_dropped: Internal log [Events dropped] is being suppressed to avoid flooding.\n\n\u2502 2024-10-16T15:51:54.441787Z ERROR sink{component_kind=\"sink\" component_id=ingress component_type=http}:request{request_id=28}: vector::sinks::util::retries: Internal log [No \u2502\n\u2502 t retriable; dropping the request.] has been suppressed 8 times.\n\nRecently,  I used http sink (openobserve) with the following config:\n  sinks:\n    ingress:\n      inputs:\n        - ingress-common\n      type: http\n      uri: \"https://<REDACTED>/api/default/ingress-common/_json\"\n      method: \"post\"\n      auth:\n        strategy: \"basic\"\n        user: \"<REDACTED>\"\n        password: \"<REDACTED>\"\n      compression: \"gzip\"\n      encoding:\n        codec: \"json\"\n        timestamp_format: \"rfc3339\"\n      healthcheck:\n        enabled: false\n\n\nthen I tried to modify requests params (for http sink):\n      request:\n        concurrency: 300\n        rate_limit_num: 1000\n        rate_limit_duration_secs: 1\n\nBut still catching the error with rate limiting. I assume I should be able to post log - but eventually it can hang/kill my log collector (openobserve).\nCould you please advise what the concrete steps I can accomplish to accomplish the issue?\nBest regards,P.",
        "url": "https://github.com/vectordotdev/vector/discussions/21520",
        "createdAt": "2024-10-16T16:03:14Z",
        "updatedAt": "2024-10-18T08:33:34Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ep4sh"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21495,
        "title": "Restructure a log message into custom dimensions",
        "bodyText": "Dear community,\nI have the following log message:\n{\n  \"data_source_name\": \"TEST\",\n  \"data_source_type\": \"shipper\",\n  \"filename\": \"parser.py\",\n  \"host\": \"DEVMACHINE\",\n  \"hostname\": \"DEVMACHINE\",\n  \"is_dry_run\": false,\n  \"is_enabled\": true,\n  \"lineno\": 101,\n  \"log_id\": \"b91f4a11-e9ec-4f97-96fc-96e611d94b07\",\n  \"module\": \"parser\",\n  \"name\": \"parse_service\",\n  \"node_id\": \"main\",\n  \"pid\": 15488,\n  \"processName\": \"MainProcess\",\n  \"service_name\": \"MASTER\",\n  \"shipper\": \"vector\",\n  \"location\": \"us-east-2\",\n  \"completed_at\": \"2021-09-30T14:00:00Z\",\n  \"timestamp\": \"2021-09-30T14:00:00Z\"\n}\nI want to reserve some of the json keys, and put all other keys in a \"customDimenssions\" key, for example:\n{\n  \"processName\": \"MainProcess\",\n  \"service_name\": \"MASTER\",\n  \"shipper\": \"vector\",\n  \"location\": \"us-east-2\",\n  \"completed_at\": \"2021-09-30T14:00:00Z\",\n  \"timestamp\": \"2021-09-30T14:00:00Z\",\n  \"customDimenssions\": {\n    \"data_source_name\": \"TEST\",\n    \"data_source_type\": \"shipper\",\n    \"filename\": \"parser.py\",\n    \"host\": \"DEVMACHINE\",\n    \"hostname\": \"DEVMACHINE\",\n    \"is_dry_run\": false,\n    \"is_enabled\": true,\n    \"lineno\": 101,\n    \"log_id\": \"b91f4a11-e9ec-4f97-96fc-96e663d94b07\",\n    \"module\": \"parser\",\n    \"name\": \"parse_service\",\n    \"node_id\": \"main\",\n    \"pid\": 15488\n  }\n}\nVector shall accept any log message, it will reserve the \"known\" keys, and place all the rest in the \"customDimenssions\" key.\nMany thanks",
        "url": "https://github.com/vectordotdev/vector/discussions/21495",
        "createdAt": "2024-10-13T08:42:24Z",
        "updatedAt": "2024-10-18T06:43:18Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "omers"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21538,
        "title": "How to concatenate strings?",
        "bodyText": "What is the correct way to concatenate .message and .foo in the image and assign them to .test? Thanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/21538",
        "createdAt": "2024-10-17T12:57:56Z",
        "updatedAt": "2024-10-24T21:40:34Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "linux-time"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21496,
        "title": "How to refer a nested JSON field in Loki sink's label?",
        "bodyText": "Hi vector,\nI try to add a label from a nested json field. But it doesn't working. How can I achieve this? I tried 2 ways, but all failed.\n\n\n\nin loki sink configure:\n\n    labels:\n        host: \"{{ host }}\"\n        method: \"{{ .message.method }}\"\n        source_type: \"{{ source_type }}\"\n\n\n\n\napi:\n  enabled: true\n  address: 0.0.0.0:8686\nsources:\n  demo_logs:\n    type: demo_logs\n    interval: 1\n    format: json\ntransforms:\n  my_throttle:\n    type: throttle\n    inputs:\n      - demo_logs\n    window_secs: 10\n    threshold: 1\n  my_remap:\n    type: remap\n    inputs:\n      - my_throttle\n    source: \".method = .host\"\nsinks:\n  console:\n    inputs:\n      - my_throttle\n    target: stdout\n    type: console\n    encoding:\n      codec: json\n  my_loki_sink:\n    type: loki\n    healthcheck:\n        enabled: false\n    encoding:\n        codec: json\n    labels:\n        host: \"{{ host }}\"\n        method: \"{{ .method }}\"\n        source_type: \"{{ source_type }}\"\n    inputs:\n      - my_remap\n    endpoint: http://loki.domain.com\n    tenant_id: vector_test",
        "url": "https://github.com/vectordotdev/vector/discussions/21496",
        "createdAt": "2024-10-13T12:45:23Z",
        "updatedAt": "2024-10-24T21:40:44Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tiantangkuohai"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18585,
        "title": "AWS source auth.assume_role isn't working",
        "bodyText": "Hey All,\nI'm having issues with configurating source and use the aut.assume_role.\nI have the following configuration:\n[sources.demo]\ntype = \"aws_sqs\"\nqueue_url = \"queueURL\"\nauth.region = \"us-east-1\"\nauth.assume_role = \"arn:aws:iam::AccountID:role/RoleName\"\nBut I receive:\nvector::internal_events::aws_sqs: Failed to fetch SQS events. error=failed to construct request error_code=\"failed_fetching_sqs_events\" .\nAny one has encountered this ? The same worked with KeyID and Secret Key.",
        "url": "https://github.com/vectordotdev/vector/discussions/18585",
        "createdAt": "2023-09-18T09:58:10Z",
        "updatedAt": "2024-10-14T14:34:01Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "RoeiSagi"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 18775,
        "title": "Issue",
        "bodyText": "my vector.yaml file is  -\nsources:\nmy_source_id:\ntype: opentelemetry\ngrpc:\naddress: 0.0.0.0:4317\nhttp:\naddress: 0.0.0.0:4318\nsinks:\nmy_sink_id:\ntype: console\ninputs:\n- my_source_id\nencoding:\ncodec: json\nbut getting this error Configuration error. error=Input \"my_sourc_id\" for sink \"my_sink_id\" doesn't match any components.\npls help",
        "url": "https://github.com/vectordotdev/vector/discussions/18775",
        "createdAt": "2023-10-05T10:27:02Z",
        "updatedAt": "2024-10-13T11:56:26Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "shivamsoni1"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21494,
        "title": "Using vector collect kubernetes logs issue",
        "bodyText": "Hello,\nI have encountered some log fetch issue when I am using vector to collect kubernetes logs.\nThe  version I use is version=\"0.40.0\" arch=\"x86_64\" revision=\"1167aa9 2024-07-29 15:08:44.028365803\"\nI deploy the vector as agent role with helm.\nHere is part of my configuration:\n   include_paths_glob_patterns: [\"/var/log/**/*.log\",\"/srv/kubernetes/logs/ntraefik-external-access.log\"]\nAnd I tried to add a init container to perform the files existed\n`initContainers:\n\nname: init-check\nimage: busybox\ncommand: ['sh', '-c', 'ls -l /srv/kubernetes/logs/ && id || exit 1']\nvolumeMounts:\n\nname: kubernetes-log-dir\nmountPath: /srv/kubernetes/logs/`\n\n\n\nMount infomation.\n`extraVolumes:\n\nname: vector-tls\nsecret:\nsecretName: vector-kubeconfig\nname: kubernetes-log-dir\nhostPath:\npath: /srv/kubernetes/logs/\ntype: DirectoryOrCreate\n\nextraVolumeMounts -- Additional Volume to mount into Vector Containers.\nextraVolumeMounts:\n\nname: vector-tls\nmountPath: /etc/vector/tls\nname: kubernetes-log-dir\nmountPath: /srv/kubernetes/logs/\n`\n\npartial logs in vector pod.\nvector::sources::kubernetes_logs: Including matching files. ret=[\"/var/log/**/*.log\", \"/srv/kubernetes/logs/ntraefik-external-access.log\"]",
        "url": "https://github.com/vectordotdev/vector/discussions/21494",
        "createdAt": "2024-10-12T10:03:45Z",
        "updatedAt": "2024-10-12T10:58:01Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "7czl"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 0
    },
    {
        "number": 21492,
        "title": "19-digit numeric string loses precision when converted to number",
        "bodyText": "to_int(\"1844928628568102120\") = 1844928628568102100",
        "url": "https://github.com/vectordotdev/vector/discussions/21492",
        "createdAt": "2024-10-12T03:14:05Z",
        "updatedAt": "2024-10-12T05:02:40Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "yesgs"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21489,
        "title": "How do you set up the `Regression Detector`?",
        "bodyText": "The automated Regression Detector Results that show up on a pull request are really interesting. How was it set up?",
        "url": "https://github.com/vectordotdev/vector/discussions/21489",
        "createdAt": "2024-10-11T19:37:18Z",
        "updatedAt": "2024-10-11T19:45:58Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "hamirmahal"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21421,
        "title": "Source AWS Cloud Watch",
        "bodyText": "Does vector have a source that can use AWS Cloud watch as a source for metrics?",
        "url": "https://github.com/vectordotdev/vector/discussions/21421",
        "createdAt": "2024-10-04T14:58:00Z",
        "updatedAt": "2024-10-10T23:33:59Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Manniac07"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21476,
        "title": "Is there a way to add custom dimensions and values to the AWS CloudWatch Metrics sink?",
        "bodyText": "Hi folks,\nI'm trying to understand if there's any out-of-the-box way to provide custom dimension keys and values to the Vector metrics that I am sending over. It doesn't seem like the CW Metrics sink itself supports this, nor does the internal metrics source I'm using to get the metrics.\nOne possibility could be a remap transform step that can somehow add this in but I will need to POC this. I'm wondering if anyone in the community here has experience with this.",
        "url": "https://github.com/vectordotdev/vector/discussions/21476",
        "createdAt": "2024-10-10T22:01:24Z",
        "updatedAt": "2024-10-11T17:19:09Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mkwan-amzn"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21371,
        "title": "IDE Plugin that offers navigation between vector components",
        "bodyText": "I couldn't find any prior discussion around implementing an IDE plugin that would allow for easy navigation between vector components similar to how IntelliJ supports navigation between classes, functions, etc. Something like this would be very useful for a large and non-linear vector pipeline.",
        "url": "https://github.com/vectordotdev/vector/discussions/21371",
        "createdAt": "2024-09-27T17:22:14Z",
        "updatedAt": "2024-10-10T20:02:31Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "nzxwang"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21277,
        "title": "CIDR range/IP range/subnet Source for HTTP Client Source (or others)",
        "bodyText": "Currently, you can only specify one IP address for an Http Client source. It would be advantageous to be able to \"fan out\" and make requests to multiple endpoints.\nI think you would also want to be able to specify concurrency parameters of IPs to hit per second, timeouts, and others.\nFrom Discord:\njszwedko \u2014 Yesterday at 3:30 PM\nAccepting multiple endpoints, I think, would be since there is precedent with some other sources like prometheus_scrape. Accepting an IP range would be a feature without precedent and so may require some more discussion, but you could certainly open a feature request to kick that discussion off",
        "url": "https://github.com/vectordotdev/vector/discussions/21277",
        "createdAt": "2024-09-11T20:55:22Z",
        "updatedAt": "2024-10-10T18:32:12Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "average-gary"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21472,
        "title": "Transforming structure",
        "bodyText": "Hi, I want to covert incoming messages like:\n{\n   \"f1\": \"AABBCC\",\n   \"f2\": \"DEF\",\n   \"f3\": 3,\n   \"feature\": [{\n       \"name\": \"nameX\",\n       \"string\": \"aaaaa\"\n   }, {\n       \"name\": \"nameY\",\n       \"long\": 333\n   }],\n   \"f10\": 3.14\n}\n\nto:\n{\n   \"f1\": \"AABBCC\",\n   \"f2\": \"DEF\",\n   \"f3\": 3,\n   \"fields\": {\n       \"nameX\": \"aaaaa\",\n       \"nameY\": 333\n   },\n   \"f10\": 3.14\n}\n\nthat is, only to alter the \"feature\" field from the above array to an object with the key \"fields\" that will hold a representation of the array items.\nAs a newbie to vrl I'm trying to do something like this:\nresult = {}\nmap_values(.feature) -> |value| {\n   key = value.name\n   val = \"wrong type\"\n   if exists(value.long) {\n     val = value.long\n   } else if exists(value.string) {\n     val = value.string\n   }  # more types here \n   result |= { \"{{ key }}\" : val }\n}\n. |= { \"fields\" : result }\n\nThe thing is that the \"{{ key }}\" is not transformed to the variable name and left as \"{{ key }}\".\nWhat am I missing?",
        "url": "https://github.com/vectordotdev/vector/discussions/21472",
        "createdAt": "2024-10-10T09:10:04Z",
        "updatedAt": "2024-10-10T14:17:08Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "CR-OrenYekutieli"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21448,
        "title": "HighLoad tweaks: vector->vector_aggregator->clickhouse",
        "bodyText": "Hello team,\nI'm using following scheme:\n30 vectors on each VM, which collect data through source:file and use sink:vector to one (+1 duplicate) global vector acting as an aggregator). Buffer type is disk. There are two types of VMs with different type of files.\nOn vector hub we have sourcevector and sink:clickhouse with following configuration:\n[sources.RSU_cdr_vector]\ntype = \"vector\"\naddress = \"192.168.48.34:40103\"\n\n[sinks.RSU_cdr_clickhouse_site1]\ntype = \"clickhouse\"\ninputs = [\"RSU_cdr_vector\"]\nendpoint = \"http://192.168.48.35:8123\"\ndatabase = \"statistics\"\ntable = \"RSU_cdr_vector\"\nskip_unknown_fields = true\ncompression = \"zstd\"\nauth.strategy = \"basic\"\nauth.user = \"vector\"\nauth.password = \"vector\"\n\n        [sinks.RSU_cdr_clickhouse_site1.buffer]\n        max_events = 2500000\n        type = \"memory\"\n        when_full = \"block\"\n\n        [sinks.RSU_cdr_clickhouse_site1.batch]\n        max_bytes = 100000000\n        timeout_secs = 3\n\nAnd\n[sources.SSU_L3_cdr_vector]\ntype = \"vector\"\naddress = \"192.168.48.34:43102\"\n\n[sinks.SSU_L3_cdr_clickhouse_site1]\ntype = \"clickhouse\"\ninputs = [\"SSU_L3_cdr_vector\"]\nendpoint = \"http://192.168.48.35:8123\"\ndatabase = \"statistics\"\ntable = \"SSU_L3_cdr_vector\"\nskip_unknown_fields = true\ncompression = \"zstd\"\nauth.strategy = \"basic\"\nauth.user = \"vector\"\nauth.password = \"vector\"\n\n        [sinks.SSU_L3_cdr_clickhouse_site1.buffer]\n        max_events = 2500000\n        type = \"memory\"\n        when_full = \"block\"\n\n        [sinks.SSU_L3_cdr_clickhouse_site1.batch]\n        max_bytes = 100000000\n        timeout_secs = 3\n\nThe same configuration on another server acting as hot backup.\n\nWhen load is not so high, vector_source and clickhouse_sink are acting simultaneously, no delays.\n\nBut as soon as we got higher traffic there is a delay in sink for type called SSU_L3. As you can see, green line is the same as before, no actual delays. But for SSU_L3 it looks like this, that it collects data for some time and only then try to upload it to sink using bigger batches.\nGrafana:\n\nVector top as well. I can see that Events In are increasing for source and sink almost 30 seconds and only then EventsOut start to sink big batches of data:\n\nFor another type called RSU_cdr it's ok, data is uploaded almost at realtime.\nInsert log from clickhouse: again, constant small upload for one type and periodical big chunks for another\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500event_time\u2500\u252c\u2500table\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u252c\u2500formatReadableSize(sum(bytes))\u2500\u252c\u2500sum(rows)\u2500\u2510\n\u2502 2024-10-08 13:53:23 \u2502 RSU_cdr_vector    \u2502       1 \u2502 37.00 MiB                      \u2502     99670 \u2502\n\u2502 2024-10-08 13:53:22 \u2502 RSU_cdr_vector    \u2502       1 \u2502 37.00 MiB                      \u2502     99671 \u2502\n\u2502 2024-10-08 13:53:21 \u2502 RSU_cdr_vector    \u2502       3 \u2502 111.03 MiB                     \u2502    298980 \u2502\n\u2502 2024-10-08 13:53:20 \u2502 RSU_cdr_vector    \u2502       1 \u2502 37.01 MiB                      \u2502     99653 \u2502\n\u2502 2024-10-08 13:53:19 \u2502 RSU_cdr_vector    \u2502       1 \u2502 37.03 MiB                      \u2502     99632 \u2502\n\u2502 2024-10-08 13:53:18 \u2502 RSU_cdr_vector    \u2502       4 \u2502 148.13 MiB                     \u2502    398496 \u2502\n\u2502 2024-10-08 13:53:17 \u2502 RSU_cdr_vector    \u2502       1 \u2502 37.02 MiB                      \u2502     99640 \u2502\n\u2502 2024-10-08 13:53:16 \u2502 RSU_cdr_vector    \u2502       1 \u2502 37.03 MiB                      \u2502     99635 \u2502\n\u2502 2024-10-08 13:53:15 \u2502 RSU_cdr_vector    \u2502       3 \u2502 111.04 MiB                     \u2502    298964 \u2502\n\u2502 2024-10-08 13:53:14 \u2502 RSU_cdr_vector    \u2502       1 \u2502 37.00 MiB                      \u2502     99672 \u2502\n\u2502 2024-10-08 13:53:13 \u2502 RSU_cdr_vector    \u2502       1 \u2502 37.01 MiB                      \u2502     99665 \u2502\n\u2502 2024-10-08 13:53:12 \u2502 RSU_cdr_vector    \u2502       1 \u2502 37.00 MiB                      \u2502     99685 \u2502\n\u2502 2024-10-08 13:53:11 \u2502 RSU_cdr_vector    \u2502       3 \u2502 111.06 MiB                     \u2502    298927 \u2502\n\u2502 2024-10-08 13:53:10 \u2502 RSU_cdr_vector    \u2502       1 \u2502 37.01 MiB                      \u2502     99653 \u2502\n\u2502 2024-10-08 13:53:09 \u2502 RSU_cdr_vector    \u2502       1 \u2502 37.01 MiB                      \u2502     99653 \u2502\n\u2502 2024-10-08 13:53:08 \u2502 SSU_L3_cdr_vector \u2502      21 \u2502 566.98 MiB                     \u2502   2451901 \u2502\n\u2502 2024-10-08 13:53:08 \u2502 RSU_cdr_vector    \u2502       2 \u2502 74.06 MiB                      \u2502    199247 \u2502\n\u2502 2024-10-08 13:53:07 \u2502 SSU_L3_cdr_vector \u2502      24 \u2502 648.19 MiB                     \u2502   2801801 \u2502\n\u2502 2024-10-08 13:53:07 \u2502 RSU_cdr_vector    \u2502       2 \u2502 74.04 MiB                      \u2502    199288 \u2502\n\u2502 2024-10-08 13:53:06 \u2502 RSU_cdr_vector    \u2502       1 \u2502 37.01 MiB                      \u2502     99661 \u2502\n\u2502 2024-10-08 13:53:06 \u2502 SSU_L3_cdr_vector \u2502      24 \u2502 648.15 MiB                     \u2502   2801858 \u2502\n\u2502 2024-10-08 13:53:05 \u2502 RSU_cdr_vector    \u2502       2 \u2502 74.04 MiB                      \u2502    199279 \u2502\n\u2502 2024-10-08 13:53:05 \u2502 SSU_L3_cdr_vector \u2502      22 \u2502 593.96 MiB                     \u2502   2568664 \u2502\n\u2502 2024-10-08 13:53:04 \u2502 SSU_L3_cdr_vector \u2502       6 \u2502 161.99 MiB                     \u2502    700554 \u2502\n\u2502 2024-10-08 13:53:04 \u2502 RSU_cdr_vector    \u2502       1 \u2502 37.00 MiB                      \u2502     99673 \u2502\n\u2502 2024-10-08 13:53:03 \u2502 SSU_L3_cdr_vector \u2502      20 \u2502 539.84 MiB                     \u2502   2335354 \u2502\n\nQuestion:\nWhich parameters should i tweak to force sink to clickhouse more often, every second or two, not 20-30.\nI thought that batch.timeout_secs will be able to do this, but no. According to number of bytes\\rows in sink inserts which can be up to millions of rows there is no problem with DB as sink, it supports higher number of rows in one batch.",
        "url": "https://github.com/vectordotdev/vector/discussions/21448",
        "createdAt": "2024-10-08T10:57:59Z",
        "updatedAt": "2024-10-08T10:58:00Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ADovgalyuk"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 21163,
        "title": "Service call failed. No retries or retries exhausted.] is being suppressed to avoid flooding.",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\ntwo vectors start to upload a log to one elasticsearch index and a similar error appears in the logs, one vector is on the first server, the second on the second and they upload to 1 elasticsearch index\nConfiguration\nNo response\nVersion\nvector 0.33.0\nDebug Output\nNo response\nExample Data\n{\"host\":\"dc2\",\"message\":\"Internal log [Service call failed. No retries or retries exhausted.] has been suppressed 9 times.\",\"metadata\":{\"kind\":\"event\",\"level\":\"ERROR\",\"module_path\":\"vector_common::internal_event::service\",\"target\":\"vector_common::internal_event::service\"},\"pid\":40900,\"source_type\":\"internal_logs\",\"timestamp\":\"2024-08-24T23:59:41.576300789Z\",\"vector\":{\"component_id\":\"es_cluster\",\"component_kind\":\"sink\",\"component_name\":\"es_cluster\",\"component_type\":\"elasticsearch\"}}\n{\"error\":\"None\",\"error_type\":\"request_failed\",\"host\":\"dc2\",\"internal_log_rate_limit\":true,\"message\":\"Service call failed. No retries or retries exhausted.\",\"metadata\":{\"kind\":\"event\",\"level\":\"ERROR\",\"module_path\":\"vector_common::internal_event::service\",\"target\":\"vector_common::internal_event::service\"},\"pid\":40900,\"request_id\":21310,\"source_type\":\"internal_logs\",\"stage\":\"sending\",\"timestamp\":\"2024-08-24T23:59:41.576309042Z\",\"vector\":{\"component_id\":\"es_cluster\",\"component_kind\":\"sink\",\"component_name\":\"es_cluster\",\"component_type\":\"elasticsearch\"}}\n{\"host\":\"dc2\",\"message\":\"Internal log [Events dropped] has been suppressed 9 times.\",\"metadata\":{\"kind\":\"event\",\"level\":\"ERROR\",\"module_path\":\"vector_common::internal_event::component_events_dropped\",\"target\":\"vector_common::internal_event::component_events_dropped\"},\"pid\":40900,\"source_type\":\"internal_logs\",\"timestamp\":\"2024-08-24T23:59:41.576339728Z\",\"vector\":{\"component_id\":\"es_cluster\",\"component_kind\":\"sink\",\"component_name\":\"es_cluster\",\"component_type\":\"elasticsearch\"}}\n{\"count\":11,\"host\":\"dc2\",\"intentional\":false,\"internal_log_rate_limit\":true,\"message\":\"Events dropped\",\"metadata\":{\"kind\":\"event\",\"level\":\"ERROR\",\"module_path\":\"vector_common::internal_event::component_events_dropped\",\"target\":\"vector_common::internal_event::component_events_dropped\"},\"pid\":40900,\"reason\":\"Service call failed. No retries or retries exhausted.\",\"source_type\":\"internal_logs\",\"timestamp\":\"2024-08-24T23:59:41.576344899Z\",\"vector\":{\"component_id\":\"es_cluster\",\"component_kind\":\"sink\",\"component_name\":\"es_cluster\",\"component_type\":\"elasticsearch\"}}\n{\"host\":\"dc2\",\"message\":\"Internal log [Not retriable; dropping the request.] is being suppressed to avoid flooding.\",\"metadata\":{\"kind\":\"event\",\"level\":\"ERROR\",\"module_path\":\"vector::sinks::util::retries\",\"target\":\"vector::sinks::util::retries\"},\"pid\":40900,\"source_type\":\"internal_logs\",\"timestamp\":\"2024-08-24T23:59:42.561304282Z\",\"vector\":{\"component_id\":\"es_cluster\",\"component_kind\":\"sink\",\"component_name\":\"es_cluster\",\"component_type\":\"elasticsearch\"}}\n{\"host\":\"dc2\",\"message\":\"Internal log [Service call failed. No retries or retries exhausted.] is being suppressed to avoid flooding.\",\"metadata\":{\"kind\":\"event\",\"level\":\"ERROR\",\"module_path\":\"vector_common::internal_event::service\",\"target\":\"vector_common::internal_event::service\"},\"pid\":40900,\"source_type\":\"internal_logs\",\"timestamp\":\"2024-08-24T23:59:42.561388700Z\",\"vector\":{\"component_id\":\"es_cluster\",\"component_kind\":\"sink\",\"component_name\":\"es_cluster\",\"component_type\":\"elasticsearch\"}}\n{\"host\":\"dc2\",\"message\":\"Internal log [Events dropped] is being suppressed to avoid flooding.\",\"metadata\":{\"kind\":\"event\",\"level\":\"ERROR\",\"module_path\":\"vector_common::internal_event::component_events_dropped\",\"target\":\"vector_common::internal_event::component_events_dropped\"},\"pid\":40900,\"source_type\":\"internal_logs\",\"timestamp\":\"2024-08-24T23:59:42.561412918Z\",\"vector\":{\"component_id\":\"es_cluster\",\"component_kind\":\"sink\",\"component_name\":\"es_cluster\",\"component_type\":\"elasticsearch\"}}\n{\"host\":\"dc2\",\"message\":\"Internal log [Not retriable; dropping the request.] has been suppressed 6 times.\",\"metadata\":{\"kind\":\"event\",\"level\":\"ERROR\",\"module_path\":\"vector::sinks::util::retries\",\"target\":\"vector::sinks::util::retries\"},\"pid\":40900,\"source_type\":\"internal_logs\",\"timestamp\":\"2024-08-24T23:59:53.581473811Z\",\"vector\":{\"component_id\":\"es_cluster\",\"component_kind\":\"sink\",\"component_name\":\"es_cluster\",\"component_type\":\"elasticsearch\"}}\n{\"host\":\"dc2\",\"internal_log_rate_limit\":true,\"message\":\"Not retriable; dropping the request.\",\"metadata\":{\"kind\":\"event\",\"level\":\"ERROR\",\"module_path\":\"vector::sinks::util::retries\",\"target\":\"vector::sinks::util::retries\"},\"pid\":40900,\"reason\":\"\"error type: document_parsing_exception, reason: [1:319] failed to parse field [context.context] of type [text] in document with id 'KoTVhpEBLQAI7rP3BpLa'. Preview of field's value: '{tum=, reqId=9fedf1b2dd8c5199f005de8ba6600b90}'\"\",\"source_type\":\"internal_logs\",\"timestamp\":\"2024-08-24T23:59:53.581502772Z\",\"vector\":{\"component_id\":\"es_cluster\",\"component_kind\":\"sink\",\"component_name\":\"es_cluster\",\"component_type\":\"elasticsearch\"}}\n{\"host\":\"dc2\",\"message\":\"Internal log [Service call failed. No retries or retries exhausted.] has been suppressed 6 times.\",\"metadata\":{\"kind\":\"event\",\"level\":\"ERROR\",\"module_path\":\"vector_common::internal_event::service\",\"target\":\"vector_common::internal_event::service\"},\"pid\":40900,\"source_type\":\"internal_logs\",\"timestamp\":\"2024-08-24T23:59:53.581575549Z\",\"vector\":{\"component_id\":\"es_cluster\",\"component_kind\":\"sink\",\"component_name\":\"es_cluster\",\"component_type\":\"elasticsearch\"}}\n{\"error\":\"None\",\"error_type\":\"request_failed\",\"host\":\"dc2\",\"internal_log_rate_limit\":true,\"message\":\"Service call failed. No retries or retries exhausted.\",\"metadata\":{\"kind\":\"event\",\"level\":\"ERROR\",\"module_path\":\"vector_common::internal_event::service\",\"target\":\"vector_common::internal_event::service\"},\"pid\":40900,\"request_id\":21317,\"source_type\":\"internal_logs\",\"stage\":\"sending\",\"timestamp\":\"2024-08-24T23:59:53.581582655Z\",\"vector\":{\"component_id\":\"es_cluster\",\"component_kind\":\"sink\",\"component_name\":\"es_cluster\",\"component_type\":\"elasticsearch\"}}\n{\"host\":\"dc2\",\"message\":\"Internal log [Events dropped] has been suppressed 6 times.\",\"metadata\":{\"kind\":\"event\",\"level\":\"ERROR\",\"module_path\":\"vector_common::internal_event::component_events_dropped\",\"target\":\"vector_common::internal_event::component_events_dropped\"},\"pid\":40900,\"source_type\":\"internal_logs\",\"timestamp\":\"2024-08-24T23:59:53.581613612Z\",\"vector\":{\"component_id\":\"es_cluster\",\"component_kind\":\"sink\",\"component_name\":\"es_cluster\",\"component_type\":\"elasticsearch\"}}\n{\"count\":14,\"host\":\"dc2\",\"intentional\":false,\"internal_log_rate_limit\":true,\"message\":\"Events dropped\",\"metadata\":{\"kind\":\"event\",\"level\":\"ERROR\",\"module_path\":\"vector_common::internal_event::component_events_dropped\",\"target\":\"vector_common::internal_event::component_events_dropped\"},\"pid\":40900,\"reason\":\"Service call failed. No retries or retries exhausted.\",\"source_type\":\"internal_logs\",\"timestamp\":\"2024-08-24T23:59:53.581619025Z\",\"vector\":{\"component_id\":\"es_cluster\",\"component_kind\":\"sink\",\"component_name\":\"es_cluster\",\"component_type\":\"elasticsearch\"}}\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/21163",
        "createdAt": "2024-08-27T07:51:17Z",
        "updatedAt": "2024-10-04T10:07:23Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "un0of"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21409,
        "title": "Kubernetes Source: Add namespace annotations to event metadata",
        "bodyText": "https://vector.dev/docs/reference/configuration/sources/kubernetes_logs/#namespace_annotation_fields\nTLDR: please add namespace_annotation_fields.namespace_annotations because labels are restricted to 63 characters which makes it hard to store much data for VRL transformations for example.\n\nWe currently give users of our platform a lot of flexibility to configure their observability pipelines via namespace labels however we need to add a new label for every bit of config and in VRL individually parse and handle each one which is a bit onerous.\nWe would like to move from this\nlabels:\n  splunk.mycompany/forwarding: enabled \n  splunk.mycompany/index: customer-dev-index\n  splunk.mycompany/source: platform\n  splunk.mycompany/sourcetype: _json\n  scalyr.mycompany/forwarding: enabled\n  scalyr.mycompany/token: <base64-encoded-token>\n  format.mycompany/my-container-name: keyvalue3\nto this\nannotations:\n  logging.mycompany/config: |\n    {\n      \"transforms\": {\n        \"my-container-name\": {\n          \"format\": \"keyvalue3\",\n          \"remove_fields\": [\"message\"],\n          \"timestamp\": {\n            \"field\": \"timestamp\",\n            \"format\": \"%+\"\n          },\n      },\n      \"sinks\": {\n        \"splunk_uk_dc\": {\n          \"index\": \"my-index-dev\",\n          \"source\": \"platform,\n          \"sourcetype\": \"_json\"\n        },\n        \"scalyr\": {\n          \"token\": \"dG9rZW4=\"\n        }\n      }\n    }\nSo we can simply parse the whole config one time. It also makes it more extensible in the future.\nWe could do this today for pod annotations as they are added to the event metadata however we typically keep the application Deployments the same across environments and depending where it is deployed (e.g. in a performance testing namespace, prod namespace, dev namespace) we want different log forward configurations. Namespace annotations would be greatly preferred if possible.\nThanks for considering, I hope this is an easy change to implement, I would love to contribute however I don't know Rust unfortunately. \ud83d\ude05",
        "url": "https://github.com/vectordotdev/vector/discussions/21409",
        "createdAt": "2024-10-03T10:08:42Z",
        "updatedAt": "2024-10-03T10:08:43Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "matt-simons"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 21387,
        "title": "How to Define Disk Buffer Storage Location and Customize the Path in Log Sinks?",
        "bodyText": "When we define the buffer type as 'disk,' where are the buffered logs stored? What is the default path, and how can we configure a custom path?\nsinks:\n  overflow_test:\n    type: blackhole\n    buffer:\n    - type: memory\n      max_events: 1000\n      when_full: overflow\n    - type: disk\n      max_size: 1073741824 # 1GiB.\n      when_full: drop_newest",
        "url": "https://github.com/vectordotdev/vector/discussions/21387",
        "createdAt": "2024-09-30T12:20:46Z",
        "updatedAt": "2024-10-02T17:15:34Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "hamedsa-78"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21378,
        "title": "How do I tell the Vector MQTT sink to use protocol 3.1.1",
        "bodyText": "I believe the Vector MQTT sink is using protocol 5. How can I configure it to use 3.1.1 instead (aka, confusingly, \"V4\")?\nWould I replace \n  \n    \n      vector/src/sinks/mqtt/config.rs\n    \n    \n         Line 4\n      in\n      438bc8b\n    \n  \n  \n    \n\n        \n          \n           use rumqttc::{MqttOptions, QoS, TlsConfiguration, Transport}; \n        \n    \n  \n\n\nuse rumqttc::{ with use rumqttc::{mqttbytes::v4::\nlike https://github.com/ErikWegner/homecontrol-ui/blob/f76f6ff26974b0c52678ba9c312e359cfed9db1b/server/src/mqtta/actor.rs#L6",
        "url": "https://github.com/vectordotdev/vector/discussions/21378",
        "createdAt": "2024-09-30T03:16:42Z",
        "updatedAt": "2024-10-01T20:33:43Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "mrbluecoat"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21373,
        "title": "Is there an easy way to glean the concurrency pattern of a vector component?",
        "bodyText": "For some components like datadog_logs, we found that the component was single-threaded after hitting performance limits so we had to retroactively replace it with a router that would round-robin assign the logs to n datadog_logs components.\nIt would be great to know the concurrency or \"threadedness\" of a component beforehand though. For example, is the sample component also single-threaded? Should I expect to have to fanout my logs to several sample components before merging them for downstream processing? What about other components like remap, throttle, etc.?",
        "url": "https://github.com/vectordotdev/vector/discussions/21373",
        "createdAt": "2024-09-27T18:47:49Z",
        "updatedAt": "2024-10-01T17:42:12Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "nzxwang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21377,
        "title": "How to Ensure Log Entries Are Checkpointed After Successful Kafka Delivery?",
        "bodyText": "I have this configuration, but I defined the wrong Kafka service. As a result, Vector updated the checkpoint, and the current logs inside log.log have been read from the source section. Now, when I define the valid Kafka server, the previous entries won't be sent to the Kafka server because they have already been checkpointed. I want the log entries to be checkpointed only after they are successfully sent to Kafka.\nI think the solution is related to using disk buffering, and I need to define a disk buffer. How do I specify the path for the buffers in the configuration?\"\ncustomConfig:\n  api:\n    enabled: true\n    address: 127.0.0.1:8686\n    playground: false\n  sources:\n    file_logs:\n      type: \"file\"\n      include:\n      - \"/vector-data-dir/log.log\"\n      read_from: \"beginning\"\n      data_dir: /vector-data-dir/\n  transforms:\n    app_logs_parser:\n      inputs:\n      - \"file_logs\"\n      type: \"remap\"\n      source: |\n        # Try to parse the JSON message\n        . = parse_json(.message) ?? null\n\n        # Check if parsing was successful\n        if is_null(.) {\n          abort # unknown type\n        }\n\n        # Handle two formats of logs: flat and nested\n        log_value =\n          if exists(.log) {\n            .log\n          } else {\n            .\n          }\n\n        # Merge log_value into the main object and add timestamp\n        . = log_value\n  sinks:\n    kafka_server:\n      type: \"kafka\"\n      inputs:\n      - \"app_logs_parser\"\n      encoding:\n        codec: \"json\"\n      bootstrap_servers: kafka.vector.svc.cluster.local:9092\n      topic: test",
        "url": "https://github.com/vectordotdev/vector/discussions/21377",
        "createdAt": "2024-09-29T15:14:48Z",
        "updatedAt": "2024-09-29T15:25:05Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "hamedsa-78"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 21376,
        "title": "Handling Log Read Position in Vector Agent Compared to Fluentd Forwarder",
        "bodyText": "In a Fluentd forwarder (which is equivalent to Vector agent), we define the pos_file in its source section. This ensures that when the pod is restarted, it reads logs from the last saved position. In this case, we do not need any buffer since the logs are stored on the host node, and we have mounted that path onto the pod. Defining the position is sufficient.\nWhat about Vector? How does the Vector agent handle this process? (My question is not related to buffering, as buffering occurs in the aggregator.) I found the following in the official Vector documentation:\nDocs Home > Reference > Configuration > Sources > File\n\nRead Position\n\nBy default, Vector will read from the beginning of newly discovered files. You can change this behavior by setting the read_from option to \"end\".\nPreviously discovered files will be checkpointed, and the read position will resume from the last checkpoint. To disable this behavior, you can set the ignore_checkpoints option to true. This will cause Vector to disregard existing checkpoints when determining the starting read position of a file.\n\nShould I configure anything else in the Vector Helm chart to handle this, or does checkpointing work the same as pos_file in Fluentd for our purposes?",
        "url": "https://github.com/vectordotdev/vector/discussions/21376",
        "createdAt": "2024-09-29T10:56:41Z",
        "updatedAt": "2024-09-29T15:06:46Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "hamedsa-78"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 21374,
        "title": "MQTTS support?",
        "bodyText": "Does the Vector MQTT sink support MQTTS (MQTT over TLS)?\nI see https://github.com/vectordotdev/vector/blob/master/src/sinks/websocket/sink.rs#L463 exists for wss but I don't see an equivalent on https://github.com/vectordotdev/vector/blob/master/src/sinks/mqtt/sink.rs",
        "url": "https://github.com/vectordotdev/vector/discussions/21374",
        "createdAt": "2024-09-27T23:38:42Z",
        "updatedAt": "2024-09-29T00:10:07Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ben-auo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 5
        },
        "upvoteCount": 1
    },
    {
        "number": 21361,
        "title": "Vector Delay in Pushing Logs to NATS",
        "bodyText": "We are currently using Vector to collect logs from a node and push them to NATS. However, we\u2019ve noticed a delay (sometimes several seconds) between when the logs are collected and when they are published to NATS.\nHas anyone experienced this issue or know how to configure Vector to reduce the delay? Any insights or recommendations would be appreciated.",
        "url": "https://github.com/vectordotdev/vector/discussions/21361",
        "createdAt": "2024-09-26T11:30:43Z",
        "updatedAt": "2024-09-26T23:22:32Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "SamyElHamass"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21357,
        "title": "My service cannot get http request from vector http sink",
        "bodyText": "This is my sink configuration\n\"sinks\": {\n        \"console_sink\": {\n            \"type\": \"console\",\n            \"inputs\": [\n               \"vector_http_test_transform\"\n            ],\n            \"encoding\": {\n                \"codec\": \"json\"\n            }\n       },\n       \"my_http_sink\": {\n           \"type\": \"http\",\n           \"inputs\": [\n              \"vector_http_test_transform\"\n           ],\n          \"method\": \"post\", \n          \"encoding\": {\n               \"codec\": \"json\"\n           },\n           \"request\": {\n               \"headers\": {\n                 \"Content-Type\": \"application/json\"\n               }\n           },\n           \"uri\": \"http://myHost:myPort/logs\"\n       }\n    }\nMy console print like this :\n{\"file\":\"/data/logs/n9e-logs/test.log\",\"host\":\"host.novalocal\",\"message\":{\"level\":\"INFO\",\"lineNumber\":\"21\",\"logDate\":\"2024-09-24 15:19:14.660963\",\"message\":\"event(c695d938d8955d triggered) push_queue: rule_id=47 tags:[__name__=mem_used_percent data_group=tok_3dc2a5e1457744379dad ]  2024-09-20 15:19:14.000 \",\"method\":\"dispatch/log.go\"},\"source_type\":\"file\",\"timestamp\":\"2024-09-26T02:19:30.405\"}\nAnd my sevice get correct request with curl\uff1a\ncurl -X POST http://myHost:myPort/logs -H \"Content-Type: application/json\" -d '{\"key\":\"value\"}'\nBut my service get no http request from vector http sink,what should i do\uff1f",
        "url": "https://github.com/vectordotdev/vector/discussions/21357",
        "createdAt": "2024-09-26T02:52:37Z",
        "updatedAt": "2024-09-26T23:17:49Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "qinruim"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21355,
        "title": "Does Vector Support Kubernetes Metadata ?",
        "bodyText": "Does Vector have a Kubernetes metadata plugin like Fluentd? I am not referring to Vector's Kubernetes logs in the source section, as it doesn't support custom pod selection. Additionally, I need it for arbitrary log paths of a pod, not just the default.",
        "url": "https://github.com/vectordotdev/vector/discussions/21355",
        "createdAt": "2024-09-25T15:08:57Z",
        "updatedAt": "2024-09-29T10:55:11Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "hamedsa-78"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21352,
        "title": "log_to_metric tag expansion",
        "bodyText": "I couldn't find any way/example to use log_to_metric transformation with dynamic tag names. Having something similar to label expansion from the Loki sink will be useful.\nThis way it would be possible to define labels in this fashion:\n[transforms.count_metrics.metrics.tags]\n\"*\"  = \"{{ labels }}\"\nSimilarly to Loki:\n[sinks.loki.labels]\n\"*\" = \"{{ labels }}\"",
        "url": "https://github.com/vectordotdev/vector/discussions/21352",
        "createdAt": "2024-09-25T09:27:47Z",
        "updatedAt": "2024-09-25T09:34:41Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "arch-xtof"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 21346,
        "title": "Is there a way to determine how much CPU, disk and network is being used by Vector?",
        "bodyText": "Hi Everyone, wondering if it is possible to figure out how much CPU, disk and network is being used specifically by Vector, and whether or not this can be broken up by Vector component. I did some testing with host metrics source but couldn't find this, although I may have missed it.\nI know the allocation tracing features lets you see memory allocation at the component level, and ultimately I'm wondering if something similar exists for the other metrics I mentioned.\nThanks in advance for the time!",
        "url": "https://github.com/vectordotdev/vector/discussions/21346",
        "createdAt": "2024-09-24T02:08:36Z",
        "updatedAt": "2024-09-25T14:04:15Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mkwan-amzn"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 21304,
        "title": "Sharing Sinks Across Multiple Configuration Files",
        "bodyText": "Hello!\nI currently work with multiple configuration files in Vector, each handling separate pipelines. It\u2019s been great for modularizing the setup, but I\u2019ve encountered a situation where I\u2019m collecting metrics from each pipeline and need to expose them on the same port, meaning I need to share the same sink.\nI referenced sources and transforms from other configuration files to achieve this, and it works, but I\u2019d like to know if this is considered acceptable practice. Are there any potential pitfalls or issues I should be aware of when sharing sinks across multiple config files?\nI appreciate any suggestions!",
        "url": "https://github.com/vectordotdev/vector/discussions/21304",
        "createdAt": "2024-09-17T10:28:00Z",
        "updatedAt": "2024-09-24T10:20:02Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "arch-xtof"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21333,
        "title": "Is there a way for me to add extra headers in loki sink configuration?",
        "bodyText": "Hi,\nassuming we want to add more headers like api token auth header like below to loki sink:\n    request:\n      headers:\n        \"api-key\" : \"token\"\n\nI think i can do above via http sink but loki sink gives me more features and etc\nI would appreciate it if anyone could help or provide guidance.\nsimilar feature request:  #21332",
        "url": "https://github.com/vectordotdev/vector/discussions/21333",
        "createdAt": "2024-09-22T13:38:09Z",
        "updatedAt": "2024-09-23T17:34:43Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20333,
        "title": "Can you convert metrics to log & then back to metrics again?",
        "bodyText": "I have a metrics source which I convert to log format with the transform metric_to_log. I can now save the metrics to files as JSON and that works fine, but if I try & convert them back to metrics using log_to_metric & setting all_metrics: true they will not convert.\nI just get error messages saying the log field is missing.\nThe documentation also says the metrics property is ignored if all_metrics: true, but the metrics property option is still mandatory, so it's not clear what it should be set to - I went with metrics: [].\nI'm not sure I actually need to do this anymore, but it also seemed to me it should have worked.",
        "url": "https://github.com/vectordotdev/vector/discussions/20333",
        "createdAt": "2024-04-18T11:23:46Z",
        "updatedAt": "2024-09-23T17:31:49Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "james-stevens"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 1
    },
    {
        "number": 21178,
        "title": "How to use query_parameters in sinks configuration",
        "bodyText": "hello, this is my config, thanks\nI guess I should use parse_query_string in transforms, but I haven't used it after reading the documentation.\n# Receiver\nsources:\n  http_in:\n    type: \"http_server\"\n    address: \"127.0.0.1:5006\"\n    method: POST\n    path: /api\n    query_parameters:\n      - log_type\n    encoding:\n      codec: \"json\"\n      \n\nsinks:\n  save_file:\n    type: file\n    encoding:\n      codec: \"json\"\n    compression: \"gzip\"\n    inputs:\n      - http_in\n    # This variable is not effective\n    path: /data/vector/{{.log_type}}-%Y-%m-%d.log.gz\n\n\n# Sender\n---\nsinks:\n  https_out:\n    type: \"http\"\n    inputs:\n      - \"file_logs\"\n    uri: \"https://example.com/api?log_type=kernel\"\n    method: post\n    compression: \"gzip\"\n    encoding:\n      codec: \"json\"\n    request:\n      timeout_secs: 5\n      retry_attempts: 10",
        "url": "https://github.com/vectordotdev/vector/discussions/21178",
        "createdAt": "2024-08-29T08:26:33Z",
        "updatedAt": "2024-09-23T16:52:31Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Ansen"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21316,
        "title": "PubSub does not support metric events?",
        "bodyText": "I am trying to use Vector agent to collect metrics from a local Prometheus server as to forward the data to Vector aggregator with GCP PubSub as a relay.\nAlso i am trying to avoid using metric_to_log and log_to_metric transforms.\nhttps://vector.dev/highlights/2022-03-31-native-event-codecs/\nWith the following architecture in mind:\nPrometheus <-- Vector Agent -> GCP PubSub <-- Vector Aggregator (Prometheus exporter sink) <-- Prometheus\n(left to right)\n\nVector agent scrapes Prometheus server\nVector agent publishes the metrics to GCP PubSub\nVector Aggregator reads metrics from PubSub\nVector Aggregator sinks data via Prometheus exporter\nPrometheus server scrapes Vector Aggregator\n\nThis is the configuration of the agent.\ncustomConfig:\n  data_dir: /vector-data-dir\n  api:\n    enabled: true\n    address: 0.0.0.0:8686\n  sources:\n    host_metrics:\n      type: host_metrics\n    prometheus_kube:\n      type: prometheus_scrape\n      endpoints:\n        - http://prometheus-kube-prometheus-prometheus.kube-prometheus.svc.cluster.local:9090/metrics\n  sinks:\n    pub_sub:\n      type: gcp_pubsub\n      project: abc\n      topic: my-topic\n      encoding:\n        codec: native\n      inputs:\n      - prometheus_kube\n\nI am getting the following error:\n2024-09-18T21:20:34.048629Z  INFO vector::app: Log level is enabled. level=\"info\"\n2024-09-18T21:20:34.050070Z  INFO vector::app: Loading configs. paths=[\"/etc/vector\"]\n2024-09-18T21:20:34.050807Z ERROR vector::cli: Configuration error. error=Data type mismatch between prometheus_kube (Metric) and pub_sub (Log)\n\nIt seems GCP PubSub does not support metric events.\nShould i resort to metric_to_log and log_to_metric?",
        "url": "https://github.com/vectordotdev/vector/discussions/21316",
        "createdAt": "2024-09-18T21:40:42Z",
        "updatedAt": "2024-09-20T20:59:09Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "genadipost"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21320,
        "title": "Is there a way for me to preserve aggregated data via transforms between crashes?",
        "bodyText": "Example config\napi:\n  enabled: true\n\nsources:\n  agent_logs:\n    type: vector\n    address: \"0.0.0.0:8700\"\n\n  vector_logs:\n    type: internal_logs\n\n\ntransforms:\n  long_term_agg:\n    type: aggregate\n    inputs:\n      - agent_logs\n    interval_ms: 1800000\n\n\nsinks:\n  vm_short:\n    type: \"prometheus_remote_write\"\n    inputs:\n      - \"agent_logs\"\n    endpoint: ${SHORT_SINKURL}\n    auth:\n      strategy: basic\n      user: test_user\n      password: test_user_123\n\n  vm_long:\n    type: \"prometheus_remote_write\"\n    inputs:\n      - \"long_term_agg\"\n    endpoint: ${LONG_SINKURL}\n    auth:\n      strategy: basic\n      user: test_user\n      password: test_user_123\n\n  console:\n    type: console\n    inputs:\n      - vector_logs\n    encoding:\n      codec: text\n\nHere im aggregating the data for 30 mins before writing to victoria metrics. What if vector crashes during this period - is there a way for me to recover the state before the crash?",
        "url": "https://github.com/vectordotdev/vector/discussions/21320",
        "createdAt": "2024-09-19T11:18:56Z",
        "updatedAt": "2024-09-19T12:43:20Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Kannaj"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21310,
        "title": "Vector alert",
        "bodyText": "Hello everyone!\nWe have a vector alert when the queue with logs is stuck, but there is no alert when everything works correctly and there is no queue.\nMaybe someone has encountered how to get an alert that the queue has already disappeared.\nThanks in advance!",
        "url": "https://github.com/vectordotdev/vector/discussions/21310",
        "createdAt": "2024-09-18T08:59:06Z",
        "updatedAt": "2024-09-18T08:59:07Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "alexr3"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 13393,
        "title": "How to replace value text in an array for specific key",
        "bodyText": "I spend 2 day to get the following text replace without any luck, can anyone advise?\nI tried with map_keys, map_values and for_each...\n{\n    \"device_id\": \"88:AA:CC:5A:F7:90\",\n    \"ssid_statuses\": {\n        \"5\": [\n            {\n                \"enabled\": 0,\n                \"ifname\": \"ra1\",\n                \"ssid\": \"ssid_guest1\"   <<< Change to \"ssid\": \"[REDACTED]\"\n            },\n            {\n                \"enabled\": 0,\n                \"ifname\": \"ra2\",\n                \"ssid\": \"ssid_guest2\"   <<< Change to \"ssid\": \"[REDACTED]\"\n            },\n            {\n                \"enabled\": 0,\n                \"ifname\": \"ra3\",\n                \"ssid\": \"ssid_guest3\"   <<< Change to \"ssid\": \"[REDACTED]\"\n            }\n        ]\n    },\n    \"uptime\":896790\n}",
        "url": "https://github.com/vectordotdev/vector/discussions/13393",
        "createdAt": "2022-06-30T12:09:23Z",
        "updatedAt": "2024-09-17T03:28:13Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "lzwaan"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 0
    },
    {
        "number": 21288,
        "title": "need help! DONE with vrl",
        "bodyText": "can anyone please explain what is wrong with my config -\napi:\n  address: 127.0.0.1:8686\n  enabled: true\ndata_dir: /vector-data-dir\nsinks:\n  es_cluster:\n    auth:\n      password: ${ELASTICSEARCH_PASSWORD}\n      strategy: basic\n      user: ${ELASTICSEARCH_USER}\n    bulk:\n      index: |\n        {{ .app }}-logs-%Y-%m-%d\n    endpoints:\n    - http://elasticsearch-master-hl.databases.svc.cluster.local:9200\n    inputs:\n    - add_dynamic_index\n    type: elasticsearch\nsources:\n  k8s_logs:\n    ignore_older_secs: 600\n    namespace_annotation_fields:\n      namespace_labels: \"\"\n    node_annotation_fields:\n      node_labels: \"\"\n    pod_annotation_fields:\n      container_id: \"\"\n      container_image: \"\"\n      pod_annotations: \"\"\n      pod_ip: \"\"\n    type: kubernetes_logs\ntransforms:\n  add_dynamic_index:\n    inputs:\n    - k8s_logs\n    source: |\n      tmp = .kubernetes.pod_owner\n      tmp_arr = split!(tmp, \"/\")\n      resource_name = tmp_arr[1]\n      if resource_name == null {\n         resource_name = \"unknown\"\n      }\n      .app = join!([.kubernetes.pod_namespace, resource_name], separator: \"-\")\n    type: remap\n\n\nALL i want to do is to have seprate index for per deployment/statefullset/replicaset\nnow the error on vector side -\n2024-09-12T20:53:52.823203Z ERROR sink{component_kind=\"sink\" component_id=es_cluster component_type=elasticsearch}:request{request_id=31}: vector::sinks::elasticsearch::service: Response contained errors. error_code=\"http_response_500\" response=Response { status: 500, version: HTTP/1.1, headers: {\"x-elastic-product\": \"Elasticsearch\", \"content-type\": \"application/json\"}, body: b\"{\\\"error\\\":{\\\"root_cause\\\":[{\\\"type\\\":\\\"json_e_o_f_exception\\\",\\\"reason\\\":\\\"Unexpected end-of-input in VALUE_STRING\\\\n at [Source: (byte[])\\\\\\\"{\\\\\\\"index\\\\\\\":{\\\\\\\"_index\\\\\\\":\\\\\\\"monitoring-prom-agent-kube-prometheus-stack-prometheus-shard-1-logs-2024-09-12\\\\\\\"; line: 1, column: 99]\\\"}],\\\"type\\\":\\\"json_e_o_f_exception\\\",\\\"reason\\\":\\\"Unexpected end-of-input in VALUE_STRING\\\\n at [Source: (byte[])\\\\\\\"{\\\\\\\"index\\\\\\\":{\\\\\\\"_index\\\\\\\":\\\\\\\"monitoring-prom-agent-kube-prometheus-stack-prometheus-shard-1-logs-2024-09-12\\\\\\\"; line: 1, column: 99]\\\"},\\\"status\\\":500}\" }\n\nerror on es cluster -\n[2024-09-12T20:54:47,014][WARN ][r.suppressed             ] [elasticsearch-master-0] path: /_bulk, params: {timeout=60s}, status: 500 com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input in VALUE_STRING\n at [Source: (byte[])\"{\"index\":{\"_index\":\"gitops-harbor-core-54cc84ff59-logs-2024-09-12\"; line: 1, column: 66]\n\nboth logs have diff pod refrence but its the same error for all pods all logs, no new index at all on es cluster",
        "url": "https://github.com/vectordotdev/vector/discussions/21288",
        "createdAt": "2024-09-12T20:56:13Z",
        "updatedAt": "2024-09-13T14:50:20Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "maipal-c"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21284,
        "title": "How to get k8s labels as value",
        "bodyText": ".app_name = .kubernetes.pod_labels.app.kubernetes.io/name\n.app_identifier = .kubernetes.pod_namespace + \"-\" + .app_name\nCan please someone confirm how to read values from k8s labels? like the / and . causing issue as a json key",
        "url": "https://github.com/vectordotdev/vector/discussions/21284",
        "createdAt": "2024-09-12T14:05:27Z",
        "updatedAt": "2024-09-12T14:35:33Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "maipal-c"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 20464,
        "title": "change vector daemonset timezone",
        "bodyText": "A note for the community\nI am using vector to collect the logs of a service and save it in an Azure storage, this saves it in one folder per day.\nThe problem I have is that the vector time is in UTC since it is the time zone of the kubernetes cluster, and my time zone is UTC-3, which gives me the problem that it creates the folder for the next day and for 3 hours saves logs from the previous day.\nIs there a way to set the correct time zone in vector?\nUse Cases\nNo response\nAttempted Solutions\nNo response\nProposal\nNo response\nReferences\nNo response\nVersion\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/20464",
        "createdAt": "2024-05-08T13:37:31Z",
        "updatedAt": "2024-09-12T06:24:32Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "pablofilgueira91"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21261,
        "title": "How to read entire JSON file and apply remap ?",
        "bodyText": "I have the use case where I have the valid JSON files are being generated in a folder.\nI want to parse those file and extract events from it.\nfile01.json and other files are all same format like this.\n{\n    \"scan_data\": {\n        \"version\": 1,\n        \"ip_info\": \"\",\n        \"hostname\": \"examplehost.example.com\"\n    },\n    \"vulnerabilities\": [\n        {\n            \"id\": 1001,\n            \"status\": \"\",\n            \"secure\": false,\n            \"details\": \"some details\",\n            \"title\": \"ICMP Responses Logged\"\n        },\n        {\n            \"id\": 1002,\n            \"status\": \"\",\n            \"secure\": false,\n            \"details\": \"some details\",\n            \"title\": \"Resolved DNS Name\"\n        }\n    ]\n}\n\nevents needs to be generated\nevent01\n {\n        \"version\": 1,\n        \"ip_info\": \"\",\n        \"hostname\": \"examplehost.example.com\",\n        \"id\": 1001,\n        \"status\": \"\",\n        \"secure\": false,\n        \"details\": \"some details\",\n        \"title\": \"\"title\": \"Resolved DNS Name\"\"\n}\n\nevent02\n{\n        \"version\": 1,\n        \"ip_info\": \"\",\n        \"hostname\": \"examplehost.example.com\",\n        \"id\": 1002,\n        \"status\": \"\",\n        \"secure\": false,\n        \"details\": \"some details\",\n        \"title\": \"ICMP Responses Logged\"\n}\n\nJust like that. I want all events to be streamed.\nhow should I write my source and transform config.\nthis is what i have but it reads line by line and I can't parse json. Is it something available out of the box ?\ndata_dir = \"/Users/folder/vector_data/\"\n\n[sources.file_source]\n  type = \"file\"\n  include = [ \"/Users/folder/hs_logs/result/*.json\" ]\n  read_from = \"beginning\"\n\n[transforms.asep_transform]\n  type = \"remap\"\n  inputs = [\"file_source\"]\n  source = '''\n    . = parse_json!(.message)\n  '''\n\n[sinks.console]\n  type = \"console\"\n  inputs = [\"asep_transform\"]\n  target = \"stdout\"\n  encoding.codec = \"json\"",
        "url": "https://github.com/vectordotdev/vector/discussions/21261",
        "createdAt": "2024-09-10T20:21:48Z",
        "updatedAt": "2024-09-11T16:51:53Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "sankyfunky"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 20631,
        "title": "Need clarity regarding support policy",
        "bodyText": "Hello. The release cadence for Vector at https://github.com/vectordotdev/vector/blob/master/RELEASES.md#cadence says\n\nStable channel\n\nEvery 6 weeks\nRelease patch fixes as needed to fix high-priority bugs and regressions from the last major or minor release\nRelease daily builds representing the latest state of Vector for feedback\n\n\nIm confused by what \"from the last major or minor release\" means in the second point. Does it mean\n\nbug fixes are published to the last minor release only\nbug fixes are published to all the current minor release and the latest minor release from the last major release\n\nThanks in advance...",
        "url": "https://github.com/vectordotdev/vector/discussions/20631",
        "createdAt": "2024-06-10T07:28:39Z",
        "updatedAt": "2024-09-11T15:36:40Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "shazib-summar"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21253,
        "title": "How to send vector proto messages via rpc ?",
        "bodyText": "Hello, I want to send messages (log messages) to vector source with my code, so I tried to use grpc, but it didn't work.\nI use event.proto and vector.proto to generate code.\n\nfunc TestVector(t *testing.T) {\n\twrapper := &event_pb.EventWrapper{\n\t\tEvent: &event_pb.EventWrapper_Log{\n\t\t\tLog: &event_pb.Log{\n\t\t\t\tValue: &event_pb.Value{\n\t\t\t\t\tKind: &event_pb.Value_RawBytes{\n\t\t\t\t\t\tRawBytes: []byte(\"hello world\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\tvar events []*event_pb.EventWrapper\n\tevents = append(events, wrapper)\n\treq := vector_pb.PushEventsRequest{\n\t\tEvents: events,\n\t}\n\taddr := \"xx.xx.xx.xx:9000\"\n\tconn, err := grpc.NewClient(addr, grpc.WithTransportCredentials(insecure.NewCredentials()))\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tcli := vector_pb.NewVectorClient(conn)\n\n\tctx := context.TODO()\n\tt.Logf(req.String())\n\tres, err := cli.PushEvents(ctx, &req)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tt.Logf(res.String())\n}\n\n// test log\n\n=== RUN   TestVector\n    vector_test.go:38: events:{log:{value:{raw_bytes:\"hello world\"}}}\n    vector_test.go:43: \n--- PASS: TestVector (0.01s)\nPASS\n\nres.String() always be \"\", is that OK?\n// vector config\nsources:\n  vector:\n    type: vector\n    address: 0.0.0.0:9000\nsinks:\n  file:\n    type: file\n    inputs:\n      - vector\n    path: /var/log/vector/output/vector.log\n    encoding:\n      codec: json\n\n// vector source prints\n{\"source_type\":\"vector\",\"timestamp\":\"2024-09-10T11:46:43.607741476Z\"}\n{\"source_type\":\"vector\",\"timestamp\":\"2024-09-10T11:48:37.998557858Z\"}\n{\"source_type\":\"vector\",\"timestamp\":\"2024-09-10T11:51:32.877463923Z\"}\n\nI can't get raw_bytes.",
        "url": "https://github.com/vectordotdev/vector/discussions/21253",
        "createdAt": "2024-09-10T12:07:48Z",
        "updatedAt": "2024-09-10T13:30:11Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "evanzhang87"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 20323,
        "title": "Will vector cause disk IO to be full?",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\n\nI ingested log data from machine files into kafka through vector. My kafka cluster was unavailable for a period of time. After my kafka cluster was deployed successfully and became available, my machine disk read IO suddenly increased.\nIs there any way to avoid this problem with vector?\nConfiguration\n[sources.${sourceName}]\ntype = \"file\"\ninclude = [\"/home/admin/xxxx/logs/biz/**/application.log*\"]\ndata_dir=\"${offsetFilePath}\"\nread_from=\"end\"\n\n[transforms.${transformName}]\ntype = \"remap\"\ninputs = [\"${sourceName}\"]\nsource = '''\ndel(.timestamp)\ndel(.source_type)\n.__file__=del(.file)\n.__host__=del(.host)\n.__raw_log__ = del(.message)\n.__project__ = \"hammal-image-scan\"\n.__dt__ = format_timestamp!(now(),\"%Y-%m-%d %H:%M:%S\",\"local\")\n.__level__ = \"info\"\n.\"string.values\", err = split(.__raw_log__, \"|\", limit: 100)\n.\"string.keys\" = [\"log_time\",\"sync\",\"region\",\"request_id\",\"uid\",\"request_from\",\"tenant_id\",\"client_ip\",\"sdk_version\",\"cfg_version\",\"user_type\",\"user_id\",\"user_nick\",\"avatar\",\"imei\",\"imsi\",\"umid\",\"ip\",\"os\",\"channel\",\"host_app_name\",\"host_package\",\"host_version\",\"biz_type\",\"scenes\",\"task_id\",\"data_id\",\"time\",\"url\",\"new_url\",\"code\",\"msg\",\"total_time\",\"download_time\",\"algo_time\",\"results\",\"algo_results\",\"hit_custom_keywords\",\"hit_system_keywords\",\"hit_custom_similar_texts\",\"hit_system_similar_texts\",\"hit_custom_images\",\"hit_system_images\",\"task_biz_type\",\"channel_uid\",\"profile_uid\",\"business_name\",\"first_product_name\",\"ai_service_codes\",\"hit_rule_ids\",\"user_data\",\"event_id\",\"event_code\",\"column_code\",\"src_id\",\"gray_flag_4_green\",\"aigc_biz\"]\ndel(.parseMsg)\n.parse_log = {}\nfor_each(.\"string.keys\") -> |_index, value| {\n    single_value, err = get(.\"string.values\", path: [to_int(_index)])\n    .parse_log = set!(value:.parse_log, [value], single_value)\n}\n. |=.parse_log\ndel(.parse_log)\ndel(.\"string.keys\")\ndel(.\"string.values\")\n'''\n\n[sinks.${sinkName}]\ntype = \"kafka\"\ninputs = [\"${transformName}\"]\nbootstrap_servers = \"${kafkaServers}\"\ntopic = \"${kafkaTopic}\"\nencoding.codec = \"json\"\nbuffer.type=\"disk\"\nbuffer.max_size=268435488\nbuffer.when_full=\"block\"\nacknowledgements.enabled=true\n\nVersion\nvector 0.31.0 (x86_64-unknown-linux-gnu 0f13b22 2023-07-06 13:52:34.591204470)\nDebug Output\nNo response\nExample Data\nNo response\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/20323",
        "createdAt": "2024-04-16T09:37:42Z",
        "updatedAt": "2024-09-08T14:04:49Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "FriskKiddo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 21208,
        "title": "How to know whether an event is a log or a metric?",
        "bodyText": "Hi team,\nSome of vector's transform components only work for a specific event subtype. For example, throttle only works for logs and tag_cardinality_limit only works for metrics. This makes sense because vector logs are a structured representation of a point-in-time event, and vector metrics are not represented as structured logs.\nHowever, it seems like when a component receives an incompatible event subtype it will drop it silently. Moreover, vector tap will print metrics as a structured log. This makes it difficult to observe what is happening when an event is being dropped because of subtype. What is the best way to know whether an event flowing through a vector topoology is a log or a metric?\nThank you",
        "url": "https://github.com/vectordotdev/vector/discussions/21208",
        "createdAt": "2024-09-04T22:24:58Z",
        "updatedAt": "2024-09-06T16:42:53Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "nzxwang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21195,
        "title": "Can the Cloudwatch Sink Modify Retention Periods?",
        "bodyText": "Hello, first time caller here.\nI've set up Vector to push logs to Cloudwatch and have it create the log group if it doesn't exist already. I've noticed that Vector can set the retention period when it creates the log group, but won't modify it if the log group already exists.\nIs this expected? I've given the proper IAM policies to 'DescribeLogGroup' and 'PutRetentionPolicy' with no success. The AWS console allows you to modify this setting anytime so I would assume Vector should be able to as well?",
        "url": "https://github.com/vectordotdev/vector/discussions/21195",
        "createdAt": "2024-09-03T01:35:06Z",
        "updatedAt": "2024-09-05T02:03:24Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bartagas"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 9615,
        "title": "Support for both starts_when and ends_when in reduce transform",
        "bodyText": "When starts_when option for the reduce transform were introduced in #4771 it's mentioned by @JeanMertz that in the future the restriction for specifying both that and ends_with at ones could be lifted if a use-case were found. I believe I have one.\nThe situation is as follows, I have log lines with the following messages where each line is a separate log event:\nline1\n##[startoftransactionmarker]line 2\nline 3\nline 4\n##[endoftransactionmarker]\nline 5\n\nFrom those 6 events I would after the reductions have the following 3 events separated by blank lines\nline1\n\n##[startoftransactionmarker]line 2\nline 3\nline 4\n##[endoftransactionmarker]\n\nline 5\n\nMy initial thought was to just have the following configuration:\n[transforms.reduce]\ntype = \"reduce\"\ninputs = [ \"my-input\" ]\nmerge_strategies.message = \"concat_newline\"\nstarts_when = \"\"\"starts_with!(.message, \"##[startoftransactionmarker]\")\"\"\"\nends_when = \"\"\"starts_with!(.message, \"##[endoftransactionmarker]\")\"\"\"\n\nHowever I found that it's currently not possible due to the restriction mentioned above resulting in an error when starting vector. Does anyone have a suggestion how you would do this using reduce without using both starts_when and ends_when? Currently I have resorted to using lua transform instead.\nI have created the following behavior-test that passes when the 3 lines that restricts usage of starts_when and ends_when are removed.\n[transforms.starts_when_and_ends_when]\n    inputs = []\n    type = \"reduce\"\n    merge_strategies.message = \"concat_newline\"\n    starts_when.type = \"vrl\"\n    starts_when.source = \"\"\"starts_with!(.message, \"##[startoftransactionmarker]\")\"\"\"\n    ends_when.type = \"vrl\"\n    ends_when.source = \"\"\"starts_with!(.message, \"##[endoftransactionmarker]\")\"\"\"\n\n[[tests]]\n  name = \"reduce_starts_when_and_ends_when\"\n\n  [[tests.inputs]]\n    insert_at = \"starts_when_and_ends_when\"\n    type = \"log\"\n    [tests.inputs.log_fields]\n      message = \"line 1\"\n\n  [[tests.inputs]]\n    insert_at = \"starts_when_and_ends_when\"\n    type = \"log\"\n    [tests.inputs.log_fields]\n      message = \"##[startoftransactionmarker]line 2\"\n\n  [[tests.inputs]]\n    insert_at = \"starts_when_and_ends_when\"\n    type = \"log\"\n    [tests.inputs.log_fields]\n      message = \"line 3\"\n\n  [[tests.inputs]]\n    insert_at = \"starts_when_and_ends_when\"\n    type = \"log\"\n    [tests.inputs.log_fields]\n      message = \"line 4\"\n\n  [[tests.inputs]]\n    insert_at = \"starts_when_and_ends_when\"\n    type = \"log\"\n    [tests.inputs.log_fields]\n      message = \"##[endoftransactionmarker]\"\n\n  [[tests.inputs]]\n    insert_at = \"starts_when_and_ends_when\"\n    type = \"log\"\n    [tests.inputs.log_fields]\n      message = \"line 5\"\n\n  [[tests.outputs]]\n    extract_from = \"starts_when_and_ends_when\"\n    [[tests.outputs.conditions]]\n      type = \"vrl\"\n      source = '''\n        message = string!(.message)\n        message == \"line 1\"\n      '''\n    [[tests.outputs.conditions]]\n      type = \"vrl\"\n      source = '''\n        message = string!(.message)\n        message == \"##[startoftransactionmarker]line 2\\nline 3\\nline 4\\n##[endoftransactionmarker]\"\n      '''\n    [[tests.outputs.conditions]]\n      type = \"vrl\"\n      source = '''\n        message = string!(.message)\n        message == \"line 5\"\n      '''",
        "url": "https://github.com/vectordotdev/vector/discussions/9615",
        "createdAt": "2021-10-14T22:07:56Z",
        "updatedAt": "2024-09-04T12:20:06Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ssboisen"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 9
    },
    {
        "number": 21198,
        "title": "export shows N/A for income events for syslog:udp",
        "bodyText": "In my configuration there is one syslog source with type:UDP\nin vector top and exporter I see N/A for input events while I can see their messages as output events for the same component.\nDid I miss sth in configuration?",
        "url": "https://github.com/vectordotdev/vector/discussions/21198",
        "createdAt": "2024-09-03T13:26:58Z",
        "updatedAt": "2024-09-04T08:24:05Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "mhbitman"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21204,
        "title": "socket to metrics",
        "bodyText": "Hello, i have a question for you.\nHow to convert data received by a socket to a metric using transforms\uff1f\nThe received data is as follows\uff1a\n{\"host\":\"127.0.0.1\",\"message\":\"{\"name\":\"node_memory_Active_anon_bytes\",\"help\":\"Memory information field Active_anon_bytes.\",\"type\":1,\"metric\":[{\"gauge\":{\"value\":4231172096}}]}\",\"port\":56348,\"source_type\":\"socket\",\"timestamp\":\"2024-09-03T12:15:09.118397820Z\"}\nI want to convert this json data into a metric. I checked the help documentation for transform and found that it doesn't seem to work well.",
        "url": "https://github.com/vectordotdev/vector/discussions/21204",
        "createdAt": "2024-09-04T01:17:14Z",
        "updatedAt": "2024-09-04T03:24:51Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "taikula-8"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21061,
        "title": "v0.40 Reduce transform breaking changes",
        "bodyText": "Hi,\nI'm not sure if this is an issue or intended breaking change to the reduce transform but I would really appreciate some guidance on how to continue supporting the following:\ntransforms:\n  reduce_scalyr:\n    type: reduce\n    inputs:\n    - my_input\n    group_by:\n    - session\n    expire_after_ms: 2000\n    max_events: 500\n    merge_strategies:\n      events: array\ntests:\n- name: map_reduce_scalyr.no_sev\n  inputs:\n  - type: vrl\n    insert_at: reduce_scalyr\n    source: |\n      . = {\n        \"events\": {\n          \"attrs\": {\n            \"msg\": \"foo\"\n          },\n          \"sev\": 3,\n          \"ts\": \"1602349656452332000\"\n        },\n        \"session\": \"session\",\n        \"sessionInfo\": {\n          \"kubernetes.cluster\": \"cluster\",\n          \"kubernetes.container.name\": \"container\",\n          \"kubernetes.namespace.name\": \"namespace\",\n          \"kubernetes.pod.name\": \"pod\",\n          \"logfile\": \"p\",\n          \"serverHost\": \"n\"\n        },\n        \"token\": \"token\"\n      }\n  - type: vrl\n    insert_at: reduce_scalyr\n    source: |\n      . = {\n        \"events\": {\n          \"attrs\": {\n            \"msg\": \"bar\"\n          },\n          \"sev\": 3,\n          \"ts\": \"1602349656460000000\"\n        },\n        \"session\": \"session\",\n        \"sessionInfo\": {\n          \"kubernetes.cluster\": \"cluster\",\n          \"kubernetes.container.name\": \"container\",\n          \"kubernetes.namespace.name\": \"namespace\",\n          \"kubernetes.pod.name\": \"pod\",\n          \"logfile\": \"p\",\n          \"serverHost\": \"n\"\n        },\n        \"token\": \"token\"\n      }\n  outputs:\n  - extract_from: reduce_scalyr\n    conditions:\n    - type: vrl\n      source: |\n        assert_eq!(., {\n          \"events\": [\n            {\n              \"ts\": \"1602349656452332000\",\n              \"sev\": 3,\n              \"attrs\": {\n                \"msg\": \"foo\",\n              },\n            },\n            {\n              \"ts\": \"1602349656460000000\",\n              \"sev\": 3,\n              \"attrs\": {\n                \"msg\": \"bar\",\n              },\n            },\n          ],\n          \"session\": \"session\",\n          \"sessionInfo\": {\n            \"kubernetes.cluster\": \"cluster\",\n            \"kubernetes.container.name\": \"container\",\n            \"kubernetes.namespace.name\": \"namespace\",\n            \"kubernetes.pod.name\": \"pod\",\n            \"logfile\": \"p\",\n            \"serverHost\": \"n\"\n          },\n          \"token\": \"token\",\n        })\nIn Vector 0.39 and previous versions this test passed and meant we could aggregate events before sending them to a http sink (https://app.scalyr.com/help/api#addEvents).\nHowever in 0.40 I think #20800 introduced breaking changes to this behaviour. The aggregated event now looks like the following:\n{\n  \"events\": {\n    \"attrs\": {\n      \"msg\": \"foo\"\n    },\n    \"sev\": 6,\n    \"ts\": \"1602349656452332000\"\n  },\n  \"session\": \"session\",\n  \"sessionInfo\": {\n    \"logfile\": \"p\",\n    \"serverHost\": \"n\"\n  },\n  \"token\": \"token\"\n}\ninstead of\n{\n  \"events\": [\n    {\n      \"attrs\": {\n        \"msg\": \"foo\"\n      },\n      \"sev\": 3,\n      \"ts\": \"1602349656452332000\"\n    },\n    {\n      \"attrs\": {\n        \"msg\": \"bar\"\n      },\n      \"sev\": 3,\n      \"ts\": \"1602349656460000000\"\n    }\n  ],\n  \"session\": \"session\",\n  \"sessionInfo\": {\n    \"kubernetes.cluster\": \"cluster\",\n    \"kubernetes.container.name\": \"container\",\n    \"kubernetes.namespace.name\": \"namespace\",\n    \"kubernetes.pod.name\": \"pod\",\n    \"logfile\": \"p\",\n    \"serverHost\": \"n\"\n  },\n  \"token\": \"token\"\n}\n\nThe events objects are no longer merged into an array\nThe sev field is summed\nThe fields which contain a \".\" are discarded\n\nIs this the new intended behaviour or a bug?\nIf this is intended how can I retain the previous behaviour?",
        "url": "https://github.com/vectordotdev/vector/discussions/21061",
        "createdAt": "2024-08-13T10:38:33Z",
        "updatedAt": "2024-09-03T15:11:00Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "matt-simons"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 20083,
        "title": "How can I convert a log line into json format?",
        "bodyText": "I came across the vector app by chance and am actually delighted with it. I run my docker container logs via the app and am completely satisfied with a few exceptions. However, there are docker container apps (e.g. Portainer) that do not send the logs in json format. Is there a way to convert them?\nCode:\nsource: |\n      structured, err = parse_json(.message)\n      if err != null { \n        log(err, level: \"error\")\n        log(string!(.message), level: \"error\")\n      }\n      . = merge!(., structured)\n      \n      .metadata.job = .source_type\n      .metadata.stream = .stream\n      .metadata.level = .level || \"other\"      \n      .metadata.compose_project = .label.\"com.docker.compose.project\"\n      .metadata.container_name = .container_name\n\n\nportainer-log\n2024/03/13 08:28AM INF deployments/deployer_remote.go:207 > Stack deployment output | output=\"{\\\"level\\\":\\\"info\\\",\\\"repository\\\":\\\"https://github.com/cgHome/homelab.stacks\\\",\\\"composePath\\\":[\\\"monitoring/compose.yml\\\"],\\\"destination\\\":\\\"/share/Container/.portainer/portainer-compose-unpacker\\\",\\\"env\\\":[],\\\"skipTLSVerify\\\":false,\\\"time\\\":1710318512,\\\"caller\\\":\\\"/home/runner/work/compose-unpacker/compose-unpacker/deploy.go:31\\\",\\\"message\\\":\\\"Deploying Compose stack from Git repository\\\"}\\n{\\\"level\\\":\\\"info\\\",\\\"user\\\":\\\"cgHome\\\",\\\"time\\\":1710318512,\\\"caller\\\":\\\"/home/runner/work/compose-unpacker/compose-unpacker/deploy.go:42\\\",\\\"message\\\":\\\"Using Git authentication\\\"}\\n{\\\"level\\\":\\\"info\\\",\\\"directory\\\":\\\"/share/Container/.portainer/portainer-compose-unpacker\\\",\\\"time\\\":1710318512,\\\"caller\\\":\\\"/home/runner/work/compose-unpacker/compose-unpacker/deploy.go:57\\\",\\\"message\\\":\\\"Checking the file system...\\\"}\\n{\\\"level\\\":\\\"info\\\",\\\"directory\\\":\\\"/share/Container/.portainer/portainer-compose-unpacker/stacks/homelab-monitoring\\\",\\\"time\\\":1710318512,\\\"caller\\\":\\\"/home/runner/work/compose-unpacker/compose-unpacker/deploy.go:83\\\",\\\"message\\\":\\\"Creating target destination directory on disk\\\"}\\n{\\\"level\\\":\\\"info\\\",\\\"repository\\\":\\\"https://github.com/cgHome/homelab.stacks\\\",\\\"path\\\":\\\"/share/Container/.portainer/portainer-compose-unpacker/stacks/homelab-monitoring/homelab.stacks\\\",\\\"url\\\":\\\"https://github.com/cgHome/homelab.stacks\\\",\\\"depth\\\":1,\\\"time\\\":1710318512,\\\"caller\\\":\\\"/home/runner/work/compose-unpacker/compose-unpacker/deploy.go:98\\\",\\\"message\\\":\\\"Cloning git repository\\\"}\\n{\\\"level\\\":\\\"info\\\",\\\"composeFilePaths\\\":[\\\"/share/Container/.portainer/portainer-compose-unpacker/stacks/homelab-monitoring/homelab.stacks/monitoring/compose.yml\\\"],\\\"workingDirectory\\\":\\\"/share/Container/.portainer/portainer-compose-unpacker/stacks/homelab-monitoring/homelab.stacks\\\",\\\"projectName\\\":\\\"homelab-monitoring\\\",\\\"time\\\":1710318512,\\\"caller\\\":\\\"/home/runner/work/compose-unpacker/compose-unpacker/deploy.go:126\\\",\\\"message\\\":\\\"Deploying Compose stack\\\"}\\n{\\\"level\\\":\\\"info\\\",\\\"time\\\":1710318516,\\\"caller\\\":\\\"/home/runner/work/compose-unpacker/compose-unpacker/deploy.go:144\\\",\\\"message\\\":\\\"Compose stack deployment complete\\\"}\\n\" \n\nresult\n2024-03-13T08:28:37.328415Z ERROR transform{component_kind=\"transform\" component_id=remap_docker component_type=remap}: vrl::stdlib::log: function call error for \"parse_json\" at (88:108): unable to parse json: trailing characters at line 1 column 5 internal_log_rate_secs=1 vrl_position=129\n2024-03-13T08:28:37.328660Z ER 2024/03/13 08:28AM INF deployments/deployer_remote.go:207 > Stack deployment output | output=\"{\\\"level\\\":\\\"info\\\",\\\"repository\\\":\\\"https://github.com/cgHome/homelab.stacks\\\",\\\"composePath\\\":[\\\"monitoring/compose.yml\\\"],\\\"destination\\\":\\\"/share/Container/.portainer/portainer-compose-unpacker\\\",\\\"env\\\":[],\\\"skipTLSVerify\\\":false,\\\"time\\\":1710318512,\\\"caller\\\":\\\"/home/runner/work/compose-unpacker/compose-unpacker/deploy.go:31\\\",\\\"message\\\":\\\"Deploying Compose stack from Git repository\\\"}\\n{\\\"level\\\":\\\"info\\\",\\\"user\\\":\\\"cgHome\\\",\\\"time\\\":1710318512,\\\"caller\\\":\\\"/home/runner/work/compose-unpacker/compose-unpacker/deploy.go:42\\\",\\\"message\\\":\\\"Using Git authentication\\\"}\\n{\\\"level\\\":\\\"info\\\",\\\"directory\\\":\\\"/share/Container/.portainer/portainer-compose-unpacker\\\",\\\"time\\\":1710318512,\\\"caller\\\":\\\"/home/runner/work/compose-unpacker/compose-unpacker/deploy.go:57\\\",\\\"message\\\":\\\"Checking the file system...\\\"}\\n{\\\"level\\\":\\\"info\\\",\\\"directory\\\":\\\"/share/Container/.portainer/portainer-compose-unpacker/stacks/homelab-monitoring\\\",\\\"time\\\":1710318512,\\\"caller\\\":\\\"/home/runner/work/compose-unpacker/compose-unpacker/deploy.go:83\\\",\\\"message\\\":\\\"Creating target destination directory on disk\\\"}\\n{\\\"level\\\":\\\"info\\\",\\\"repository\\\":\\\"https://github.com/cgHome/homelab.stacks\\\",\\\"path\\\":\\\"/share/Container/.portainer/portainer-compose-unpacker/stacks/homelab-monitoring/homelab.stacks\\\",\\\"url\\\":\\\"https://github.com/cgHome/homelab.stacks\\\",\\\"depth\\\":1,\\\"time\\\":1710318512,\\\"caller\\\":\\\"/home/runner/work/compose-unpacker/compose-unpacker/deploy.go:98\\\",\\\"message\\\":\\\"Cloning git repository\\\"}\\n{\\\"level\\\":\\\"info\\\",\\\"composeFilePaths\\\":[\\\"/share/Container/.portainer/portainer-compose-unpacker/stacks/homelab-monitoring/homelab.stacks/monitoring/compose.yml\\\"],\\\"workingDirectory\\\":\\\"/share/Container/.portainer/portainer-compose-unpacker/stacks/homelab-monitoring/homelab.stacks\\\",\\\"projectName\\\":\\\"homelab-monitoring\\\",\\\"time\\\":1710318512,\\\"caller\\\":\\\"/home/runner/work/compose-unpacker/compose-unpacker/deploy.go:126\\\",\\\"message\\\":\\\"Deploying Compose stack\\\"}\\n{\\\"level\\\":\\\"info\\\",\\\"time\\\":1710318516,\\\"caller\\\":\\\"/home/runner/work/compose-unpacker/compose-unpacker/deploy.go:144\\\",\\\"message\\\":\\\"Compose stack deployment complete\\\"}\\n\" internal_log_rate_secs=1 vrl_position=156 \n\n\nTHX for help\nChris",
        "url": "https://github.com/vectordotdev/vector/discussions/20083",
        "createdAt": "2024-03-13T07:44:37Z",
        "updatedAt": "2024-09-02T16:09:14Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "cgHome"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21184,
        "title": "How I can catch all events that failed to be written in sink (Elasticearch)?",
        "bodyText": "Hi all! I have pretty big flow of events which written into different Elasticsearch (ES) indices based on some fields. And it happens that events cannot be written because of schema errors or mapping exceptions from ES side. The question is what is the conventional way to catch these events, which failed to be written into ES, and write it into file sink for example?",
        "url": "https://github.com/vectordotdev/vector/discussions/21184",
        "createdAt": "2024-08-29T19:38:12Z",
        "updatedAt": "2024-08-29T23:37:55Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "gl1ridae"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21180,
        "title": "Source `internal_logs` - Transform event to readable message",
        "bodyText": "To monitor Vector instances I want to ship internal logs. The event can have different fields from event to event. For example\n{\n  \"error\": \"\\\"function call error for \\\\\\\"parse_json\\\\\\\" at (4:34): function call error for \\\\\\\"string\\\\\\\" at (16:33): expected string, got null\\\"\",\n  \"error_type\": \"conversion_failed\",\n  \"internal_log_rate_limit\": true,\n  \"message\": \"Mapping failed with event.\",\n  \"metadata\": {\n    \"kind\": \"event\",\n    \"level\": \"ERROR\",\n    \"module_path\": \"vector::internal_events::remap\",\n    \"target\": \"vector::internal_events::remap\"\n  },\n  \"stage\": \"processing\",\n  \"timestamp\": \"2024-08-29T14:40:27.919533Z\",\n  \"vector\": {\n    \"component_id\": \"transform_opensearch\",\n    \"component_kind\": \"transform\",\n    \"component_type\": \"remap\"\n  }\n}\ncompared to\n{\n  \"arch\": \"x86_64\",\n  \"debug\": \"false\",\n  \"message\": \"Vector has started.\",\n  \"metadata\": {\n    \"kind\": \"event\",\n    \"level\": \"INFO\",\n    \"module_path\": \"vector::internal_events::process\",\n    \"target\": \"vector\"\n  },\n  \"revision\": \"a9392b0 2024-08-26 14:35:19.223750502\",\n  \"timestamp\": \"2024-08-29T14:45:50.259364Z\",\n  \"version\": \"0.40.1\"\n}\nCould you please provide configuration to transform this to readable log messages like the ones printed in the console? The documentation is incomplete and does not describe all the possible fields and combinations.\nvector: Vector has started. debug=\"false\" version=\"0.40.1\" arch=\"x86_64\" revision=\"a9392b0 2024-08-26 14:35:19.223750502\"\n\ntransform{component_kind=\"transform\" component_id=transform_opensearch component_type=remap}: vector::internal_events::remap: Mapping failed with event. error=\"function call error for \\\"parse_json\\\" at (4:34): function call error for \\\"string\\\" at (16:33): expected string, got null\" error_type=\"conversion_failed\" stage=\"processing\" internal_log_rate_limit=true",
        "url": "https://github.com/vectordotdev/vector/discussions/21180",
        "createdAt": "2024-08-29T15:13:14Z",
        "updatedAt": "2024-08-29T22:30:14Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bartweber"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21160,
        "title": "How to understand how the reduce transform works with kubernetes_logs?",
        "bodyText": "Hi!\nWe have a Kubernetes cluster v1.29.5, Vector 0.40.0 (helm chart), OpenSearch 2.15 and we want to get pod logs, where the .message field can contain data in JSON format, as well as multi-line data. Many services, many logs in different formats.\nThe idea according to which we configure Vector:\n\nGet data from pods using the kubernetes_logs source.\nPass data from kubernetes_logs to the remap transform.\nUsing the remap transform, we determine the type of data in the .message field (JSON or not) in order to parse the JSON, remove unnecessary fields, etc.\nPass data from remap to the reduce transform to process the .message field in order to merge multi-line logs with a regular expression pattern.\nPass data to the Elasticsearch (Opensearch) sink.\n\nSimply put: K8s - remap (parse .message as JSON if .message is_json = true, otherwise do nothing and pass the event on) - reduce (if .message matches the regexp pattern, otherwise do nothing and pass the event on) - Opensearch.\nThe main problem is that after reduce we lose some fields.\nWe performed the following experiment. In parallel, we took data from each transform in Opensearch and saw duplicate events in it, which differ in the set of fields (see to screens).\nscreen_1\n\nscreen_2\n\nscreen_3\n\nThe screenshots show that the events are different, processed by different transforms, and their set of fields is different (.kubernetes.node_labels, .kubernetes.pod_labels, etc.)\nWe want to understand, is this normal behavior, or a bug? How can we achieve behavior when after the reduce transform we will save the fields that get into the reduce input?\nHere is the Vector configuration, from which we wanted to get the expected behavior.\nvalues.yaml:\nrole: \"Agent\"\n\nservice:\n  enabled: false\n\ncustomConfig:\n    data_dir: /vector-data-dir\n    api:\n      enabled: false\n      address: 127.0.0.1:8686\n      playground: false\n    log_schema:\n      timestamp_key: \"@timestamp\"\n    sources:\n      kubernetes_logs:\n        type: kubernetes_logs\n        extra_namespace_label_selector: \"kubernetes.io/metadata.name notin (kube-system)\"\n#        node_annotation_fields:\n#          node_labels: \"\"\n#        namespace_annotation_fields:\n#          namespace_labels: \"\"\n#        pod_annotation_fields:\n#          pod_annotations: \"\"\n#          pod_owner: \"\"\n#          pod_ips: \"\"\n#          pod_uid: \"\"\n\n\n    transforms:\n      kubernetes_logs_transformer:\n        type: remap\n        inputs:\n          - kubernetes_logs\n#          - reduce_multiline\n        source: |\n            .kubernetes.pod_labels = map_keys(object!(.kubernetes.pod_labels), recursive: true) -> |key| { replace(key, \".\", \"_\") }\n            if exists(.message) {\n               #.message = string!(.message)\n               if is_json(string!(.message)) {\n                    parsed, err = parse_json(string!(.message))\n                        if err != null {\n                             log(err, level: \"error\")\n                        }\n                          del(.message)\n                          .json = parsed\n               } # else {\n                #    .wrong_json = .message\n               #}\n            }\n            .transformer = \"kubernetes_logs_transformer (remap)\" # for test dropped labels via reduce transform\n            if exists(.kubernetes.pod_labels.creationTimestamp) {\n                 .kubernetes.pod_labels.creationTimestamp = from_unix_timestamp!(to_int!(.kubernetes.pod_labels.creationTimestamp))\n            }\n            if exists(.json.ts) {\n                 del(.json.ts)\n            }\n            if exists(.json.level) && !is_string(.json.level) {\n                 .json.level = to_string!(.json.level)\n            }\n            if exists(.kubernetes.pod_labels.\"pod-template-hash\") {\n                 del(.kubernetes.pod_labels.\"pod-template-hash\")\n            }\n\n\n      reduce_multiline:\n        type: reduce\n        inputs:\n          - kubernetes_logs\n#          - kubernetes_logs_transformer\n        merge_strategies:\n          message: concat_newline\n        starts_when: |\n            #.message = string!(.message)\n            if exists(.message) && match(string!(.message), r'^\\{|^time=|^\\[33m|^\\[[0-9]{4}-[0-9]{2}-[0-9]{2}|^[0-9]{4}-[0-9]{2}-[0-9]{2}|^[0-9]{2}/[[:alpha:]]+/[0-9]{4}|^\\[[0-9]{2}-[[:alpha:]]+-[0-9]{4}|^\\d+\\.\\d+\\.\\d+\\.\\d+|^ts=|^\\[GIN\\]|^\\d+/\\d+/\\d+[[:space:]]\\d+:\\d+:\\d+|^[[:alpha:]]\\d+[[:space:]]\\d+:\\d+:\\d+\\.\\d+') {\n                #if err != null {\n                #  false;\n                #} else {\n                #  matched;\n                #}\n                 true;\n            } else {\n                 false;\n              }\n        max_events: 100\n        group_by:\n          - file\n\n    sinks:\n      stdout:\n        type: console\n        inputs: [kubernetes_logs]\n        encoding:\n          codec: json\n      es_cluster:\n        inputs:\n#          - \"kubernetes_logs_transformer\"\n#          - \"kubernetes_logs\"\n          - \"reduce_multiline\"\n#           - \"*\" # for test\n        type: \"elasticsearch\"\n        api_version: v8\n        endpoints:\n          - \"https://test-opensearch.example.com:9200\"\n        bulk:\n          index: \"vector-test-%Y-%m-%d\"\n        tls:\n          ca_file: \"/etc/ssl/ca.crt\"\n          verify_hostname: false\n        auth:\n          strategy: \"basic\"\n          user: \"vector_user\"\n          password: \"vector_password\"\n        encoding:\n          timestamp_format: \"rfc3339\"",
        "url": "https://github.com/vectordotdev/vector/discussions/21160",
        "createdAt": "2024-08-27T10:50:14Z",
        "updatedAt": "2024-08-28T14:08:42Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bmday"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16645,
        "title": "Vector run on arm system with error",
        "bodyText": "uname -a\nLinux localhost.localdomain 4.19.90-2201.4.0.0135.up1.uel20.aarch64 #1 SMP Mon Feb 21 18:36:06 UTC 2022 aarch64 aarch64 aarch64 GNU/Linux\n./vector\n: Unsupported system page size\n: Unsupported system page size\nmemory allocation of 5 bytes failed\nAborted (core dumped)\nvector vesion\n0.27.0 gnu",
        "url": "https://github.com/vectordotdev/vector/discussions/16645",
        "createdAt": "2023-03-01T06:12:29Z",
        "updatedAt": "2024-08-27T07:10:21Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "FengZh61"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21146,
        "title": "\"overflow\" buffer type not ready for production?",
        "bodyText": "The docs mention \"Overflow buffers are not yet suitable for production workloads and may contain bugs that ultimately lead to data loss.\"\nhttps://vector.dev/docs/about/under-the-hood/architecture/buffering-model/\nIs this statement still accurate? What could be done to make overflow buffer configurations production-ready?",
        "url": "https://github.com/vectordotdev/vector/discussions/21146",
        "createdAt": "2024-08-23T21:15:51Z",
        "updatedAt": "2024-08-23T21:30:27Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "leshow"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21053,
        "title": "TLS configuration: library has no ciphers:ssl/ssl_lib.c",
        "bodyText": "Working on TLS as per https://vector.dev/docs/reference/configuration/tls/, vector is to connect with kafka, Loki and cloudwatch. But get the following error when creating a new image. Building sinks is failing.\nConfiguration error. error=Sink \"kafka_sink\": creating kafka producer failed: Client creation error: SSL_CTX_new() failed: ssl/ssl_lib.c:3955:(unknown function) error:0A0000A1:SSL routines::library has no ciphers\n\nERROR vector::topology::builder: Configuration error. error=Sink \"loki_sink\": Failed to build TLS connector: Could not build TLS connector: error:0A0000A1:SSL routines:(unknown function):library has no ciphers:ssl/ssl_lib.c:3955:\n\nSo in the openssl.cnf file, are we also supposed to pass cipher to be used.\nconfig_diagnostics = 1\nopenssl_conf = openssl_init\n\n.include /path/to/fipsmodule.cnf\n\n[openssl_init]\nproviders = provider_sect\nalg_section = algorithm_sect\n\n[provider_sect]\nfips = fips_sect\nbase = base_sect\n\n[base_sect]\nactivate = 1\n\n[algorithm_sect]\ndefault_properties = fips=yes",
        "url": "https://github.com/vectordotdev/vector/discussions/21053",
        "createdAt": "2024-08-12T08:39:45Z",
        "updatedAt": "2024-08-23T14:09:41Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "gg2code"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20510,
        "title": "Getting WARNING logs related to kafka sink",
        "bodyText": "I am getting these logs for kafka sink which doesn't make sense. Does anybody know about them?\nI am on vector version 0.38.0\n2024-05-17T06:16:51.858517Z  WARN librdkafka: librdkafka: CONFWARN [thrd:app]: Configuration property queue.buffering.max.ms is a producer property and will be ignored by this consumer instance\n2024-05-17T06:16:51.858546Z  WARN librdkafka: librdkafka: CONFWARN [thrd:app]: Configuration property batch.size is a producer property and will be ignored by this consumer instance\n2024-05-17T06:16:51.858550Z  WARN librdkafka: librdkafka: CONFWARN [thrd:app]: Configuration property request.required.acks is a producer property and will be ignored by this consumer instance\n2024-05-17T06:16:51.858552Z  WARN librdkafka: librdkafka: CONFWARN [thrd:app]: Configuration property message.timeout.ms is a producer property and will be ignored by this consumer instance",
        "url": "https://github.com/vectordotdev/vector/discussions/20510",
        "createdAt": "2024-05-17T06:18:39Z",
        "updatedAt": "2024-08-22T01:36:02Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ShahroZafar"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21123,
        "title": "vectorr tap is giving this error command terminated with exit code 137",
        "bodyText": "We upgraded the EKS cluster from 1.22 to 1.28 after that we get 137 code.  Its giving the same for 0.34, 0,38 version.\nkubectl exec -it vector-ha-deploy-1a-585c954f7-sgzv6 -n vector-events  -- vector help\ncommand terminated with exit code 137\nAny one faced this issue ? Will be very difficult to debug without access to vector tap.",
        "url": "https://github.com/vectordotdev/vector/discussions/21123",
        "createdAt": "2024-08-20T17:04:07Z",
        "updatedAt": "2024-08-20T17:17:18Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "vibhavachar"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 21122,
        "title": "VRL Redis API for log enrichment",
        "bodyText": "Probably this is something that can be done with Lua, but It will be great if you could call a redis from VRL to CRUD some data.\nIn my head, the API would be something like this\nredis_command(conn_id: <string>, command: <string>, key: <string>, args: <array>) :: <array> , <error>\n\nand would be used similarly to this\noutput, err = redis_command(conn_id: \"redis_pro\", command: \"hset\", key: .event_id, args: [\"foo\", .foo, \"bar\", .bar]) \n\nWhere redis_pro would be some kind of configuration reference like enrichment tables.\nWhat do you think? Is this crazy? Does it make sense? Is there a much simpler way to do this?\nFeedback is appreciated\nRegards!",
        "url": "https://github.com/vectordotdev/vector/discussions/21122",
        "createdAt": "2024-08-20T16:53:24Z",
        "updatedAt": "2024-08-20T17:04:53Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "rsrdesarrollo"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21107,
        "title": "Skip empty lines",
        "bodyText": "I have a rule where I want to log slow queries from my nginx into a log file. It looks like this:\nsources:\n  nginx_access:\n    type: file\n    include:\n      - /var/log/nginx/access.log\n\ntransforms:\n  nginx_access_t:\n    inputs:\n      - nginx_access\n    type: \"remap\"\n    source: |\n            . = parse_regex(.message, r'(?P<remote_addr>[^ ]*) - - \\[(?P<time_local>[^\\]]*)\\] \"(?P<request>[^\"]*)\" (?P<status>[^ ]*) (?P<body_bytes_sent>[^ ]*) \"(?P<http_referer>[^\"]*)\" \"(?P<http_user_agent>[^\"]*)\" (?P<request_time>[^ ]*) (?P<upstream_response_time>[^ ]*)') ?? {}\n            .time_local = parse_timestamp(.time_local, \"%d/%b/%Y:%H:%M:%S %z\") ?? now()\n            .request = parse_regex(.request, r'(?P<request_method>[^ ]*) (?P<request_uri>[^ ]*) (?P<request_protocol>[^ ]*)') ?? {}\n            .status = parse_int(.status) ?? 0\n            .body_bytes_sent = parse_int(.body_bytes_sent) ?? 0\n            .request_time = parse_float(.request_time) ?? 0\n            .upstream_response_time = parse_float(.upstream_response_time) ?? 0\n\n  nginx_access_t_02_keep_slow_queries:\n    inputs:\n      - nginx_access_t\n    type: \"filter\"\n    condition: .upstream_response_time > 0.5 ?? true\n\n  nginx_access_t_03_remove_empty_messages:\n    inputs:\n      - nginx_access_t_02_keep_slow_queries\n    type: \"filter\"\n    condition: .message != \"\"\n\nsinks:\n  out:\n    type: file\n    inputs:\n      - nginx_access_t\n    path: /var/log/nginx-access-slow.log\n    encoding:\n      codec: \"text\"\n\nfor nginx configured with:\nlog_format timed_combined '$remote_addr - $remote_user [$time_local] '\n\t'\"$request\" $status $body_bytes_sent '\n\t'\"$http_referer\" \"$http_user_agent\" '\n\t'$request_time $upstream_response_time $pipe';\n\naccess_log /var/log/nginx/access.log timed_combined;\n\nNow, this works, however what I see is that vector keeps putting empty lines into my /var/log/ log file. How do I tell it to stop doing that?",
        "url": "https://github.com/vectordotdev/vector/discussions/21107",
        "createdAt": "2024-08-18T15:57:51Z",
        "updatedAt": "2024-08-20T07:34:47Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "k-bx"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21104,
        "title": "playground. Filter out logs for condition",
        "bodyText": "I'm new to VRL and using the playground. I have an input event:\n{\n  \"foo\": \"bar\",\n  \"incoming_status\": \"INFO\"\n}\n\nI want a rule which ignores (drops) any event where incoming_status != ERROR\nIn other words, I'm expecting output to be {} or None or null (or however VRL represents \"nothing\").\nI can't find anything relevant in the filter documentation",
        "url": "https://github.com/vectordotdev/vector/discussions/21104",
        "createdAt": "2024-08-17T07:15:24Z",
        "updatedAt": "2024-08-19T11:36:58Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "agardnerIT"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21052,
        "title": "Vector sink with Loki gives 401 error",
        "bodyText": "I'm using vector v0.40.0, trying to connect to loki using the loki sink from openshift cluster, but getting 401 authentication error. I'm using basic auth.",
        "url": "https://github.com/vectordotdev/vector/discussions/21052",
        "createdAt": "2024-08-12T06:42:54Z",
        "updatedAt": "2024-08-19T06:10:58Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "gg2code"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21106,
        "title": "How to add additional labels when use `prometheus_scape`?",
        "bodyText": "Here is my config.toml:\n[sources.my_prometheus_source]\ntype = \"prometheus_scrape\"\nendpoints = [\"http://localhost:4000/metrics\"]\ninstance_tag = \"instance\"\nendpoint_tag = \"endpoint\"\nhonor_labels = true\n\n[sinks.my_database_sink]\ntype = \"prometheus_remote_write\"\ninputs = [\"my_prometheus_source\"]\nendpoint = \"http://localhost:4000/v1/prometheus/write?db=public\"\nIf I need some additional labels that are the same as the original prometheus scrape job, for example:\n\njob\nnamespace\ncontainer\n...\n\nCan vector support add some additional labels in the prometheus_scrape source?",
        "url": "https://github.com/vectordotdev/vector/discussions/21106",
        "createdAt": "2024-08-18T15:51:47Z",
        "updatedAt": "2024-08-19T04:07:02Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "zyy17"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21095,
        "title": "Metrics from logs json parser issue",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nHi,\nI have am trying to parse a below event to Vector via s3 and trying to fetch metrics out of it.\n#sample json event\n\nI'm uploading this event to s3 and trying to get path and request_count.\nIt is getting ingested BUT while testing it by printing it out to internal console logs before creating metrics can see that each event message is getting parsed as single event, attached snippet below\n\nConfiguration\ntesting_te_file_remap:\n        type: remap\n        inputs:\n          - route_s3_normalized.mobile_logs\n        source: |-\n           . = parse_json!(.message)\n\nI want this to be parsed as single event like below\n\n{\"bucket\":\"life360-mobile-logs-dev\",\"request_type\":\"mobile_logs\",\"source_type\":\"aws_s3\",\"message\":\"      \n\n {\n    \"path\": \"/\",\n    \"request_count\": 14,\n    \"average_duration\": 1138.4285714285713,\n    \"total_data_transferred\": 550994,\n    \"methods_distribution\": [\n      {\n        \"method\": \"POST\",\n        \"count\": 14\n      }\n    ]\n  }\n,\"region\":\"us-east-1\",\"object\":\"33.json\"}\n\nVersion\n\"0.36.0-distroless-libc\nDebug Output\nNo response\nExample Data\nAttached above\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/21095",
        "createdAt": "2024-08-16T16:15:27Z",
        "updatedAt": "2024-08-16T19:34:40Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "nthomaslife360"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21081,
        "title": "How to pass file_key from source to loki sink?",
        "bodyText": "a source like this:\nserver_log_all_file:\n  type: file\n  file_key: path\n  include:\n    - /var/log/server/*/*/*.log\n  ignore_older_secs: 600\n  glob_minimum_cooldown_ms: 10000\n\na sink like this:\nserver_log_all_to_loki:\n  inputs:\n    - server_log_all_file\n  type: loki\n  encoding:\n    codec: text\n  endpoint: \"http://loki_addr:3100\"\n  labels:\n    logfile: \"?\"\n\nHow to set the logfile label?",
        "url": "https://github.com/vectordotdev/vector/discussions/21081",
        "createdAt": "2024-08-15T08:38:31Z",
        "updatedAt": "2025-01-02T15:48:16Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "shikiroot"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21063,
        "title": "How to  improve the consumption capacity",
        "bodyText": "I am using Vector to collect Kafka messages, with two sinks: one is an HTTP-based OpenObserve interface, and the other is a single-node Kafka. The current issue is that the consumption capacity is insufficient, with Vector pulling around 15,000 messages per second, totaling about 5MB in size. In the case of multiple sinks, could one of the sinks impact the overall consumption speed? If it's a configuration issue with Vector, what configurations can be optimized to improve the consumption capacity?\n\nvector version : 0.38",
        "url": "https://github.com/vectordotdev/vector/discussions/21063",
        "createdAt": "2024-08-13T15:14:51Z",
        "updatedAt": "2024-08-14T20:02:27Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "loveyang2012"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21075,
        "title": "VRL: coerce string into array",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nUse Cases\nHello everyone!\nI have a csv file where some fields can be either empty or array. For example:\n2022-12-23 13:01:16;111;['Main','Rule','Velocity'];['routerprotocol','rule','rule'];[2,1,0];[0,0,0];'','VelocityCheck',''];0;;3;-1.00;174;-1;tr-4;13;2;\n\nI'm using function parse_csv that results in array with strings inside, but array fields from csv are also doublequoted as strings.\nCode:\narr = parse_csv!(.message, delimiter: \";\")\n        .=compact(\n            {\n                \"host\" : \"Test\",\n                \"app\" : \"Test\",\n                \"dateTime\" : {{ arr[0]}},\n                \"Id\" : {{ arr[1]}},\n                \"OneFieldWithArr\" : {{ arr[2]}},\n                \"AnotherFieldWithArr\" : {{ arr[3]}},\n....\n\nResult\n{\"app\":\"BlaBlaApp\",\"dateTime\":\"2022-12-23 13:01:16\",\"host\":\"HostExample\",\"OneFieldWithArr\":\"['Main','Rule','Velocity']\", \"AnotherFieldWithArr\":\"['routerprotocol','rule','rule']\" ......}\n\nI want to coerce value back to Array from String, because they have incorrect type after VRL transforms.\nThanks for the help in advance!\nAttempted Solutions\nI do understand that I can do it via Lua script or with additional string manipulations, but still it's better and more \"clean\" to do it as additional function.\nProposal\nNo response\nReferences\nBased on discussion\nVersion\n0.40.0",
        "url": "https://github.com/vectordotdev/vector/discussions/21075",
        "createdAt": "2024-08-14T11:55:41Z",
        "updatedAt": "2024-08-14T18:48:26Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ADovgalyuk"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21027,
        "title": "InfluxDB decoding in sources",
        "bodyText": "Hi, with the release of the influxDB decoding option in source decoding.codec=influxdb I thought I could use it to generate metrics from all kinds of sources, for example, with this config:\nsources:\n  file:\n    type: exec\n    command: [\"cat\", \"sampling.txt\"]\n    decoding:\n      codec: \"influxdb\"\n    mode: streaming\nsinks:\n  console:\n    type: console\n    inputs: [\"file\"]\n    encoding:\n      codec: \"json\"\nwhere sampling.txt are lines of valid influxDB line protocol messages.\nIt seems that this fails due to the fact that log sources cannot output metric types?\nvector --config vector-influx.yaml --watch-config\n2024-08-08T10:12:14.407424Z  INFO vector::app: Log level is enabled. level=\"info\"\n2024-08-08T10:12:14.413269Z  INFO vector::config::watcher: Creating configuration file watcher.\n2024-08-08T10:12:14.416737Z  INFO vector::config::watcher: Watching configuration files.\n2024-08-08T10:12:14.417237Z  INFO vector::app: Loading configs. paths=[\"vector-influx.yaml\"]\nthread 'main' panicked at lib/vector-core/src/config/mod.rs:128:9:\nassertion failed: ty.contains(DataType::Log)\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\nIf thats true and metrics cannot be generated from log sources, what are the use cases of the decoding.codec: influxdb option? Can that codec be really used in text-based sources? Or am I missing something?\nFor example, I would like to use a kafka source that can only output logs, but parse the messages with the influxdb codec.\nI see this was discussed here #11337 (comment) but I don't see that the limitation of ty.contains(DataType::Log) for codecs is lifted in these cases, as the influxdb codec only outputs metrics and would fail for log-only output sources\nI don't think it has sense to limit the output of a source based on the source type (as it is done here \n  \n    \n      vector/lib/vector-core/src/config/mod.rs\n    \n    \n         Line 127\n      in\n      b2c9f27\n    \n  \n  \n    \n\n        \n          \n           pub fn new_logs(ty: DataType, schema_definition: schema::Definition) -> Self { \n        \n    \n  \n\n) ... It should be based on the codec, for example, if we were using decoding.codec=native_json, the Kafka source could output metrics too, and in the documentation it states that the output is exclusivelly logs output: logs\n\nI think it would be a better fit that this was called default output: logs or so, because the \"real\" output depends on the  codec we are using\nIn this case, I think any log-only output source with a metrics-only codec should be valid",
        "url": "https://github.com/vectordotdev/vector/discussions/21027",
        "createdAt": "2024-08-08T10:35:52Z",
        "updatedAt": "2024-08-19T10:32:12Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "jorgehermo9"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 20944,
        "title": "CrowdStrike Logs Multiline Json Not Working Properly",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nHi Team,\nI am attempting to integrate CrowdStrike SIEM logs into Elasticsearch using Vector. I am encountering issues with transforming multilined JSON format logs. I have attached an error screenshot and configuration details for reference. Please assist.\n\n\n\nConfiguration\nsources:\n  apilogs:\n    type: \"file\"\n    include:\n      - \"./test.json\"\n    multiline:\n       start_pattern: \".*\"\n       condition_pattern: \"}$\"\n       mode: \"halt_with\"\n       timeout_ms: 1000\ntransforms:\n  remap_syslog:\n    inputs: [\"apilogs\"]\n    type: \"remap\"\n    source: |\n      .message  = parse_json!(.message)\n      .timestamp = now()\nsinks:\n  print:\n    type: \"console\"\n    inputs:\n      - \"remap_syslog\"\n    encoding:\n      codec: \"json\"\n  my_sink_id:\n    type: elasticsearch\n    inputs:\n      - \"remap_syslog\"\n    endpoints: [\"https://11.11.1.1:9200\"]\n    auth:\n      user: \"111111\"\n      password: \"1111111\"\n      strategy: \"basic\"\n    tls:\n      verify_certificate: false\n\nVersion\nvector 0.39.0 (x86_64-unknown-linux-gnu 73da9bb 2024-06-17 16:00:23.791735272)\nDebug Output\n2024-07-26T02:19:12.411635Z ERROR transform{component_kind=\"transform\" component_id=remap_syslog component_type=remap}: vector::internal_events::remap: Mapping failed with event. error=\"function call error for \\\"parse_json\\\" at (12:33): unable to parse json: EOF while parsing a list at line 36 column 6\" error_type=\"conversion_failed\" stage=\"processing\" internal_log_rate_limit=true\n2024-07-26T02:19:12.411741Z ERROR transform{component_kind=\"transform\" component_id=remap_syslog component_type=remap}: vector::internal_events::remap: Internal log [Mapping failed with event.] is being suppressed to avoid flooding.\n{\"file\":\"test.json\",\"host\":\"vmi1767413.contaboserver.net\",\"message\":\"{\\n  \\\"metadata\\\":{\\n   \\\"customerIDString\\\":\\\"956f887xxxxxx\\\",\\n   \\\"offset\\\":6068,\\n   \\\"eventType\\\":\\\"AuthActivityAuditEvent\\\",\\n   \\\"eventCreationTime\\\":1656862026109,\\n   \\\"version\\\":\\\"1.0\\\"\\n  },\\n  \\\"event\\\":{\\n   \\\"UserId\\\":\\\"api-client-id:24c6e2395axxxxxxx\\\",\\n   \\\"UserIp\\\":\\\"101.0.60.100\\\",\\n   \\\"OperationName\\\":\\\"streamStarted\\\",\\n   \\\"ServiceName\\\":\\\"Crowdstrike Streaming API\\\",\\n   \\\"Success\\\":true,\\n   \\\"UTCTimestamp\\\":1656862026,\\n   \\\"AuditKeyValues\\\":[\\n     {\\n      \\\"Key\\\":\\\"APIClientID\\\",\\n      \\\"ValueString\\\":\\\"24c6e2395a0c4a2eb2xxxxxxx\\\"\\n     },\\n     {\\n      \\\"Key\\\":\\\"partition\\\",\\n      \\\"ValueString\\\":\\\"0\\\"\\n     },\\n     {\\n      \\\"Key\\\":\\\"offset\\\",\\n      \\\"ValueString\\\":\\\"6067\\\"\\n     },\\n     {\\n      \\\"Key\\\":\\\"appId\\\",\\n      \\\"ValueString\\\":\\\"siem-connect\\\"\\n     },\\n     {\\n      \\\"Key\\\":\\\"eventType\\\",\\n      \\\"ValueString\\\":\\\"[MobileDetectionSummaryEvent LoginAuditEvent DetectionSummaryEvent FirewallMatchEvent ReconNotificationSummaryEvent IdentityProtectionEvent ScheduledReportNotificationEvent IncidentSummaryEvent CSPMIOAStreamingEvent CustomerIOCEvent XdrDetectionSummaryEvent RemoteResponseSessionEndEvent AuthActivityAuditEvent RemoteResponseSessionStartEvent HashSpreadingEvent CSPMSearchStreamingEvent UserActivityAuditEvent]\\\"\\n     }\",\"source_type\":\"file\",\"timestamp\":\"2024-07-26T02:19:12.411318553Z\"}\n{\"file\":\"test.json\",\"host\":\"vmi1767413.contaboserver.net\",\"message\":\"   ]\\n  }\",\"source_type\":\"file\",\"timestamp\":\"2024-07-26T02:19:12.411356353Z\"}\n{\"file\":\"test.json\",\"host\":\"vmi1767413.contaboserver.net\",\"message\":\"}\",\"source_type\":\"file\",\"timestamp\":\"2024-07-26T02:19:13.412922522Z\"}\n2024-07-26T02:19:13.602466Z ERROR sink{component_kind=\"sink\" component_id=my_sink_id component_type=elasticsearch}:request{request_id=2}: vector::sinks::elasticsearch::service: Response contained errors. error_code=\"http_response_200\" response=Response { status: 200, version: HTTP/1.1, headers: {\"content-type\": \"application/json; charset=UTF-8\", \"content-length\": \"506\"}, body: b\"{\\\"took\\\":1,\\\"errors\\\":true,\\\"items\\\":[{\\\"index\\\":{\\\"_index\\\":\\\"vector-2024.07.26\\\",\\\"_id\\\":\\\"J4nV7JABgEIWWdEvzpvy\\\",\\\"status\\\":400,\\\"error\\\":{\\\"type\\\":\\\"mapper_parsing_exception\\\",\\\"reason\\\":\\\"object mapping for [message] tried to parse field [message] as object, but found a concrete value\\\"}}},{\\\"index\\\":{\\\"_index\\\":\\\"vector-2024.07.26\\\",\\\"_id\\\":\\\"KInV7JABgEIWWdEvzpvy\\\",\\\"status\\\":400,\\\"error\\\":{\\\"type\\\":\\\"mapper_parsing_exception\\\",\\\"reason\\\":\\\"object mapping for [message] tried to parse field [message] as object, but found a concrete value\\\"}}}]}\" }\n2024-07-26T02:19:13.602711Z ERROR sink{component_kind=\"sink\" component_id=my_sink_id component_type=elasticsearch}:request{request_id=2}: vector::sinks::util::retries: Not retriable; dropping the request. reason=\"error type: mapper_parsing_exception, reason: object mapping for [message] tried to parse field [message] as object, but found a concrete value\" internal_log_rate_limit=true\n2024-07-26T02:19:13.602738Z ERROR sink{component_kind=\"sink\" component_id=my_sink_id component_type=elasticsearch}:request{request_id=2}: vector_common::internal_event::service: Service call failed. No retries or retries exhausted. error=None request_id=2 error_type=\"request_failed\" stage=\"sending\" internal_log_rate_limit=true\n2024-07-26T02:19:13.602766Z ERROR sink{component_kind=\"sink\" component_id=my_sink_id component_type=elasticsearch}:request{request_id=2}: vector_common::internal_event::component_events_dropped: Events dropped intentional=false count=2 reason=\"Service call failed. No retries or retries exhausted.\" internal_log_rate_limit=true\n2024-07-26T02:19:14.602308Z ERROR sink{component_kind=\"sink\" component_id=my_sink_id component_type=elasticsearch}:request{request_id=3}: vector::sinks::elasticsearch::service: Response contained errors. error_code=\"http_response_200\" response=Response { status: 200, version: HTTP/1.1, headers: {\"content-type\": \"application/json; charset=UTF-8\", \"content-length\": \"270\"}, body: b\"{\\\"took\\\":1,\\\"errors\\\":true,\\\"items\\\":[{\\\"index\\\":{\\\"_index\\\":\\\"vector-2024.07.26\\\",\\\"_id\\\":\\\"KYnV7JABgEIWWdEv0pva\\\",\\\"status\\\":400,\\\"error\\\":{\\\"type\\\":\\\"mapper_parsing_exception\\\",\\\"reason\\\":\\\"object mapping for [message] tried to parse field [message] as object, but found a concrete value\\\"}}}]}\" }\n2024-07-26T02:19:14.602442Z ERROR sink{component_kind=\"sink\" component_id=my_sink_id component_type=elasticsearch}:request{request_id=3}: vector::sinks::util::retries: Internal log [Not retriable; dropping the request.] is being suppressed to avoid flooding.\n2024-07-26T02:19:14.602472Z ERROR sink{component_kind=\"sink\" component_id=my_sink_id component_type=elasticsearch}:request{request_id=3}: vector_common::internal_event::service: Internal log [Service call failed. No retries or retries exhausted.] is being suppressed to avoid flooding.\n2024-07-26T02:19:14.602495Z ERROR sink{component_kind=\"sink\" component_id=my_sink_id component_type=elasticsearch}:request{request_id=3}: vector_common::internal_event::component_events_dropped: Internal log [Events dropped] is being suppressed to avoid flooding.\n\nExample Data\n{\n\"metadata\":{\n\"customerIDString\":\"956f887xxxxxx\",\n\"offset\":6068,\n\"eventType\":\"AuthActivityAuditEvent\",\n\"eventCreationTime\":1656862026109,\n\"version\":\"1.0\"\n},\n\"event\":{\n\"UserId\":\"api-client-id:24c6e2395axxxxxxx\",\n\"UserIp\":\"101.0.60.100\",\n\"OperationName\":\"streamStarted\",\n\"ServiceName\":\"Crowdstrike Streaming API\",\n\"Success\":true,\n\"UTCTimestamp\":1656862026,\n\"AuditKeyValues\":[\n{\n\"Key\":\"APIClientID\",\n\"ValueString\":\"24c6e2395a0c4a2eb2xxxxxxx\"\n},\n{\n\"Key\":\"partition\",\n\"ValueString\":\"0\"\n},\n{\n\"Key\":\"offset\",\n\"ValueString\":\"6067\"\n},\n{\n\"Key\":\"appId\",\n\"ValueString\":\"siem-connect\"\n},\n{\n\"Key\":\"eventType\",\n\"ValueString\":\"[MobileDetectionSummaryEvent LoginAuditEvent DetectionSummaryEvent FirewallMatchEvent ReconNotificationSummaryEvent IdentityProtectionEvent ScheduledReportNotificationEvent IncidentSummaryEvent CSPMIOAStreamingEvent CustomerIOCEvent XdrDetectionSummaryEvent RemoteResponseSessionEndEvent AuthActivityAuditEvent RemoteResponseSessionStartEvent HashSpreadingEvent CSPMSearchStreamingEvent UserActivityAuditEvent]\"\n}\n]\n}\n}\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/20944",
        "createdAt": "2024-07-26T02:27:05Z",
        "updatedAt": "2024-08-10T09:27:16Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "navein-kumar"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 1
    },
    {
        "number": 21035,
        "title": "Is there any to do stateful dedupe (across restart)",
        "bodyText": "Hello there,\nIs there any to do stateful dedupe (across restart)? Currently, the state is flush on restart.",
        "url": "https://github.com/vectordotdev/vector/discussions/21035",
        "createdAt": "2024-08-09T06:40:58Z",
        "updatedAt": "2024-08-09T15:30:18Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "BhautikChudasama"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 21026,
        "title": "How to merge multiple lines of logs with special format and drop the other logs?",
        "bodyText": "Log example:\ntimestamp hostname [pid][file.py][xid][rpc][INFO]:info msg\n... lots of INFO ...\ntimestamp hostname [pid][file.py][xid][rpc][ERROR]:error msg\ntimestamp hostname [pid][WARNING]:[TRACE][0]Traceback (most recent call last):\ntimestamp hostname [pid][WARNING]:[TRACE][1]trace msg\ntimestamp hostname [pid][WARNING]:[TRACE][2]trace msg\n... lots of TRACE ...\ntimestamp hostname [pid][WARNING]:[TRACE][n]Traceback end\ntimestamp hostname [pid][file.py][xid][rpc][INFO]:info msg\n... lots of INFO ...\n\nOur requirement is to merge multiple lines of logs with special format into one event and send it to Loki. In this example, we want to drop other logs, and merge the logs from ERROR to Traceback end, and send it to Loki, how should I write the config file?\nBtw, we may also have some single line ERROR logs without TRACE, and those need to be dropped too.\nI've tried the 2 configurations, but they do not work:\n    multiline:\n      start_pattern: \"ERROR\"\n      mode: continue_through\n      condition_pattern: \"TRACE\"\n      timeout_ms: 500\n\n    multiline:\n      start_pattern: \"^\\\\d{4}-\\\\d{2}-\\\\d{2} \\\\d{2}:\\\\d{2}:\\\\d{2} \\\\S+ \\\\[\\\\d+\\\\]\\\\[\\\\w+\\\\.py\\\\]\\\\[\\\\d+\\\\]\\\\[.*?\\\\]\\\\[ERROR\\\\]:(.*)$\"\n      mode: halt_before\n      condition_pattern: \"^\\\\d{4}-\\\\d{2}-\\\\d{2} \\\\d{2}:\\\\d{2}:\\\\d{2} \\\\S+ \\\\[\\\\d+\\\\]\\\\[\\\\w+\\\\.py\\\\]\\\\[\\\\d+\\\\]\\\\[.*?\\\\]\\\\[.*\\\\]:(.*)$\"\n      timeout_ms: 500\n\nBefore the vector, we are using td-agent, the config is like this:\n<source>\n  @type tail\n  path /var/log/server/%Y-%m-%d/*.log\n  read_from_head true\n  pos_file /var/log/td-agent/pos/trace\n  tag trace\n  <parse>\n    @type none\n  </parse>\n</source>\n\n<filter trace>\n  @type grep\n  <regexp>\n    key message\n    pattern /ERROR|TRACE/\n  </regexp>\n</filter>\n\n<filter trace>\n  @type concat\n  key message\n  separator \"\\n\"\n  flush_interval 1\n  multiline_start_regexp /ERROR/\n  continuous_line_regexp /TRACE/\n  multiline_end_regexp /Traceback end/\n  timeout_label @NORMAL\n</filter>\n\n<label @NORMAL>\n  <match>\n    @type null\n  </match>\n</label>\n\n<match trace>\n  @type loki\n  url ''\n  drop_single_key true\n  flush_interval 10s\n  flush_at_shutdown true\n  buffer_chunk_limit 10m\n</match>",
        "url": "https://github.com/vectordotdev/vector/discussions/21026",
        "createdAt": "2024-08-08T09:02:41Z",
        "updatedAt": "2025-01-02T15:48:31Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "shikiroot"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 21017,
        "title": "Problems with vector receiving data",
        "bodyText": "In the vector configuration file set up two sink (are set up the domain name), one of the domain name can not be accessed, the other can be accessed normally, in the vector after the start of the reception of data for a period of time after the data can not be received in the data, but the vector process is still alive in the state, and the source of the port is also listening to the state, how to ask the How to deal with this?",
        "url": "https://github.com/vectordotdev/vector/discussions/21017",
        "createdAt": "2024-08-07T05:46:04Z",
        "updatedAt": "2024-08-07T16:24:19Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "taikula-8"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20997,
        "title": "Negative value of vector_buffer_events",
        "bodyText": "I see a negative value of vector_buffer_events metric. If went into negative after the conclusion of a test where the outbound connection to kafka using kafka sink was stopped to test buffering and buffer block setting. Once the connection was re-instated, vector lead logs from disk but the above metric went in negative.\nWhat does a negative value of vector_buffer_events  mean?\n    acknowledgements:\n      enabled: true\n    api:\n      address: 0.0.0.0:8686\n      enabled: true\n      playground: false\n    data_dir: /vector-data-dir\n    expire_metrics_secs: 300\n    sinks:\n      kafka:\n        batch:\n          max_bytes: 1000000\n          max_events: 10000\n          timeout_secs: 2\n        bootstrap_servers: kafka:9092\n        buffer:\n          max_events: 4000\n          type: memory\n          when_full: block\n        compression: zstd\n        encoding:\n          codec: json\n        inputs:\n        - kubernetes_logs\n        librdkafka_options:\n          client.id: vector\n          request.required.acks: \"1\"\n        message_timeout_ms: 0\n        topic: ingress\n        type: kafka\n      prometheus_exporter:\n        address: 0.0.0.0:9090\n        buffer:\n          max_events: 500\n          type: memory\n          when_full: block\n        flush_period_secs: 60\n        inputs:\n        - internal_metrics\n        type: prometheus_exporter\n    sources:\n      internal_metrics:\n        type: internal_metrics\n      kubernetes_logs:\n        glob_minimum_cooldown_ms: 5000\n        include_paths_glob_patterns:\n        - /var/log/pods/**/*\n        ingestion_timestamp_field: ingest_timestamp\n        type: kubernetes_logs",
        "url": "https://github.com/vectordotdev/vector/discussions/20997",
        "createdAt": "2024-08-04T18:13:53Z",
        "updatedAt": "2024-08-07T16:21:36Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ShahroZafar"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21018,
        "title": "Send traces from opentelemetry source to kafka sink",
        "bodyText": "Hi everyone. We want to send traces from opentelemetry source to kafka sink with encoding protobuf. Right now it looks like:\nsources:\n    otel_traces:\n      type: opentelemetry\n      grpc:\n        address: 0.0.0.0:4317\n      http:\n        address: 0.0.0.0:4318\n\nsinks:\n    kafka_traces:\n      type: kafka\n      inputs:\n        - otel_traces.traces\n      bootstrap_servers: <some_server>\n      topic: traces-topic\n      encoding:\n        codec: protobuf\n        protobuf:\n          message_type: \"opentelemetry.proto.trace.v1.Span\"\n          desc_file: \"/etc/vector/opentelemetry.desc\"\n      compression: lz4\n\nBut I get error like:\n2024-08-06T17:20:07.620634Z ERROR vector::cli: Configuration error. error=Data type mismatch between otel_traces.traces (Trace) and kafka_traces (Log)\nAre there some ways to send traces to Kafka? Consumer of kafka topic is quickwit which expected protobuf format for work with OTEL traces.",
        "url": "https://github.com/vectordotdev/vector/discussions/21018",
        "createdAt": "2024-08-07T08:51:16Z",
        "updatedAt": "2024-08-07T14:28:40Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "otani88"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21015,
        "title": "How to find a bottleneck in simple file->vrl->blackhole config?",
        "bodyText": "Hi, I'm checking a simple flow of reading some key-value dumps, transforming them using VRL, and sending them to blackhole:\n[sources.logfmt]\ntype = \"file\"\ninclude = [\"logfmt\"]\nread_from = \"beginning\"\n\n[transforms.prepare_logfmt]\ntype = \"remap\"\ninputs = [\"logfmt\"]\n\nsource = '''\nraw = parse_logfmt!(.message)\n. = {}\n.actor_group = raw.g\n.actor_key = raw.k\n.node_no = raw.n\n.sequence_no = raw.s\n.trace_id = raw.t\n.thread_id = raw.th\n.platform_time = raw.ts\n.direction = raw.d\n.class = raw.cl\n.message_name = raw.mn\n.message_protocol = raw.mp\n.message_kind = raw.mk\n.message = raw.m\n.correlation_id = raw.c\n'''\n\n[sinks.blackhole_logfmt]\ntype = \"blackhole\"\ninputs = [\"prepare_logfmt\"]\nI observe suspiciously low performance (~80-90k/s or ~70MiB/s) with following utilization metrics:\nvector_utilization{component_id=\"blackhole_logfmt\",component_kind=\"sink\",component_type=\"blackhole\",host=\"loydbook\"} 0.1193229907855674 1722978231779\nvector_utilization{component_id=\"prepare_logfmt\",component_kind=\"transform\",component_type=\"remap\",host=\"loydbook\"} 0.6343437837793554 1722978231779\n\nCPU Usage: 520%, so it's only 17k/s per core.\nI don't understand why utilization is so low, and have several questions:\n\nWhy is the utilization of the transform only 63%?\nDoes this metric calculate the cumulative utilization of all concurrent transformers?\nWhat can I use to understand where the bottleneck is?",
        "url": "https://github.com/vectordotdev/vector/discussions/21015",
        "createdAt": "2024-08-06T21:29:07Z",
        "updatedAt": "2024-08-07T14:25:55Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "loyd"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20865,
        "title": "kubernetes_logs source in GCP doesn't work",
        "bodyText": "They do not work at all and there is no flow of logs.\nI am using Helm Chart for vector agents as daemon-sets.\nThe directory on the node /var/log/pods is already provided by hostPath by the Helm Chart.\n\nThe same setup works on AWS.\nIs there anything to be done on GCP side for access or something?",
        "url": "https://github.com/vectordotdev/vector/discussions/20865",
        "createdAt": "2024-07-16T07:13:40Z",
        "updatedAt": "2024-08-07T07:45:43Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "madhur-df"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 21012,
        "title": "Unable to validate encoding field",
        "bodyText": "Issue\n# ...\nsinks:\n  my_sink_id:\n    type: http\n    inputs: [\"kubernetes_logs\"]\n    uri: https://webhook.site/7e7707ca-ce37-464d-8540-e2745c5a3f0c\n    encoding:\n      codec: json\nI am getting Configuration error. error=unknown field encoding.codec error in validation step.\nversion\nvector 0.40.0 (x86_64-apple-darwin 1167aa9 2024-07-29 15:08:44.028365803)",
        "url": "https://github.com/vectordotdev/vector/discussions/21012",
        "createdAt": "2024-08-06T11:30:07Z",
        "updatedAt": "2024-08-06T11:42:57Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "BhautikChudasama"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20951,
        "title": "vector support for sasl.mechanism = oauthbearer with librdkafka",
        "bodyText": "Hi Vector Team,\nJust wondering if there is support for  sasl.mechanism = oauthbearer which comes from confluent kafka. Because I ran into following error while configuring sasl properties under librdkafka_options: inside vector.yaml.\n2024-07-27T22:13:31.673098Z ERROR vector::topology::builder: Configuration error. error=Sink \"logging_kafka\": creating kafka producer failed: Client config error: Configuration property \"sasl.oauthbearer.client.id\" not supported in this build: OAuth/OIDC depends on libcurl and OpenSSL which were not available at build time sasl.oauthbearer.client.id 0oafq15axz9WMbkJd1d7",
        "url": "https://github.com/vectordotdev/vector/discussions/20951",
        "createdAt": "2024-07-27T23:22:17Z",
        "updatedAt": "2024-08-04T15:52:08Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "CH-raghavendergogi"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 1
    },
    {
        "number": 20993,
        "title": "log_to_metric: I can't set the timestamp",
        "bodyText": "log_to_metric keeps defaulting the output to the current time.  I've tried formatting the input multiple ways:\nUndesired Output\n{\"name\":\"foo\",\"namespace\":\"bar\",\"timestamp\":\"2024-08-02T20:11:42.262485106Z\",\"kind\":\"absolute\",\"gauge\":{\"value\":4.299362}}\n\nAttempt 1\n{\"file\":\"sourceFile.json\",\"host\":\"machine\",\"message\":{\"foo\":4.299362},\"source_type\":\"file\",\"timestamp\":\"2024-07-02T20:10:31.316053532Z\"}\n\nfrom https://vector.dev/docs/about/under-the-hood/architecture/data-model/log/\nAttempt 2\nHow I normally transform a json log events before sending to log_to_metric.\n{\"foo\":4.299362,\"timestamp\":\"2024-07-02T20:10:31.316053532Z\"}\n\nAttempt 3\nGrasping at straws, I even tried embedding the timestamp in a tag and transforming after the conversion\n  transform-correct-timestamp:\n    type: remap\n    inputs:\n      - transform-log-to-metric-json\n    source: |\n      .name = .tags.correcttimestamp # Just to test if remap is doing anything\n      .timestamp = .tags.correcttimestamp\n\n{\n  \"name\": \"2024-07-02T20:10:31.316053532Z\",\n  \"namespace\": \"bar\",\n  \"tags\": {\n    \"correcttimestamp\": \"2024-07-02T20:10:31.316053532Z\"\n  },\n  \"timestamp\": \"2024-08-02T20:45:17.633498712Z\",\n  \"kind\": \"absolute\",\n  \"gauge\": {\n    \"value\": 4.299362\n  }\n}",
        "url": "https://github.com/vectordotdev/vector/discussions/20993",
        "createdAt": "2024-08-02T21:21:47Z",
        "updatedAt": "2024-08-02T22:12:57Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "DePinto"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20983,
        "title": "VRL: camel case?",
        "bodyText": "I see upcase and downcase functions in VRL. I don't see a camelcase. Is this possible or do you need to write your own camel case transformation?",
        "url": "https://github.com/vectordotdev/vector/discussions/20983",
        "createdAt": "2024-08-01T19:15:52Z",
        "updatedAt": "2024-08-02T21:49:49Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "leshow"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20964,
        "title": "Anyone connected Vector to Azure Log Analytics Workspace",
        "bodyText": "Hi,\nhas anyone connected vector with azure_monitor_logs, I don't get any errors when watching Vector top but log does not show in custom table.\nanyone have some cookbook how to configure log analytics workspace and vector to work?",
        "url": "https://github.com/vectordotdev/vector/discussions/20964",
        "createdAt": "2024-07-30T07:33:58Z",
        "updatedAt": "2024-07-31T12:00:21Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Idriel"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20967,
        "title": "Vector not buffering in case of message_timeout_ms=0",
        "bodyText": "We have a setup where kubernetes_logs source is used to read and push logs to kafka via kafka sink. In case of outage of kafka, with memory buffer in place. I see that queued messages keeps on increasing instead of storing them to buffer. This leads to continuous memory increase and OOMKill\nConfigs are like this\n    acknowledgements:\n      enabled: true\n    api:\n      address: 0.0.0.0:8686\n      enabled: true\n      playground: false\n    data_dir: /vector-data-dir\n    expire_metrics_secs: 300\n    sinks:\n      kafka:\n        batch:\n          max_bytes: 1000000\n          max_events: 10000\n          timeout_secs: 2\n        bootstrap_servers: kafka:9092\n        buffer:\n          max_events: 4000\n          type: memory\n          when_full: block\n        compression: zstd\n        encoding:\n          codec: json\n        inputs:\n        - kubernetes_logs\n        librdkafka_options:\n          client.id: vector\n          request.required.acks: \"1\"\n        message_timeout_ms: 0\n        topic: ingress\n        type: kafka\n      prometheus_exporter:\n        address: 0.0.0.0:9090\n        buffer:\n          max_events: 500\n          type: memory\n          when_full: block\n        flush_period_secs: 60\n        inputs:\n        - internal_metrics\n        type: prometheus_exporter\n    sources:\n      internal_metrics:\n        type: internal_metrics\n      kubernetes_logs:\n        glob_minimum_cooldown_ms: 5000\n        include_paths_glob_patterns:\n        - /var/log/pods/**/*\n        ingestion_timestamp_field: ingest_timestamp\n        type: kubernetes_logs",
        "url": "https://github.com/vectordotdev/vector/discussions/20967",
        "createdAt": "2024-07-30T10:58:38Z",
        "updatedAt": "2024-07-30T21:23:12Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ShahroZafar"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 20969,
        "title": "meaning of vector_source_lag_time_seconds",
        "bodyText": "We are replacing logstash with Vector, our filebeat send data to vector via logstash source, but it looks not very fast.\nvector_source_lag_time_seconds_bucket{component_id=\"logstash\",component_kind=\"source\",component_type=\"logstash\",le=\"0.015625\"} 0\nvector_source_lag_time_seconds_bucket{component_id=\"logstash\",component_kind=\"source\",component_type=\"logstash\",le=\"0.03125\"} 0\nvector_source_lag_time_seconds_bucket{component_id=\"logstash\",component_kind=\"source\",component_type=\"logstash\",le=\"0.0625\"} 0\nvector_source_lag_time_seconds_bucket{component_id=\"logstash\",component_kind=\"source\",component_type=\"logstash\",le=\"0.125\"} 12\nvector_source_lag_time_seconds_bucket{component_id=\"logstash\",component_kind=\"source\",component_type=\"logstash\",le=\"0.25\"} 301\nvector_source_lag_time_seconds_bucket{component_id=\"logstash\",component_kind=\"source\",component_type=\"logstash\",le=\"0.5\"} 59930\nvector_source_lag_time_seconds_bucket{component_id=\"logstash\",component_kind=\"source\",component_type=\"logstash\",le=\"1\"} 623390\nvector_source_lag_time_seconds_bucket{component_id=\"logstash\",component_kind=\"source\",component_type=\"logstash\",le=\"2\"} 1254199\nvector_source_lag_time_seconds_bucket{component_id=\"logstash\",component_kind=\"source\",component_type=\"logstash\",le=\"4\"} 1624082\nvector_source_lag_time_seconds_bucket{component_id=\"logstash\",component_kind=\"source\",component_type=\"logstash\",le=\"8\"} 2325502\nvector_source_lag_time_seconds_bucket{component_id=\"logstash\",component_kind=\"source\",component_type=\"logstash\",le=\"16\"} 2971113\nvector_source_lag_time_seconds_bucket{component_id=\"logstash\",component_kind=\"source\",component_type=\"logstash\",le=\"32\"} 2976016\nvector_source_lag_time_seconds_bucket{component_id=\"logstash\",component_kind=\"source\",component_type=\"logstash\",le=\"64\"} 2979728\nvector_source_lag_time_seconds_bucket{component_id=\"logstash\",component_kind=\"source\",component_type=\"logstash\",le=\"128\"} 2979811\nvector_source_lag_time_seconds_bucket{component_id=\"logstash\",component_kind=\"source\",component_type=\"logstash\",le=\"256\"} 2979812\nvector_source_lag_time_seconds_bucket{component_id=\"logstash\",component_kind=\"source\",component_type=\"logstash\",le=\"512\"} 2979834\nvector_source_lag_time_seconds_bucket{component_id=\"logstash\",component_kind=\"source\",component_type=\"logstash\",le=\"1024\"} 2979834\nvector_source_lag_time_seconds_bucket{component_id=\"logstash\",component_kind=\"source\",component_type=\"logstash\",le=\"2048\"} 2979834\nvector_source_lag_time_seconds_bucket{component_id=\"logstash\",component_kind=\"source\",component_type=\"logstash\",le=\"4096\"} 2979834\nvector_source_lag_time_seconds_bucket{component_id=\"logstash\",component_kind=\"source\",component_type=\"logstash\",le=\"+Inf\"} 2979834\n\nThe Doc explained: The difference between the timestamp recorded in each event and the time when it was ingested, expressed as fractional seconds.\nData sent by filebeat will have a time key @timestamp, so is the metric comes from here?\nFurthermore, according to my metric value, is the logstash source a bottleneck?",
        "url": "https://github.com/vectordotdev/vector/discussions/20969",
        "createdAt": "2024-07-30T12:30:49Z",
        "updatedAt": "2024-09-04T02:57:40Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "evanzhang87"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20971,
        "title": "AWS managed prometheus integration with vector sink prometheus_remote_write",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nI am trying to connect my AWS managed prometheus instance with my vector agent running on a ec2 instance with a ecs agent running. I am trying to send ecs_metrics to prometheus. I am able to do so with a prometheus server running on the same host machine and using the prometheus_exporter sink but not with the remote_write sink. I enabled VECTOR_LOG=aws_config=debug on my vector agent and have verified that I am able to authenticate with AWS to receive the correct credentials only when I pass the env variables with the access key, session token, secret key.\nIs there a way for the vector agent to assume the role that is attached to the task that vector is running in?\nI am running the docker image: timberio/vector:0.39.0-alpine\nConfiguration\n#                                    __   __  __\n#                                    \\ \\ / / / /\n#                                     \\ V / / /\n#                                      \\_/  \\/\n#\n#                                    V E C T O R\n#                                   Configuration\n#\n# ------------------------------------------------------------------------------\n# Website: https://vector.dev\n# Docs: https://vector.dev/docs\n# Chat: https://chat.vector.dev\n# ------------------------------------------------------------------------------\n\n# Change this to use a non-default directory for Vector data storage:\n# data_dir: \"/var/lib/vector\"\n\n# Random Syslog-formatted logs\nsources:\n  docker_content:\n    type: docker_logs\n    # include_images:\n    #   - nginx:latest\n    include_labels: [\"com.amazonaws.ecs.cluster=test\"] \n\n  ecs_metrics:\n    type: aws_ecs_metrics\n\n# Parse Syslog logs\n\ntransforms:\n\n  filter_out_backslashes:\n    type: lua\n    inputs:\n      - docker_content\n    version: \"2\"\n    hooks:\n      process: |-\n        function (event, emit)\n          -- Check if the message field contains backslashes\n          if string.match(event.log.message, \"\\\\\") then\n            -- Drop the log by not emitting it\n            return\n          end\n          -- Pass the log through\n          emit(event)\n        end\nsinks:\n  prometheus_write:\n    type: prometheus_remote_write\n    inputs:\n      - ecs_metrics\n    endpoint: https://aps-workspaces.us-east-1.amazonaws.com/workspaces/ws-2a9dded1-5d43-43d0-9deb-77a78d86d1d9/api/v1/remote_write\n    default_namespace: vector\n    healthcheck: false\n    auth:\n      region: us-east-1\n      strategy: aws\n    aws:\n      region: us-east-1\n      endpoint: https://aps-workspaces.us-east-1.amazonaws.com/workspaces/ws-2a9dded1-5d43-43d0-9deb-77a78d86d1d9/api/v1/remote_write\n\nVersion\ntimberio/vector:0.39.0-alpine\nDebug Output\n`2024-07-30T12:20:46.740109Z DEBUG hyper::proto::h1::io: flushed 990 bytes\n2024-07-30T12:20:47.222155Z DEBUG transform{component_kind=\"transform\" component_id=filter_out_backslashes component_type=lua}: vector::utilization: utilization=0.00008934048952547165\n2024-07-30T12:20:47.405741Z DEBUG source{component_kind=\"source\" component_id=ecs_metrics component_type=aws_ecs_metrics}:http: vector::internal_events::http_client: Sending HTTP request. uri=http://169.254.170.2/v3/6a7d9d39-5892-455c-923d-941891cb2220/task/stats method=GET version=HTTP/1.1 headers={\"user-agent\": \"Vector/0.39.0 (x86_64-unknown-linux-musl 73da9bb 2024-06-17 16:00:23.791735272)\", \"accept-encoding\": \"identity\"} body=[empty]\n2024-07-30T12:20:47.405806Z DEBUG source{component_kind=\"source\" component_id=ecs_metrics component_type=aws_ecs_metrics}:http: hyper::client::connect::http: connecting to 169.254.170.2:80\n2024-07-30T12:20:47.405962Z DEBUG source{component_kind=\"source\" component_id=ecs_metrics component_type=aws_ecs_metrics}:http: hyper::client::connect::http: connected to 169.254.170.2:80\n2024-07-30T12:20:47.406046Z DEBUG hyper::proto::h1::io: flushed 209 bytes\n2024-07-30T12:20:47.406558Z DEBUG sink{component_kind=\"sink\" component_id=prometheus_write component_type=prometheus_remote_write}: vector::utilization: utilization=0.00000034015692348652186\n2024-07-30T12:20:47.406579Z DEBUG hyper::proto::h1::io: parsed 7 headers\n2024-07-30T12:20:47.406585Z DEBUG hyper::proto::h1::conn: incoming body is content-length (1689 bytes)\n2024-07-30T12:20:47.406612Z DEBUG sink{component_kind=\"sink\" component_id=prometheus component_type=prometheus_exporter}: vector::utilization: utilization=0.0000004127891402505394\n2024-07-30T12:20:47.406614Z DEBUG hyper::proto::h1::conn: incoming body completed\n2024-07-30T12:20:47.406633Z DEBUG source{component_kind=\"source\" component_id=ecs_metrics component_type=aws_ecs_metrics}:http: hyper::client::pool: pooling idle connection for (\"http\", 169.254.170.2)\n2024-07-30T12:20:47.406649Z DEBUG source{component_kind=\"source\" component_id=ecs_metrics component_type=aws_ecs_metrics}:http: vector::internal_events::http_client: HTTP response. status=200 OK version=HTTP/1.1 headers={\"content-type\": \"application/json\", \"x-rate-limit-duration\": \"1\", \"x-rate-limit-limit\": \"40.00\", \"x-rate-limit-request-forwarded-for\": \"\", \"x-rate-limit-request-remote-addr\": \"10.14.81.72:57828\", \"date\": \"Tue, 30 Jul 2024 12:20:47 GMT\", \"content-length\": \"1689\"} body=[1689 bytes]\n2024-07-30T12:20:47.515632Z DEBUG source{component_kind=\"source\" component_id=prome_export component_type=prometheus_scrape}:http: vector::internal_events::http_client: Sending HTTP request. uri=http://10.14.81.72:9091/metrics method=GET version=HTTP/1.1 headers={\"accept\": \"text/plain\", \"user-agent\": \"Vector/0.39.0 (x86_64-unknown-linux-musl 73da9bb 2024-06-17 16:00:23.791735272)\", \"accept-encoding\": \"identity\"} body=[empty]\n2024-07-30T12:20:47.515698Z DEBUG source{component_kind=\"source\" component_id=prome_export component_type=prometheus_scrape}:http: hyper::client::pool: reuse idle connection for (\"http\", 10.14.81.72:9091)\n2024-07-30T12:20:47.515792Z DEBUG hyper::proto::h1::io: flushed 189 bytes\n2024-07-30T12:20:47.515846Z DEBUG hyper::proto::h1::io: parsed 4 headers\n2024-07-30T12:20:47.515862Z DEBUG hyper::proto::h1::conn: incoming body is empty\n2024-07-30T12:20:47.515895Z DEBUG sink{component_kind=\"sink\" component_id=prometheus component_type=prometheus_exporter}:http-request{method=GET path=/metrics}: vector::internal_events::http: Internal log [Received HTTP request.] is being suppressed to avoid flooding.\n2024-07-30T12:20:47.516103Z DEBUG hyper::proto::h1::io: flushed 8262 bytes\n2024-07-30T12:20:47.516141Z DEBUG hyper::proto::h1::io: parsed 3 headers\n2024-07-30T12:20:47.516146Z DEBUG hyper::proto::h1::conn: incoming body is chunked encoding\n2024-07-30T12:20:47.516158Z DEBUG hyper::proto::h1::decode: incoming chunked header: 0x1FBC (8124 bytes)\n2024-07-30T12:20:47.516182Z DEBUG source{component_kind=\"source\" component_id=prome_export component_type=prometheus_scrape}:http: vector::internal_events::http_client: HTTP response. status=200 OK version=HTTP/1.1 headers={\"content-type\": \"text/plain; version=0.0.4\", \"transfer-encoding\": \"chunked\", \"date\": \"Tue, 30 Jul 2024 12:20:47 GMT\"} body=[unknown]\n2024-07-30T12:20:47.516220Z DEBUG hyper::proto::h1::conn: incoming body completed\n2024-07-30T12:20:47.516232Z DEBUG hyper::client::pool: pooling idle connection for (\"http\", 10.14.81.72:9091)\n2024-07-30T12:20:48.408422Z DEBUG sink{component_kind=\"sink\" component_id=prometheus_write component_type=prometheus_remote_write}:request{request_id=5}:provide_credentials{provider=default_chain}: aws_config::meta::credentials::chain: loaded credentials provider=Environment\n2024-07-30T12:20:48.408524Z DEBUG sink{component_kind=\"sink\" component_id=prometheus_write component_type=prometheus_remote_write}:request{request_id=5}:http: vector::internal_events::http_client: Sending HTTP request. uri=https://aps-workspaces.us-east-1.amazonaws.com/workspaces/ws-2a9dded1-5d43-43d0-9deb-77a78d86d1d9/api/v1/remote_write method=POST version=HTTP/1.1 headers={\"x-prometheus-remote-write-version\": \"0.1.0\", \"content-type\": \"application/x-protobuf\", \"content-encoding\": \"snappy\", \"x-amz-date\": \"20240730T122048Z\", \"authorization\": Sensitive, \"x-amz-security-token\": Sensitive, \"user-agent\": \"Vector/0.39.0 (x86_64-unknown-linux-musl 73da9bb 2024-06-17 16:00:23.791735272)\", \"accept-encoding\": \"identity\"} body=[1331 bytes]\n2024-07-30T12:20:48.408565Z DEBUG sink{component_kind=\"sink\" component_id=prometheus_write component_type=prometheus_remote_write}:request{request_id=5}:http: hyper::client::pool: reuse idle connection for (\"https\", aps-workspaces.us-east-1.amazonaws.com)\n2024-07-30T12:20:48.408817Z DEBUG hyper::proto::h1::io: flushed 2977 bytes\n2024-07-30T12:20:48.441793Z DEBUG hyper::proto::h1::io: parsed 5 headers\n2024-07-30T12:20:48.441815Z DEBUG hyper::proto::h1::conn: incoming body is empty\n2024-07-30T12:20:48.441904Z DEBUG sink{component_kind=\"sink\" component_id=prometheus_write component_type=prometheus_remote_write}:request{request_id=5}:http: hyper::client::pool: pooling idle connection for (\"https\", aps-workspaces.us-east-1.amazonaws.com)`\n\nExample Data\nNo response\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/20971",
        "createdAt": "2024-07-30T12:24:44Z",
        "updatedAt": "2024-07-30T18:46:25Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "jarfral"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20926,
        "title": "Health Check for \"*Sink*\" failed: deadline has elapsed",
        "bodyText": "I encountering an error when I running the \"vector validate --config-yaml\" command, the error is health check for \"sink\" failed: deadline has elapsed. The sink I I used is vector, has anyone have encounter this error?\nAlso, I can telnet the port of the vector from the server where the vector agent installed.\nerror:\n\nVector config:",
        "url": "https://github.com/vectordotdev/vector/discussions/20926",
        "createdAt": "2024-07-25T02:52:27Z",
        "updatedAt": "2024-07-30T03:52:41Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "krisss24"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20959,
        "title": "Incorrect timestamp parsing",
        "bodyText": "I'm having trouble parsing a timestamp correctly in vector\n$ parse_timestamp!(\"2024-07-29T14:13:44.573709634Z\", \"%Y-%m-%dT%H:%M:%S.%fZ\")\nt'2024-07-29T19:13:44.573709634Z'\n\nAs far as I can tell, this should resolve to an hour of 14 not 19 (I'm 5 hours behind UTC) and when I test it here I get the right output (adjusted for my local time zone)\n\nHow do I tell vector that this string is in fact time UTC time and not my local time zone?",
        "url": "https://github.com/vectordotdev/vector/discussions/20959",
        "createdAt": "2024-07-29T14:46:59Z",
        "updatedAt": "2024-07-29T14:55:04Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "paymog"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20952,
        "title": "ERROR vector::topology::builder: Configuration error. error=Sink \"azure\": OpenSSL error",
        "bodyText": "Getting errors OpenSSL error when trying to use Azure_monitor_logs, using official Docker Debian version\nsinks:\n  azure:\n    inputs:\n      - demo_logs\n    type: azure_monitor_logs\n    customer_id: xxxxxxx\n    log_type: custom_table_test_1_CL\n    shared_key: xxxxxxxx\n    time_generated_key: time_generated\n    tls:\n      verify_certificate: false",
        "url": "https://github.com/vectordotdev/vector/discussions/20952",
        "createdAt": "2024-07-28T16:26:25Z",
        "updatedAt": "2024-07-29T08:53:24Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Idriel"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 20925,
        "title": "AWS Prometheus_remote_write failed to receive credentials",
        "bodyText": "Hi, first of all: I'm a total vector novice so bear with me,\nI'm trying to send node_exporter metrics of an EC2 instance to a AWS managed prometheus on the same VPC,\nI'm using docker image of the vector timberio/vector:0.39.0-debian\nafter configuring the vector by vector.yaml and running the container I receive the following logs:\nERROR sink{component_kind=\"sink\" component_id=prometheus_remote_write component_type=prometheus_remote_write}:request{request_id=2}: vector::sinks::util::retries: Unexpected error type; dropping the request. error=the credential provider was not enabled internal_log_rate_limit=true\nWARN sink{component_kind=\"sink\" component_id=prometheus_remote_write component_type=prometheus_remote_write}:request{request_id=2}: vector::sinks::util::adaptive_concurrency::controller: Unhandled error response. error=the credential provider was not enabled internal_log_rate_limit=true\nERROR sink{component_kind=\"sink\" component_id=prometheus_remote_write component_type=prometheus_remote_write}:request{request_id=2}: vector_common::internal_event::service: Service call failed. No retries or retries exhausted. error=Some(CredentialsNotLoaded(CredentialsNotLoaded { source: Some(\"no providers in chain provided credentials\") })) request_id=2 error_type=\"request_failed\" stage=\"sending\" internal_log_rate_limit=true\nERROR sink{component_kind=\"sink\" component_id=prometheus_remote_write component_type=prometheus_remote_write}:request{request_id=2}: vector_common::internal_event::component_events_dropped: Events dropped intentional=false count=499 reason=\"Service call failed. No retries or retries exhausted.\" internal_log_rate_limit=true\n\nmy vector.yaml is like:\nsources:\n  my_source_id:\n    type: prometheus_scrape\n    endpoints:\n      - http://exporter:9100/metrics\n    scrape_interval_secs: 15\n    scrape_timeout_secs: 5\n\nsinks:\n  prometheus_remote_write:\n    type: \"prometheus_remote_write\"\n    inputs:\n      - my_source_id\n    endpoint: \"https://aps-workspaces.eu-central-1.amazonaws.com/workspaces/ws-<the_WS_ID>/api/v1/remote_write\"\n    auth:\n      strategy: \"aws\"\n      region: \"eu-central-1\"\n    aws:\n      region: \"eu-central-1\"\nall the policy, IAM Role and attaching Role to the EC2 is done, I send curl request like the following to verify that credentials are accessible in EC2:\ncurl http://169.254.169.254/latest/meta-data/iam/info\ncurl http://169.254.169.254/latest/meta-data/iam/security-credentials/\ncurl http://169.254.169.254/latest/meta-data/iam/security-credentials/vector-prome-remote-write\nthe results are respectively:\n{\n  \"Code\" : \"Success\",\n  \"LastUpdated\" : \"2024-07-24T21:34:34Z\",\n  \"InstanceProfileArn\" : \"arn:aws:iam::<XXXXX>:instance-profile/vector-prome-remote-write\",\n  \"InstanceProfileId\" : \"AIPEF5D6XCCQYXYXYXY\"\n}\nvector-prome-remote-write\n\n{\n  \"Code\" : \"Success\",\n  \"LastUpdated\" : \"2024-07-24T21:35:22Z\",\n  \"Type\" : \"AWS-HMAC\",\n  \"AccessKeyId\" : \"ASIAT5D6XCCDSFVGHIOD\",\n  \"SecretAccessKey\" : \"bIjDVDVDVDVDVDVDkKpKbdwyU\",\n  \"Token\" : \"<long_string_of_characters>\"\nThe vector sink document for Sink Prometheus remote write said \"aws.region\" is optional but without it I received error on \"if you are using aws authentication then you must have aws.region\".\nDid I miss anything in configuration file?\nThank you in advance for your time and help.",
        "url": "https://github.com/vectordotdev/vector/discussions/20925",
        "createdAt": "2024-07-24T22:11:27Z",
        "updatedAt": "2024-07-26T21:07:32Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "aggholamyy"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20941,
        "title": "Looking for help with configuring okta oauth bearer token using librdkafka options in vector.",
        "bodyText": "Hi All,\nI have been trying to configure following configuration in librdkafka_options for vector kafka sink but it seems to be not working. Could you please provide any working example with SASL_SSL and OAUTH bearer token? Also, please let me know how to check librdkafka version in vector as seems to be bundled inside\n'sasl.mechanisms': 'OAUTHBEARER',\n'sasl.oauthbearer.method': 'oidc',\n'sasl.oauthbearer.client.id': args.client_id,\n'sasl.oauthbearer.client.secret': args.client_secret,\n'sasl.oauthbearer.token.endpoint.url': args.token_url,\n'sasl.oauthbearer.scope': ' '.join(args.scopes)\nHere are the logs:\nlimit=true\n2024-07-26T15:19:12.439522Z ERROR sink{component_kind=\"sink\" component_id=logging_kafka component_type=kafka}:request{request_id=5850}: vector_common::internal_event::component_events_dropped: Internal log [Events dropped] has been suppressed 9 times.\n2024-07-26T15:19:12.439528Z ERROR sink{component_kind=\"sink\" component_id=logging_kafka component_type=kafka}:request{request_id=5850}: vector_common::internal_event::component_events_dropped: Events dropped intentional=false count=1 reason=\"Service call failed. No retries or retries exhausted.\" internal_log_rate_limit=true\n2024-07-26T15:19:12.813388Z ERROR rdkafka::client: librdkafka: Global error: AllBrokersDown (Local: All broker connections are down): 1/1 brokers are down\n2024-07-26T15:19:13.440153Z ERROR sink{component_kind=\"sink\" component_id=logging_kafka component_type=kafka}:request{request_id=5851}: vector_common::internal_event::service: Internal log [Service call failed. No retries or retries exhausted.] is being suppressed to avoid flooding.\n2024-07-26T15:19:13.440245Z ERROR sink{component_kind=\"sink\" component_id=logging_kafka component_type=kafka}:request{request_id=5851}: vector_common::internal_event::component_events_dropped: Internal log [Events dropped] is being suppressed to avoid flooding.\n2024-07-26T15:19:14.582969Z  INFO vector::topology::running: Shutting down... Waiting on running components. remaining_components=\"logging_kafka\" time_remaining=\"54 seconds left\"\n2024-07-26T15:19:19.583030Z  INFO vector::topology::running: Shutting down... Waiting on running components. remaining_components=\"logging_kafka\" time_remaining=\"49 seconds left\"\n2024-07-26T15:19:19.812183Z  INFO vector::app: Log level is enabled. level=\"info\"\n2024-07-26T15:19:19.813642Z  INFO vector::app: Loading configs. paths=[\"/etc/vector/vector.yaml\"]\n2024-07-26T15:19:19.838447Z  WARN sink{component_kind=\"sink\" component_id=logging_kafka component_type=kafka}: librdkafka: librdkafka: CONFWARN [thrd:app]: Configuration property fetch.error.backoff.ms is a consumer property and will be ignored by this producer instance\n2024-07-26T15:19:19.838548Z  INFO vector::topology::running: Running healthchecks.\n2024-07-26T15:19:19.838632Z  INFO vector: Vector has started. debug=\"false\" version=\"0.39.0\" arch=\"aarch64\" revision=\"73da9bb 2024-06-17 16:00:23.791735272\"\n2024-07-26T15:19:19.839642Z  INFO vector::internal_events::api: API server running. address=0.0.0.0:8686 playground=http://0.0.0.0:8686/playground graphql=http://0.0.0.0:8686/graphql\n2024-07-26T15:19:19.845558Z  WARN librdkafka: librdkafka: CONFWARN [thrd:app]: Configuration property message.send.max.retries is a producer property and will be ignored by this consumer instance\n2024-07-26T15:19:19.845566Z  WARN librdkafka: librdkafka: CONFWARN [thrd:app]: Configuration property request.required.acks is a producer property and will be ignored by this consumer instance\n2024-07-26T15:19:22.852996Z ERROR vector::topology::builder: msg=\"Healthcheck failed.\" error=Meta data fetch error: OperationTimedOut (Local: Timed out) component_kind=\"sink\" component_type=\"kafka\" component_id=logging_kafka\n2024-07-26T15:19:25.039866Z  WARN librdkafka: librdkafka: FAIL [thrd:ssl://pkc-n98pk.us-west-2.aws.confluent.cloud:9092/bootstrap]: ssl://pkc-n98pk.us-west-2.aws.confluent.cloud:9092/bootstrap: Disconnected (after 5043ms in state UP)\n2024-07-26T15:19:25.040153Z ERROR rdkafka::client: librdkafka: Global error: BrokerTransportFailure (Local: Broker transport failure): ssl://pkc-n98pk.us-west-2.aws.confluent.cloud:9092/bootstrap: Disconnected (after 5043ms in state UP)\n2024-07-26T15:19:25.040237Z ERROR rdkafka::client: librdkafka: Global error: AllBrokersDown (Local: All broker connections are down): 1/1 brokers are down\n2024-07-26T15:19:30.300260Z  WARN librdkafka: librdkafka: FAIL [thrd:ssl://pkc-n98pk.us-west-2.aws.confluent.cloud:9092/bootstrap]: ssl://pkc-n98pk.us-west-2.aws.confluent.cloud:9092/bootstrap: Disconnected (after 5108ms in state UP, 1 identical error(s) suppressed)\n2024-07-26T15:19:30.300444Z ERROR rdkafka::client: librdkafka: Global error: BrokerTransportFailure (Local: Broker transport failure): ssl://pkc-n98pk.us-west-2.aws.confluent.cloud:9092/bootstrap: Disconnected (after 5108ms in state UP, 1 identical error(s) suppressed)\n2024-07-26T15:19:30.300470Z ERROR rdkafka::client: librdkafka: Global error: AllBrokersDown (Local: All broker connections are down): 1/1 brokers are dow",
        "url": "https://github.com/vectordotdev/vector/discussions/20941",
        "createdAt": "2024-07-26T15:20:19Z",
        "updatedAt": "2024-07-26T17:28:59Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "CH-raghavendergogi"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20943,
        "title": "multiline support for kubernetes_logs source",
        "bodyText": "Hi All,\nI have java program running on k8s, I wish to combine multiline exceptions.\nI know file source support multiline and I am currently using it: https://vector.dev/docs/reference/configuration/sources/file/#multiline\nHow to do the same via kubernetes_logs source?\ntransforms via Lua?",
        "url": "https://github.com/vectordotdev/vector/discussions/20943",
        "createdAt": "2024-07-26T16:34:22Z",
        "updatedAt": "2024-07-26T17:16:48Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "gwang-JNPR"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20931,
        "title": "How to extract Windows logs and insert them into a MySQL database?",
        "bodyText": "I have been reading about how to filter and extract Windows events to insert them into a MySQL database, but I found that there isn't a specific sink for MySQL. How accurate is this?",
        "url": "https://github.com/vectordotdev/vector/discussions/20931",
        "createdAt": "2024-07-25T22:00:45Z",
        "updatedAt": "2024-07-25T22:00:46Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tssingtampa"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 20928,
        "title": "How do I display the body message of sinks.http in the Vector Agent console?",
        "bodyText": "I want to see in the console about the body message sent to the post method over the https protocol.\nCan you tell me plz.",
        "url": "https://github.com/vectordotdev/vector/discussions/20928",
        "createdAt": "2024-07-25T11:45:09Z",
        "updatedAt": "2024-07-25T14:29:09Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "yeonjoo-ahn"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20912,
        "title": "performance",
        "bodyText": "Does outputting to two different places at the same time during sinks lead to decreased performance?\nMy config file is  like this:\n# Print parsed logs to stdout\nsinks:\n  elastic_sink:\n    type: \"elasticsearch\"\n    inputs: [\"json_parser\"]\n    api_version: \"v8\"\n    auth:\n      strategy: \"basic\"\n      user: \"elastic\"\n      password: \"password\"\n    bulk:\n      index: \"{{ event_name }}_logs\"\n    endpoints: [\"https://10.173.67.100:9200\",\"https://10.173.67.101:9200\", \"https://10.173.67.102:9200\"]\n    tls:\n      ca_file: \"/opt/logstash/config/ca.pem\"\n      verify_certificate: false\n  to_vector:\n    type: \"vector\"\n    inputs:  [\"json_parser\"]\n    address: \"10.173.67.117:8000\"\n\nThe source I configured comes from Kafka. When it doesn't sink to the next-level vector, there is almost no delay in Kafka. However, after I added a to_vector, the data from Kafka began to experience",
        "url": "https://github.com/vectordotdev/vector/discussions/20912",
        "createdAt": "2024-07-23T02:59:02Z",
        "updatedAt": "2024-07-23T02:59:02Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "alex-dengx"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 20901,
        "title": "Collect JAVA logs from multiple projects, multiple lines need to be merged, is there any room for optimization in this configuration?",
        "bodyText": "data_dir = \"/var/lib/vector\"\n[api]\nenabled = false\naddress = \"127.0.0.1:8686\"\n[sources.app1]\ntype = \"file\"\ninclude = [ \"/var/log/app1/java.log\" ]\nread_from = \"beginning\"\nhost_key = \"host\"\nfile_key = \"logfile_path\"\nmultiline.start_pattern = \"^\\d+\\-\\d+\\-\\d+\"\nmultiline.condition_pattern = \"^\\d+\\-\\d+\\-\\d+\"\nmultiline.mode = \"halt_before\"\nmultiline.timeout_ms = 1000\n[sources.app2]\ntype = \"file\"\ninclude = [ \"/var/log/app2/java.log\" ]\nread_from = \"beginning\"\nhost_key = \"host\"\nfile_key = \"logfile_path\"\nmultiline.start_pattern = \"^\\d+\\-\\d+\\-\\d+\"\nmultiline.condition_pattern = \"^\\d+\\-\\d+\\-\\d+\"\nmultiline.mode = \"halt_before\"\nmultiline.timeout_ms = 1000\n[transforms.app1]\ninputs = [\"app1\"]\ntype = \"remap\"\nsource = '''\n????\n'''\n[transforms.app2]\ninputs = [\"app2\"]\ntype = \"remap\"\nsource = '''\n????\n'''\n[sinks.app1_sink_id]\ntype = \"clickhouse\"\ninputs = [ \"app1\" ]\ncompression = \"gzip\"\ndatabase = \"mydatabase\"\nendpoint = \"http://localhost:8123\"\nformat = \"json_each_row\"\ntable = \"app1\"\n[sinks.app2_sink_id]\ntype = \"clickhouse\"\ninputs = [ \"app2\" ]\ncompression = \"gzip\"\ndatabase = \"mydatabase\"\nendpoint = \"http://localhost:8123\"\nformat = \"json_each_row\"\ntable = \"app2\"",
        "url": "https://github.com/vectordotdev/vector/discussions/20901",
        "createdAt": "2024-07-22T06:09:25Z",
        "updatedAt": "2024-07-22T06:09:26Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "qooke"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 20874,
        "title": "How to use parse_nginx_log in my example",
        "bodyText": "Example log:\n[\n{\n\"hostname\": \"myhostname01\",\n\"message\": \"10.218.15.12 - - [17/Jul/2024:03:40:44 +0800] \"POST /udoorkeeper/report/heartbeat HTTP/1.1\" 200 2 \"-\" \"remote-zuche-java\" \"-\" 0.009 udoorkeepertest04.xxx.com 10.218.22.11:8080 200 0.009 http - - - -\",\n\"my_ip\": \"10.218.20.105\",\n\"@timestamp\": \"2024-07-17T03:40:44+08:00\"\n},\n{\n\"hostname\": \"myhostname02\",\n\"message\": \"10.218.15.192 - - [17/Jul/2024:03:40:44 +0800] \"GET /healthCheck HTTP/2.0\" 200 19 \"-\" \"Blackbox Exporter/0.23.0\" \"-\" 0.001 zeusnewtest04.xxx.com 10.218.18.139:8080 200 0.003 https - - - -\",\n\"my_ip\": \"10.218.20.105\",\n\"@timestamp\": \"2024-07-17T03:40:44+08:00\"\n}\n]\nMy conf:\ndata_dir = \"/opt/vector/data\"\n[sources.access_log  ]\ntype = \"kafka\"\nbootstrap_servers = \"xxxx\u201d\ngroup_id = \"accesslog\"\ntopics = [ \"access_log\" ]\nauto_offset_reset = \"largest\"\nsession_timeout_ms = 10000\n[transforms.access_parser]\ninputs = [\"access_log\"]\ntype   = \"remap\"\nsource = '''\n. = parse_json!(.message)\n#., err = merge(., parse_nginx_log!(.message, \"combined\"))  ##It doesn't work here\n'''\n[sinks.print]\ntype = \"console\"\ninputs = [ \"access_parser\" ]\nencoding.codec = \"json\"",
        "url": "https://github.com/vectordotdev/vector/discussions/20874",
        "createdAt": "2024-07-17T14:07:04Z",
        "updatedAt": "2024-07-19T14:09:35Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "YangTao0"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20844,
        "title": "Source idea: HTTP API collector for Fastly Signal Science",
        "bodyText": "I've been playing with the idea of using Vector to try and pull log data from Fastly Signal Science solution using their API. https://docs.fastly.com/en/ngwaf/extract-your-data\nIt seems that the HTTP Client source would be a good base for this but their API throws up some interesting issues to solve.\nFirstly, the responses are paginated so it would need to be aware of that and look for the next page URL given in the response and automatically fetch that, eventually getting all the pages for the full data set. So I guess the the source would be making multiple HTTP requests to the API as needed.\nAlso the from/until timestamp - as these require UTC timestamp values they'd need to be dynamic and based on the current time that the source collection start. I was thinking about having it expressed as an negative offset from \"now\" in minutes. i.e from = now-10m, until = now-5m then run the source with a 5m interval. I was also thinking if now should be aligned to the nearest 5 minute to account for any delay from the desired start time and the actual time.\nSo yeah, I don't think this is possible out-of-the box and requires modifying the default HTTP Client source but I'd love some feed back on possible approaches or gotchas.",
        "url": "https://github.com/vectordotdev/vector/discussions/20844",
        "createdAt": "2024-07-11T13:21:24Z",
        "updatedAt": "2024-07-12T18:42:52Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "NeilJed"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20833,
        "title": "\u00bfVector > HAProxy > datadoghq.eu?",
        "bodyText": "Hello there,\nHere is what I want to achieve:\nvector (sink: datadog_logs) > (datadog-agent/)haproxy > datadoghq.eu\nSome context:\n\nI have datadog agents (I am a Datadog customer) sending their metrics to a designated datadog agent with HAProxy (only one machine is authorized to send data to DD platform)\nI have Vector sending digested logs to a Kafka topic, works great\n\nMy objective:\n\nSend the same logs from Vector to the datadog \"proxy\" that will in its turn send these logs to DD platform\n\nIs this achievable?\nThis vector config does not work:\nsink_datadog:\n  type: datadog_logs\n    site: \"datadoghq.eu\"\n    inputs:\n      - my_great_input\n    default_api_key: \"super_secret_dd_api_key\"\n    proxy:\n      enabled: true\n      http: \"http://fqdn_for_dd_agent:3838\"\n\nThe 3838 port is found here: https://docs.datadoghq.com/agent/configuration/proxy/?tab=linux#haproxy\nHAProxy is configured just like in the official docs, I use default ports.\nThis is excerpt from /etc/haproxy/haproxy.cfg:\n# This declares the endpoint where your Agents connects for\n# sending Logs (e.g the value of \"logs.config.logs_dd_url\")\n# If sending logs with force_use_http: true\nfrontend logs_http_frontend\n    bind *:3838\n    mode http\n    option tcplog\n    default_backend datadog-logs-http\n\nThanks for your help!",
        "url": "https://github.com/vectordotdev/vector/discussions/20833",
        "createdAt": "2024-07-10T08:57:59Z",
        "updatedAt": "2024-07-12T18:04:22Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "julgsk"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20845,
        "title": "route to different sinks with different response times",
        "bodyText": "sources:\n  sourceA:\n    type: logstash\n\ntransforms:\n  dispatcher:\n    type: route\n    inputs:\n      - sourceA\n    reroute_unmatched: false\n    route:\n      r1: .type == \"sys\"\n      r2: .type != \"sys\"\n\nsinks:\n  sinkA: // faster response\n    type: kafka\n    inputs:\n      - dispatcher.r1\n  sinkB:  // slower response\n    type: kafka\n    inputs:\n      - dispatcher.r2\n\nWill this have a buckets effect?  Will sourceA's response time depend on sinkB?",
        "url": "https://github.com/vectordotdev/vector/discussions/20845",
        "createdAt": "2024-07-11T14:19:04Z",
        "updatedAt": "2024-07-12T03:10:13Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "evanzhang87"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20837,
        "title": "Clarification on the correct usage of `get_enrichment_table_record` function in the remap transform to fetch GeoIP data.",
        "bodyText": "I am trying to configure GeoIP enrichment in my pipeline configuration but facing difficulties. Below is my current setup:\nenrichment_tables:\n  geoip:\n    path: ./GeoLite2-City.mmdb\n    type: geoip\n\nsources:\n  test_logs:\n    type: demo_logs\n    format: json\n    interval: 1\n\ntransforms:\n  test_remap:\n    type: remap\n    inputs:\n      - test_logs\n    source: |\n      .GeoData, err = get_enrichment_table_record(\"geoip\", { \"8.8.8.8\": .\"sourceAddress\" })\n\nsinks:\n  console_all:\n    type: console\n    inputs:\n      - test_remap\n    target: stdout\n    encoding:\n      codec: json\n\n\nI am attempting to utilize GeoIP enrichment using the GeoLite2-City.mmdb file, but I am encountering issues with the configuration. Specifically, I am unsure about how to correctly integrate the get_enrichment_table_record function to retrieve GeoIP data based on the source IP address (8.8.8.8 in this case).\nI have reviewed the closed issues and guides but did not find sufficient guidance on setting up GeoIP enrichment specifically. there have been recent changes and updates to the documentation that make some conflicts.\ncan anybody help me figure this out ?\nThank you for your assistance.",
        "url": "https://github.com/vectordotdev/vector/discussions/20837",
        "createdAt": "2024-07-10T17:02:28Z",
        "updatedAt": "2024-07-11T21:27:44Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20827,
        "title": "filter error",
        "bodyText": "my config\uff0cvector vecsion is 0.29.1\n[transforms.filter_no_httplog]\ntype = \"filter\"\ninputs = [\"info_logs\"]\ncondition = '!contains(string!(.message), \"httpLog\") && contains(string!(.app), \"gateway\")'\n\nAnd I run the vector\uff0cthe error logs is as follows:\n2024-07-09T06:09:36.900302Z ERROR transform{component_kind=\"transform\" component_id=filter_no_httplog component_type=filter component_name=filter_no_httplog}: vector::internal_events::conditions: VRL condition execution failed. error=function call error for \"contains\" at (43:77): function call error for \"string\" at (52:65): expected string, got null error_type=\"script_failed\" stage=\"processing\" internal_log_rate_limit=true\n2024-07-09T06:09:36.900969Z ERROR transform{component_kind=\"transform\" component_id=filter_no_httplog component_type=filter component_name=filter_no_httplog}: vector::internal_events::conditions: Internal log [VRL condition execution failed.] is being rate limited.\n\nPlease\uff0canyone help me,thank u very much",
        "url": "https://github.com/vectordotdev/vector/discussions/20827",
        "createdAt": "2024-07-09T06:10:38Z",
        "updatedAt": "2024-07-09T13:31:13Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "uglyliu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20808,
        "title": "Ballpark value of vector_utilization metric to consider vector is choked",
        "bodyText": "Vector have a metric named vector_utilization. We have a system which is very critical for which we are reading logs and cannot lose logs.\nWe want to get alerted when vector is lagging behind / fully choked and is applying backpressure. At what value of this metric should we consider that vector is working slowly and something needs to be done",
        "url": "https://github.com/vectordotdev/vector/discussions/20808",
        "createdAt": "2024-07-08T08:29:01Z",
        "updatedAt": "2024-07-30T12:55:36Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ShahroZafar"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20797,
        "title": "How do acknowledgements work with the File Source and a HTTP Sink?",
        "bodyText": "Hi everyone, I have a setup as follows:\n\nFile source w/ remove_after_secs set\nHTTP sink w/ acknowledgements enabled\n\nI made some assumptions here about how E2E acknowledgements here would work. I assumed that the File source's remove_after_secs flag would only delete a file if it had reached EOF and all the data in the File source had been acknowledged by the sink. However, I'm seeing that even when the data has hit the HTTP sink,and the HTTP sink is receiving some retryable error and is still in the process of retrying it, the File source will have already deleted the data files it originally read from. This is concerning because this opens up the possibility of data loss, where retries get exhausted and Vector loses this data, and the original data files that were read have been deleted.\nIt's likely that I am misunderstanding how the acknowledgements should work in this scenario. My main questions are:\n\nHow do acknowledgements work for a File source?\nIs there a way to ensure that files are only deleted via the remove_after_secs flag after the HTTP sink has received a non-error code response for all the data in the file?\n\nIdeally, I would want to be able to guard against data loss even in the case of a restart of Vector",
        "url": "https://github.com/vectordotdev/vector/discussions/20797",
        "createdAt": "2024-07-04T14:06:08Z",
        "updatedAt": "2024-07-08T15:12:27Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mkwan-amzn"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20770,
        "title": "how can I parse json with null",
        "bodyText": "the Original json is like this :```\n{\n\"cmdline\": \"forfiles  /p \"C:\\users\\administrator\\AppData\\Roaming\\Microsoft\\Signatures\\Deloitte short_CN_files\" /c \"cmd /c del /f /q @path\" /d -15/06/2016\",\n\"company_id\": \"65b8b9cb881d8861bc00bf82\",\n\"ctc_version\": \"2.13.8.4\",\n\"datetime\": 1719931361203,\n\"elevation\": 1,\n\"elevation_sz\": \"elevated\",\n\"event_name\": \"process_create\",\n\"event_version\": 1,\n\"hardware_id\": \"DB2BFB4C-27E9-11B2-A85C-CDF98482CE32-9C2DCD0F899E\",\n\"headers\": {},\n\"integrity\": 12288,\n\"integrity_sz\": \"high\",\n\"machine_name\": \"CNPC2KRXR6\",\n\"message_key\": null,\n\"mitre_ids\": [\n{\n\"categories\": [\n\"Execution\"\n],\n\"id\": \"T1059\",\n\"name\": \"Command and Scripting Interpreter\"\n},\n{\n\"categories\": [\n\"Defense Evasion\"\n],\n\"id\": \"T1218\",\n\"name\": \"System Binary Proxy Execution\"\n},\n{\n\"categories\": [\n\"Privilege Escalation\",\n\"Defense Evasion\"\n],\n\"id\": \"T1548\",\n\"name\": \"Abuse Elevation Control Mechanism\",\n\"subtechniques\": [\n{\n\"id\": \"T1548.002\",\n\"name\": \"Bypass User Account Control\"\n}\n]\n}\n],\n\"offset\": 3308225475,\n\"os_family\": \"windows\",\n\"os_platform\": \"x64\",\n\"os_type\": \"client\",\n\"os_version\": \"Windows 10\",\n\"parent_cmdline\": \"C:\\WINDOWS\\system32\\cmd.exe /c \"\"C:\\WINDOWS\\ccmcache\\1k\\Install_user.cmd\"\"\",\n\"parent_elevation\": 1,\n\"parent_elevation_sz\": \"elevated\",\n\"parent_integrity\": 12288,\n\"parent_integrity_sz\": \"high\",\n\"parent_pid\": 4552,\n\"parent_process_path\": \"c:\\windows\\syswow64\\cmd.exe\",\n\"parent_user_name\": \"administrator\",\n\"partition\": 29,\n\"pid\": 23856,\n\"process_md5\": \"d95c443851f70f77427b3183b1619dd3\",\n\"process_path\": \"c:\\windows\\syswow64\\forfiles.exe\",\n\"process_sha\": \"7074d2a9c3d669a15d5b3a7ba1226dbba05888cc537cf055fed6371f32f0c1f5\",\n\"product_version\": \"7.9.12.418\",\n\"source_type\": \"kafka\",\n\"timestamp\": \"2024-07-02T14:41:59.944Z\",\n\"topic\": \"test\",\n\"user_name\": \"administrator\",\n\"user_sid\": \"S-1-5-21-776561741-1482476501-682003330-2268628\"\n}",
        "url": "https://github.com/vectordotdev/vector/discussions/20770",
        "createdAt": "2024-07-02T14:50:52Z",
        "updatedAt": "2024-07-08T13:06:31Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "alex-dengx"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 0
    },
    {
        "number": 20782,
        "title": "What are the plans for Vector?",
        "bodyText": "Dear Datadog team,\nAre there any future plans to close this project and integrate it into Datadog stack?\nWe have a large deployment stack of Vector, and would be happy to understand what are the commercial future plans of it\nMany thanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/20782",
        "createdAt": "2024-07-03T07:45:35Z",
        "updatedAt": "2024-07-04T05:24:06Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "omers"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20771,
        "title": "Json With null value",
        "bodyText": "How can I parse json from kafka that some key may have null value ? like following json , the message_key is null so that I get an error message:\nERROR transform{component_kind=\"transform\" component_id=json_parser component_type=remap}: vector::internal_events::remap: Mapping failed with event. error=\"function call error for \"parse_json\" at (4:25): expected string, got null\" error_type=\"conversion_failed\" stage=\"processing\" internal_log_rate_limit=true\nthe json is like this:\n{\n    \"cmdline\": \"forfiles  /p \\\"C:\\\\users\\\\administrator\\\\AppData\\\\Roaming\\\\Microsoft\\\\Signatures\\\\Deloitte short_CN_files\\\" /c \\\"cmd /c del /f /q @path\\\" /d -15/06/2016\",\n    \"company_id\": \"65b8b9cb881d8861bc00bf82\",\n    \"ctc_version\": \"2.13.8.4\",\n    \"datetime\": 1719931361203,\n    \"elevation\": 1,\n    \"elevation_sz\": \"elevated\",\n    \"event_name\": \"process_create\",\n    \"event_version\": 1,\n    \"hardware_id\": \"DB2BFB4C-27E9-11B2-A85C-CDF98482CE32-9C2DCD0F899E\",\n    \"headers\": {},\n    \"integrity\": 12288,\n    \"integrity_sz\": \"high\",\n    \"machine_name\": \"CNPC2KRXR6\",\n     \"message_key\": null,\n    \"mitre_ids\": [\n        {\n            \"categories\": [\n                \"Execution\"\n            ],\n            \"id\": \"T1059\",\n            \"name\": \"Command and Scripting Interpreter\"\n        },\n        {\n            \"categories\": [\n                \"Defense Evasion\"\n            ],\n            \"id\": \"T1218\",\n            \"name\": \"System Binary Proxy Execution\"\n        },\n        {\n            \"categories\": [\n                \"Privilege Escalation\",\n                \"Defense Evasion\"\n            ],\n            \"id\": \"T1548\",\n            \"name\": \"Abuse Elevation Control Mechanism\",\n            \"subtechniques\": [\n                {\n                    \"id\": \"T1548.002\",\n                    \"name\": \"Bypass User Account Control\"\n                }\n            ]\n        }\n    ],\n    \"offset\": 3308225475,\n    \"os_family\": \"windows\",\n    \"os_platform\": \"x64\",\n    \"os_type\": \"client\",\n    \"os_version\": \"Windows 10\",\n    \"parent_cmdline\": \"C:\\\\WINDOWS\\\\system32\\\\cmd.exe /c \\\"\\\"C:\\\\WINDOWS\\\\ccmcache\\\\1k\\\\Install_user.cmd\\\"\\\"\",\n    \"parent_elevation\": 1,\n    \"parent_elevation_sz\": \"elevated\",\n    \"parent_integrity\": 12288,\n    \"parent_integrity_sz\": \"high\",\n    \"parent_pid\": 4552,\n    \"parent_process_path\": \"c:\\\\windows\\\\syswow64\\\\cmd.exe\",\n    \"parent_user_name\": \"administrator\",\n    \"partition\": 29,\n    \"pid\": 23856,\n    \"process_md5\": \"d95c443851f70f77427b3183b1619dd3\",\n    \"process_path\": \"c:\\\\windows\\\\syswow64\\\\forfiles.exe\",\n    \"process_sha\": \"7074d2a9c3d669a15d5b3a7ba1226dbba05888cc537cf055fed6371f32f0c1f5\",\n    \"product_version\": \"7.9.12.418\",\n    \"source_type\": \"kafka\",\n    \"timestamp\": \"2024-07-02T14:41:59.944Z\",\n    \"topic\": \"test\",\n    \"user_name\": \"administrator\",\n    \"user_sid\": \"S-1-5-21-776561741-1482476501-682003330-2268628\"\n}\n\nmy transform config is this but the del function can not solve this problem\ntransforms:\n  json_parser:\n    type: \"remap\"\n    inputs: [\"kafka_consumer\"]\n    source : |\n      . = parse_json!(.message)\n      del(.message_key)",
        "url": "https://github.com/vectordotdev/vector/discussions/20771",
        "createdAt": "2024-07-02T15:12:13Z",
        "updatedAt": "2024-07-02T17:09:18Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "alex-dengx"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20767,
        "title": "how can I configure the URI address of the HTTP sink using variables extracted from the remap transforms?",
        "bodyText": "I send logs to openobserve by sink.http;\nthe original log like this:\n{\"contents\": {\"content\": \"prodnginx 200 499 0.020 180.120.167.213 96 [28/Jun/2024:18:27:51 +0800] \"POST /mc/device/\",\"host\": \"dmz-k-paas-16c-e-9.novalocal\",\"namespace\": \"pushcenter\",\"pod\": \"nginx-pushcenter-88d99c45b-dz5pg\",\"service\": \"nginx-pushcenter\"}}\nthe right configure is like this :\n_[sources.a]\ntype = \"file\"\ninclude = [ \"/data/vector/json.log\" ]\n[transforms.b]\ntype = \"remap\"\ninputs = [ \"a\" ]\nsource = \"\"\"\n. = parse_json!(string!(.message))\n\"\"\"\n[transforms.c]\ntype = \"remap\"\ninputs = [ \"b\" ]\nsource = \"\"\"\n. = .contents\n\"\"\"\n[sinks.openobserver]\ntype = \"http\"\ninputs = [ \"c\" ]\nuri = \"http://192.168.168.137:5080/api/default/nginx/_json\"\nmethod = \"post\"\nauth.strategy = \"basic\"\nauth.user = \"root@example.com\"\nauth.password = \"GzoDXJ7RXJMOTnoU\"\ncompression = \"gzip\"\nencoding.codec = \"json\"\nencoding.timestamp_format = \"rfc3339\"\nhealthcheck.enabled = false_\nbut now I want to use a variable to config the uri ,like this\n_uri = \"http://192.168.168.137:5080/api/default/{{  .service  }}/json\"\nit not works .\nhow can I do it ,now ?",
        "url": "https://github.com/vectordotdev/vector/discussions/20767",
        "createdAt": "2024-07-02T07:04:07Z",
        "updatedAt": "2024-07-02T13:28:53Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "loveyang2012"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20722,
        "title": "use an environment variable to determine which application's logs to collect",
        "bodyText": "In a Kubernetes environment, there are many application logs, and it seems that Vector collects all of them. However, I only want to collect the logs of my own application. Can I use an environment variable to determine which application's logs to collect? For example, when I start an Nginx application, I configure an environment variable during startup, and Vector detects this environment variable to collect Nginx logs.",
        "url": "https://github.com/vectordotdev/vector/discussions/20722",
        "createdAt": "2024-06-24T08:21:55Z",
        "updatedAt": "2024-07-02T06:01:41Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "loveyang2012"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20705,
        "title": "Is it recommened to use `--watch-config` in production?",
        "bodyText": "Hi,\nI have a use case to dynamically populate metadata to logs and route to different syncs. We can probably do it by dynamically updating configuration that transform and route and use --watch-config global option on vector to reload when configuration is changed.\nI have following questions of --watch-config and reloading of vector and its performance impact:\n\nIs it recommend to use --watch-config in production where vector handling 1M msg/s\nhow intensive is reload of vector, length of configuration file has a factor on this?\nwill there be a downtime or dip in # of events that vector accepts when its reloading?\nvector reload cause any back pressure on systems writing to it?\nIs it possible that vector discord events during reload?\n\nAlso, is there a limit on # of routes / transform in config for optimal vector performance\nThanks for the help!",
        "url": "https://github.com/vectordotdev/vector/discussions/20705",
        "createdAt": "2024-06-21T07:28:54Z",
        "updatedAt": "2024-06-30T13:25:59Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "pgollangi"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18137,
        "title": "Strange logs from vector\uff0cIs this normal?",
        "bodyText": "I deployed 4 instances of Vector using systemd, and when I checked /var/log/messages, one of the Vector instances outputted strange logs. Is this coming from Vector? Is it an Easter egg?\nAug  2 16:53:42 dev-consul vector: 2023-08-02T08:53:42.503461Z  INFO source{component_kind=\"source\" component_id=file component_type=file component_name=file}:file_server: file_source::checkpointer: Loaded checkpoint data.\nAug  2 16:53:42 dev-consul vector: {\"appname\":\"devankoshal\",\"facility\":\"auth\",\"hostname\":\"names.us\",\"message\":\"A bug was encountered but not in Vector, which doesn't have bugs\",\"msgid\":\"ID461\",\"procid\":2435,\"severity\":\"alert\",\"timestamp\":\"2023-08-02T08:53:42.503Z\",\"version\":2}\nAug  2 16:53:42 dev-consul vector: 2023-08-02T08:53:42.507906Z  INFO vector::internal_events::api: API server running. address=127.0.0.1:8686 playground=http://127.0.0.1:8686/playground\nAug  2 16:53:42 dev-consul vector: 2023-08-02T08:53:42.508802Z  INFO vector::topology::builder: Healthcheck passed.\nAug  2 16:53:43 dev-consul vector: {\"appname\":\"meln1ks\",\"facility\":\"mail\",\"hostname\":\"random.org\",\"message\":\"Great Scott! We're never gonna reach 88 mph with the flux capacitor in its current state!\",\"msgid\":\"ID464\",\"procid\":3539,\"severity\":\"notice\",\"timestamp\":\"2023-08-02T08:53:43.504Z\",\"version\":1}\nAug  2 16:53:44 dev-consul vector: {\"appname\":\"benefritz\",\"facility\":\"kern\",\"hostname\":\"make.net\",\"message\":\"#hugops to everyone who has to deal with this\",\"msgid\":\"ID688\",\"procid\":4260,\"severity\":\"crit\",\"timestamp\":\"2023-08-02T08:53:44.503Z\",\"version\":2}\nAug  2 16:53:48 dev-consul vector: {\"appname\":\"jesseddy\",\"facility\":\"lpr\",\"hostname\":\"for.de\",\"message\":\"A bug was encountered but not in Vector, which doesn't have bugs\",\"msgid\":\"ID986\",\"procid\":4095,\"severity\":\"debug\",\"timestamp\":\"2023-08-02T08:53:48.503Z\",\"version\":1}",
        "url": "https://github.com/vectordotdev/vector/discussions/18137",
        "createdAt": "2023-08-02T09:03:04Z",
        "updatedAt": "2024-06-26T15:25:55Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "SongJinZe1"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 20714,
        "title": "Whether URIs in http sink support template syntax",
        "bodyText": "Whether URIs in http sink support template syntax. For example, when I send the data to openobserve support \"http://localhost:5080/api/ {{organization}} {{stream}} / _json\"",
        "url": "https://github.com/vectordotdev/vector/discussions/20714",
        "createdAt": "2024-06-22T10:24:43Z",
        "updatedAt": "2024-06-24T19:25:57Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "mumu-lab"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20718,
        "title": "Use vector to transfer OpenTelemetry ProtoBuf Data from S3 to Kafka",
        "bodyText": "I everyone\nI use OpenTelemetry Collector Contrib with exporters to AWS S3 with otlp_proto, data formated as ExportLogsServiceRequest. Then I use vector AWS S3 to read from S3 with SQS event and copy proto event to Kafka.\nFirst. I create desc file from\nhttps://github.com/open-telemetry/opentelemetry-proto/blob/main/opentelemetry/proto/collector/logs/v1/logs_service.proto#L36\nprotoc --descriptor_set_out=logs_service.desc --include_imports opentelemetry/proto/collector/logs/v1/logs_service.proto\n\nand try to decode successfully\nprotoc --decode=opentelemetry.proto.collector.logs.v1.ExportLogsServiceRequest --descriptor_set_in=../logs_service.desc  < ../data/logs_101951345.binpb\n\nand this i my vector config\nschema:\n  log_namespace: true\n\nsources:\n  s3_logs:\n    type: aws_s3\n    compression: auto\n    decoding:\n      codec: protobuf\n      protobuf:\n        desc_file: ./logs_service.desc\n        message_type: opentelemetry.proto.collector.logs.v1.ExportLogsServiceRequest\n    strategy: sqs\n    sqs:\n      delete_message: true\n      queue_url: \"https://sqs.ap-southeast-1.amazonaws.com/123456789012/pvcb-infinity-dev-s3-events-log-sqs-queue\"\nsinks:\n  stdout:\n    type: console\n    inputs:\n      - s3_logs\n    encoding:\n      codec: protobuf\n      protobuf:\n        desc_file: ./logs_service.desc\n        message_type: opentelemetry.proto.collector.logs.v1.ExportLogsServiceRequest\n\n\nBut error are show as below\n2024-06-23T18:21:27.647922Z ERROR source{component_kind=\"source\" component_id=s3_logs component_type=aws_s3}: vector::internal_events::codecs: Failed deserializing frame. error=Error parsing protobuf: DecodeError { description: \"unexpected end group tag\", stack: [] } error_code=\"decoder_deserialize\" error_type=\"parser_failed\" stage=\"processing\" internal_log_rate_limit=true\n2024-06-23T18:21:27.648015Z ERROR source{component_kind=\"source\" component_id=s3_logs component_type=aws_s3}: vector::internal_events::codecs: Internal log [Failed deserializing frame.] is being suppressed to avoid flooding.\n2024-06-23T18:21:37.648059Z ERROR source{component_kind=\"source\" component_id=s3_logs component_type=aws_s3}: vector::internal_events::codecs: Internal log [Failed deserializing frame.] has been suppressed 923297 times.\n2024-06-23T18:21:37.648085Z ERROR source{component_kind=\"source\" component_id=s3_logs component_type=aws_s3}: vector::internal_events::codecs: Failed deserializing frame. error=Error parsing protobuf: DecodeError { description: \"unexpected end group tag\", stack: [] } error_code=\"decoder_deserialize\" error_type=\"parser_failed\" stage=\"processing\" internal_log_rate_limit=true\nDoes vector support to transfer OpenTelemetry Protobuf message to Kafka ? Is my configuration wrong ?\nThank",
        "url": "https://github.com/vectordotdev/vector/discussions/20718",
        "createdAt": "2024-06-23T18:31:26Z",
        "updatedAt": "2024-06-23T18:33:23Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "scila1996"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 2
    },
    {
        "number": 20675,
        "title": "How to disable /health endpoint logging?",
        "bodyText": "I want to disable /health endpoint logs:\n{\"appname\":\"meln1ks\",\"facility\":\"ntp\",\"hostname\":\"we.am\",\"message\":\"You're not gonna believe what just happened\",\"msgid\":\"ID93\",\"procid\":2650,\"severity\":\"err\",\"timestamp\":\"2024-06-14T11:47:45.354Z\",\"version\":1} {\"appname\":\"Scarface\",\"facility\":\"local3\",\"hostname\":\"up.cw\",\"message\":\"There's a breach in the warp core, captain\",\"msgid\":\"ID89\",\"procid\":2198,\"severity\":\"crit\",\"timestamp\":\"2024-06-14T11:47:46.354Z\",\"version\":1} {\"appname\":\"jesseddy\",\"facility\":\"uucp\",\"hostname\":\"some.ieee\",\"message\":\"We're gonna need a bigger boat\",\"msgid\":\"ID853\",\"procid\":1323,\"severity\":\"alert\",\"timestamp\":\"2024-06-14T11:47:47.354Z\",\"version\":2} {\"appname\":\"benefritz\",\"facility\":\"syslog\",\"hostname\":\"make.nowruz\",\"message\":\"You're not gonna believe what just happened\",\"msgid\":\"ID798\",\"procid\":9527,\"severity\":\"notice\",\"timestamp\":\"2024-06-14T11:47:48.355Z\",\"version\":2} {\"appname\":\"b0rnc0nfused\",\"facility\":\"authpriv\",\"hostname\":\"some.property\",\"message\":\"Maybe we just shouldn't use computers\",\"msgid\":\"ID520\",\"procid\":4639,\"severity\":\"info\",\"timestamp\":\"2024-06-14T11:47:49.354Z\",\"version\":1} {\"appname\":\"KarimMove\",\"facility\":\"clockd\",\"hostname\":\"up.jnj\",\"message\":\"You're not gonna believe what just happened\",\"msgid\":\"ID803\",\"procid\":6539,\"severity\":\"err\",\"timestamp\":\"2024-06-14T11:47:50.354Z\",\"version\":2} {\"appname\":\"jesseddy\",\"facility\":\"local2\",\"hostname\":\"for.gratis\",\"message\":\"You're not gonna believe what just happened\",\"msgid\":\"ID564\",\"procid\":4672,\"severity\":\"info\",\"timestamp\":\"2024-06-14T11:47:51.354Z\",\"version\":1} {\"appname\":\"BronzeGamer\",\"facility\":\"local1\",\"hostname\":\"we.stockholm\",\"message\":\"A bug was encountered but not in Vector, which doesn't have bugs\",\"msgid\":\"ID695\",\"procid\":2074,\"severity\":\"alert\",\"timestamp\":\"2024-06-14T11:47:52.354Z\",\"version\":2} {\"appname\":\"AmbientTech\",\"facility\":\"user\",\"hostname\":\"random.rehab\",\"message\":\"Take a breath, let it go, walk away\",\"msgid\":\"ID437\",\"procid\":3457,\"severity\":\"err\",\"timestamp\":\"2024-06-14T11:47:53.354Z\",\"version\":1} {\"appname\":\"jesseddy\",\"facility\":\"user\",\"hostname\":\"random.kerrylogistics\",\"message\":\"Maybe we just shouldn't use computers\",\"msgid\":\"ID183\",\"procid\":5791,\"severity\":\"notice\",\"timestamp\":\"2024-06-14T11:47:54.353Z\",\"version\":2} {\"appname\":\"BronzeGamer\",\"facility\":\"news\",\"hostname\":\"up.fashion\",\"message\":\"Maybe we just shouldn't use computers\",\"msgid\":\"ID653\",\"procid\":4913,\"severity\":\"notice\",\"timestamp\":\"2024-06-14T11:47:55.353Z\",\"version\":2} \nAny ideas?",
        "url": "https://github.com/vectordotdev/vector/discussions/20675",
        "createdAt": "2024-06-14T11:49:48Z",
        "updatedAt": "2024-06-19T11:45:55Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "stefanitsky"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20691,
        "title": "Anyone using Vector, InfluxDB2, and/or Grafana as their metrics/logs stack?",
        "bodyText": "Basically what the title says. Have you used Vector to push metrics and logs to InfluxDB2 (not 1) and then configured your monitoring dashboards and alerts either in InfluxDB's system, or in Grafana?\nI've been experimenting with it. Metrics seem very doable in InfluxDB's UI if I can figure out Flux and where all the data Vector ships goes. Logs I'm not so sure on. It's harder to actually get the explorer UI to display them, and I have not figured out how to properly search them.\nThat makes me wonder if Grafana would be the way to go. But Grafana's InfluxDB2/Flux support is in beta (I think beta...), and there is precisely 1 shared dashboard for non-metric/log data. (AFAICT)\nAnyway, if you done this, or something similar, I'd love to hear how it went. What issues you ran into, the pros and cons, etc. :)\nThanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/20691",
        "createdAt": "2024-06-18T17:34:22Z",
        "updatedAt": "2024-06-18T17:34:23Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "jerrac"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 20684,
        "title": "Why are Docker container labels causing \"can't merge a non object mapping\" errors?",
        "bodyText": "Hey all,\nI did do a bit of searching on this, and I think I know what is going on, but I wanted to confirm.\nWhen I have vector watching my docker container logs using the docker_logs source, my vector logs fill up with a lot of errors like this:\n2024-06-17T16:14:15.246467Z ERROR sink{component_kind=\"sink\" component_id=test_alertdrop_es_logs component_type=elasticsearch}:request{request_id=21}: vector::sinks::elasticsearch::service: Response contained errors. error_code=\"http_response_200\" response=Response { status: 200, version: HTTP/1.1, headers: {\"x-elastic-product\": \"Elasticsearch\", \"content-type\": \"application/json\", \"transfer-encoding\": \"chunked\"}, body: b\"{\\\"errors\\\":true,\\\"took\\\":38,\\\"items\\\":[{\\\"index\\\":{\\\"_index\\\":\\\"vector-logs-2024.06.17\\\",\\\"_id\\\":\\\"8m76JpABEaCUMIymSE1f\\\",\\\"status\\\":400,\\\"error\\\":{\\\"type\\\":\\\"illegal_argument_exception\\\",\\\"reason\\\":\\\"can't merge a non object mapping [label.com.docker.swarm.task] with an object mapping\\\"}}},{\\\"index\\\":{\\\"_index\\\":\\\"vector-logs-2024.06.17\\\",\\\"_id\\\":\\\"8276JpABEaCUMIymSE1f\\\",\\\"status\\\":400,\\\"error\\\":{\\\"type\\\":\\\"illegal_argument_exception\\\",\\\"reason\\\":\\\"can't merge a non object mapping [label.com.docker.swarm.task] with an object mapping\\\"}}},{\\\"index\\\":{\\\"_index\\\":\\\"vector-logs-2024.06.17\\\",\\\"_id\\\":\\\"9G76JpABEaCUMIymSE1f\\\",\\\"status\\\":400,\\\"error\\\":{\\\"type\\\":\\\"illegal_argument_exception\\\",\\\"reason\\\":\\\"can't merge a non object mapping [label.com.docker.swarm.task] with an object mapping\\\"}}},{\\\"index\\\":{\\\"_index\\\":\\\"vector-logs-2024.06.17\\\",\\\"_id\\\":\\\"9W76JpABEaCUMIymSE1f\\\",\\\"status\\\":400,\\\"error\\\":{\\\"type\\\":\\\"illegal_argument_exception\\\",\\\"reason\\\":\\\"can't merge a non object mapping [label.com.docker.swarm.task] with an object mapping\\\"}}},{\\\"index\\\":{\\\"_index\\\":\\\"vector-logs-2024.06.17\\\",\\\"_id\\\":\\\"9m76JpABEaCUMIymSE1f\\\",\\\"status\\\":400,\\\"error\\\":{\\\"type\\\":\\\"illegal_argument_exception\\\",\\\"reason\\\":\\\"can't merge a non object mapping [label.com.docker.swarm.task] with an object mapping\\\"}}},{\\\"index\\\":{\\\"_index\\\":\\\"vector-logs-2024.06.17\\\",\\\"_id\\\":\\\"9276JpABEaCUMIymSE1f\\\",\\\"status\\\":400,\\\"error\\\":{\\\"type\\\":\\\"illegal_argument_exception\\\",\\\"reason\\\":\\\"can't merge a non object mapping [label.com.docker.swarm.task] with an object mapping\\\"}}},{\\\"index\\\":{\\\"_index\\\":\\\"vector-logs-2024.06.17\\\",\\\"_id\\\":\\\"-G76JpABEaCUMIymSE1f\\\",\\\"status\\\":400,\\\"error\\\":{\\\"type\\\":\\\"illegal_argument_exception\\\",\\\"reason\\\":\\\"can't merge a non object mapping [label.com.docker.swarm.task] with an object mapping\\\"}}},{\\\"index\\\":{\\\"_index\\\":\\\"vector-logs-2024.06.17\\\",\\\"_id\\\":\\\"-W76JpABEaCUMIymSE1f\\\",\\\"status\\\":400,\\\"error\\\":{\\\"type\\\":\\\"illegal_argument_exception\\\",\\\"reason\\\":\\\"can't merge a non object mapping [label.com.docker.swarm.task] with an object mapping\\\"}}},{\\\"index\\\":{\\\"_index\\\":\\\"vector-logs-2024.06.17\\\",\\\"_id\\\":\\\"-m76JpABEaCUMIymSE1f\\\",\\\"status\\\":400,\\\"error\\\":{\\\"type\\\":\\\"illegal_argument_exception\\\",\\\"reason\\\":\\\"can't merge a non object mapping [label.com.docker.swarm.task] with an object mapping\\\"}}},{\\\"index\\\":{\\\"_index\\\":\\\"vector-logs-2024.06.17\\\",\\\"_id\\\":\\\"-276JpABEaCUMIymSE1f\\\",\\\"status\\\":400,\\\"error\\\":{\\\"type\\\":\\\"illegal_argument_exception\\\",\\\"reason\\\":\\\"can't merge a non object mapping [label.com.docker.swarm.task] with an object mapping\\\"}}},{\\\"index\\\":{\\\"_index\\\":\\\"vector-logs-2024.06.17\\\",\\\"_id\\\":\\\"_G76JpABEaCUMIymSE1f\\\",\\\"status\\\":400,\\\"error\\\":{\\\"type\\\":\\\"illegal_argument_exception\\\",\\\"reason\\\":\\\"can't merge a non object mapping [label.com.docker.swarm.task] with an object mapping\\\"}}},{\\\"index\\\":{\\\"_index\\\":\\\"vector-logs-2024.06.17\\\",\\\"_id\\\":\\\"_W76JpABEaCUMIymSE1f\\\",\\\"status\\\":400,\\\"error\\\":{\\\"type\\\":\\\"illegal_argument_exception\\\",\\\"reason\\\":\\\"can't merge a non object mapping [label.com.docker.swarm.task] with an object mapping\\\"}}},{\\\"index\\\":{\\\"_index\\\":\\\"vector-logs-2024.06.17\\\",\\\"_id\\\":\\\"_m76JpABEaCUMIymSE1f\\\",\\\"status\\\":400,\\\"error\\\":{\\\"type\\\":\\\"illegal_argument_exception\\\",\\\"reason\\\":\\\"can't merge a non object mapping [label.com.docker.swarm.task] with an object mapping\\\"}}},{\\\"index\\\":{\\\"_index\\\":\\\"vector-logs-2024.06.17\\\",\\\"_id\\\":\\\"_276JpABEaCUMIymSE1f\\\",\\\"_version\\\":1,\\\"result\\\":\\\"created\\\",\\\"_shards\\\":{\\\"total\\\":2,\\\"successful\\\":2,\\\"failed\\\":0},\\\"_seq_no\\\":1391,\\\"_primary_term\\\":1,\\\"status\\\":201}},{\\\"index\\\":{\\\"_index\\\":\\\"vector-logs-2024.06.17\\\",\\\"_id\\\":\\\"AG76JpABEaCUMIymSE5f\\\",\\\"status\\\":400,\\\"error\\\":{\\\"type\\\":\\\"illegal_argument_exception\\\",\\\"reason\\\":\\\"can't merge a non object mapping [label.com.docker.swarm.task] with an object mapping\\\"}}}]}\" }\n\nThere was another post a while back about a similar error with Kubernetes logs. The solution, I think, was to drop the labels or transform them into something without dots.\nSo, is Vector looking at the labels and thinking that each individual label with dots in it is actually an object?\nIs there a way to tell Vector not to do that? Docker itself adds labels with dots in them, and I've seen many other systems do the same.\nPart of why I'm asking is that when I run docker inspect on a container, the json output clearly shows the labels as keys, not objects. So I kinda think it's a little odd that Vector would try to turn them into objects. And if it isn't doing that, then I need to figure out what is actually causing those errors...\nThanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/20684",
        "createdAt": "2024-06-17T16:29:49Z",
        "updatedAt": "2024-06-17T17:42:28Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "jerrac"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20676,
        "title": "Vector source is slow",
        "bodyText": "Hello.\nI'm trying to use a Vector Source with my gRPC client which use PushEventsRequest from vector.proto as in the issue New grpc source\nMy config file:\nsources:  \n  my_grpc_server:\n    type: \"vector\"\n    version: \"2\"\n    address: \"0.0.0.0:8310\"\n    acknowledgements:\n      enable: false\n\n  vector_metrics:\n    type: internal_metrics\n    scrape_interval_secs: 70\n\nsinks:\n  stdout_vector_metrics:\n    inputs:\n      - \"vector_metrics\"\n    type: \"console\"\n    encoding:\n      codec: \"text\"\n\nBut I have a problem that the vector processes only 30~50 requests per second in one gRPC connection (client can send ~10k requests):\n2024-06-14T12:43:58.000Z vector_component_received_events_total{component_id=\"my_grpc_server\",component_kind=\"source\",component_type=\"vector\",host=\"host\"} = 0\n...\n2024-06-14T12:43:59.000Z vector_component_received_events_total{component_id=\"my_grpc_server\",component_kind=\"source\",component_type=\"vector\",host=\"host\"} = 45\n\nExample of request in json format:\n{\n  \"events\": [\n    {\n      \"metric\": {\n        \"name\": \"metric\",\n        \"timestamp\": {\n          \"seconds\": 12323213123,\n          \"nanos\": 0\n        },\n        \"tags_v1\": {\n          \"System\": \"system\"\n        },\n        \"tags_v2\": {},\n        \"kind\": 0,\n        \"counter\": {\n          \"value\": 1.4\n        },\n        \"namespace\": \"a\",\n        \"interval_ms\": 100\n      }\n    }]\n}\n\nCan I improve this with some vector configuration?",
        "url": "https://github.com/vectordotdev/vector/discussions/20676",
        "createdAt": "2024-06-14T13:08:20Z",
        "updatedAt": "2024-06-14T20:43:08Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "paulrozhkin"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20674,
        "title": "Include Only one namespace from kubernetes logs",
        "bodyText": "good day\nis it possible to only get logs for sources kubernetes_logs for only one namespace\ni dont need whole Kubernetes  cluster logs\nmaybe someone can help\nnow i use source like this :\n  kubernetes_logs:\n    type: kubernetes_logs\n    pod_annotation_fields:\n      pod_namespace: \"kubernetes.namespace_name\"\n      pod_labels:    \"kubernetes.labels\"\n      pod_uid:       \"kubernetes.pod_id\"\n      pod_node_name: \"kubernetes.host\"\n    max_line_bytes: 1048576",
        "url": "https://github.com/vectordotdev/vector/discussions/20674",
        "createdAt": "2024-06-14T09:33:15Z",
        "updatedAt": "2024-06-14T09:33:31Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "therus000"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 2
    },
    {
        "number": 20652,
        "title": "Vector Remap Language UDF support",
        "bodyText": "Can I define custom function with VRL. I want to reuse some code for the multiple fields.",
        "url": "https://github.com/vectordotdev/vector/discussions/20652",
        "createdAt": "2024-06-12T08:27:46Z",
        "updatedAt": "2024-06-12T14:12:46Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "artemklevtsov"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20655,
        "title": "high latency of log collect",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nWe use Vector to collect Kubernetes pod logs. For pods in specific namespaces, we aim to obtain \"real-time\" logs. Vector collects logs from the pod's standard output and sends them to VictoriaLogs. Currently, there is a latency of 30 to 120 seconds between the pod output and the ability to query in VictoriaLogs. How can we reduce this latency?\nConfiguration\nNo response\nVersion\ndocker.io/timberio/vector:0.31.0-distroless-libc\nDebug Output\nNo response\nExample Data\nNo response\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/20655",
        "createdAt": "2024-06-12T13:00:20Z",
        "updatedAt": "2024-06-12T14:32:12Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "code1704"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 20623,
        "title": "Adding multiple AWS Credentials in Vector Helm chart Deployment",
        "bodyText": "Hello everyone,\nWe're currently deploying Vector using the Helm chart and injecting AWS credentials via environment variables. However, we need to send logs to an AWS S3 bucket located in a different AWS account. We've tried creating an IAM Role pointing to the other account's S3 bucket, but it hasn't worked as expected as the pod couldn't assume the correct role. Additionally, we're unsure how to handle these additional credentials within the Helm chart.\nWe've also attempted to create a separate file containing the other credentials, but we're unsure of the correct approach for integrating it via the Helm chart.\nAny guidance or suggestions would be greatly appreciated!\nThanks,\nGerm\u00e1n",
        "url": "https://github.com/vectordotdev/vector/discussions/20623",
        "createdAt": "2024-06-07T12:18:46Z",
        "updatedAt": "2024-06-07T21:42:52Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "garceger"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20620,
        "title": "is there any way to change the value of timestamp field?",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nThe loki record value timestamp and  timestamp in message are different, is there any way to change the value of timestamp?\nConfiguration\ndata_dir = \"/vector-data-dir\"\n# I'm afraid to open it in case it triggers some bugs.\nschema.log_namespace = false\napi.enabled = true\napi.address = \"0.0.0.0:8686\"\n[sources.in]\ntype = \"kafka\"\nbootstrap_servers = \"xxxxx\"\ntopics = [\"a\"]\ngroup_id = \"xxx\"\ndecoding.codec = \"json\"\n[sources.in.librdkafka_options]\n\"allow.auto.create.topics\" = \"false\"\n\"auto.offset.reset\" = \"earliest\"\n\n[transforms.trans]\ntype = \"remap\"\ninputs = [\"in\"]\n# event value is a json, e.g., {\"message\": \"hello\", \"@collect_time\":  \"2024-06-06T07:09:23.015146042Z\"}\nsource = '''\n        .timestamp=to_unix_timestamp(parse_timestamp!(.@collect_time, format: \"%F T %T%.9f Z\"), unit: \"nanoseconds\")\n    '''\n\n[sinks.out]\ntype = \"loki\"\ninputs = [\"trans\"]\nendpoint = \"http://xxxxx\"\nencoding.codec = \"json\"\ncompression = \"snappy\"\nencoding.only_fields = [\"message\", \"timestamp\"]\nout_of_order_action = \"accept\"\nremove_label_fields = true\nremove_timestamp = false\n\n[sinks.out.labels]\njob = \"test\"\n\nVersion\n0.38.0\nDebug Output\nNo response\nExample Data\n{\n    \"status\": \"success\",\n    \"data\": {\n        \"resultType\": \"streams\",\n        \"result\": [\n            {\n                \"stream\": {\n                    \"job\": \"test\"\n                },\n                \"values\": [\n                    [\n                        \"1717658642800327233\",\n                        \"{\\\"message\\\":\\\"hello\\\",\\\"timestamp\\\":1717657763015146042}\"\n                    ]\n                ]\n            }\n        ],\n        \"stats\": {\n            \"summary\": {\n                \"bytesProcessedPerSecond\": 6652,\n                \"linesProcessedPerSecond\": 130,\n                \"totalBytesProcessed\": 51,\n                \"totalLinesProcessed\": 1,\n                \"execTime\": 0.007666,\n                \"queueTime\": 0.000344,\n                \"subqueries\": 0,\n                \"totalEntriesReturned\": 1,\n                \"splits\": 2,\n                \"shards\": 0,\n                \"totalPostFilterLines\": 1,\n                \"totalStructuredMetadataBytesProcessed\": 0\n            },\n            \"querier\": {\n                \"store\": {\n                    \"totalChunksRef\": 0,\n                    \"totalChunksDownloaded\": 0,\n                    \"chunksDownloadTime\": 0,\n                    \"chunk\": {\n                        \"headChunkBytes\": 0,\n                        \"headChunkLines\": 0,\n                        \"decompressedBytes\": 0,\n                        \"decompressedLines\": 0,\n                        \"compressedBytes\": 0,\n                        \"totalDuplicates\": 0,\n                        \"postFilterLines\": 0,\n                        \"headChunkStructuredMetadataBytes\": 0,\n                        \"decompressedStructuredMetadataBytes\": 0\n                    },\n                    \"chunkRefsFetchTime\": 2816982\n                }\n            },\n            \"ingester\": {\n                \"totalReached\": 6,\n                \"totalChunksMatched\": 1,\n                \"totalBatches\": 7,\n                \"totalLinesSent\": 1,\n                \"store\": {\n                    \"totalChunksRef\": 0,\n                    \"totalChunksDownloaded\": 0,\n                    \"chunksDownloadTime\": 0,\n                    \"chunk\": {\n                        \"headChunkBytes\": 51,\n                        \"headChunkLines\": 1,\n                        \"decompressedBytes\": 0,\n                        \"decompressedLines\": 0,\n                        \"compressedBytes\": 0,\n                        \"totalDuplicates\": 0,\n                        \"postFilterLines\": 1,\n                        \"headChunkStructuredMetadataBytes\": 0,\n                        \"decompressedStructuredMetadataBytes\": 0\n                    },\n                    \"chunkRefsFetchTime\": 0\n                }\n            },\n            \"cache\": {\n                \"chunk\": {\n                    \"entriesFound\": 0,\n                    \"entriesRequested\": 0,\n                    \"entriesStored\": 0,\n                    \"bytesReceived\": 0,\n                    \"bytesSent\": 0,\n                    \"requests\": 0,\n                    \"downloadTime\": 0\n                },\n                \"index\": {\n                    \"entriesFound\": 0,\n                    \"entriesRequested\": 0,\n                    \"entriesStored\": 0,\n                    \"bytesReceived\": 0,\n                    \"bytesSent\": 0,\n                    \"requests\": 0,\n                    \"downloadTime\": 0\n                },\n                \"result\": {\n                    \"entriesFound\": 0,\n                    \"entriesRequested\": 0,\n                    \"entriesStored\": 0,\n                    \"bytesReceived\": 0,\n                    \"bytesSent\": 0,\n                    \"requests\": 0,\n                    \"downloadTime\": 0\n                },\n                \"statsResult\": {\n                    \"entriesFound\": 0,\n                    \"entriesRequested\": 0,\n                    \"entriesStored\": 0,\n                    \"bytesReceived\": 0,\n                    \"bytesSent\": 0,\n                    \"requests\": 0,\n                    \"downloadTime\": 0\n                }\n            }\n        }\n    }\n}\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/20620",
        "createdAt": "2024-06-06T07:47:25Z",
        "updatedAt": "2024-06-07T08:50:57Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "1123183721"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20613,
        "title": "how convert a iso8601_ns string to timestamp?",
        "bodyText": ".a=\"2024-06-05T08:40:02.015146042Z\"\n.timestamp=to_unix_timestamp(parse_timestamp!(.a, format: \"%F T %T%.9f%z\"), unit: \"nanoseconds\")\n\nThe log:\n2024-06-06T02:41:02.502003Z ERROR transform{component_kind=\"transform\" component_id=trans component_type=remap}: vector::internal_events::remap: Mapping failed with event. error=\"function call error for \\\"to_unix_timestamp\\\" at (66:151): function call error for \\\"parse_timestamp\\\" at (84:129): Invalid timestamp \\\"2024-06-05T08:40:02.015146042Z\\\": input contains invalid characters\" error_type=\"conversion_failed\" stage=\"processing\" internal_log_rate_limit=true",
        "url": "https://github.com/vectordotdev/vector/discussions/20613",
        "createdAt": "2024-06-05T08:58:33Z",
        "updatedAt": "2024-06-06T02:58:03Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "1123183721"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20603,
        "title": "Need help configuring Vector to transform nested JSON",
        "bodyText": "Hi all,\nI'm running Vector(0.38.0) + Loki(3.0.0) on vanilla k8s to experiment with the tech. I have some demo apps deployed that are generating logs but I cannot seem to find a way to transform(parse) the JSON from the app logs and extract the all the json objects as labels so it would be easier to search in Grafana. I've tried all the ways that are mentioned in the documentation about json transformations but none of them seem to work with my case.\nI've attached my config.yaml and few raw json logs samples\nconfig.txt\nraw_log_http_request_sample.json\nraw_log_http_response_sample.json\nraw_log_http_response_sample1.json\nAny help or a nudge in the right direction is much appreciated\nThank you.",
        "url": "https://github.com/vectordotdev/vector/discussions/20603",
        "createdAt": "2024-06-03T16:00:17Z",
        "updatedAt": "2024-06-05T22:22:57Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "dimidevops"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 0
    },
    {
        "number": 20600,
        "title": "High Memory Usage when reading from Disk Buffer",
        "bodyText": "Is there a way to know / control the memory usage while reading from buffer?\nWe have observed that when vector for any reason stores the logs in disk for buffering and starts to read back from it, we see OOMKilled on pods even when the memory limit is configured to be 1024Mi as recommended.\nWe want to know how buffer uses memory and if there is a way using which we can limit that this much memory can be used while processing buffered events\nOur setup\nWe have kubernetes_logs source which reads the logs of pods and then push it to kafka via kafka sink.\nConfig file\nacknowledgements:\n  enabled: true\napi:\n  address: 0.0.0.0:8686\n  enabled: true\n  playground: false\ndata_dir: /vector-data-dir\nexpire_metrics_secs: 900\nsinks:\n  kafka:\n    batch:\n      max_bytes: 1000000\n      max_events: 10000\n      timeout_secs: 3\n    bootstrap_servers: kafka:9092\n    buffer:\n      max_size: 5000000000\n      type: disk\n      when_full: block\n    compression: zstd\n    encoding:\n      codec: json\n    inputs:\n    - dedot_keys\n    librdkafka_options:\n      client.id: vector\n      request.required.acks: \"1\"\n    message_timeout_ms: 0\n    topic: vector\n    type: kafka\n  prometheus_exporter:\n    address: 0.0.0.0:9090\n    buffer:\n      max_size: 5000000000\n      type: disk\n      when_full: block\n    flush_period_secs: 60\n    inputs:\n    - internal_metrics\n    type: prometheus_exporter\nsources:\n  internal_metrics:\n    type: internal_metrics\n  kubernetes_logs:\n    glob_minimum_cooldown_ms: 2000\n    ingestion_timestamp_field: ingest_timestamp\n    type: kubernetes_logs\ntransforms:\n  dedot_keys:\n    inputs:\n    - kubernetes_logs\n    source: |\n      . = map_keys(., recursive: true) -> |key| { replace(key, \".\", \"_\") }\n    type: remap\n\n\nIt can be seen here",
        "url": "https://github.com/vectordotdev/vector/discussions/20600",
        "createdAt": "2024-06-03T05:56:05Z",
        "updatedAt": "2024-06-04T18:07:06Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ShahroZafar"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20601,
        "title": "Support batch consumption of historical data in NATS JetStream",
        "bodyText": "Currently, it only consumes real-time data from NATS and does not perform ack confirmation. I am wondering if it can support batch consumption of historical data in NATS JetStream with ack confirmation.",
        "url": "https://github.com/vectordotdev/vector/discussions/20601",
        "createdAt": "2024-06-03T09:16:24Z",
        "updatedAt": "2024-06-03T09:16:24Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bluedreamcloud"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 20590,
        "title": "syslog source to loki sink - HTTP 400 Bad Request",
        "bodyText": "Hi,\nIm trying to ingest syslog messages from access points using the syslog source, filter for a specific message, then use loki as the source sink.\nTried various options but everytime loki just seems to respond with a HTTP 400 - Bad Request\nheres my config:\n[sources.aps]\ntype = \"syslog\"\naddress = \"0.0.0.0:5143\"\nmode = \"tcp\"\n\n\n[transforms.ap_connects]\ntype = \"filter\"\ninputs = [\"aps\"]\ncondition = '.appname == \"WIFI-4-CLIENT-CONNECTED\"'\n\n[sinks.loki]\ntype = \"loki\"\ninputs = [\"ap_connects\"]\n#remove_timestamp = true\nendpoint = \"http://loki:3100\"\n\n[sinks.loki.labels]\njob = \"vector\"\n\n[sinks.loki.encoding]\ncodec = \"json\"\n\n\n[sinks.my_console]\ntype = \"console\"\ninputs = [ \"ap_connects\" ]\ntarget = \"stdout\"\n\n[sinks.my_console.encoding]\ncodec = \"json\"\n\nheres one of the syslog messages sent to stdout by the console sink, which is correctly showing only the transformed messages:\n{\"appname\":\"WIFI-4-CLIENT-CONNECTED\",\"facility\":\"user\",\"host\":\"zzz-zzz-zzz-zz-z\",\"hostname\":\"blahblah\",\"message\":\"Client [XX-XX-XX-XX-XX-XX] connected to WLAN [MY SSID] on radio [2]\",\"severity\":\"warning\",\"source_ip\":\"192.168.0.2\",\"source_type\":\"syslog\",\"timestamp\":\"2024-05-31T16:17:21Z\"}\nvector log output:\n2024-05-31T15:18:32.122489Z DEBUG sink{component_kind=\"sink\" component_id=loki component_type=loki}:request{request_id=35}:http: vector::internal_events::http_client: HTTP response. status=400 Bad Request version=HTTP/1.1 headers={\"content-type\": \"text/plain; charset=utf-8\", \"x-content-type-options\": \"nosniff\", \"date\": \"Fri, 31 May 2024 15:18:32 GMT\", \"content-length\": \"201\"} body=[201 bytes]\n2024-05-31T15:18:32.122557Z ERROR sink{component_kind=\"sink\" component_id=loki component_type=loki}:request{request_id=35}: vector::sinks::util::retries: Internal log [Non-retriable error; dropping the request.] is being suppressed to avoid flooding.\n2024-05-31T15:18:32.122591Z ERROR sink{component_kind=\"sink\" component_id=loki component_type=loki}:request{request_id=35}: vector_common::internal_event::service: Internal log [Service call failed. No retries or retries exhausted.] is being suppressed to avoid flooding.\n2024-05-31T15:18:32.122616Z ERROR sink{component_kind=\"sink\" component_id=loki component_type=loki}:request{request_id=35}: vector_common::internal_event::component_events_dropped: Internal log [Events dropped] is being suppressed to avoid flooding.\n\nI cant work out what i am doing wrong. using vector 0.38.0 and loki 2.8.0\nany suggestions?\nmany thanks\nJamie",
        "url": "https://github.com/vectordotdev/vector/discussions/20590",
        "createdAt": "2024-05-31T15:22:29Z",
        "updatedAt": "2024-06-03T04:59:28Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "j4mbob"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20593,
        "title": "How to handle NULL Byte Padded Log format (Meta Trader 4 Server / Trading Platform)?",
        "bodyText": "Someone already explained this in their best effort here: https://discuss.elastic.co/t/how-to-read-logs-with-permanent-null-characters-at-the-end-of-file/324408\nTo sum it up briefly, Meta Trader 4 Server, a trading platform by MetaQuotes, writes latest log files in quite a peculiar way:\nIt pads the file with some Megabytes of NUL Bytes (\\x000) in one long line, and upon writing some log lines / messages, it replaces some NUL bytes, and appends the remaining NUL bytes to the end/last line of the log.\nThis effectively confuses the file source in vector, and I could not find a way around it. Vector follows some, but then stops seeing new log lines / messages altogether at some point.\nWhat I've tried:\n\nattempted to filter the line beginning with NUL byte before remap:\n\n[sources.mt4_main_logs]\ntype = \"file\"\ninclude = [\"D:\\\\MT4Server\\\\logs\\\\*.log\"]]\n\n[transforms.mt4_main_logs_filter]\ntype = \"filter\"\ninputs = [ \"mt4_main_logs\" ]\ncondition = \"\"\"\n    starts_with(string!(.message), \"\\\\0\") == false\n\"\"\"\n\nresult: warnings are still printed to stdout frequently by vector about discarding line over max allowed bytes (102400?), and vector stops seeing new messages / log lines after some minutes (effectively stopping sending to sink the latest log lines). So the problem is still at the file source.\n\nTried setting ignore_checkpoints = true to the file source\n\nresult: same as above. warnings about discarding of line above max allowed bytes, and vector stops seeing new log messages after some minutes, and has to be restarted to re-read the current log and start sending to the sink what it has missed , from where it left off.\nAny ideas or input would be much appreciated!\nCheers",
        "url": "https://github.com/vectordotdev/vector/discussions/20593",
        "createdAt": "2024-06-01T13:33:00Z",
        "updatedAt": "2024-06-02T23:57:37Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tamer-hassan"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 20584,
        "title": "`http_server` source to ingest clickstream events \u2013 reasonable use case?",
        "bodyText": "Hello all,\nI see that moving log data around is the primary use case for vector, but I've been wondering if it could be used to ingest clickstream events into Kafka and then from Kafka into ClickHouse. The pipeline would roughly be:\nHTTP clients --- (http_server source > kafka sink) ---> Kafka --- (kafka source > clickhouse sink) ---> Clickhouse\n\nThe http_source endpoint wouldn't be publicly exposed directly. For a production setup we'll have a load balancer in front of the vector processes and some sort of WAF in front of everything for security.\nWe're trying to avoid building and maintaining our own HTTP->Kafka solution, so that's why we're interested in using vector. Main questions I have:\n\nHas anyone experimented or deployed something like this with vector and could share their experience?\nIs this a \"bad\" use of the http_server source? I can't seem to find a reason why it would be.\n\nFurthermore, I ran some quick stress tests with hey using the following settings:\n# 100K requests, 250 connections, HTTP2 enabled, posting a single log entry as JSON.\n$ hey -n 100000 -c 250 -h2 -m POST -D fake_log.txt -T application/json http://127.0.0.1:3010/\n\n# vector.yaml -- Vector 0.38.0 running as a Docker container\nsources:\n  http:\n    type: http_server\n    address: 0.0.0.0:3010\n    encoding: json\n    headers:\n      - User-Agent\n    path_key: vector_http_path\n\nsinks:\n  out:\n    inputs:\n      - \"http\"\n    type: \"console\"\n    encoding:\n      codec: json\n\nResults:\nResponse time histogram:\n  0.000 [1]\t|\n  0.011 [11650]\t|\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\n  0.022 [80229]\t|\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\n  0.033 [6896]\t|\u25a0\u25a0\u25a0\n  0.044 [681]\t|\n  0.055 [132]\t|\n  0.066 [70]\t|\n  0.077 [28]\t|\n  0.088 [83]\t|\n  0.099 [61]\t|\n  0.110 [129]\t|\n\nPerformance was great but a few hundred requests returned non-200 with the error below. If I increase concurrency, I get more errors. Could it be hitting an open connection limit? I couldn't find a way to inspect or tweak this in the source settings.\n  [1]\tPost \"http://127.0.0.1:3010/\": read tcp 127.0.0.1:61797->127.0.0.1:3010: read: connection reset by peer`.\n\nedit 1: I did test using only the blackhole sink and got the same results.\nedit 2: ran hey from within a container in the same docker network and didn't see any issues with 500 connections. Probably something funky from running all those connections from host -> container.",
        "url": "https://github.com/vectordotdev/vector/discussions/20584",
        "createdAt": "2024-05-30T22:42:45Z",
        "updatedAt": "2024-05-31T00:00:21Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "mksm"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 2
    },
    {
        "number": 20581,
        "title": "Is there systemd unit state source?",
        "bodyText": "I just discovered vector and am contemplating whether to move monitoring to it instead of fluent-bit. Currently I use promtail & node_exporter, but I'm looking for something new and was playing around with fluent-bit when I came across vector. One important monitor for me is the node_systemd_unit_state metric provided by node_exporter (and which fluent-bit also supports as a drop-in replacement).\nFor vector, I noticed host metrics exist, but they don't seem to support systemd metrics and I can't find any metrics source for systemd, only journald for logs. Am I missing something? Does this source exist? If not, am I even asking the right question? How would you find out if a systemd unit has failed with Vector?\nThanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/20581",
        "createdAt": "2024-05-30T02:42:13Z",
        "updatedAt": "2024-05-30T22:09:28Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Javex"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20570,
        "title": "What's the best way to transmit data over a high-latency networks",
        "bodyText": "I want to transmit data between two places that are far apart, eg: EU -> ASIA, and I want to send msg from vector to vector,\ninput -> [EU]vector-source -> (encode)[EU]vector-sink ---> (decode)[ASIA]vector-source -> [ASIA]vector-sink \n\nIf vector->vector too slow, queue will block, and it will affect input rate, how to mitigate this situation?\nThere are a lot of source and sink pairs for me to use, eg: vector->vector, http->http-server...  Which is recommended to use?\nThere are my configs:\nsinks:\n  vector:\n    type: vector\n    inputs:\n      - input\n    address: xxx\n    compression: true\n    acknowledgements: false\n    request:\n      retry_attempts: 1\n\nsources:\n  vector:\n    type: vector\n    address: 0.0.0.0:6000",
        "url": "https://github.com/vectordotdev/vector/discussions/20570",
        "createdAt": "2024-05-28T09:55:25Z",
        "updatedAt": "2024-05-29T03:27:29Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "evanzhang87"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20566,
        "title": "Push logs of different namespaces to different kafka clusters",
        "bodyText": "We have a use case where we want to send logs of different namespaces to different kafka clusters. Is it possible to dynamically set the inputs or broker endpoints to achieve this?",
        "url": "https://github.com/vectordotdev/vector/discussions/20566",
        "createdAt": "2024-05-27T08:56:38Z",
        "updatedAt": "2024-05-28T18:26:09Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ShahroZafar"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20531,
        "title": "Throttle",
        "bodyText": "We use vector to move logs between Kafka and our logging system. We need to enable rate limiting. But we our applications have very spikey log production. Instead of fixed rate limiting per topic we'd like to be able to set limits and then share unused capacity from other topics. Do you have any insight on how to have more dynamic control of the Throttle transform?",
        "url": "https://github.com/vectordotdev/vector/discussions/20531",
        "createdAt": "2024-05-20T17:33:54Z",
        "updatedAt": "2024-05-28T13:49:45Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "dturn"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20533,
        "title": "Memory usage exceeds expectations!",
        "bodyText": "I am using vector to consume messages from Kafka and write them to ClickHouse.\nThe buffer size is 40k, and the batch size is 80k.\n800k kafka message size is approximately 700MB, and the converted ClickHouse request size is around 1GB.\nBut the machine monitoring shows peak memory usage of 30GB, which is about double the expected usage.\nconfig\n[sources.in]\nsession_timeout_ms = 120000\ntype = \"kafka\"\nbootstrap_servers = \"#\"\ngroup_id = \"flume-clickhouse-event\"\ntopics = [\"#\"]\nlibrdkafka_options = { \"auto.offset.reset\" = \"latest\", \"fetch.min.bytes\" = \"10240\" }\n\n[transforms.balance]\ntype = \"remap\"\ninputs = [\"in\"]\ndrop_on_error = true\nsource = '.route_id=random_int(0,3)'\n\n[transforms.route]\ntype = \"route\"\ninputs = [\"balance\"]\nreroute_unmatched = false\nroute.first = '.route_id==0'\nroute.second = '.route_id==1'\nroute.third = '.route_id==2'\n\n\n[sinks.out-clickhouse-1]\ntype = \"clickhouse\"\ninputs = [\"route.first\"]\ncompression = \"none\"\nendpoint = \"http://localhost:8125\"\ndatabase = \"mytable\"\nbatch.max_events = 800000\nbatch.max_bytes = 16106127360\nbatch.timeout_secs = 120\nrequest.timeout_secs = 120\nrequest.adaptive_concurrency.initial_concurrency=4\n# request.adaptive_concurrency.max_concurrency_limit=4\nbuffer.type = \"memory\"\nbuffer.max_events = 400000\nbuffer.when_full = \"block\"\n\n[sinks.out-clickhouse-2]\ntype = \"clickhouse\"\ninputs = [\"route.second\"]\ncompression = \"none\"\nendpoint = \"http://localhost:8125\"\ndatabase = \"mytable\"\n\nbatch.max_events = 800000\nbatch.max_bytes = 16106127360\nbatch.timeout_secs = 120\n\nrequest.timeout_secs = 120\nrequest.adaptive_concurrency.initial_concurrency=4\n# request.adaptive_concurrency.max_concurrency_limit=4\n\nbuffer.type = \"memory\"\nbuffer.max_events = 400000\nbuffer.when_full = \"block\"\n\n[sinks.out-clickhouse-3]\ntype = \"clickhouse\"\ninputs = [\"route.third\"]\ncompression = \"none\"\nendpoint = \"http://localhost:8125\"\ndatabase = \"mytable\"\nbatch.max_events = 800000\nbatch.max_bytes = 16106127360\nbatch.timeout_secs = 120\nrequest.timeout_secs = 120\nrequest.adaptive_concurrency.initial_concurrency=4\nbuffer.type = \"memory\"\nbuffer.max_events = 400000\nbuffer.when_full = \"block\"\n\n\nmemory monitor",
        "url": "https://github.com/vectordotdev/vector/discussions/20533",
        "createdAt": "2024-05-16T08:42:23Z",
        "updatedAt": "2024-05-23T16:24:45Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "biuboombiuboom"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 20535,
        "title": "SSL handshake failed on sinks.kafka",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\n`/etc/vector/bin/vector validate  /etc/vector/vector.toml\n\u221a Loaded [\"/etc/vector/vector.toml\"]\n\u221a Component configuration\n2024-05-17T07:23:50.609338Z ERROR rdkafka::client: librdkafka: Global error: SSL (Local: SSL error): ssl://kafka-1-external.kafka:30001/bootstrap: SSL handshake failed: error:0A000086:SSL routines::certificate verify failed: broker certificate could not be verified, verify that ssl.ca.location is correctly configured or root CA certificates are installed (install ca-certificates package) (after 21ms in state SSL_HANDSHAKE)\n2024-05-17T07:23:51.594908Z ERROR rdkafka::client: librdkafka: Global error: SSL (Local: SSL error): ssl://kafka-0-external.kafka:30000/bootstrap: SSL handshake failed: error:0A000086:SSL routines::certificate verify failed: broker certificate could not be verified, verify that ssl.ca.location is correctly configured or root CA certificates are installed (install ca-certificates package) (after 9ms in state SSL_HANDSHAKE)\n2024-05-17T07:23:52.590942Z ERROR rdkafka::client: librdkafka: Global error: SSL (Local: SSL error): ssl://kafka-2-external.kafka:30002/bootstrap: SSL handshake failed: error:0A000086:SSL routines::certificate verify failed: broker certificate could not be verified, verify that ssl.ca.location is correctly configured or root CA certificates are installed (install ca-certificates package) (after 7ms in state SSL_HANDSHAKE)\n2024-05-17T07:23:52.591050Z ERROR rdkafka::client: librdkafka: Global error: AllBrokersDown (Local: All broker connections are down): 3/3 brokers are down\n2024-05-17T07:23:53.586483Z ERROR vector::topology::builder: msg=\"Healthcheck failed.\" error=Meta data fetch error: BrokerTransportFailure (Local: Broker transport failure) component_kind=\"sink\" component_type=\"kafka\" component_id=kafka\nx Health check for \"kafka\" failed: Meta data fetch error: BrokerTransportFailure (Local: Broker transport failure)\n2024-05-17T07:23:53.592329Z ERROR rdkafka::client: librdkafka: Global error: SSL (Local: SSL error): ssl://kafka-1-external.kafka:30001/bootstrap: SSL handshake failed: error:0A000086:SSL routines::certificate verify failed: broker certificate could not be verified, verify that ssl.ca.location is correctly configured or root CA certificates are installed (install ca-certificates package) (after 9ms in state SSL_HANDSHAKE, 1 identical error(s) suppressed)`\nConfiguration\n[sinks.kafka]\ntype = \"kafka\"\ninputs = [ \"filter_comments\"]\n#Configure the list of kafka servers along with port numbers\nbootstrap_servers = \"kafka-0-external.kafka:30000,kafka-1-external.kafka:30001,kafka-2-external.kafka:30002\"\ntopic = \"data_8_1\"\ncompression = \"none\"\nkey_field = \"host\"\nencoding.codec = \"json\"\nbatch = {max_events=50000}\nbuffer = {max_events=50000, type=\"memory\", when_full=\"block\"}\nlibrdkafka_options = {\"linger.ms\"=\"1000\",\"queue.buffering.max.kbytes\"=\"1048576\",\"security.protocol\" = \"ssl\",\"ssl.ca.location\" = \"/home/ubuntu/kafka24/client/root.crt\",\"ssl.certificate.location\" = \"/home/ubuntu/kafka24/client/client.crt\",\"ssl.key.location\" = \"/home/ubuntu/kafka24/client/client.key\",\"enable.ssl.certificate.verification\" = \"true\",\"ssl.endpoint.identification.algorithm\" = \"none\"}\n\nVersion\n0.34.2\nDebug Output\nNo response\nExample Data\nNo response\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/20535",
        "createdAt": "2024-05-17T07:41:34Z",
        "updatedAt": "2024-05-20T18:03:49Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "lavis11"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 20532,
        "title": "Vector - Prometheus flattening the field.",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nHello,\nI hope all is well. I am trying to get prometheus data in kafka. This is working well, but I am trying to take the output and flatten the fields. I am using the prometheus_scrape plugin for this, and the data is ending up in kafka.\nWhat I am trying to do is flatten the \"tags\" object, so that it in a more accessable format for me. Instead of everything being under the tag object, i want to flatten it.\nThank you again!\nConfiguration\n\u276f cat vector2.yml\nenabled: true\nenv:\n  - name: VECTOR_REQUIRE_HEALTHY\n    value: \"true\"\nfullnameOverride: \"vector\"\nservice:\n  type: \"LoadBalancer\"\n  ports:\n    - name: syslognonstandard\n      port: 5141\n      targetPort: 5141\n      protocol: \"UDP\"\n    - name: syslog\n      port: 5140\n      targetPort: 5140\n      protocol: \"TCP\"\n    - name: fluentd\n      port: 9000\n      targetPort: 9000\n      protocol: \"TCP\"\ncustomConfig:\n  data_dir: /vector\n  api:\n    enabled: true\n    address: 127.0.0.1:8686\n    playground: false\n  sources:\n    rsyslog-tcp:\n      address: 0.0.0.0:5140\n      mode: tcp\n      type: syslog\n    rsyslog-udp:\n      address: 0.0.0.0:5141\n      mode: udp\n      type: syslog\n    k8s-clustersrc:\n      type: fluent\n      address: 0.0.0.0:9000\n    k8s-metricssrc:\n      type: prometheus_scrape\n      scrape_interval_secs: 60\n      endpoints:\n        - http://prometheus-operated.monitoring:9090/federate?match[]={job!=%22%22}\n  transforms:\n    syslog_timestamp_add:\n      type: remap\n      inputs: [rsyslog-tcp, rsyslog-udp]\n      timezone: UTC\n      source: |\n        .ingestion_timestamp = now()\n    prom:\n      type: remap\n      inputs: [k8s-metricssrc]\n      source: |\n        .test = (.tags.job)\n  sinks:\n    client_rsyslog:\n      type: kafka\n      inputs: [syslog_timestamp_add]\n      bootstrap_servers: kafka:9092\n      topic: vector-lines-0\n      compression: none\n      healthcheck: true\n      encoding:\n        codec: json\n    internal_k8s_logs:\n      type: kafka\n      inputs: [k8s-clustersrc]\n      bootstrap_servers: kafka:9092\n      topic: internal-k8s-logs\n      compression: none\n      healthcheck: true\n      encoding:\n        codec: json\n    internal_k8s_metrics:\n      type: kafka\n      inputs: [prom]\n      bootstrap_servers: kafka:9092\n      topic: internal-k8s-metrics\n      compression: none\n      healthcheck: true\n      encoding:\n        codec: json\nlivenessProbe:\n  exec:\n    command:\n    - /usr/bin/vector\n    - validate\nreadinessProbe:\n  exec:\n    command:\n    - /usr/bin/vector\n    - validate\n\n\n\n### Version\n\nvector-0.33.0 helm chart. 0.38.0-distroless-libc. \n\n### Debug Output\n\n```text\n{\"name\":\"apiserver_request_duration_seconds_bucket\",\"tags\":{\"component\":\"apiserver\",\"endpoint\":\"https\",\"group\":\"acme.cert-manager.io\",\"instance\":\"192.168.122.88:6443\",\"job\":\"apiserver\",\"le\":\"45\",\"namespace\":\"default\",\"prometheus\":\"monitoring/mon-kube-prometheus-stack-prometheus\",\"prometheus_replica\":\"prometheus-mon-kube-prometheus-stack-prometheus-0\",\"resource\":\"orders\",\"scope\":\"cluster\",\"service\":\"kubernetes\",\"verb\":\"LIST\",\"version\":\"v1\"},\"timestamp\":\"2024-05-15T18:40:11.465Z\",\"kind\":\"absolute\",\"gauge\":{\"value\":31.0}}\n\n\n\n### Example Data\n\n_No response_\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_",
        "url": "https://github.com/vectordotdev/vector/discussions/20532",
        "createdAt": "2024-05-15T18:45:39Z",
        "updatedAt": "2024-05-20T17:45:33Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ryansliceup"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20487,
        "title": "--",
        "bodyText": "",
        "url": "https://github.com/vectordotdev/vector/discussions/20487",
        "createdAt": "2024-05-13T09:18:58Z",
        "updatedAt": "2024-05-13T19:13:36Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "hajdukda"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 20476,
        "title": "an error occur when vector cant query geo info from geo_city_lite.mmdb",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\n\nversion\n\nvector 0.38\n\n\n\nhi, I push data from kafka by vector, and I used transformer to conver ip info, and write it to file.\nhere is all my vector config:\nmost times,  it works well, but few times, it will reflect on error:\n\n2024-05-10T05:56:56.496641Z ERROR transform{component_kind=\"transform\" component_id=geo_ip_transfer component_type=remap}: vector::internal_events::remap: Mapping failed with event. error=\"function call error for \"get_enrichment_table_record\" at (36:123): IP not found\" error_type=\"conversion_failed\" stage=\"processing\" internal_log_rate_limit=true\nand then i will get an wrong result.\n\nCan I detect when the transformer is doing the conversion, and if an error is encountered, give .geo_ip a default value? just like:\n  .geo_ip,err = get_enrichment_table_record!(\"geoip_table\",{\"ip\": .dissect.remote_ip})\n  if (err !=null){\n    .geo_ip={\"city_name\":\"unknon\"}\n  }\nIn this way, I can get the correct results.\nThanks!\nConfiguration\nsources:\n  dev_kafka:\n    type: kafka\n    bootstrap_servers: 10.11.1.170:9092\n    topics:\n      - web-logs\n    group_id: web-logs\ntransforms:\n  geo_ip_transfer:\n    type: remap\n    inputs:\n      - dev_kafka\n    source: |\n      . = parse_json!(.message)\n      .geo_ip = get_enrichment_table_record!(\"geoip_table\",{\"ip\": .dissect.remote_ip})\n      del(.@metadata)\nsinks:\n  to_file:\n    inputs: [\"geo_ip_transfer\"]   \n    type: file\n    encoding:\n      codec: json\n    path: ./business-%Y-%m-%d.log\nenrichment_tables:\n  geoip_table:\n    path: \"./GeoLite2-City.cdn.mmdb\"\n    type: geoip\n    locale: zh-CN\n\n\n\n### Version\n\n0.38\n\n### Debug Output\n\n```text\n2024-05-10T06:59:02.137959Z DEBUG sink{component_kind=\"sink\" component_id=to_file component_type=file}: vector::topology::builder: Sink starting.\n2024-05-10T06:59:02.139301Z DEBUG sink{component_kind=\"sink\" component_id=to_file component_type=file}: vector::utilization: utilization=0.6459871647614359\n2024-05-10T06:59:02.152279Z DEBUG vector::sources::kafka: Consuming partition web-logs:0.\n2024-05-10T06:59:07.139394Z DEBUG sink{component_kind=\"sink\" component_id=to_file component_type=file}: vector::utilization: utilization=0.06459877335472873\n2024-05-10T06:59:10.187682Z DEBUG transform{component_kind=\"transform\" component_id=geo_ip_transfer component_type=remap}: vector::utilization: utilization=0.00042708953430647956\n2024-05-10T06:59:10.188359Z ERROR transform{component_kind=\"transform\" component_id=geo_ip_transfer component_type=remap}: vector::internal_events::remap: Mapping failed with event. error=\"function call error for \\\"get_enrichment_table_record\\\" at (36:106): IP not found\" error_type=\"conversion_failed\" stage=\"processing\" internal_log_rate_limit=true\n2024-05-10T06:59:12.139095Z DEBUG sink{component_kind=\"sink\" component_id=to_file component_type=file}: vector::utilization: utilization=0.006754124998050746\n2024-05-10T06:59:17.139247Z DEBUG sink{component_kind=\"sink\" component_id=to_file component_type=file}: vector::utilization: utilization=0.0006754655981937604\n2024-05-10T06:59:22.139401Z DEBUG sink{component_kind=\"sink\" component_id=to_file component_type=file}: vector::utilization: utilization=0.00006760145813916612\n\nExample Data\n\nraw data from kafka\n\n{\n  \"@timestamp\": \"2024-05-10T06:11:45.833Z\",\n  \"message\": {\n    \"dissect\": {\n      \"UA\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\",\n      \"access_date\": \"2024-04-09\",\n      \"access_time\": \"18:00:27\",\n      \"http_code\": \"302\",\n      \"http_host\": \"abc.com\",\n      \"referer\": \"-\",\n      \"remote_ip\": \"103.190.179.11\",\n      \"request_bytes\": \"113\",\n      \"request_method\": \"GET\",\n      \"resp_bytes\": \"267\",\n      \"resp_cost\": \"0\",\n      \"uri\": \"/index.do\"\n    }\n  }\n}\n\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/20476",
        "createdAt": "2024-05-10T07:03:15Z",
        "updatedAt": "2024-05-11T01:35:19Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "saltfishh"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 20432,
        "title": "Detect when Vector is lagging",
        "bodyText": "Is there a way using which we can somehow detect by how much the vector is falling behind reading events from a particular file?\nCurrently we want to use vector to use file source to push logs of a very critical application (Almost 0 log loss) to kafka and we want to get alerted if vector is lagging behind and is not keeping up with the load.\nThe deployment is in kubernetes (We are currently not using kubernetes_logs source as log enrichment doesn't matter and we want to keep the log entry size as small as possible)",
        "url": "https://github.com/vectordotdev/vector/discussions/20432",
        "createdAt": "2024-05-04T19:57:02Z",
        "updatedAt": "2024-05-07T06:28:49Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ShahroZafar"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20436,
        "title": "Sorting JSON value [transform]",
        "bodyText": "Hi. After looking at the documentation, I did not find an answer to my question. How can I sort the values of the groups alphabetically?\nExample:\n{\n\"json\": {\n\"groups\": [\n\"group2\",\n\"group3\",\n\"group1\"\n]\n}\n}",
        "url": "https://github.com/vectordotdev/vector/discussions/20436",
        "createdAt": "2024-05-06T11:29:14Z",
        "updatedAt": "2024-05-07T05:09:15Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "RFskynet"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20423,
        "title": "Need assistance - how can i detect the ABSENCE of a given string in a log file, and only report if its been over 10 minutes missing?",
        "bodyText": "I am struggling to wrap my head around some of this - I can understand easily how to detect a string in a log file, i.e. report when an error shows up in our log file to our endpoint.\nFor some reason I am struggling to do the OPPOSITE - which is detect when a constantly reoccuring message is NOT there. I want to only alert when its been 10+ minutes obviously for any false positives.\nAny help is appreciated...",
        "url": "https://github.com/vectordotdev/vector/discussions/20423",
        "createdAt": "2024-05-02T20:33:08Z",
        "updatedAt": "2024-05-02T20:33:09Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ngacordex"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 20405,
        "title": "Is it possible to get the source host with the http_server source?",
        "bodyText": "The socket source returns the source host and port of the incoming event but there doesn't seem to be a way to make the http_server source do the same.",
        "url": "https://github.com/vectordotdev/vector/discussions/20405",
        "createdAt": "2024-04-30T21:25:36Z",
        "updatedAt": "2024-04-30T21:25:37Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "brucejxz"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 20342,
        "title": "How to parese mysql query slow log?",
        "bodyText": "1.slow log\nTime: 2024-04-19T08:14:30.240692Z\nUser@Host: root[root] @ localhost []  Id: 695248\nQuery_time: 5.000470  Lock_time: 0.000000 Rows_sent: 1  Rows_examined: 0\nuse uni_mydb;\nSET timestamp=1713514470;\nselect sleep(5);\n2.I want to convert the above text into json as follows\u3002Can mysql slow query log be parsed? Please give an example. Thanks.\n{\u201ctime\u201c\uff1a\u201c2024-04-19 16:40:30\u201d,\n\u201cuser\u201d:\u201croot\u201d,\n\"host\":\"localhost\",\n\"query_time\",\"5.000\",\n\"lock_time\":\"0.000\",\n\"rows_sent\":1\uff0c\n\"rows_examined\u201d:0\uff0c\n\"sql_text\u201d:\"select sleep(5);\"}",
        "url": "https://github.com/vectordotdev/vector/discussions/20342",
        "createdAt": "2024-04-19T08:26:10Z",
        "updatedAt": "2024-04-25T05:44:34Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "spihiker"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15762,
        "title": "Kubernetes audit logs",
        "bodyText": "is there way to use a logs from audit-logs as a source in vector\nand what the best type to use to sink to kafka",
        "url": "https://github.com/vectordotdev/vector/discussions/15762",
        "createdAt": "2022-12-28T11:56:11Z",
        "updatedAt": "2024-04-24T13:55:15Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "therus000"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20363,
        "title": "log_to_metric field not used?",
        "bodyText": "Am I crazy or is the only purpose of the field attribute to check if the attribute exists and is not null?  Feels like I gotta be missing something.  In the docs it says that if the value of the field is null then it will not emit a metric, but I've found that it throws an error when the value is null.  Can someone tell me how this is supposed to work?",
        "url": "https://github.com/vectordotdev/vector/discussions/20363",
        "createdAt": "2024-04-23T17:51:12Z",
        "updatedAt": "2024-04-23T19:23:31Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "syntastical"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20362,
        "title": "How to read Prometheus text files?",
        "bodyText": "Hi guys! I've some systems that generate their metrics through Prometheus text files. The idea is to expose those metrics using the Prometheus Exporter sink. I've seen that Vector doesn't have a Prometheus File source, and the raw File source outputs just logs, so I would have to parse line by line and transform it to metrics with the respective metric type (it seems inviable). Is there any alternative? Am I missing something?",
        "url": "https://github.com/vectordotdev/vector/discussions/20362",
        "createdAt": "2024-04-23T17:37:22Z",
        "updatedAt": "2024-04-23T17:58:55Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "brunomrpx"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20360,
        "title": "vector collects zipkin logs through kafka",
        "bodyText": "Original log\n[{\u201ctraceId\u201d:\u201c5982fe77008310cc80f1da5e10147517\u201d,\u201cname\u201d:\u201cget\u201d,\u201cid\u201d:\u201cbd7a977555f6b982\u201d,\u201ctimestamp\u201d:1458702548467000,\u201cduration\u201d:386000,\u201clocalEndpoint\u201d:{\u201cserviceName\u201d:\u201czipkin-query\u201d,\u201cipv4\u201d:\u201c192.168.1.2\u201d,\u201cport\u201d:9411},\u201cannotations\u201d:[{\u201ctimestamp\u201d:1458702548467000,\u201cvalue\u201d:\u201csr\u201d},{\u201ctimestamp\u201d:1458702548853000,\u201cvalue\u201d:\u201css\u201d}]},{\u201ctraceId\u201d:\u201c5982fe77008310cc80f1da5e10147517\u201d,\u201cname\u201d:\u201cget-traces\u201d,\u201cid\u201d:\u201cebf33e1a81dc6f71\u201d,\u201cparentId\u201d:\u201cbd7a977555f6b982\u201d,\u201ctimestamp\u201d:1458702548478000,\u201cduration\u201d:354374,\u201clocalEndpoint\u201d:{\u201cserviceName\u201d:\u201czipkin-query\u201d,\u201cipv4\u201d:\u201c192.168.1.2\u201d,\u201cport\u201d:9411},\u201ctags\u201d:{\u201clc\u201d:\u201cJDBCSpanStore\u201d,\u201crequest\u201d:\u201cQueryRequest{serviceName=zipkin-query,spanName=null,annotations=[],binaryAnnotations={},minDuration=null,maxDuration=null,endTs=1458702548478,lookback=86400000,limit=1}\u201d}},{\u201ctraceId\u201d:\u201c5982fe77008310cc80f1da5e10147517\u201d,\u201cname\u201d:\u201cquery\u201d,\u201cid\u201d:\u201cbe2d01e33cc78d97\u201d,\u201cparentId\u201d:\u201cebf33e1a81dc6f71\u201d,\u201ctimestamp\u201d:1458702548786000,\u201cduration\u201d:13000,\u201clocalEndpoint\u201d:{\u201cserviceName\u201d:\u201czipkin-query\u201d,\u201cipv4\u201d:\u201c192.168.1.2\u201d,\u201cport\u201d:9411},\u201cremoteEndpoint\u201d:{\u201cserviceName\u201d:\u201cspanstore-jdbc\u201d,\u201cipv4\u201d:\u201c127.0.0.1\u201d,\u201cport\u201d:3306},\u201cannotations\u201d:[{\u201ctimestamp\u201d:1458702548786000,\u201cvalue\u201d:\u201ccs\u201d},{\u201ctimestamp\u201d:1458702548799000,\u201cvalue\u201d:\u201ccr\u201d}],\u201ctags\u201d:{\u201cjdbc.query\u201d:\u201cselectdistinctzipkin_spans.trace_idfromzipkin_spansjoinzipkin_annotationson(zipkin_spans.trace_id=zipkin_annotations.trace_idandzipkin_spans.id=zipkin_annotations.span_id)where(zipkin_annotations.endpoint_service_name=?andzipkin_spans.start_tsbetween?and?)orderbyzipkin_spans.start_tsdesclimit?\u201d,\u201csa\u201d:\u201ctrue\u201d}},{\u201ctraceId\u201d:\u201c5982fe77008310cc80f1da5e10147517\u201d,\u201cname\u201d:\u201cquery\u201d,\u201cid\u201d:\u201c13038c5fee5a2f2e\u201d,\u201cparentId\u201d:\u201cebf33e1a81dc6f71\u201d,\u201ctimestamp\u201d:1458702548817000,\u201cduration\u201d:1000,\u201clocalEndpoint\u201d:{\u201cserviceName\u201d:\u201czipkin-query\u201d,\u201cipv4\u201d:\u201c192.168.1.2\u201d,\u201cport\u201d:9411},\u201cremoteEndpoint\u201d:{\u201cserviceName\u201d:\u201cspanstore-jdbc\u201d,\u201cipv4\u201d:\u201c127.0.0.1\u201d,\u201cport\u201d:3306},\u201cannotations\u201d:[{\u201ctimestamp\u201d:1458702548817000,\u201cvalue\u201d:\u201ccs\u201d},{\u201ctimestamp\u201d:1458702548818000,\u201cvalue\u201d:\u201ccr\u201d}],\u201ctags\u201d:{\u201cjdbc.query\u201d:\u201cselectzipkin_spans.trace_id,zipkin_spans.id,zipkin_spans.name,zipkin_spans.parent_id,zipkin_spans.debug,zipkin_spans.start_ts,zipkin_spans.durationfromzipkin_spanswherezipkin_spans.trace_idin(?)\u201d,\u201csa\u201d:\u201ctrue\u201d}},{\u201ctraceId\u201d:\u201c5982fe77008310cc80f1da5e10147517\u201d,\u201cname\u201d:\u201cquery\u201d,\u201cid\u201d:\u201c37ee55f3d3a94336\u201d,\u201cparentId\u201d:\u201cebf33e1a81dc6f71\u201d,\u201ctimestamp\u201d:1458702548827000,\u201cduration\u201d:2000,\u201clocalEndpoint\u201d:{\u201cserviceName\u201d:\u201czipkin-query\u201d,\u201cipv4\u201d:\u201c192.168.1.2\u201d,\u201cport\u201d:9411},\u201cremoteEndpoint\u201d:{\u201cserviceName\u201d:\u201cspanstore-jdbc\u201d,\u201cipv4\u201d:\u201c127.0.0.1\u201d,\u201cport\u201d:3306},\u201cannotations\u201d:[{\u201ctimestamp\u201d:1458702548827000,\u201cvalue\u201d:\u201ccs\u201d},{\u201ctimestamp\u201d:1458702548829000,\u201cvalue\u201d:\u201ccr\u201d}],\u201ctags\u201d:{\u201cjdbc.query\u201d:\u201cselectzipkin_annotations.trace_id,zipkin_annotations.span_id,zipkin_annotations.a_key,zipkin_annotations.a_value,zipkin_annotations.a_type,zipkin_annotations.a_timestamp,zipkin_annotations.endpoint_ipv4,zipkin_annotations.endpoint_port,zipkin_annotations.endpoint_service_namefromzipkin_annotationswherezipkin_annotations.trace_idin(?)orderbyzipkin_annotations.a_timestampasc,zipkin_annotations.a_keyasc\u201d,\u201csa\u201d:\u201ctrue\u201d}}]\n\nExpect log expansion to increase timestamp\ntransforms:\nsource: |\n. = parse_json!(.message)\nThis rule has no time\n.time = now\ncannot be added. Is there any way to add it?\n. = parse_json!(.message)\nprint\n{ \"duration\": 1431, \"id\": \"352bff9a74ca9ad2\", \"kind\": \"SERVER\", \"localEndpoint\": { \"ipv4\": \"192.168.99.1\", \"port\": 3306, \"serviceName\": \"backend\" }, \"name\": \"get /api\", \"parentId\": \"6b221d5bc9e6496c\", \"remoteEndpoint\": { \"ipv4\": \"172.19.0.2\", \"port\": 58648 }, \"tags\": { \"http.method\": \"GET\", \"http.path\": \"/api\" }, \"timestamp\": 1556604172355737, \"traceId\": \"5af7183fb1d4cf5f\" }\n. = parse_json!(.message)\n.timestamp = now()\nWhat is printed is\n{\"timestamp\":\"2024-04-22T15:38:53.553515654Z\"}\nCan't satisfy\n.message = parse_json!(.message)\n. = unnest!(.message)",
        "url": "https://github.com/vectordotdev/vector/discussions/20360",
        "createdAt": "2024-04-23T14:20:37Z",
        "updatedAt": "2024-04-25T05:53:51Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "jiaozi07"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 20355,
        "title": "some questions about the use of parse_grok",
        "bodyText": "A note for the community\nI use vector consume from kafka, then parse log format by parse_grok, but it can not work\nsuch as my example log,  regex pattern infomation:\n\n\nexample log\n[2024-04-22 14:43:43.385] [ INFO] [hs-msc-proxy,TID:ace6121994f8449ebc7965a8f2815ff4.121.17137682228980013,,] [1] [ XNIO-1 task-12] [c.r.m.p.channel.service.HsYCardService.batchYiDCardQuery(90)] : [batchYiDCardQuery]\u83b7\u53d6\u7b7e\u540dSign\uff1akGA0xSObIDNvlMElsopm1KJ43JSfiDCGFVj3ctz5yIkIHWT6GdIX3cx1q/fBS2jSdCe02eEOScTjRM5CSIVCAT3gdE5J8Nm76ANQPaQefUd9A+XB3UBoN3AboZCaI4V/X0ljjuVB14K1JfCh6QV7gWmwPe+/mvSlCSbF/WCsMeM=\n\n\nregex pattern\n^[%{TIMESTAMP_ISO8601:logtime}]\\s*[\\s*%{LOGLEVEL:loglevel}]\\s*[%{USERNAME:appname},(?[a-zA-Z0-9.:/]+)?,(?[a-zA-Z0-9.:]+)?,(?\\w+)?]\\s*[%{INT:pid}]\\s*[\\s*(?[a-zA-Z0-9_. -]+)]\\s*%{NOTSPACE:logger}\\s*:(?.*)\n\n\nabove regex pattern refer logstash",
        "url": "https://github.com/vectordotdev/vector/discussions/20355",
        "createdAt": "2024-04-22T14:30:07Z",
        "updatedAt": "2024-04-22T16:51:19Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "yanming-zhang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 20353,
        "title": "vector collects zipkin logs",
        "bodyText": "vector collects zipkin logs\nThe log is in list format, and the list needs to be parsed.\n[\n  {\n    \"id\": \"352bff9a74ca9ad2\",\n    \"traceId\": \"5af7183fb1d4cf5f\",\n    \"parentId\": \"6b221d5bc9e6496c\",\n    \"name\": \"get /api\",\n    \"timestamp\": 1556604172355737,\n    \"duration\": 1431,\n    \"kind\": \"SERVER\",\n    \"localEndpoint\": {\n      \"serviceName\": \"backend\",\n      \"ipv4\": \"192.168.99.1\",\n      \"port\": 3306\n    },\n    \"remoteEndpoint\": {\n      \"ipv4\": \"172.19.0.2\",\n      \"port\": 58648\n    },\n    \"tags\": {\n      \"http.method\": \"GET\",\n      \"http.path\": \"/api\"\n    }\n  },\n  {\n    \"id\": \"352bff9a74ca9ad2\",\n    \"traceId\": \"5af7183fb1d4cf5f\",\n    \"parentId\": \"6b221d5bc9e6496c\",\n    \"name\": \"get /api\",\n    \"timestamp\": 1556604172355737,\n    \"duration\": 1431,\n    \"kind\": \"SERVER\",\n    \"localEndpoint\": {\n      \"serviceName\": \"backend\",\n      \"ipv4\": \"192.168.99.1\",\n      \"port\": 3306\n    },\n    \"remoteEndpoint\": {\n      \"ipv4\": \"172.19.0.2\",\n      \"port\": 58648\n    },\n    \"tags\": {\n      \"http.method\": \"GET\",\n      \"http.path\": \"/api\"\n    }\n  }\n]\n\nExpect all dictionaries in the list to be flattened.\n\ntransforms:\nsource: |\n. = parse_json!(.message)\nThis rule has no time\n.time = now\ncannot be added. Is there any way to add it?\n\n. = parse_json!(.message)\nprint\n{ \"duration\": 1431, \"id\": \"352bff9a74ca9ad2\", \"kind\": \"SERVER\", \"localEndpoint\": { \"ipv4\": \"192.168.99.1\", \"port\": 3306, \"serviceName\": \"backend\" }, \"name\": \"get /api\", \"parentId\": \"6b221d5bc9e6496c\", \"remoteEndpoint\": { \"ipv4\": \"172.19.0.2\", \"port\": 58648 }, \"tags\": { \"http.method\": \"GET\", \"http.path\": \"/api\" }, \"timestamp\": 1556604172355737, \"traceId\": \"5af7183fb1d4cf5f\" }\n . = parse_json!(.message) .timestamp = now()\nWhat is printed is\n{\"timestamp\":\"2024-04-22T15:38:53.553515654Z\"}",
        "url": "https://github.com/vectordotdev/vector/discussions/20353",
        "createdAt": "2024-04-22T13:35:11Z",
        "updatedAt": "2024-04-23T14:23:21Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "jiaozi07"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 20330,
        "title": "How to unit test multiple events (type: reduce)",
        "bodyText": "I want to unit test my reduce config by sending multiple events and then check that I get one combined event out. How can I do that?\nI tried with multiple input events but that seems to pass only one event at the time:\ntransforms:\n  kubernetes_transform:\n    type: remap\n    inputs:\n      - kubernetes_logs\n      ...\n  kubernetes_logs_full:\n    type: reduce\n    inputs:\n      - kubernetes_transform\n    ends_when: \".LogEntryType == \\\"F\\\"\"\n    ...\n  - name: \"kube log parsing\"\n    inputs:\n      - insert_at: \"kubernetes_transform\"\n        type: \"log\"\n        log_fields:\n          .file: /var/log/containers/influxdb-influxdb-864c4f54cb-7vhct_influxdb_influxdb-influxdb-ed9971dddc34ccafd6f572659aa40cb895d4fa013fe1aa8fc9c805febdbd8a0e.log\n          .host: vector-9fxtp\n          .message: 2022-02-11T17:39:27.684933558Z stderr P line1\n          .source_type: file\n      - insert_at: \"kubernetes_transform\"\n        type: \"log\"\n        log_fields:\n          .file: /var/log/containers/influxdb-influxdb-864c4f54cb-7vhct_influxdb_influxdb-influxdb-ed9971dddc34ccafd6f572659aa40cb895d4fa013fe1aa8fc9c805febdbd8a0e.log\n          .host: vector-9fxtp\n          .message: 2022-02-11T17:39:27.684933558Z stderr F line2\n          .source_type: file\n    outputs:\n      - extract_from: \"kubernetes_logs_full\"\n        conditions:\n          - type: \"vrl\"\n            source: |\n              assert!(is_timestamp(.timestamp))\n              assert_eq!(.LogMessage, \"line1line2\")\n\nI also tried with vrl source to pass an array:\ntransforms:\n  kubernetes_transform:\n    type: remap\n    inputs:\n      - kubernetes_logs\n      ...\n  kubernetes_logs_full:\n    type: reduce\n    inputs:\n      - kubernetes_transform\n    ends_when: \".LogEntryType == \\\"F\\\"\"\n    ...\n  - name: \"kube log parsing\"\n    inputs:\n      - insert_at: \"kubernetes_transform\"\n        type: \"vrl\"\n        source: |\n          . = [\n            {\n              \"file\": \"/var/log/containers/influxdb-influxdb-864c4f54cb-7vhct_influxdb_influxdb-influxdb-ed9971dddc34ccafd6f572659aa40cb895d4fa013fe1aa8fc9c805febdbd8a0e.log\",\n              \"host\": \"vector-9fxtp\",\n              \"message\": \"2022-02-11T17:39:27.684933558Z stderr P line1\",\n              \"source_type\": \"file\"\n            },\n            {\n              \"file\": \"/var/log/containers/influxdb-influxdb-864c4f54cb-7vhct_influxdb_influxdb-influxdb-ed9971dddc34ccafd6f572659aa40cb895d4fa013fe1aa8fc9c805febdbd8a0e.log\",\n              \"host\": \"vector-9fxtp\",\n              \"message\": \"2022-02-11T17:39:27.684933558Z stderr F line2\",\n              \"source_type\": \"file\"\n            }\n          ]\n    outputs:\n      - extract_from: \"kubernetes_logs_full\"\n        conditions:\n          - type: \"vrl\"\n            source: |\n              assert!(is_timestamp(.timestamp))\n              assert_eq!(.LogMessage, \"line1line2\")",
        "url": "https://github.com/vectordotdev/vector/discussions/20330",
        "createdAt": "2024-04-18T07:12:48Z",
        "updatedAt": "2024-04-18T23:22:22Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "yvespp"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20327,
        "title": "How to construct Vector's new Datakey deb signing key in Ansible",
        "bodyText": "I have an Ansible playbook where I was installing Vector through the original timber URLs:\n- name: Packages | Vector | Ensure GPG Key\n  become: true\n  ansible.builtin.get_url:\n    url: \"https://repositories.timber.io/public/vector/gpg.3543DB2D0A2BC4B8.key\"\n    dest: \"/usr/share/keyrings/timber-vector-archive-keyring.asc\"\n    mode: '0644'\n    owner: root\n  register: vector_gpg_key\n\n- name: Packages | Vector | Add Repository\n  become: true\n  ansible.builtin.apt_repository:\n    repo: \"deb [signed-by={{ vector_gpg_key.dest }}] https://repositories.timber.io/public/vector/deb/ubuntu {{ ansible_distribution_release }} main\"\n    state: present\n    filename: timber-vector\n    mode: '0644'\nbut I have recently been getting an error that the timber URL no longer had a jammy Release file. I finally took a look and saw the migration notice that took effect back in February and that vector's new installation instructions are a curl | bash script and are now signed under Datadog's main keys. I was digging into https://s3.amazonaws.com/dd-agent/scripts/install_script_vector0.sh to update these Ansible tasks but I'm confused how the /usr/share/keyrings/datadog-archive-keyring.gpg file is getting built inside that script.\nI expected something like this to work:\n- name: Packages | Vector | Ensure Repo Signing Key\n  become: true\n  ansible.builtin.get_url:\n    url: \"https://keys.datadoghq.com/DATADOG_APT_KEY_CURRENT.public\"\n    dest: \"/usr/share/keyrings/datadog-archive-keyring.gpg\"\n    mode: '0644'\n    owner: root\n  register: vector_gpg_key\n\n- name: Packages | Vector | Add Repository\n  become: true\n  ansible.builtin.apt_repository:\n    repo: \"deb [signed-by={{ vector_gpg_key.dest }}] https://apt.vector.dev/ stable vector-{{ vector_major_version }}\"\n    state: present\n    filename: vector\n    mode: '0644'\nbut that's a PGP public key and running the install script inside a container shows the resulting /usr/share/keyrings/datadog-archive-keyring.gpg is binary. I ended up copying that binary file over and commenting out the get_url step of the Ansible script to get vector back up and running, but that's a temporary patch. What do I need to do instead to idempotently generate the correct /usr/share/keyrings/datadog-archive-keyring.gpg binary file? Running the installer script inside the Ansible playbook is not an option.",
        "url": "https://github.com/vectordotdev/vector/discussions/20327",
        "createdAt": "2024-04-18T03:05:46Z",
        "updatedAt": "2024-04-18T03:26:55Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "artis3n"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 20308,
        "title": "How to collect k8s container log file(not stdout and stderr )",
        "bodyText": "How to collect k8s pod/container log files (not stdout and stderr )?\nLike app write logs to container path /app/logs/test.log\nI try to use kubernetes_logs, but it seems to only collect logs from /var/log/pods/\nk8s container runtime: containerd/docker\nMaybe config like:\nsources:\n  my_source_id:\n    type: kubernetes_logs\n    # like file source\n    include:\n      - /app/logs/*.log\nFound real path like\n/var/lib/docker/overlay2/9e048600f8da25798790333d8c0d128ba9a9f5c3110c530239fde2737ec0a1b8/diff/app/logs/xxx.log",
        "url": "https://github.com/vectordotdev/vector/discussions/20308",
        "createdAt": "2024-04-16T03:21:07Z",
        "updatedAt": "2024-04-17T02:26:09Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "fcfangcc"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 2
    },
    {
        "number": 20306,
        "title": "Log_to_metric convert mcs to seconds in field",
        "bodyText": "Is my field syntax correct. Or is there a better way to convert mcs to second\nwfm_scheduleViewerAction_latency-prod:\ntype: log_to_metric inputs:\n\nfilter_wfm_scheduleVtewerAction_prod_message\nmetrics:\ntype: histogram\nfield: http.request.request_time_mcs/1000000\nname: nice_apache_access_agent_adherence increment_by_value: true namespace: vector",
        "url": "https://github.com/vectordotdev/vector/discussions/20306",
        "createdAt": "2024-04-15T18:37:23Z",
        "updatedAt": "2024-04-16T17:30:02Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Manniac07"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20293,
        "title": "Parse nested fields for kinesis sink partition key?",
        "bodyText": "I'm currently configuring Vector sinks to send data to an AWS Kinesis Stream. My input data consists of JSON objects with nested fields, and I need to use one of these nested fields (specifically _session_id) as the partition key for the Kinesis Stream. Here is a sample of the data:\n{\n  \"timestamp\": \"2024-04-12T14:48:13.846548459Z\",\n  \"other fields\": \"xyz\"\n  \"data\": [\n    {\n      \"event_type\": \"some_event\",\n      \"attributes\": {\n        \"_session_id\": \"3bcf74c1-20240412-132033990\",\n        \"_session_start_timestamp\": 1712928033990,\n      }\n    }\n  ],\n  ...\n}\n\nPreviously, I had the partition_key_field set to \"timestamp\", and the configuration was working fine and sank my data to kinesis correctly. However, I now need to use a nested field, specifically \"data[0].attributes._session_id\", as the partition key.\nIn my scenario, the data field comes in as a list, but only ever contains 1 list item in the list which is why I can safely use data[0] to attempt to parse that record from the list. When I made this adjustment, the parsing failed with an error indicating that the partition key does not exist.\n2024-04-12T15:32:21.306393Z ERROR sink{component_kind=\"sink\" component_id=kinesis_sink component_type=aws_kinesis_streams}: vector::internal_events::aws_kinesis: Partition key does not exist. partition_key_field=data[0].attributes._session_id error_type=\"parser_failed\" stage=\"processing\" internal_log_rate_limit=true\n\nHere's a snippet of my current configuration:\n[sinks.kinesis_sink]\ntype = \"aws_kinesis_streams\"\ninputs = [\"json_parser\"]\npartition_key_field = \"data[0].attributes._session_id\"\ncompression = \"none\"\nregion = \"%%AWS_REGION%%\"\nstream_name = \"%%AWS_KINESIS_STREAM_NAME%%\"\n\nacknowledgements.enabled = true\nrequest_retry_partial = true\nrequest.retry_attempts = 4\nrequest.retry_max_duration_secs = 3\n\n[sinks.kinesis_sink.encoding]\ncodec = \"json\"\nand:\n[sources.nginx_http_post]\ntype = \"http_server\"\naddress = \"0.0.0.0:8685\"\nstrict_path = false\nheaders = [ \"X_Uri\", \"X_User_Agent\", \"X_Forwarded_For\", \"X_Date\", \"X_Request_ID\", \"X_Method\"]\nquery_parameters = [ \"platform\", \"appId\", \"compression\", \"fakeIp\", \"upload_timestamp\" ]\nmethod = \"POST\"\nencoding = \"binary\"\n\n[transforms.json_parser]\ninputs = [\"nginx_http_post\"]\ntype   = \"remap\"\nsource = '''\n.date = del(.X_Date)\n.uri = del(.X_Uri)\n.ua = del(.X_User_Agent)\nif get_env_var!(\"DEV_MODE\") == \"Yes\" && !is_null(.fakeIp) {\n  .ip = del(.fakeIp)\n} else {\n  .ip = del(.X_Forwarded_For)\n}\nif !is_null(.upload_timestamp) {\n  .client_timestamp = del(.upload_timestamp)\n}\n.rid = del(.X_Request_ID)\n.method = del(.X_Method)\n.data = del(.message)\n.ingest_time = to_unix_timestamp(now()) * 1000\n.server_ingest_time = to_unix_timestamp(now()) * 1000\n'''\n\nIs there a way to configure Vector sinks to properly parse nested fields for the Kinesis Stream partition key? Alternatively, are there any adjustments needed in my Vector configuration or transformation logic to ensure the correct parsing of nested fields?\nOr the kinesis sink config is not capable of parsing the _session_id within that part of the config, is it possible to change the json_parser transformation to somehow get _session_id out of .message and add it to the top level alongside ingest_time and server_ingest_time ?\nAny insights or suggestions would be greatly appreciated. Thank you!",
        "url": "https://github.com/vectordotdev/vector/discussions/20293",
        "createdAt": "2024-04-12T16:08:12Z",
        "updatedAt": "2024-04-12T16:08:13Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "AdamUrbanfox"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 2
    },
    {
        "number": 20262,
        "title": "if field match regex",
        "bodyText": "I'd like to do the following:\nregex_public_ip = r'^10\\.10\\.10\\.(?:[0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])$'\nif .source_ip != regex_public_ip {\ndo something\n}\nSo the problem is, this ifnot don't catch the regex but only like a hard coded private IP address for testing purposes. I need something like if not match, but I dont know how to wirte. Any ideas?\nThe application is OpenObserve and gives me the following identifiers to choose from:\nexpected one of: \"\\n\", \"!=\", \"&&\", \"*\", \"+\", \"-\", \"/\", \"<\", \"<=\", \"==\", \">\", \">=\", \"??\", \"{\", \"|\", \"|\", \"||\"",
        "url": "https://github.com/vectordotdev/vector/discussions/20262",
        "createdAt": "2024-04-08T21:45:59Z",
        "updatedAt": "2024-04-08T22:14:11Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "filetto"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20239,
        "title": "splunk_hec not showing its listening on port",
        "bodyText": "api:\n      enabled: true\n      address: 127.0.0.1:8687\n      playground: true\n\n    data_dir: ./data_dir\n\n    # LOG DATAPLANE port 6000\n    # METRICS DATAPLANE port 6001\n    sources:\n      # datadog_agent:\n      #   address: 0.0.0.0:8282\n      #   type: datadog_agent\n      # fluent:\n      #   address: 0.0.0.0:24224\n      #   type: fluent\n      # internal_metrics:\n      #   type: internal_metrics\n      # logstash:\n      #   address: 0.0.0.0:5044\n      #   type: logstash\n      splunk_hec:\n        address: 0.0.0.0:8088\n        type: splunk_hec\n      # statsd:\n      #   address: 0.0.0.0:8125\n      #   mode: tcp\n      #   type: statsd\n      # syslog:\n      #   address: 0.0.0.0:9000\n      #   mode: tcp\n      #   type: syslog\n      # vector:\n      #   address: 0.0.0.0:6000\n      #   type: vector\n    sinks:\n      splunk:\n        type: console\n        inputs:\n          - splunk_hec\n        encoding:\n          codec: json",
        "url": "https://github.com/vectordotdev/vector/discussions/20239",
        "createdAt": "2024-04-05T00:37:32Z",
        "updatedAt": "2024-04-05T01:13:17Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "rajsinghtech"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 20133,
        "title": "How to improve the vector performance of writing to Kafka Sink?",
        "bodyText": "My pipeline configuration consists of a source node and sink node,  from source kafka to sink kafka. the configuration is as shown in the figure. Due to the large traffic from the source kafka, it was found that there was a lag between the source and the sink kafka. When I run vector on a host with 16 CPUs and 32G memory, the configured in this pipeline writes to the sink kafka at a speed of 1.1Gib/s, and the CPU usage is only 25% at this time. When the number of copies of the same pipeline configuration is increased to 4, the writing the sink kafka speed can reach 2.9Gib/s, and the CPU usage reaches 90% at this time. I want to know how to in one configure kafka sink to improve write speed and concurrency\uff1f\nBTW: I have tried increasing the number of sink partitions but it has no effect. and setting environmental VECTOR_EXPERIMENTAL_REQUEST_BUILDER_CONCURRENCY to increase the number  has  also no effect",
        "url": "https://github.com/vectordotdev/vector/discussions/20133",
        "createdAt": "2024-03-19T07:47:50Z",
        "updatedAt": "2024-04-02T15:29:06Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ppmars"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 15172,
        "title": "Global variables",
        "bodyText": "I would like to know, does vector support global variables? I can create variable, for example in every remap transform, but it would be  great to create variable once and use it everywhere (at least within configuration file).",
        "url": "https://github.com/vectordotdev/vector/discussions/15172",
        "createdAt": "2022-11-10T14:24:18Z",
        "updatedAt": "2024-03-28T08:54:13Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Radcriminal"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 4
    },
    {
        "number": 20174,
        "title": "Parsing log line which could be 1 of N types (Apache Combined or Err or Custom, etc)",
        "bodyText": "I've had a look through the discussions here and the docs, and I'm not sure what the best approach here is. Basically, I'm running a bunch of services (that I don't own - self-hosting binge), and the logs are frequently intermingled, and could be any of, say,\n\nApache Combined\nApache Error\nPHP Composer logs\nA log format I don't know\n\nAs far as I can tell, I need to do something like this:\n[sources.source_firefly_iii_core]\ntype = \"docker_logs\"\ninclude_containers = [\"firefly_iii_core\"]\n\n[transforms.transform_firefly_iii_core]\ntype = \"route\"\ninputs = [ \"source_firefly_iii_core\" ]\n\n[transforms.transform_firefly_iii_core.route]\napache = '''\n    _, err = parse_apache_log(.message, \"combined\")\n    err == null\n'''\napache_error = '''\n    _, err = parse_apache_log(.message, \"error\")\n    err == null\n'''\napplication = '''\n    _, err = parse_groks(\n        .message,\n        patterns: [\n            \"\\\\[%{TIMESTAMP_ISO8601:timestamp}\\\\] %{FIREFLY_LOG_LEVEL}: %{GREEDYDATA:message}\"\n        ],\n        aliases: {\n            \"FIREFLY_LOG_LEVEL\": \"(?:%{DATA:host}\\\\.%{LOGLEVEL:level})\"\n        }\n    )\n    err == null\n'''\n\n[transforms.transform_firefly_iii_core_apache_combined_remap]\ntype = \"remap\"\ninputs = [ \"transform_firefly_iii_core.apache\" ]\nsource = '''\n    . |= parse_apache_log!(.message, \"combined\")\n'''\n[transforms.transform_firefly_iii_core_apache_err_remap]\ntype = \"remap\"\ninputs = [ \"transform_firefly_iii_core.apache_error\" ]\nsource = '''\n    . |= parse_apache_log!(.message, \"error\")\n'''\n[transforms.transform_firefly_iii_core_application_remap]\ntype = \"remap\"\ninputs = [ \"transform_firefly_iii_core.application\" ]\nsource = '''\n    . |= parse_groks!(\n        .message,\n        patterns: [\n            \"\\\\[%{TIMESTAMP_ISO8601:timestamp}\\\\] %{FIREFLY_LOG_LEVEL}: %{GREEDYDATA:message}\"\n        ],\n        aliases: {\n            \"FIREFLY_LOG_LEVEL\": \"(?:%{DATA:host}\\\\.%{LOGLEVEL:level})\"\n        }\n    )\n'''\n\n[sinks.console_test_out]\ntype = \"console\"\ninputs = [ \"transform_firefly_iii_core_*_remap\" ]\nencoding.codec = \"json\"\nThis isn't really ideal, since I have to parse the log twice in the best-case scenario, or four times in the worst-case, plus the necessary parsing code has to be duplicated. Is there a better way of handling this, given that I can't reasonably go and make a PR for every single service I've got deployed to get them to use JSON?\nI suppose I could also write some horribly nested VRL, along the lines of\n  logs, err = parse_apache_log(.message, \"combined\")\n  if err == null {\n    . |= logs\n  } else {\n    logs, err = parse_apache_log(.message, \"error\")\n    if err == null {\n      . |= logs\n    } else {\n      logs, err = parse_groks(\n          .message,\n          patterns: [\n              \"\\\\[%{TIMESTAMP_ISO8601:timestamp}\\\\] %{FIREFLY_LOG_LEVEL}: %{GREEDYDATA:message}\"\n          ],\n          aliases: {\n              \"FIREFLY_LOG_LEVEL\": \"(?:%{DATA:host}\\\\.%{LOGLEVEL:level})\"\n          }\n      )\n      if err == null {\n        . |= logs\n      }\n    }\n  }\n\nBut don't think that's any better, particularly since I'm not sure I can route it to a separate destination like that (e.g., I want Apache logs to go somewhere else, separate from application logs).",
        "url": "https://github.com/vectordotdev/vector/discussions/20174",
        "createdAt": "2024-03-26T01:09:40Z",
        "updatedAt": "2024-03-27T16:13:30Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ipsi"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20157,
        "title": "Is it possible to template a boolean value in a sink configuration",
        "bodyText": "Hi,\nCurrently trying to set the value of auto_extract_timestamp which expects a boolean in the Splunk Hec sink based on a boolean value in the event. Eg '{{ .config.logs.auto_extract_timestamp }}'.\nThis fails validation as the template is parsed and expressed as a string. validate fails validation with x invalid type: string \"{{ .config.logs.auto_extract_timestamp }}\", expected a boolean\nSo I'm wondering if there any other way I could set this value in the configuration based on the value in the event field?\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/20157",
        "createdAt": "2024-03-24T22:51:36Z",
        "updatedAt": "2025-03-06T22:54:30Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tbenade"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20123,
        "title": "Vector supports multiple config files",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nUse Cases\nHi, I wonder if I understand it correctly, but I am using vector with all components in one config file (vector.yaml at /etc/vector), which makes it too large to handle a large system. Is there a way we can store the structure in several config files and make the structure more clear?\nThanks!\nAttempted Solutions\nNo response\nProposal\nNo response\nReferences\nNo response\nVersion\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/20123",
        "createdAt": "2024-03-18T10:46:37Z",
        "updatedAt": "2024-04-28T09:55:42Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Joshualy94"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 20143,
        "title": "Why is my `<=` filter including values greater than the test value?",
        "bodyText": "Hey all, still testing Vector a bit, and I'm not even sure how to search on this one...\nAs far as I can tell my filter transform is doing the opposite of what I think it should.\nMy config:\nsources:\n  base_host_metrics:\n    type: host_metrics\n\ntransforms:\n  load5valuethreshold:\n    type: filter\n    inputs:\n      - base_host_metrics\n    condition: exists(.name) && .name == \"load5\" && to_float!(.gauge.value) <= 0.01\n  load5valuethreshold_to_log:\n    type: metric_to_log\n    inputs:\n      - load5valuethreshold\n\nsinks:\n  load5valuethreshold__file:\n    type: file\n    inputs:\n      - load5valuethreshold_to_log\n    path: /home/vagrant/metrics/load5valuethreshold-%Y-%m-%d.log\n    encoding:\n      codec: json\nResults in the output file looking like:\n{\"gauge\":{\"value\":1.0},\"host\":\"sw-dev-node-01\",\"kind\":\"absolute\",\"name\":\"load5\",\"namespace\":\"host\",\"tags\":{\"collector\":\"load\"},\"timestamp\":\"2024-03-20T18:45:36.567502707Z\"}\n{\"gauge\":{\"value\":1.0},\"host\":\"sw-dev-node-01\",\"kind\":\"absolute\",\"name\":\"load5\",\"namespace\":\"host\",\"tags\":{\"collector\":\"load\"},\"timestamp\":\"2024-03-20T18:45:51.567415239Z\"}\n{\"gauge\":{\"value\":1.0},\"host\":\"sw-dev-node-01\",\"kind\":\"absolute\",\"name\":\"load5\",\"namespace\":\"host\",\"tags\":{\"collector\":\"load\"},\"timestamp\":\"2024-03-20T18:46:06.567562208Z\"}\nI should be getting values less than 0.01, right? Why am I getting the opposite?\nIs to_float!(.gauge.value) doing something to the 1.0 value?\nThanks in advance.",
        "url": "https://github.com/vectordotdev/vector/discussions/20143",
        "createdAt": "2024-03-20T18:55:43Z",
        "updatedAt": "2024-03-22T22:19:14Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "jerrac"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20150,
        "title": "Stress Tools",
        "bodyText": "My Bro\uff0cCould you please tell me what tools you have that can be used for stress testing? We are going to use vector to collect logs and put them on disk.The daily log volume is estimated to be 30T.",
        "url": "https://github.com/vectordotdev/vector/discussions/20150",
        "createdAt": "2024-03-22T02:21:06Z",
        "updatedAt": "2024-03-22T02:21:06Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "uglyliu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 20141,
        "title": "systemd Vector Service Configuration with Environment Variables",
        "bodyText": "Hi,\nI was hoping to use some templated configuration only swapping out the value of environment variables  in /etc/default/vector on Linux EC2 instances running vector as a systemd service. We already used /etc/default/vector to configure some VECTOR_* values.\nDeployed a quick POC and ran great. Then noticed in the validate output that we have a deprecation warning.\nLoaded with warnings [\"/etc/vector\"]\n------------------------------------\n~ Unknown environment variable in config. This is DEPRECATED and will become an error in future versions. name = \"SPLUNK_INDEX\"\n\nTwo questions then. Maybe this guidance may be misleading for anyone comes across it. https://github.com/vectordotdev/vector/blob/master/distribution/systemd/vector.default#L2\nNext question is what is the appropriate mechanism for despatching environment variables for a vector systemd service if not /etc/default/vector please. Apologies if this is purely a linux question, systemd is not in my regular skillset.\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/20141",
        "createdAt": "2024-03-20T10:04:06Z",
        "updatedAt": "2024-03-21T12:41:21Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tbenade"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20144,
        "title": "Strategies for running templated configuration - Vector Aggregator Service - Linux Service",
        "bodyText": "Hi, wondering if there any documented or well known solutions for running templated configurations that can be migrated between environments for example test & prod. I haven't been able to get environment variables to work so assuming Im not the first person here so would love to know how others are templating their configuration and reusing it in different environments.\nFor example if you had a different Splunk index for each environment? Thanks",
        "url": "https://github.com/vectordotdev/vector/discussions/20144",
        "createdAt": "2024-03-21T03:27:44Z",
        "updatedAt": "2024-03-21T03:27:45Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tbenade"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 18897,
        "title": "Trying to capture protobufServer() output from PowerDNS recursor, but not seeing anything (v0.33.0)",
        "bodyText": "I'm using the PowerDNS recursor to implement DNS filtering using an RPZ. I should be able to see RPZ block events logged via their protobuf interface. Looking at its internal metrics, vector is reporting that it is receiving bytes, but not logging anything to file, but I can get it to log the dnstap data from pdns_recursor fine.\nhttps://docs.powerdns.com/recursor/lua-config/protobuf.html\nPowerDNS provide the proto file (see link), which I compiled,using protoc, into a desc file and vector validate is happy with picking up the message type from the desc.\nMy vector config\nhealthchecks:\n  require_healthy: true\n\nsources:\n  src_my_prom:\n    type: internal_metrics\n\n  dnstap_src:\n    type: dnstap\n    raw_data_only: false\n    socket_path: /run/vector/dnstap.sock\n    socket_file_mode: 0o777\n\n  dns_msg:\n    type: socket\n    address: \"0.0.0.0:4100\"\n    mode: \"tcp\"\n    decoding:\n      codec: protobuf\n      protobuf:\n        desc_file: /etc/vector/dnsmessage.desc\n        message_type: PBDNSMessage\n\n\nsinks:\n  output_my_prom:\n    type: prometheus_exporter\n    address: 0.0.0.0:9598\n    inputs: [ src_my_prom ]\n\n  msg_to_files:\n    type: file\n    inputs: [ dns_msg ]\n    healthcheck:\n      enabled: true\n    framing:\n      method: \"newline_delimited\"\n    path: \"/opt/dnsmsg/%Y/%m/%d/%H/%M/dnsmsg_%s.log\"\n    encoding:\n      codec: json\n      metric_tag_values: full\n\n  tap_to_files:\n    type: file\n    inputs: [ dnstap_src ]\n    healthcheck:\n      enabled: true\n    framing:\n      method: \"newline_delimited\"\n    path: \"/opt/dnstap/%Y/%m/%d/%H/%M/dnstap_%s.log\"\n    encoding:\n      codec: json\n      metric_tag_values: full\n\nThen I have protobufServer(\"127.0.0.1:4100\") in my recursor.lua to enable the pdns-recursor end.\nUsing netstat -nap I can see the two applications are connected to each other.\nWhen I curl the metrics from vector, and look for vector_component_received_bytes_total & dns_msg, I can see bytes coming in, but I never see any files logged in /opt/dnsmsg (directory is owned by vector).\nThe metric vector_component_received_event_bytes_total for msg_to_files is also zero.\nThere's nothing logged by either application in syslog or systemctl",
        "url": "https://github.com/vectordotdev/vector/discussions/18897",
        "createdAt": "2023-10-20T16:15:10Z",
        "updatedAt": "2024-03-20T09:46:18Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "james-stevens"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 20128,
        "title": "Is there any output plugin to remove all 404 lines in log files, before forwarding to aggregator",
        "bodyText": "I would like to a map reduce on certain HTTP error codes and HTTP verbs. Are there any plugins that accomplish this?",
        "url": "https://github.com/vectordotdev/vector/discussions/20128",
        "createdAt": "2024-03-19T02:14:36Z",
        "updatedAt": "2024-03-19T02:14:37Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "sirdatta"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 20103,
        "title": "How to determine whether a field is empty or null",
        "bodyText": "When I test on this website https://playground.vrl.dev/, my configuration is as follows, and I always get an error\n.t1 = \"-\"\n.t2 = 3\n\nif is_nullish(.t1) || is_empty(.t1) {\n  .t3 = 0\n} else {\n   .t3 = .t2 - .t1\n}\n\n\n\nThe error is reported as follows\nerror[E103]: unhandled fallible assignment\n  \u250c\u2500 :9:10\n  \u2502\n9 \u2502    .t3 = .t2 - .t1\n  \u2502    ----- ^^^^^^^^^ this expression is fallible because at least one argument's type cannot be verified to be valid\n  \u2502    \u2502\n  \u2502    or change this to an infallible assignment:\n  \u2502    .t3, err = .t2 - .t1\n  \u2502\n  = see documentation about error handling at https://errors.vrl.dev/#handling\n  = see functions characteristics documentation at https://vrl.dev/expressions/#function-call-characteristics\n  = learn more about error code 103 at https://errors.vrl.dev/103\n  = see language documentation at https://vrl.dev\n  = try your code in the VRL REPL, learn more at https://vrl.dev/examples\n\n\nThen I changed the configuration to look like this\uff0cit\u2018s ok\n.t1 = \"-\"\n.t2 = 3\n\nif is_nullish(.t1) || is_empty(.t1) {\n  .t3 = 0\n} else {\n   .t3, err = .t2 - .t1\n}\n\nThen I changed the configuration to the following\n.t1 = 2\n.t2 = 3\n\nif is_nullish(.t1) || is_empty(.t1) {\n  .t3 = 0\n} else {\n   .t3, err = .t2 - .t1\n}\n\n\n\nThe error is reported as follows\nerror[E110]: invalid argument type\n  \u250c\u2500 :6:32\n  \u2502\n6 \u2502 if is_nullish(.t1) || is_empty(.t1) {\n  \u2502                                ^^^\n  \u2502                                \u2502\n  \u2502                                this expression resolves to the exact type integer\n  \u2502                                but the parameter \"value\" expects one of string, array or object\n  \u2502\n  = learn more about error code 110 at https://errors.vrl.dev/110\n  = see language documentation at https://vrl.dev\n  = try your code in the VRL REPL, learn more at https://vrl.dev/examples\n\nHow should this be solved? Thank you very much",
        "url": "https://github.com/vectordotdev/vector/discussions/20103",
        "createdAt": "2024-03-16T10:27:00Z",
        "updatedAt": "2024-03-18T13:15:36Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "uglyliu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20094,
        "title": "Can elasticsearch be used as a data source?",
        "bodyText": "I would like to ask everyone, can vector support pulling data from the elasticsearch cluster, cleaning the data in the middle, and finally outputting it to the elasticsearch cluster?",
        "url": "https://github.com/vectordotdev/vector/discussions/20094",
        "createdAt": "2024-03-14T12:48:21Z",
        "updatedAt": "2024-03-18T13:10:11Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "evannwang1996"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20106,
        "title": "GPU acceleration?",
        "bodyText": "Curious if GPU acceleration was on the table. I saw a issue around simd-json, and I'm assuming that Vector's goals include super-performant parsing and data transforms. Have people looked into libraries like cuDF (which is a wrapper around a native libcudf)? Or am I completely off the mark?",
        "url": "https://github.com/vectordotdev/vector/discussions/20106",
        "createdAt": "2024-03-17T21:46:25Z",
        "updatedAt": "2024-03-18T12:47:52Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "wiwa"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20104,
        "title": "How to completely collect multi-line logs, like filebeat",
        "bodyText": "My configuration is as follows\ndata_dir = \"/data/svr/vector/data\"\ntimezone = \"local\"\n\n[api]\nenabled = false\naddress = \"[::]:8686\"\nplayground = false\n\n[sources.file1]\ninclude = [ \"/data/log/**/*.log\" ]\ntype = \"file\"\nignore_checkpoints = true\nread_from = \"end\"\n\n  [sources.file1.fingerprint]\n  strategy = \"device_and_inode\"\n\n  [sources.file1.multiline]\n  start_pattern = '^\\[0-9]{4}-[0-9]{2}-[0-9]{2}'\n  mode = \"continue_past\"\n  condition_pattern = '^\\[0-9]{4}-[0-9]{2}-[0-9]{2}'\n  timeout_ms = 1000\n\n[transforms.add_host]\ninputs = [\"file1\"]\ntype = \"remap\"\nsource = \"\"\"\n.pod_ip = \"${POD_IP}\"\n.host_ip = \"${HOST_IP}\"\n\n\"\"\"\n\n\n[sources.internal_metrics]\ntype = \"internal_metrics\"\n\n[sinks.prom_exporter]\ntype = \"prometheus_exporter\"\ninputs = [ \"internal_metrics\" ]\naddress = \"[::]:9092\"\n\n[sinks.stdout]\ntype = \"console\"\ninputs = [\"add_host\"]\n[sinks.stdout.encoding]\n  codec = \"json\"\n\nWhen I execute, the log output by the console is as follows\uff0cNot treating the complete log as one line\uff0cSorry, how should I modify the configuration?\n{\"file\":\"/data/log/user-center/user-center-20240317.log\",\"host\":\"ip-10-65-1-163.ap-northeast-1.compute.internal\",\"host_ip\":\"122.168.1.1\",\"message\":\"2022-04-01 10:15:31.234 - ERROR - An error occurred: NullPointerException\",\"pod_ip\":\"192.168.1.1\",\"source_type\":\"file\",\"timestamp\":\"2024-03-17T02:43:04.398117897Z\"}\n{\"file\":\"/data/log/user-center/user-center-20240317.log\",\"host\":\"ip-10-65-1-163.ap-northeast-1.compute.internal\",\"host_ip\":\"122.168.1.1\",\"message\":\"\\tat com.example.MyClass.method1(MyClass.java:25)\",\"pod_ip\":\"192.168.1.1\",\"source_type\":\"file\",\"timestamp\":\"2024-03-17T02:43:04.398160037Z\"}\n{\"file\":\"/data/log/user-center/user-center-20240317.log\",\"host\":\"ip-10-65-1-163.ap-northeast-1.compute.internal\",\"host_ip\":\"122.168.1.1\",\"message\":\"\\tat com.example.MyClass.method2(MyClass.java:35)\",\"pod_ip\":\"192.168.1.1\",\"source_type\":\"file\",\"timestamp\":\"2024-03-17T02:43:04.398163307Z\"}\n{\"file\":\"/data/log/user-center/user-center-20240317.log\",\"host\":\"ip-10-65-1-163.ap-northeast-1.compute.internal\",\"host_ip\":\"122.168.1.1\",\"message\":\"\\tat com.example.MyClass.method2(MyClass.java:35)\",\"pod_ip\":\"192.168.1.1\",\"source_type\":\"file\",\"timestamp\":\"2024-03-17T02:43:04.398165848Z\"}",
        "url": "https://github.com/vectordotdev/vector/discussions/20104",
        "createdAt": "2024-03-17T02:35:17Z",
        "updatedAt": "2024-03-18T11:25:11Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "uglyliu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20077,
        "title": "how to pass custom variables",
        "bodyText": "I set an environment variable pod_ip on the vector agent server\uff0cI want to pass the pod_ip field to the vector server \uff0cthe vector agent client configuration is as follows\ndata_dir = \"/data/svr/vector/data\"\ntimezone = \"local\"\n\n[api]\nenabled = false\naddress = \"[::]:8686\"\nplayground = false\n\n[sources.file1]\ninclude = [ \"/data/log/**/*.log\" ]\ntype = \"file\"\nignore_checkpoints = true\nread_from = \"end\"\n\n  [sources.file1.fingerprint]\n  strategy = \"device_and_inode\"\n\n\n[sources.internal_metrics]\ntype = \"internal_metrics\"\n\n[sinks.prom_exporter]\ntype = \"prometheus_exporter\"\ninputs = [ \"internal_metrics\" ]\naddress = \"[::]:9092\"\n\n[sinks.vector1]\ntype = \"vector\"\ninputs = [ \"file1\" ]\naddress = \"10.165.1.24:8688\"\nversion = \"2\"\n\n\nThe results I printed on the server side are as follows\n{\"file\":\"/data/log/user-center/user-center-20240313.log\",\"host\":\"ip-10-65-1-163.ap-northeast-1.compute.internal\",\"message\":\"2024-03-13 info test102\",\"source_type\":\"file\",\"timestamp\":\"2024-03-13T00:44:22.448217020Z\"}\n\nHow to achieve the following effect\n{\"file\":\"/data/log/user-center/user-center-20240313.log\",\"host\":\"ip-10-65-1-163.ap-northeast-1.compute.internal\",\"message\":\"2024-03-13 info test102\",\"source_type\":\"file\",\"timestamp\":\"2024-03-13T00:44:22.448217020Z\",\"pod_ip\":\"192.168.1.1\"}",
        "url": "https://github.com/vectordotdev/vector/discussions/20077",
        "createdAt": "2024-03-13T00:55:31Z",
        "updatedAt": "2024-03-17T02:37:24Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "uglyliu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20093,
        "title": "config parsing interpose envvars early, sometimes causing invalid yaml",
        "bodyText": "We discovered this by having a password env-var starting with grawlix @!*%..., and worked around by always \"\"-enclosing variable expansion. yq conversion didn't use the quotes.\nI suggest that the config-convert subcommand eagerly wrap strings in quote for defense.\nThe symptom is syntax error in yaml parsing, pointing at ${ character. Example config, without component-specific interpolation:\ntimezone: ${T}\n#2345678901\n# workaround: timezone: \"${T}\"\n\n# irrelevant minimal pipe\ndata_dir: \".\"\nsources:\n  stdin:\n    type: stdin\nsinks:\n  blackhole:\n    type: blackhole\n    inputs: [stdin]\nNo issue:\n\u276f env T=Etc/GMT vector validate --config-yaml config.yaml\n\u221a Loaded [\"config.yaml\"]\n\u221a Component configuration\n\u221a Health check \"blackhole\"\n--------------------------\n                 Validated\n\nYAML parse error:\n\u276f env T=@ vector validate --config-yaml config.yaml\nFailed to load [\"config.yaml\"]\n------------------------------\nx found character that cannot start any token at line 1 column 11, while scanning for the next token\n\nWith \"\"-wrapping, sensible config error:\n\u276f env T=@ vector validate --config-yaml config.yaml\nFailed to load [\"config.yaml\"]\n------------------------------\nx No such time zone\n\ncheers!",
        "url": "https://github.com/vectordotdev/vector/discussions/20093",
        "createdAt": "2024-03-14T11:50:34Z",
        "updatedAt": "2024-03-14T14:15:01Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "hdhoang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20088,
        "title": "Is the configuration of external secrets via SECRET[backend_aws_secrets_manager.*] universally supported?",
        "bodyText": "Hi,\nTrying to use the external secrets placeholder/backend with little luck in the datadog_metrics sink. The secrets retrieval works great for our Splunk sink but for 'datadog_metrics' we only get the 403 fiobidden. If I replace SECRET[backend_aws_secrets_manager.datadog_api_key] with the value  stored in AWS secrets manager it works fine.\ntype: datadog_metrics\ninputs:\n  - internal_metrics\ndefault_api_key: \"SECRET[backend_aws_secrets_manager.datadog_api_key]\"\ndefault_namespace: vector-aggregator-service\n\nBefore I burn much more time on this thought I would check that parsing the SECRET[*.*] was actually supported in the datadog_metrics sink.\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/20088",
        "createdAt": "2024-03-14T04:01:56Z",
        "updatedAt": "2024-03-14T13:49:33Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tbenade"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 20078,
        "title": "Turn off color in the validate CLI",
        "bodyText": "Hi, is there any way to turn of colored output when calling vector validate please.\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/20078",
        "createdAt": "2024-03-13T02:58:47Z",
        "updatedAt": "2024-03-13T22:46:20Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tbenade"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20071,
        "title": "How to add content to the log",
        "bodyText": "My config is as followers:\ndata_dir = \"/data/svr/vector/data\"\ntimezone = \"local\"\n\n[api]\nenabled = false\naddress = \"[::]:8687\"\nplayground = false\n\n[sources.vector]\ntype = \"vector\"\naddress = \"0.0.0.0:8688\"\nversion = \"2\"\n\n[sources.internal_metrics]\ntype = \"internal_metrics\"\n\n[transforms.trans1]\ntype = \"remap\"\ninputs = [ \"vector\" ]\nsource = \"\"\"\n.service = split!(.file, \"/\")[3]\n.timestamp = now()\n.unix_time = to_int!(format_timestamp!(.timestamp, format: \"%s\"))\n.utc8_time = .unix_time + 28800\n.t = from_unix_timestamp!(.utc8_time, unit: \"seconds\")\n.log_file_ts = format_timestamp!(.t, format: \"%Y.%m.%d\")\"\"\"\n\n[transforms.filter1]\ntype = \"filter\"\ninputs = [ \"trans1\" ]\n\n  [transforms.filter1.condition]\n  type = \"vrl\"\n  source = 'starts_with!(.message, \"WARN\") || starts_with!(.message, \"ERROR\") || starts_with!(.message, \"FATAL\") || contains!(.file, \"_err.log\")'\n\n[sinks.prom_exporter]\ntype = \"prometheus_exporter\"\ninputs = [ \"internal_metrics\" ]\naddress = \"[::]:9091\"\n\n[sinks.allfile]\ntype = \"file\"\ninputs = [ \"trans1\" ]\npath = \"/log-s3/{{ service }}/{{ host }}/{{ service }}-{{ log_file_ts }}.log\"\n\n  [sinks.allfile.encoding]\n  codec = \"text\"\n\n[sinks.errfile]\ntype = \"file\"\ninputs = [ \"filter1\" ]\npath = \"/log-s3/{{ service }}/{{ host }}/{{ service }}-error-{{ log_file_ts }}.log\"\n\n  [sinks.errfile.encoding]\n  codec = \"text\"\n\n[sinks.stdout]\ntype = \"console\"\ninputs = [ \"filter1\" ]\n\n  [sinks.stdout.encoding]\n  codec = \"json\"\n\nMy source log is\n2024-03-12 20:48:15.287   INFO   360 --- [           Thread-47] cddfrgrgafafa: XXXXXXXX accountType=35\n\nMy question is how to add a field to the log\uff0cfor example, the effect I want is as follows\n2024-03-12 20:48:15.287  pod_ip=172.16.2.2  host_ip=172.23.2.2   INFO   360 --- [           Thread-47] cddfrgrgafafa: XXXXXXXX accountType=35",
        "url": "https://github.com/vectordotdev/vector/discussions/20071",
        "createdAt": "2024-03-12T12:51:49Z",
        "updatedAt": "2024-03-17T02:38:22Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "uglyliu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19905,
        "title": "Can vector (inside a docker container as a service) connect to other docker services directly without HTTP Sink connection?",
        "bodyText": "Hi\nIs there a way to create a sink that can connect to Docker service directly, without exposing the other service as an HTTP to the outside world?\nE.g. I have two docker services: vector and analytics, where analytics service is not exposed to the outside world, but lies in the same docker network where vector service lies.\nSo instead of creating an HTTP sink, like:\nsinks:\n   analytics:\n      type: \"http\"\n      inputs:\n         - logs\n      encoding:\n         codec: \"json\"\n      method: \"post\"\n      request:\n         retry_max_duration_secs: 10\n      uri: \"http://${SERVICE_ANALYTICS_NAME:-analytics}:${ANALYTICS_PORT:-3000}/api/logs?source_name=gotrue.logs.prod&api_key=your-super-secret-and-long-logflare-key\"\nCan I make the above work with service names directly.",
        "url": "https://github.com/vectordotdev/vector/discussions/19905",
        "createdAt": "2024-02-20T03:21:16Z",
        "updatedAt": "2024-03-11T18:54:15Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "phoenisx"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19991,
        "title": "Can Vector's Loki `sink` accept Docker Container name as `endpoint`, instead of HTTP url?",
        "bodyText": "Is it possible to set sink type: loki to use docker container/service names as endpoints?\nSimilar to how we use container names in exclude_containers for Docker logs source?\n\nI have never used Vector log collector before, thus if the question I'm asking here seems to obvious please ignore \ud83d\ude47\ud83c\udffd .",
        "url": "https://github.com/vectordotdev/vector/discussions/19991",
        "createdAt": "2024-03-01T16:31:24Z",
        "updatedAt": "2024-03-09T10:09:07Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "phoenisx"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20031,
        "title": "Does vector support SNI on TLS configuration.",
        "bodyText": "Hi everyone,\nsomeone know if we can use SNI (server name Identication) with TLS between two vector or on a sink with a tls option? Does alpn option can be used to implement SNI ?\nWe try to configure something similar to fluenbit/D tls.vhost  https://docs.fluentbit.io/manual/v/1.9-pre/administration/security\nThanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/20031",
        "createdAt": "2024-03-07T17:10:44Z",
        "updatedAt": "2024-03-08T09:25:06Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "piellick"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 20029,
        "title": "Whether the Address field can be configured with multiple instances",
        "bodyText": "My config as follewers\n\ndata_dir: /data/svr/vector/data\ntimezone: \"local\"\napi:\nenabled: false\naddress: \"[::]:8686\"\nplayground: false\nsources:\nfile1:\ninclude:\n- \"/data/log/**/*.log\"\ntype: file\nfingerprint:\nstrategy: \"device_and_inode\"\nignore_checkpoints: true\nread_from: end\ninternal_metrics:\ntype: internal_metrics\nsinks:\nprom_exporter:\ntype: prometheus_exporter\ninputs:\n- internal_metrics\naddress: \"[::]:9090\"\nvector1:\ntype: vector\ninputs:\n- file1\naddress: \"10.35.2.213:8688\"\nversion: \"2\"",
        "url": "https://github.com/vectordotdev/vector/discussions/20029",
        "createdAt": "2024-03-07T12:21:51Z",
        "updatedAt": "2024-03-07T15:21:28Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "uglyliu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 20025,
        "title": "The checksum error metric is occurring from kubernetes_logs source. (metric name : vector_checksum_errors_total)",
        "bodyText": "hello.\nI am collecting Kubernetes container logs via Kubernetes Log source.\nI am getting a checksum error.\n\nWhat does this mean and how can I resolve this error?\nAnd this is my config.\n  sources:\n    internal-metrics:\n      type: internal_metrics\n      namespace: vector\n      scrape_interval_secs: 15\n\n    k8s:\n      type: kubernetes_logs\n      exclude_paths_glob_patterns:\n        - \"**/kube-system*/**\"\n...\n        - \"**/*.gz\"\n        - \"**/*.tmp\"\n\n      glob_minimum_cooldown_ms: 5000\n      read_from: beginning\n      pod_annotation_fields:\n        pod_name: \"kubernetes.pod_name\"\n        pod_owner: \"\"\n        pod_ips: \"\"\n        pod_ip: \"\"\n        pod_annotations: \"\"\n        pod_namespace: \"kubernetes.pod_namespace\"\n        pod_node_name: \"\"\n        pod_uid: \"\"\n        container_name: \"kubernetes.container_name\"\n        container_image_id: \"\"\n        container_image: \"\"\n        container_id: \"\"\n      node_annotation_fields:\n        node_labels: \"\"\n      namespace_annotation_fields:\n        namespace_labels: \"\"\n\n\nVector : 0.36.0",
        "url": "https://github.com/vectordotdev/vector/discussions/20025",
        "createdAt": "2024-03-07T01:05:15Z",
        "updatedAt": "2024-03-07T15:06:02Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "pingping95"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 20013,
        "title": "SQS attribute parameters",
        "bodyText": "Are you guys considering include message_attributes for SQS synk?",
        "url": "https://github.com/vectordotdev/vector/discussions/20013",
        "createdAt": "2024-03-05T21:24:55Z",
        "updatedAt": "2024-03-06T23:41:28Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "cmpacheco30"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19930,
        "title": "12% speed-up to vector incremental builds",
        "bodyText": "12% speed-up to vector.dev incremental builds\nSummary\nI have forked & modified the Rust compiler, achieving 12% faster incremental builds for vector.dev. The modded compiler can cache proc macro expansion; as you know, heavy proc macro dependencies impact compilation time and cause lags in rust-analyzer (in VSCode). vector.dev depends on proc-macro heavy crates like async-graphql, and thus benefits from this optimization. I have profiled vector's incremental builds, and have found that with zero configuration I achieve 12% faster builds (via cargo check):\n\nMore details\nI learned that the Rust compiler re-expands every macro in a crate, even on one-line edits, which is a disaster for the edit-compile-run cycle. Taking a similar approach to Rust's incremental compilation, I modified the Rust compiler to cache these expansions (in particular, the ones without side-effects; the counter! macros in vector.dev are the only ones that have side-effects) so that we don't need to re-compute them when unrelated code changes. This proc macro cache is the first tool in a toolkit that I am building -- all with the goal of improving Rust developer productivity!\nChat?\nI'd love to chat and/or have you try it out! I've recently packaged it into a remote build tool, which allows you to try it out in <3 minutes.",
        "url": "https://github.com/vectordotdev/vector/discussions/19930",
        "createdAt": "2024-02-22T05:43:27Z",
        "updatedAt": "2024-03-05T01:33:13Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "kapilsinha"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 16210,
        "title": "Filter log with regular expressions",
        "bodyText": "Hi all.\ni add tag label on my logs and\ni need filter some tag by using specific regex in transform vector but doesn't work with below configuration. would you please help me figure this out. tag:  log-test-[name]\ntransforms:\n  to_json:\n    type: remap\n    inputs:\n      - input_logs\n    source: |\n      . = parse_json!(.message)\n\n  block_test_logs:\n    type: filter\n    inputs:\n      - to_json\n    condition:\n      type: vrl\n      source:  .tag != r'log-test-(.*)'",
        "url": "https://github.com/vectordotdev/vector/discussions/16210",
        "createdAt": "2023-01-31T12:15:05Z",
        "updatedAt": "2024-03-02T14:32:07Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 19989,
        "title": "vector \u65e5\u5fd7\u5206\u8bcd\u5668",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nUse Cases\n\u73b0\u5728\u6709\u5982\u4e0b\u65e5\u5fd7\u6570\u636e\uff1a\n{\n_timestamp:1709261275287183,\nfile:/data1/kubernetes/logs/im/zero-user-wx/zero-user-wx-base-7d6cb69f56-nsrrb/app.log,\nhost:vector-wt6vv,\nmessage:{\"@timestamp\":\"2024-03-01T10:47:54.689+08:00\",\"caller\":\"load/sheddingstat.go:61\",\"content\":\"(api) shedding_stat [1m], cpu: 0, total: 40, pass: 40, drop: 0\",\"level\":\"stat\"},\noffset:518386817,\nsource_type:file,\ntimestamp:2024-03-01T02:47:54.818238220Z\n}\n\u73b0\u5728\u6709\u5982\u4e0b\u9700\u6c42\uff1a\n\u8bf7\u95ee\u5728vector\u914d\u7f6e\u4e2d\u5982\u4f55\u91c7\u7528\u5206\u8bcd\u529f\u80fd\uff0c\u63d0\u53d6file\u5b57\u6bb5\u4e2d\uff1aim\u4e3akubernetes_pod_namespace\u5b57\u6bb5\u7684\u503c\uff0czero-user-wx\u4e3akubernetes_pod_labels_deploy\u7684\u503c\uff0czero-user-wx-base-7d6cb69f56-nsrrb\u4e3akubernetes_pod_name\u7684\u503c\u3002\u5373\u5b9e\u73b0\u5982\u4e0b\u6548\u679c\uff1a\n{\n_timestamp:1709261275287183,\nfile:/data1/kubernetes/logs/im/zero-user-wx/zero-user-wx-base-7d6cb69f56-nsrrb/app.log,\nkubernetes_pod_name: im,\nkubernetes_pod_labels_deploy: zero-user-wx,\nkubernetes_pod_name: zero-user-wx-base-7d6cb69f56-nsrrb,\nhost:vector-wt6vv,\nmessage:{\"@timestamp\":\"2024-03-01T10:47:54.689+08:00\",\"caller\":\"load/sheddingstat.go:61\",\"content\":\"(api) shedding_stat [1m], cpu: 0, total: 40, pass: 40, drop: 0\",\"level\":\"stat\"},\noffset:518386817,\nsource_type:file,\ntimestamp:2024-03-01T02:47:54.818238220Z\n}\n\u5176\u4e2d,vector 0.35\u7248\u672c\u4e0d\u652f\u6301regex_parser\u63d2\u4ef6\nAttempted Solutions\nNo response\nProposal\nNo response\nReferences\nNo response\nVersion\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/19989",
        "createdAt": "2024-03-01T03:25:45Z",
        "updatedAt": "2024-03-01T12:47:42Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ZbZn905888"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19980,
        "title": "File to GCS event dropped",
        "bodyText": "Hi\nI am using vector 0.34.0 to send logs file to GCP Cloud Storage without any issues for a month. But recently got an error below and some log was disappeared.\nNot sure if this is an issue or if there is any existing configuration option to handle this?\nExpected behavior: Retry sending to GCS in case of failure and no log was dropped\nerror1\n{\n  \"span\": {\n    \"name\": \"request\",\n    \"request_id\": 607\n  },\n  \"error\": \"Failed to make HTTP(S) request: error writing a body to connection: Connection reset by peer (os error 104)\",\n  \"message\": \"Unexpected error type; dropping the request.\",\n  \"target\": \"vector::sinks::util::retries\",\n  \"internal_log_rate_limit\": true,\n  \"spans\": [\n    {\n      \"component_kind\": \"sink\",\n      \"name\": \"sink\",\n      \"component_id\": \"start_event\",\n      \"component_type\": \"gcp_cloud_storage\"\n    },\n    {\n      \"name\": \"request\",\n      \"request_id\": 607\n    }\n  ],\n  \"level\": \"ERROR\",\n  \"timestamp\": \"2024-02-27T13:22:46.637798Z\"\n}\nerror2\n{\n  \"span\": {\n    \"request_id\": 607,\n    \"name\": \"request\"\n  },\n  \"stage\": \"sending\",\n  \"message\": \"Service call failed. No retries or retries exhausted.\",\n  \"request_id\": 607,\n  \"timestamp\": \"2024-02-27T13:22:46.643160Z\",\n  \"spans\": [\n    {\n      \"name\": \"sink\",\n      \"component_id\": \"start_event\",\n      \"component_type\": \"gcp_cloud_storage\",\n      \"component_kind\": \"sink\"\n    },\n    {\n      \"name\": \"request\",\n      \"request_id\": 607\n    }\n  ],\n  \"internal_log_rate_limit\": true,\n  \"error\": \"Some(CallRequest { source: hyper::Error(BodyWrite, Os { code: 104, kind: ConnectionReset, message: \\\"Connection reset by peer\\\" }) })\",\n  \"target\": \"vector_common::internal_event::service\",\n  \"level\": \"ERROR\",\n  \"error_type\": \"request_failed\"\n}\nerror3\n{\n  \"message\": \"Events dropped\",\n  \"timestamp\": \"2024-02-27T13:22:46.643221Z\",\n  \"count\": 491,\n  \"target\": \"vector_common::internal_event::component_events_dropped\",\n  \"internal_log_rate_limit\": true,\n  \"reason\": \"Service call failed. No retries or retries exhausted.\",\n  \"intentional\": false,\n  \"span\": {\n    \"name\": \"request\",\n    \"request_id\": 607\n  },\n  \"spans\": [\n    {\n      \"component_id\": \"start_event\",\n      \"name\": \"sink\",\n      \"component_kind\": \"sink\",\n      \"component_type\": \"gcp_cloud_storage\"\n    },\n    {\n      \"request_id\": 607,\n      \"name\": \"request\"\n    }\n  ],\n  \"level\": \"ERROR\"\n}\nconfig\n    data_dir: \"/usr/share/vector/data\"\n    sources:\n      logs:\n        type: \"file\"\n        include:\n          - \"/log/*\"\n        ignore_older_secs: 7200\n        offset_key: offset\n      vector_metrics:\n        type: \"internal_metrics\"\n    transforms:\n      jsonParse:\n        type: remap\n        inputs:\n          - logs\n        source: |-\n          .message = parse_json!(.message)\n      eventFilter:\n        type: filter\n        inputs: \n          - jsonParse\n        condition: '.message.s_event==\"START\"'\n    sinks:\n      prometheus:\n        type: prometheus_exporter\n        inputs:\n          - vector_metrics\n      start_event:\n        type: gcp_cloud_storage\n        inputs:\n          - eventFilter\n        bucket: <GCS BUCKET>\n        encoding:\n          codec: json\n        batch:\n          max_bytes: 268435488\n          max_events: 40000\n          timeout_secs: 30\n        buffer:\n          type: disk\n          max_size: 2684354880\n          when_full: block\n        key_prefix: \"start/date=%F/\"\n        filename_extension: ndjson\n        framing:\n          method: newline_delimited",
        "url": "https://github.com/vectordotdev/vector/discussions/19980",
        "createdAt": "2024-02-29T09:45:01Z",
        "updatedAt": "2024-03-01T07:00:51Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "braden00"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19820,
        "title": "Degredation of PVCs",
        "bodyText": "Hi!\nWe have been using PVC-backed data buffers, mostly because we were unsure about undertaking the operational burden of keeping kafka happy.  We have since decided the operational flexibility is worth the effort, so will be redesigning our system as such, but recently I've run into a more pressing problem with the PVC solution: it appears to be unstable after time.\nUnfortunately, I don't have a ton of information here... there is nothing outstanding in the logs and really all I see is CPU utilization going through the roof and discarded events being reported.  If all I do is remove the pod, it comes back (with its PVC) and continues its high CPU utilization and discarded errors.  We're using HPA, so all it takes is for one pod to throw off the average, which triggers HPA scale-out.  Over time, it happens to more and more pods.  My only recourse is to delete the pod and its corresponding PVC.\nRecently, this happened to 6 of 20 pods (we only had 20 pods, up from the usual 8, because multiple pods started exhibiting this behavior).  Here's what I saw:\nNAME                                    CPU(cores)   MEMORY(bytes)\nvector-writer-0                         323m         415Mi\nvector-writer-1                         402m         501Mi\nvector-writer-10                        359m         457Mi\nvector-writer-11                        641m         516Mi\nvector-writer-12                        402m         481Mi\nvector-writer-13                    --> 2535m        554Mi \nvector-writer-14                    --> 2578m        525Mi\nvector-writer-15                        436m         469Mi\nvector-writer-16                        544m         442Mi\nvector-writer-17                        318m         461Mi\nvector-writer-18                        611m         486Mi\nvector-writer-19                    --> 1802m        471Mi\nvector-writer-2                         366m         335Mi\nvector-writer-3                     --> 2729m        559Mi  \nvector-writer-4                     --> 2426m        538Mi \nvector-writer-5                         360m         504Mi\nvector-writer-6                     --> 3966m        506Mi\nvector-writer-7                         451m         548Mi\nvector-writer-8                         501m         438Mi\nvector-writer-9                         244m         460Mi\n\nI cleaned up the 6 indicated via this script:\n#!/bin/bash\n\nset -exu\n\nfor n in 13 14 19 3 4 6; do\n    kubectl delete pod/vector-writer-$n pvc/data-vector-writer-$n\ndone\n\nThe chaos stopped, and HPA scaled back down with 8 pods hovering around 800m CPU.\nThe PVC data has to be the culprit here... I just don't know how to diagnose this issue.  I've saved the /vector-data-dir from one of the instances, but not sure how to go about troubleshooting it.\nBefore we get to the kafka-based solution (which we're working on), I'm wondering whether it's worth applying some workaround:\n\nUse some hook to rm -rf /vector-data-dir/* on \"clean\" shutdown, and possibly do regular rolling restarts\nDrop the stateful set entirely and just accept the relatively low likelihood of data loss\nTake it one step further and switch to memory buffers\n\nAny guidance here would be great!",
        "url": "https://github.com/vectordotdev/vector/discussions/19820",
        "createdAt": "2024-02-07T13:49:07Z",
        "updatedAt": "2024-02-29T20:14:17Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "srstrickland"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 3
    },
    {
        "number": 19982,
        "title": "SSL failures during connection attempts to Kafka brokers",
        "bodyText": "Hello,\nI'm seeking help with unclear behavior with Kafka sink. I have got SSL error during establish connection with Kafka broker. Trying with different Vector versions, both fail but with different errors.\nEnvironments:\n\nVector Version: 0.28.1\nError Message:\n\nERROR librdkafka: librdkafka: FAIL [thrd:sasl_ssl://KAFKA_BROKER_IP:9093/bootstrap]: sasl_ssl://KAFKA_BROKER_IP:9093/bootstrap: SSL handshake failed: error:0A000126:SSL routines::unexpected eof while reading (after 100ms in state SSL_HANDSHAKE, 13 identical error(s) suppressed)\nERROR rdkafka::client: librdkafka: Global error: SSL (Local: SSL error): sasl_ssl://KAFKA_BROKER_IP:9093/bootstrap: SSL handshake failed: error:0A000126:SSL routines::unexpected eof while reading (after 100ms in state SSL_HANDSHAKE, 13 identical error(s) suppressed)\n\n\nVector Version: 0.34.1\nError Message:\n\nERROR rdkafka::client: librdkafka: Global error: BrokerTransportFailure (Local: Broker transport failure): sasl_ssl://KAFKA_BROKER_IP:443/bootstrap: Disconnected (after 76074ms in state UP)\n\nSimilarity:\n\nBoth environments are running Red Hat Enterprise Linux 9.3.\nBoth errors involve SSL failures during connection attempts to Kafka brokers.\nOpenSSL version: 3.0.7 (in both environments)\n\nWhile the root cause appears to be the same, Vector handles the issue differently based on the version.\nError 1: Unexpected EOF during SSL handshake.\nError 2: Broker transport failure after connection established\nI came across this GitHub issue #openssl/issues/18866#issuecomment-1194219601 that mentions potential compatibility issues with certain configurations and OpenSSL 3.x. Could this be relevant to my situation? More generally, might the OpenSSL version be contributing to these errors?\nI would greatly appreciate any insights or suggestions you might have to help me troubleshoot and resolve these issues. Additionally, please let me know if you require further information such as specific Kafka versions or Vector configurations.\nThank you for your time and assistance.",
        "url": "https://github.com/vectordotdev/vector/discussions/19982",
        "createdAt": "2024-02-29T18:34:30Z",
        "updatedAt": "2024-02-29T18:34:30Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "vparfonov"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 19902,
        "title": "Parse CloudTrail log and remap each event",
        "bodyText": "Dear Community,\nI am planing to use vector to ingest cloudtrail logs into clickhouse.\nThe batch of events looks like:\n{\"Records\": [{\n    \"eventVersion\": \"1.08\",\n    \"userIdentity\": {\n        \"type\": \"IAMUser\",\n        \"principalId\": \"AIDA6ON6E4XEGITEXAMPLE\",\n        \"arn\": \"arn:aws:iam::888888888888:user/Mary\",\n        \"accountId\": \"888888888888\",\n        \"accessKeyId\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"userName\": \"Mary\",\n        \"sessionContext\": {\n            \"sessionIssuer\": {},\n            \"webIdFederationData\": {},\n            \"attributes\": {\n                \"creationDate\": \"2023-07-19T21:11:57Z\",\n                \"mfaAuthenticated\": \"false\"\n            }\n        }\n    },\n    \"eventTime\": \"2023-07-19T21:25:09Z\",\n    \"eventSource\": \"iam.amazonaws.com\",\n    \"eventName\": \"CreateUser\",\n    \"awsRegion\": \"us-east-1\",\n    \"sourceIPAddress\": \"192.0.2.0\",\n    \"userAgent\": \"aws-cli/2.13.5 Python/3.11.4 Linux/4.14.255-314-253.539.amzn2.x86_64 exec-env/CloudShell exe/x86_64.amzn.2 prompt/off command/iam.create-user\",\n    \"requestParameters\": {\n        \"userName\": \"Richard\"\n    },\n    \"responseElements\": {\n        \"user\": {\n            \"path\": \"/\",\n            \"arn\": \"arn:aws:iam::888888888888:user/Richard\",\n            \"userId\": \"AIDA6ON6E4XEP7EXAMPLE\",\n            \"createDate\": \"Jul 19, 2023 9:25:09 PM\",\n            \"userName\": \"Richard\"\n        }\n    },\n    \"requestID\": \"2d528c76-329e-410b-9516-EXAMPLE565dc\",\n    \"eventID\": \"ba0801a1-87ec-4d26-be87-EXAMPLE75bbb\",\n    \"readOnly\": false,\n    \"eventType\": \"AwsApiCall\",\n    \"managementEvent\": true,\n    \"recipientAccountId\": \"888888888888\",\n    \"eventCategory\": \"Management\",\n    \"tlsDetails\": {\n        \"tlsVersion\": \"TLSv1.2\",\n        \"cipherSuite\": \"ECDHE-RSA-AES128-GCM-SHA256\",\n        \"clientProvidedHostHeader\": \"iam.amazonaws.com\"\n    },\n    \"sessionCredentialFromConsole\": \"true\"\n},\n{\n    \"eventVersion\": \"1.08\",\n    \"userIdentity\": {\n        \"type\": \"IAMUser\",\n        \"principalId\": \"AIDA6ON6E4XEGITEXAMPLE\",\n        \"arn\": \"arn:aws:iam::888888888888:user/Mary\",\n        \"accountId\": \"888888888888\",\n        \"accessKeyId\": \"AKIAIOSFODNN7EXAMPLE\",\n        \"userName\": \"Mary\",\n        \"sessionContext\": {\n            \"sessionIssuer\": {},\n            \"webIdFederationData\": {},\n            \"attributes\": {\n                \"creationDate\": \"2023-07-19T21:11:57Z\",\n                \"mfaAuthenticated\": \"false\"\n            }\n        }\n    },\n    \"eventTime\": \"2023-07-19T21:25:09Z\",\n    \"eventSource\": \"iam.amazonaws.com\",\n    \"eventName\": \"CreateUser\",\n    \"awsRegion\": \"us-east-1\",\n    \"sourceIPAddress\": \"192.0.2.0\",\n    \"userAgent\": \"aws-cli/2.13.5 Python/3.11.4 Linux/4.14.255-314-253.539.amzn2.x86_64 exec-env/CloudShell exe/x86_64.amzn.2 prompt/off command/iam.create-user\",\n    \"requestParameters\": {\n        \"userName\": \"Richard\"\n    },\n    \"responseElements\": {\n        \"user\": {\n            \"path\": \"/\",\n            \"arn\": \"arn:aws:iam::888888888888:user/Richard\",\n            \"userId\": \"AIDA6ON6E4XEP7EXAMPLE\",\n            \"createDate\": \"Jul 19, 2023 9:25:09 PM\",\n            \"userName\": \"Richard\"\n        }\n    },\n    \"requestID\": \"2d528c76-329e-410b-9516-EXAMPLE565dc\",\n    \"eventID\": \"ba0801a1-87ec-4d26-be87-EXAMPLE75bbb\",\n    \"readOnly\": false,\n    \"eventType\": \"AwsApiCall\",\n    \"managementEvent\": true,\n    \"recipientAccountId\": \"888888888888\",\n    \"eventCategory\": \"Management\",\n    \"tlsDetails\": {\n        \"tlsVersion\": \"TLSv1.2\",\n        \"cipherSuite\": \"ECDHE-RSA-AES128-GCM-SHA256\",\n        \"clientProvidedHostHeader\": \"iam.amazonaws.com\"\n    },\n    \"sessionCredentialFromConsole\": \"true\"\n}\n]}    \nMy configuration is:\nsources:\n  cloudtrailsource:\n    type: http\n    address: 0.0.0.0:8090\n    method: POST\n    path: /cloudtrail\n    encoding: text\n    response_code: 200\n\ntransforms:\n  transformcloudtrail:\n    type: remap\n    inputs:\n      - cloudtrailsource\n    source: |\n      . = parse_json!(.message)\n      .active = true\nsinks:\n  stdout:\n    type: console\n    inputs:\n      - transformcloudtrail\n    encoding:\n      codec: json\nHow can I iterate over each log record, and do transformation for each log event?",
        "url": "https://github.com/vectordotdev/vector/discussions/19902",
        "createdAt": "2024-02-19T10:47:41Z",
        "updatedAt": "2024-03-10T14:45:12Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "omers"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 19922,
        "title": "Vector events in disk buffer not preserved after pod restart",
        "bodyText": "When restarting pod with disk buffer persistence enabled vector buffer doesn't try to send already got events from kafka but starts from 0 with current consumer, so events are lost. How this issue could be resolved or I missed something?\nUse 0.31.0 chart version (the same issue was on 0.26.0 version too, so bump didn't help)\nPersistence is enabled:\n          persistence:\n            enabled: true\n            storageClassName: premium-rwo\n            size: 10Gi\n\nTypical sink config.\n            sinks:  \n              clickhouse:\n                type: clickhouse\n                healthcheck: true\n                acknowledgements:\n                  enabled: true\n                inputs: [\"kafka\"]\n                endpoint: _lorem-ipsum_\n                database: _lorem-ipsum_\n                table: _lorem-ipsum_\n                skip_unknown_fields: true\n                auth:\n                  strategy: basic\n                  user: vector\n                  password: _lorem-ipsum_\n                buffer:\n                  type: disk\n                  max_size: 5368709120\n                  when_full: block\n                batch:\n                  max_bytes: 536870912\n                  timeout_secs: 5\n                request:\n                  timeout_secs: 120\n\nP.S. Using Aggregator role with 1 replica",
        "url": "https://github.com/vectordotdev/vector/discussions/19922",
        "createdAt": "2024-02-21T13:47:33Z",
        "updatedAt": "2024-02-21T20:33:34Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "viktordebulat"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 19896,
        "title": "Handling bursty/batch/bulk data dumps",
        "bodyText": "Vector is currently handling log shipping for our \"realtime\" environments, where logs are streamed out from running processes.  But we have a number of other \"offline\" processes which generate logs in a disconnected environment that then need to be uploaded in batch.  It seems fairly trivial to set up an S3 / SQS ingestion mechanism for this where users or applications just dump logfiles to S3 (and in fact I've already built & verified this), but I'm worried about the bursty nature of this data and how it might affect the other realtime things.\nVector is built to be fast, and for most applications \"as fast as possible\" or \"as fast as the sinks will accept\" is the answer.  But I'm not finding many ways to slow things down, \"smooth out\" data delivery, or control flows from different sources (in an attempt to be \"fair\").  What happens if someone drops a 1TB log file to s3?  It'll be read as fast as possible, and very possibly starve the data coming from the (arguably more important) realtime applications.  And generally this data is going to be very bursty, in stark contrast to the rest of the system.\nAnd to be fair, there are things I can build outside of vector to help with this (like an ingestion pipeline to kafka with intentional rate limits built by hand), and I could build a completely separate pipeline with different resource allocations (which might be a good idea anyways), but I just wanted to see if anyone else has had similar use cases.  Fairness and flow control might be generally useful, and if I can keep everything inside vector, it makes things a lot simpler operationally.\nBackpressure & adaptive concurrency are natural for vector, so it got me thinking along a few lines:\n\nIf VRL had some kind of sleep mechanism, I could sort of manually control flow through a remap transform by doing something like:\n\n# approximately every 10000 items, sleep for 1s\nif (random_float(0.0, 1.0) < 0.0001) { sleep(1) }\n\n\nMaybe sources could expose a configuration for some kind of speed limit (either bytes/sec or events/sec)?\nI initially thought the throttle transform might be a good home for this, with a \"block\" option, but seems really tricky because throttle is (optionally) key-based, and blocking per key seems... impossible.  But maybe some rate_limit transform which just operates on the entire stream?\n\nAny other ideas?  Thoughts?  Am I way off base?",
        "url": "https://github.com/vectordotdev/vector/discussions/19896",
        "createdAt": "2024-02-17T19:03:17Z",
        "updatedAt": "2024-02-20T20:17:32Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "srstrickland"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19917,
        "title": "Better CI with RunsOn?",
        "bodyText": "I notice that the test.yml workflow is using outdated ubuntu20 self-hosted runner, with a lot of preparation steps to install all the required software.\nSince it looks like it's already running on a self-hosted runner on AWS, maybe you should have a look at RunsOn to simplify the setup, and greatly improve the speed + cost.\nAlso RunsOn launches ephemeral runners (boot time 20s for ARM64, 40s for x64), which alleviates the security issues with long-running self-hosted runners in public repositories.\nHappy to help in any way I can!",
        "url": "https://github.com/vectordotdev/vector/discussions/19917",
        "createdAt": "2024-02-20T17:16:57Z",
        "updatedAt": "2024-02-20T17:16:58Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "crohr"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 19903,
        "title": "how to ship logs and metrics and keep the name of the cluster source not the one vector lives on ?",
        "bodyText": "hello there, i have a strange behavior using vector,\nvector gets metrics and logs for external clusters and ship those metrics to prom and logs to loki, i've discovered that in this case for logs and metrics, the cluster label value is the one where vector lives and not the one where the data and originally from.\nIs there anyway to change that behavior ?",
        "url": "https://github.com/vectordotdev/vector/discussions/19903",
        "createdAt": "2024-02-19T15:28:28Z",
        "updatedAt": "2024-02-19T15:28:28Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "xinity"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 17650,
        "title": "S3 sort in reverse chronological order",
        "bodyText": "Hey all,\nusing the S3 sink, is there a way to format the file names so they are naturally sorted in reverse chronological order? The main usecase is I'm creating many small batches of events and want to make it faster to iterate through the recent ones.\nOne work-around I am doing is calculating the timestamp of the events and subtracting it from a large number (similar to 9s complement), but the issue with that approach is that I cannot pad the result with leading zeroes.\nAlternatively, if I can format integers in VRL to pad with leading zeroes that would be helpful as well.",
        "url": "https://github.com/vectordotdev/vector/discussions/17650",
        "createdAt": "2023-06-08T21:05:24Z",
        "updatedAt": "2024-02-17T18:28:34Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "alanwguo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17449,
        "title": "Support both histograms and summaries in distributions_as_summaries in Prometheus Sink",
        "bodyText": "We've got folks migrating from older aggregators / statsd to prometheus.\nThey've come to rely on quantile generation.\nQuantiles have caveats when vector runs on multiple instances, they cannot be aggregated (avg(p95, p95) ... is not a \"valid\" p95).\nHistograms also render nicely in tools like grafana.\nI'd like to maintain continuity, while offering a forward-looking path to adopting histograms.\n\nIt'd be nice if this option supported either/or. The metric names/dimensions are different such that you could differentiate and have both on at once, I believe.",
        "url": "https://github.com/vectordotdev/vector/discussions/17449",
        "createdAt": "2023-05-19T23:42:19Z",
        "updatedAt": "2024-02-15T21:58:30Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "jacobstr"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 19685,
        "title": "Enhancement for sink config to refer to (the resolved) global timezone name",
        "bodyText": "I'm using the http sink. I want to let the server know the timezone name (e.g., America/Chicago) that is configured for this vector system (the value of the global timezone configuration entry, or if the timezone is configured to \"local\" ideally vector would get and use the actual timezone name (e.g., America/Chicago) from the host OS).\nThis can help the server do the right thing if the server is parsing/processing unstructured textual log data that it receives, and it needs to know how to treat text timestamps that it's detecting/parsing if they do not have an explicit timezone. Right now vector configuration can refer to environment variables or secrets, but it doesn't appear it can refer to global configuration values. Is it currently possible for a sink configuration value (e.g., the http sink uri or header) to refer to the value of the timezone?\nPossible ways this could be implemented:\n\nA special \"environment variable\" (name of TIMEZONE or maybe more unique like VECTOR.TIMEZONE) could be automatically added after global config is resolved that has the dynamic value of the global timezone configuration (including trying to resolve TimeZone::Local to the actual local timezone name). This does overload environment variable interpolation with a special value, so maybe it's not as clean an approach as would be desired. On the other hand, setting an actual environment variable could be beneficial for other places, like the exec source. That could get messy after a while if a lot of configuration values became set environment variables...\nAlternatively, a new type of setting interpolation could be added just like secret interpolation, e.g., CONFIG[timezone] -- on quick inspection, this would probably require a lot more code changes than just adding a special environment variable with the resolved timezone name.\nAlternatively, it would be easy enough to develop a custom sink that wrapped the http sink and directly used/transformed the global cx.globals.timezone into a request header, but that is less general, and it feels like other vector users might also want to send what the locally configured timezone is to a sink.",
        "url": "https://github.com/vectordotdev/vector/discussions/19685",
        "createdAt": "2024-01-23T01:09:54Z",
        "updatedAt": "2024-02-13T04:29:21Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "klondikedragon"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19829,
        "title": "How to listen for dnstap source from network",
        "bodyText": "Source type dnstap seems to enforce socket_path .\nI've tried putting 0.0.0.0:6053 as the path ,  but all that did was created a socket file on disk.\nCan someone advise ?",
        "url": "https://github.com/vectordotdev/vector/discussions/19829",
        "createdAt": "2024-02-08T02:48:54Z",
        "updatedAt": "2024-02-09T02:22:49Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "peterand-pa"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 19815,
        "title": "Vector not accepting connections on sink failure",
        "bodyText": "I'm using Vector v0.35.0 in an aggregator configuration, where it accepts json via HTTP, from FluentBit agents, and inserting it into ClickHouse using the sink. I'm hosting it on AWS ECS Fargate in front of an NLB with health-checks enabled based on the API /health endpoint.\nWhenever the sink starts to experience a large amount of failures, in my case a number of \"connection resets\" from ClickHouse Cloud whenever it scales up, Vector will soon stop accepting connections resulting in an unhealthy task and being killed off by AWS. Vector also shoots up in memory, but there's nothing indicating in the logs (level=info) to indicate that.\nI tried buffering to filesystem to see how it would perform and it resulted in a 2-3x performance decrease, which is a considerable amount at our scale, so I haven't tried that out with ClickHouse Cloud scaling up.\nExample error:\n2024-02-06T08:57:49.515492Z  WARN sink{component_kind=\"sink\" component_id=clickhouse_envoy component_type=clickhouse}:request{request_id=9069}:http: vector::internal_events::http_client: HTTP error. error=connection closed before message completed error_type=\"request_failed\" stage=\"processing\" internal_log_rate_limit=true\n\nVector config:\napi:\n  enabled: true\n  address: \"0.0.0.0:8686\" \n\nsources:\n  envoy_in:\n    type: http_server\n    address: 0.0.0.0:8123\n    encoding: ndjson\n    path_key: __vector_http_path\n\nsinks:\n  clickhouse_envoy:\n    type: clickhouse\n    inputs:\n      - envoy_in\n    endpoint: https://<host>.us-west-2.vpce.aws.clickhouse.cloud:8443\n    database: default\n    table: envoy\n    auth:\n      strategy: basic\n      user: default\n      password: <password>\n    batch:\n      max_bytes: 30000000\n      timeout_secs: 2\n# -- later tried --\n    buffer:\n      type: disk\n      max_size: 4294967296 # 4 GB",
        "url": "https://github.com/vectordotdev/vector/discussions/19815",
        "createdAt": "2024-02-06T23:01:16Z",
        "updatedAt": "2024-02-12T16:35:30Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "jakegut"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19791,
        "title": "Unable to delete timestamp and source_type fields",
        "bodyText": "Hi Team,\nI tried to use del(.field1) as mentioned in VRL examples but was not able to delete timestamp and source_type fields. I am also validating JSON format in the .vrl files which seems correct.\nKindly suggest on how to remove these parameters. I am using vector version 0.32.0",
        "url": "https://github.com/vectordotdev/vector/discussions/19791",
        "createdAt": "2024-02-05T09:34:49Z",
        "updatedAt": "2024-02-07T06:04:20Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "AmitJ1303"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 1
    },
    {
        "number": 19792,
        "title": "Does vector need read permissions for s3 sink ?",
        "bodyText": "Hi, I have a vector instance that has an s3 sink, the s3 key associated to this vector instance only has write permissions.\nToday I've found out that I am getting 403 from my s3 and by checking my s3 logs I've noticed that vector had tried to read something from s3.\nThe situation that this happened was not normal because alongside my s3 sink I have an elasticsearch sink and the elasticsearch was not responding at the moment of vector tried to read from s3. So I'm suspicious that maybe there is some fault-tolerance mechanism that needs read access.\nI've reviewed the code for s3 sinks but have not found anything.",
        "url": "https://github.com/vectordotdev/vector/discussions/19792",
        "createdAt": "2024-02-05T10:31:29Z",
        "updatedAt": "2024-02-06T14:55:37Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mehrdad-khojastefar"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 19781,
        "title": "(best way to) estimate event size",
        "bodyText": "Recently we had a rogue process start spamming megabyte log lines, and needless to say bad things started happening.  We installed a quick fix at the agent level to just drop anything whose .message field exceeds some threshold (via strlen), and it got us out of the woods.  Maybe in practice, checking the message field is sufficient, since all log messages start there no matter the source.  But we have a distributed topology, and don't control all the agents (configs) in our ecosystem, so I need something similar enforced at the \"central\" layer (which receives data from all the various agents).  And technically at this point, it's possible that some upstream agent has already parsed a gigantic message into other fields, so it may be insufficient to check only the message field.\nThe simplest way I've found to estimate payload size is to just encode as json and take a strlen.  Obviously this is an overshot, but since a lot of data is serialized as JSON on the way out, it's not unreasonable.  But adding a full serialization just to count the bytes feels like overkill, and I'm wondering if there's a better way.  I couldn't find anything in the VRL docs, so at the moment this is all I have:\nif (strlen(encode_json(.)) > some_threshold)  # could use length() too\n\nIs there another option?  I figure since vector is doing a lot of monitoring around bytes moving through the system, this might be an easy opportunity to expose that information via a function like estimate_size(<any>).  It feels like this would be far more efficient than allocating & using memory just for a string to be counted.\nThanks in advance for any pointers!",
        "url": "https://github.com/vectordotdev/vector/discussions/19781",
        "createdAt": "2024-02-03T16:36:18Z",
        "updatedAt": "2024-02-05T21:11:17Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "srstrickland"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 3
    },
    {
        "number": 19757,
        "title": "I would like to ask a question about thread concurrency",
        "bodyText": "We want to use Vector to collect Kubenetes logs, but there are 500 pods on a node. To use vector demonset to collect vector, do we need to start independent threads to collect 500 files? Can such a high thread count be supported? I want to know the detailed threading model of this piece. I hope I can get a reply. Thank you. Oh, by the way, we have replaced logstash.\nVector feels great",
        "url": "https://github.com/vectordotdev/vector/discussions/19757",
        "createdAt": "2024-01-31T06:24:28Z",
        "updatedAt": "2024-02-05T14:37:51Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "xiaoxiongxyy"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19782,
        "title": "When are enrichment tables reloaded?",
        "bodyText": "If a file referenced by an enrichment table changes, is the change automatically detected and the contents reloaded, or does vector need to be restarted? I ask as I don't see any log events emitted to indicate that changes to an enrichment table have been detected, despite using VECTOR_WATCH_CONFIG=true.\nAlso, does it make a difference if enrichment table files are stored in the VECTOR_CONFIG_DIR or elsewhere?",
        "url": "https://github.com/vectordotdev/vector/discussions/19782",
        "createdAt": "2024-02-04T00:08:23Z",
        "updatedAt": "2024-02-06T10:21:08Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "codersaur"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19796,
        "title": "Summary of recent use",
        "bodyText": "A note for the community\n\nProblem\n\ngeoip Acquire   Some ip, such as 146.0.233.147, does not have country_name, but has field value, which is empty, and is not given field in logstash\ntag_cardinality_limit is used  The official example is wrong\nHow are configuration files classified if helm manages them\uff1f\n\nConfiguration\nopenresty_metrics_limit:\n      type: tag_cardinality_limit\n      inputs:\n        - openresty_metrics_counter\n      mode: exact\n      value_limit: 5000\n      limit_exceeded_action: drop_tag\n\nVersion\nvector-0.34.2-1.x86_64\nDebug Output\nNo response\nExample Data\nNo response\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/19796",
        "createdAt": "2024-02-04T09:00:10Z",
        "updatedAt": "2024-02-05T14:36:13Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "baiyibing123"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 19776,
        "title": "Vector Agent as Library",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nDoes Vector have an option to be loaded as a library instead of being used as a agent? I ask due to some architectures moving away from the sidecar approach, things like proxyless grpc or eBPF variants of tools (like Istio). There of course would be pros and cons of this approach.\nI noticed other tickets (for Vector developers) discussing use of crates (or not) to help with build efforts. Maybe a subset could be used in such a form.\nAny thoughts (or links to prior conversations - I didn't find any) appreciated.\nConfiguration\nNo response\nVersion\nvector 0.35.0 (x86_64-unknown-linux-gnu e57c0c0 2024-01-08 14:42:10.103908779)\nDebug Output\nNo response\nExample Data\nNo response\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/19776",
        "createdAt": "2024-02-02T02:36:30Z",
        "updatedAt": "2024-02-02T15:40:27Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ronaldpetty"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 19764,
        "title": "Didn't work",
        "bodyText": "2024-02-01T06:50:43.711605Z ERROR transform{component_kind=\"transform\" component_id=openresty_metrics component_type=log_to_metric}: vector::internal_events::parser: Field does not exist. field=.geoip.geo.country_name error_code=\"field_not_found\" error_type=\"condition_failed\" stage=\"processing\" internal_log_rate_limit=true",
        "url": "https://github.com/vectordotdev/vector/discussions/19764",
        "createdAt": "2024-02-01T09:07:36Z",
        "updatedAt": "2024-02-02T09:23:43Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "baiyibing123"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 19765,
        "title": "Not come into effect",
        "bodyText": "",
        "url": "https://github.com/vectordotdev/vector/discussions/19765",
        "createdAt": "2024-02-01T09:09:14Z",
        "updatedAt": "2024-02-01T09:11:50Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "baiyibing123"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19747,
        "title": "How to add multiple values in vrl",
        "bodyText": "Original log   \uff1a  \"upstream_response_time\":\"0.103,0.203\"\nCompleted using lua\n  my_lua_transform:\n    inputs:\n    - my_transform\n    type: lua\n    version: \"2\"\n    hooks:\n      process: |-\n        function (event, emit)\n          local upstream_response_time = event.log.message.upstream_response_time\n          if type(upstream_response_time) == \"string\" then\n            if string.find(upstream_response_time, \",\") then\n              local result = 0\n              for i in string.gmatch(upstream_response_time, '([^,]+)') do\n                result = result + tonumber(i)\n              end\n              upstream_response_time = result\n            else\n              upstream_response_time = tonumber(upstream_response_time)\n            end\n          end\n          event.log.message.upstream_response_time = upstream_response_time\n          emit(event)\n        end\n\nHow to implement it using vrl \uff1f\neventually need to be achieved\n\"upstream_response_time\":\"0.103,0.203\"  => \"upstream_response_time\":0.306",
        "url": "https://github.com/vectordotdev/vector/discussions/19747",
        "createdAt": "2024-01-30T07:05:35Z",
        "updatedAt": "2024-01-31T06:00:23Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "jiaozi07"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 19702,
        "title": "Expression that can't fail, but fail",
        "bodyText": "Sorry, this is probably a silly thing I'm doing, but I don't get it:\nUsing \"vector vrl\":\nHappy path:\n$ .test = 123\n123\n\n$ .test = to_string(.test)\n\"123\"\n\nBasic failure:\n$ .test = {\"hello\": \"world\"}\n{ \"hello\": \"world\" }\n\n$ .test = to_string(.test)\n\nerror[E103]: unhandled fallible assignment\n  \u250c\u2500 :1:9\n  \u2502\n1 \u2502 .test = to_string(.test)\n  \u2502 ------- ^^^^^^^^^^^^^^^^ this expression is fallible because at least one argument's type cannot be verified to be valid\n  \u2502 \u2502\n  \u2502 or change this to an infallible assignment:\n  \u2502 .test, err = to_string(.test)\n  \u2502\n  = see documentation about error handling at https://errors.vrl.dev/#handling\n  = see functions characteristics documentation at https://vrl.dev/expressions/#function-call-characteristics\n  = learn more about error code 103 at https://errors.vrl.dev/103\n  = see language documentation at https://vrl.dev\n  = try your code in the VRL REPL, learn more at https://vrl.dev/examples\n\n\n$\n\n=> ok, fair enough: let me add \", err\":\n$ .test = {\"hello\": \"world\"}\n{ \"hello\": \"world\" }\n\n$ .test, err = to_string(.test)\n\"function call error for \\\"to_string\\\" at (13:29): unable to coerce { hello: string } into string\"\n\nok, it's happy - but let me pass 123 again now:\n$ .test = 123\n123\n\n$ .test, err = to_string(.test)\n\nerror[E104]: unnecessary error assignment\n  \u250c\u2500 :1:8\n  \u2502\n1 \u2502 .test, err = to_string(.test)\n  \u2502 -----  ^^^   ---------------- because this expression can't fail\n  \u2502 \u2502      \u2502\n  \u2502 \u2502      this error assignment is unnecessary\n  \u2502 use: .test = to_string(.test)\n  \u2502\n  = see documentation about error handling at https://errors.vrl.dev/#handling\n  = learn more about error code 104 at https://errors.vrl.dev/104\n  = see language documentation at https://vrl.dev\n  = try your code in the VRL REPL, learn more at https://vrl.dev/examples\n\n$\n\nsooooo : \"to_string(.test)\" is both faillible and can't fail ?\n=> what am I doing wrong ? and how to make it work for both cases ?\nNotes:\n\nI guess I can add a \"if is_string(.test) or is_float(.test) [...etc]\" before, but that sounds silly\nI tried with \"to_string(.test) ?? \"\"\" with the same results\n\nVector version:\n/ # vector --version\nvector 0.35.0 (x86_64-unknown-linux-musl e57c0c0 2024-01-08 14:42:10.103908779)",
        "url": "https://github.com/vectordotdev/vector/discussions/19702",
        "createdAt": "2024-01-24T23:39:56Z",
        "updatedAt": "2024-01-29T15:28:47Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "jgournet"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19707,
        "title": "Support multi grpc service in one server?",
        "bodyText": "vector/src/sources/util/grpc/mod.rs\n    \n    \n         Line 24\n      in\n      ee9d182\n    \n  \n  \n    \n\n        \n          \n           pub async fn run_grpc_server<S>( \n        \n    \n  \n\n\nHow can we change this fn so that it support multiple grpc service? Now I can just pass one specific service, but what if I have multiple services?\npub async fn run_grpc_server<S>(\n    address: SocketAddr,\n    tls_settings: MaybeTlsSettings,\n    service: S,\n    shutdown: ShutdownSignal,\n) -> crate::Result<()>\nwhere\n    S: Service<Request<Body>, Response = Response<BoxBody>, Error = Infallible>\n        + NamedService\n        + Clone\n        + Send\n        + 'static,\n    S::Future: Send + 'static,\n{\n    let (tx, rx) = tokio::sync::oneshot::channel::<ShutdownSignalToken>();\n    let listener = tls_settings.bind(&address).await?;\n    let stream = listener.accept_stream();\n\n    info!(%address, \"Building gRPC server.\");\n\n    Server::builder()\n        .add_service(service)\n        .serve_with_incoming_shutdown(stream, shutdown.map(|token| tx.send(token).unwrap()))\n        .await?;\n\n    drop(rx.await);\n\n    Ok(())\n}\ntonic itself supports we do like this:\n    Server::builder()\n        .add_service(service1)\n        .add_service(service2)\n        .add_service(...)\nBut I'm not an expert in rust, just I want to know how to change this function to achieve so",
        "url": "https://github.com/vectordotdev/vector/discussions/19707",
        "createdAt": "2024-01-25T12:07:26Z",
        "updatedAt": "2024-01-25T15:52:40Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "caibirdme"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 19665,
        "title": "Hosted Version Documentation",
        "bodyText": "Is there a site that hosts the documentation by released version?",
        "url": "https://github.com/vectordotdev/vector/discussions/19665",
        "createdAt": "2024-01-19T19:14:48Z",
        "updatedAt": "2024-01-28T03:02:24Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "jcantrill"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19655,
        "title": "Converting key-values pairs",
        "bodyText": "Hi Team,\nWe have an requirement where we want to convert one of the JSON field into key-value pairs\nHere we want to convert value of resource into key-value pairs\nInput:\n{\"Name\":\"List\",\"Tier\":\"Hot\",\"average\":20,\"count\":1,\"maximum\":1,\"metricName\":\"Availability\",\"minimum\":100,\"resource\":\"/key1/value1/key2/value2/key3/values3\",\"time\":\"2023-12-14T13:22:00.0000000Z\",\"total\":1}\nExcepted Output:\n{\"Name\":\"List\",\"Tier\":\"Hot\",\"average\":20,\"count\":1,\"maximum\":1,\"metricName\":\"Availability\",\"minimum\":100,\"key1\":\"value1\",\"key2\":\"value2\",\"key3\":\"value3\",\"time\":\"2023-12-14T13:22:00.0000000Z\",\"total\":1}\nPlease help us understand how can we achieve this using vector.",
        "url": "https://github.com/vectordotdev/vector/discussions/19655",
        "createdAt": "2024-01-18T07:31:25Z",
        "updatedAt": "2024-01-18T14:45:29Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "KrishnaJyothika"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19647,
        "title": "What's the fastest source",
        "bodyText": "Hi \ud83d\udc4b\nI am wondering what the fastest source vector can ingest from? Is that source still the fastest if you need to filter out half of the logs for one destination and the other half for another?",
        "url": "https://github.com/vectordotdev/vector/discussions/19647",
        "createdAt": "2024-01-18T00:14:04Z",
        "updatedAt": "2024-01-18T00:14:04Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "enricoschaaf"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 19594,
        "title": "Unit Test Assertion for Counter Fails",
        "bodyText": "Hi,\nI'm trying to write a unit test for an aggregate that aggregates counter metrics. My assertions are as follows:\n    [[tests.outputs.conditions]]\n    type = \"vrl\"\n    source = '''\n    assert_eq!(.tags.service, \"product_1.component_1\")\n    assert_eq!(.tags.location, \"environment_1.datacenter_1.shard_1\")\n    assert_eq!(.counter.value, 2)\n    '''\nThe first 2 assertions on the tag work correctly, but the third assertion fails with a blatantly contradicting error message (the output payload explicitly shows the value of .counter.value being equal to 2.0:\nRunning tests\ntest logs_usage_audit ... failed\n\nfailures:\n\ntest logs_usage_audit:\n\ncheck[0] for transforms [\"minutely_logs_usage_audit\"] failed conditions:\n\n  condition[0]: source execution failed: \nerror[E000]: function call error for \"assert_eq\" at (128:151): assertion failed: null == 2\n  \u250c\u2500 :3:5\n  \u2502\n3 \u2502     assert_eq!(.counter.value, 2)\n  \u2502     ^^^^^^^^^^^^^^^^^^^^^^^ assertion failed: null == 2\n  \u2502\n  = see language documentation at https://vrl.dev\n  = try your code in the VRL REPL, learn more at https://vrl.dev/examples\n\n\noutput payloads from [\"minutely_logs_usage_audit\"] (events encoded as JSON):\n  {\"name\":\"logs_usage_audit\",\"tags\":{\"location\":\"environment_1.datacenter_1.shard_1\",\"service\":\"product_1.component_1\"},\"timestamp\":\"2024-01-11T18:14:47.209476506Z\",\"kind\":\"incremental\",\"counter\":{\"value\":2.0}}\n\nThis also fails and claims a value of null when I try assert_eq!(.counter, 2). It seems like what I'm doing is very similar to \n  \n    \n      vector/tests/behavior/transforms/task_transform.toml\n    \n    \n         Line 90\n      in\n      2448a72\n    \n  \n  \n    \n\n        \n          \n                     assert_eq!(.counter, 2, \"incorrect counter\") \n        \n    \n  \n\n. Any insights appreciated!",
        "url": "https://github.com/vectordotdev/vector/discussions/19594",
        "createdAt": "2024-01-11T18:21:40Z",
        "updatedAt": "2024-01-12T17:14:37Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "nzxwang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19592,
        "title": "API url",
        "bodyText": "Is there a way to configure the API url, say to serve the graphql endpoint under a subpath?",
        "url": "https://github.com/vectordotdev/vector/discussions/19592",
        "createdAt": "2024-01-11T17:18:31Z",
        "updatedAt": "2024-01-11T18:08:37Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tr11"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19590,
        "title": "Vector with Kafka",
        "bodyText": "Something strange with Vector:\nI have 3 VM with Vector container and 1-st can't push logs to Kafka.\nMy config:\ndata_dir = \"/var/lib/vector\"\n\n[api]\n    enabled = true # optional, default\n    address = \"0.0.0.0:8686\" # optional, default\n    playground = true # optional, default\n\n[sources.vector_metrics]\n    type = \"internal_metrics\"\n    scrape_interval_secs = 15\n\n[sinks.my_prometheus_exporter_sink]\n    type = \"prometheus_exporter\"\n    inputs = [ \"vector_metrics\" ]\n    flush_period_secs = 30\n    address = \"0.0.0.0:8086\"\n\n[sources.internal_logs_source]\n    type = \"internal_logs\"\n\n[transforms.internal_logs_remap]\n    type = \"remap\"\n    inputs = [\"internal_logs_source\"]\n    source = '''\n    .dc = \"my_dc\"\n    .env = \"my_env\"\n    .group = \"my_group\"\n    .inst = \"my_instance\"\n    if exists(.system) && .system != null {\n      .system = downcase!(.system)\n    }\n    if !exists(.@timestamp) {\n      if exists(.timestamp) && .timestamp != null {\n        .@timestamp = .timestamp\n      } else {\n        .@timestamp = now()\n      }\n    }\n    del(.timestamp)\n\n    .system = \"vector\"\n    del(.level)\n    '''\n\n[sources.netdata_docker_source]\n    type = \"docker_logs\"\n    include_containers = [\"netdata\"]\n\n[transforms.netdata_logs_remap]\n    type = \"remap\"\n    inputs = [\"netdata_docker_source\"]\n    source = '''\n    .dc = \"my_dc\"\n    .env = \"my_env\"\n    .group = \"my_group\"\n    .inst = \"my_instance\"\n    if exists(.system) && .system != null {\n      .system = downcase!(.system)\n    }\n    if !exists(.@timestamp) {\n      if exists(.timestamp) && .timestamp != null {\n        .@timestamp = .timestamp\n      } else {\n        .@timestamp = now()\n      }\n    }\n    del(.timestamp)\n\n    .system = \"netdata\"\n    '''\n\n[sources.dockerd_logs]\n    type = \"journald\" # required\n    current_boot_only = false # optional, default\n    journalctl_path = \"/etc/vector/journalctl\"\n    exclude_units = [] # optional, default\n    include_units = [\"docker.service\"] # optional, default\n\n[transforms.dockerd_logs_add_fields]\n    type = \"remap\"\n    inputs = [\"dockerd_logs\"]\n    source = '''\n    .dc = \"ds\"\n    .env = \"qa\"\n    .group = \"my_group\"\n    .inst = \"my_instance\"\n    if exists(.system) && .system != null {\n      .system = downcase!(.system)\n    }\n    if !exists(.@timestamp) {\n      if exists(.timestamp) && .timestamp != null {\n        .@timestamp = .timestamp\n      } else {\n        .@timestamp = now()\n      }\n    }\n    del(.timestamp)\n\n    .system = \"dockerd\"\n    '''\n\n[sources.sshd_logs]\n    type = \"journald\" # required\n    current_boot_only = false # optional, default\n    journalctl_path = \"/etc/vector/journalctl\"\n    exclude_units = [] # optional, default\n    include_units = [\"ssh.service\"] # optional, default\n\n[transforms.sshd_logs_add_fields]\n    type = \"remap\"\n    inputs = [\"sshd_logs\"]\n    source = '''\n    .dc = \"my_dc\"\n    .env = \"my_env\"\n    .group = \"my_group\"\n    .inst = \"my_instance\"\n    if exists(.system) && .system != null {\n      .system = downcase!(.system)\n    }\n    if !exists(.@timestamp) {\n      if exists(.timestamp) && .timestamp != null {\n        .@timestamp = .timestamp\n      } else {\n        .@timestamp = now()\n      }\n    }\n    del(.timestamp)\n\n    .system = \"sshd\"\n    '''\n\n[sources.keydb_docker_source]\n  type = \"docker_logs\"\n  include_containers = [\"keydb\"]\n\n[sources.sentinel_docker_source]\n  type = \"docker_logs\"\n  include_containers = [\"sentinel\"]\n\n[transforms.keydb_logs_remap]\n  type = \"remap\"\n  inputs = [\"keydb_docker_source\"]\n  source = '''\n    .dc = \"my_dc\"\n    .env = \"my_env\"\n    .group = \"my_group\"\n    .inst = \"my_instance\"\n    if exists(.system) && .system != null {\n      .system = downcase!(.system)\n    }\n    if !exists(.@timestamp) {\n      if exists(.timestamp) && .timestamp != null {\n        .@timestamp = .timestamp\n      } else {\n        .@timestamp = now()\n      }\n    }\n    del(.timestamp)\n\n  .system = \"my_system\"\n  .cluster = \"my_cluster\"\n  '''\n\n[transforms.sentinel_logs_remap]\n  type = \"remap\"\n  inputs = [\"sentinel_docker_source\"]\n  source = '''\n    .dc = \"my_dc\"\n    .env = \"my_env\"\n    .group = \"my_group\"\n    .inst = \"my_instance\"\n    if exists(.system) && .system != null {\n      .system = downcase!(.system)\n    }\n    if !exists(.@timestamp) {\n      if exists(.timestamp) && .timestamp != null {\n        .@timestamp = .timestamp\n      } else {\n        .@timestamp = now()\n      }\n    }\n    del(.timestamp)\n\n  .system = \"my_system\"\n  .cluster = \"My_cluster\"\n  '''\n\n\n[sinks.log_sink_kafka]\n    type = \"kafka\"\n                                            inputs = [\"internal_logs_remap\",\"netdata_logs_remap\",\"sshd_logs_add_fields\",\"dockerd_logs_add_fields\",\"sentinel_logs_remap\",\"keydb_logs_remap\"]\n        bootstrap_servers = \"Kafka-nodes\"\n    key_field = \"my_key\"\n    topic = \"my_topik\"\n    message_timeout_ms = 300000 \n    socket_timeout_ms = 60000 \n    compression = \"gzip\"\n\n    buffer.max_size = 1049000000\n    buffer.type = \"disk\"\n\n    librdkafka_options.\"client.id\" = \"client_id\"\n    librdkafka_options.\"message.max.bytes\" = \"1000000\"\n    librdkafka_options.\"max.in.flight.requests.per.connection\" = \"5\"\n    librdkafka_options.\"metadata.request.timeout.ms\" = \"300000\"\n    librdkafka_options.\"socket.timeout.ms\" = \"60000\"\n    librdkafka_options.\"socket.keepalive.enable\" = \"true\"\n    librdkafka_options.\"socket.max.fails\" = \"1\"\n    librdkafka_options.\"api.version.request\" = \"false\"\n    librdkafka_options.\"transaction.timeout.ms\" = \"2147483647\"\n    librdkafka_options.\"enable.idempotence\" = \"false\"\n    librdkafka_options.\"queue.buffering.max.messages\" = \"100000\"\n    librdkafka_options.\"queue.buffering.max.kbytes\" = \"100100000000000000\"\n    librdkafka_options.\"queue.buffering.max.ms\" = \"50\"\n    librdkafka_options.\"message.send.max.retries\" = \"10000\"\n    librdkafka_options.\"compression.codec\" = \"gzip\"\n    librdkafka_options.\"compression.type\" = \"gzip\"\n    librdkafka_options.\"compression.level\" = \"4\"\n    librdkafka_options.\"batch.num.messages\" = \"100\"\n    librdkafka_options.\"batch.size\" = \"2147483647\"\n    librdkafka_options.\"request.required.acks\" = \"-1\"\n    librdkafka_options.\"request.timeout.ms\" = \"900000\"\n    librdkafka_options.\"message.timeout.ms\" = \"300000\"\n    librdkafka_options.\"log_level\" = \"0\"\n\n    encoding.codec = \"json\"\n    encoding.except_fields = [\"label\",\"container_created_at\", \"container_id\", \"container_name\", \"image\", \"source_type\", \"stream\",\"timestamp\",\"LVL\"]\n\nAnd my error:\n{\"timestamp\":\"2024-01-11T11:23:31.149786Z\",\"level\":\"ERROR\",\"message\":\"Configuration error.\",\"error\":\"Sink \\\"log_sink_kafka\\\": error occurred when building buffer: failed to build individual stage 0: failed to load/create ledger: failed to lock buffer.lock; is another Vector process running and using this buffer?\",\"target\":\"vector::topology::builder\"}\n\nBefore I use version image 0.32.0-debian and now upgrade to 0.35.0-debian",
        "url": "https://github.com/vectordotdev/vector/discussions/19590",
        "createdAt": "2024-01-11T12:44:37Z",
        "updatedAt": "2024-01-11T15:09:12Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "quwassar"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 19587,
        "title": "Is there a way to debug or route a log to a sink only if the send failed?",
        "bodyText": "Hi,\nWe are seeing some failures to send events to GCP PubSub sink. The error is about the event schema Invalid data in message: Message failed schema validation.. There are some details but we would like to see the event itself or at least get the event ID in order to find the source of the malformed events.\nIs there a way to dump the event or a specific field to Vector stdout logs when an error to send happens? I would imagine an option to route only failed sends to a specific sinks. Maybe if we could query the result of the send in VRL or something.\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/19587",
        "createdAt": "2024-01-11T10:08:06Z",
        "updatedAt": "2024-01-23T21:00:17Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "doron-cohen"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19577,
        "title": "Parsing unix milisecond timestamps",
        "bodyText": "My JSON events log their timestamp as a unix timestamp in miliseconds, i.e. 1704895500140. I want to parse these into a timestamp object so I can reformat them as something more human readable in the output.\nWith parse_timestamp I've tried both %s%fand %s%3f`for the format string but just get Invalid Timestamp errors.\nCan anyone suggest a way to parse these correctly?",
        "url": "https://github.com/vectordotdev/vector/discussions/19577",
        "createdAt": "2024-01-10T14:12:33Z",
        "updatedAt": "2024-01-10T16:02:07Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "NeilJed"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 19564,
        "title": "how to Force an object type into a string field",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nUse Cases\nNo response\nAttempted Solutions\nNo response\nProposal\nNo response\nReferences\nNo response\nVersion\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/19564",
        "createdAt": "2024-01-09T06:01:05Z",
        "updatedAt": "2024-01-10T06:00:39Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "frankbaozun"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19507,
        "title": "Supports capturing network packets as a data source",
        "bodyText": "Hi, there! It seems that Vector does not support capturing network packets from a specific network interface as Source, any plans on it? This way we can analyze network traffic and store the results, like src/dest ip address/port, tcp flags... and so on, which may be useful to monitor network traffic in some situations",
        "url": "https://github.com/vectordotdev/vector/discussions/19507",
        "createdAt": "2024-01-03T08:30:00Z",
        "updatedAt": "2024-01-04T02:20:26Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bitcapybara"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 17640,
        "title": "compile error when I do 'cargo build'",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nwhen I execute \u2018cargo build\u2019, it reports an error.\n\"[failed to run custom build command for vector-core v0.1.0](error: failed to run custom build command for vector-core v0.1.0)\"\nCaused by:\nprocess didn't exit successfully: /vector_back/target/debug/build/vector-core-ee2ba90d479c8fb4/build-script-build (exit status: 101)\n--- stdout\ncargo:rerun-if-changed=proto/event.proto\n--- stderr\nthread 'main' panicked at 'called Result::unwrap() on an Err value: Custom { kind: Other, error: \"protoc failed: Unknown flag: --experimental_allow_proto3_optional\\n\" }', lib/vector-core/build.rs:8:10\nnote: run with RUST_BACKTRACE=1 environment variable to display a backtrace\nConfiguration\nNo response\nVersion\nmaster branch\nDebug Output\nNo response\nExample Data\nNo response\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/17640",
        "createdAt": "2023-06-08T11:30:02Z",
        "updatedAt": "2023-12-26T16:54:46Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Lempossible"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 19409,
        "title": "Load Balancing issue with HTTP source/sink and Vector source/sink",
        "bodyText": "Hi\nIssue 1: Load Balancing with HTTP Source and Sink in Kubernetes Environment\nDescription:\nI am currently facing an issue with load balancing when using two vectors that connect to each other with HTTP as the source and sink in a Kubernetes environment. Specifically, when I increase the replica count of the HTTP server vector, the traffic does not seem to be evenly distributed among the replicas. I have checked for keep alive configuration and other related settings but haven't found a solution.\nI would appreciate any guidance or assistance in figuring out how to achieve effective load balancing for the HTTP traffic when increasing the replica count in a Kubernetes environment.\nIssue 2: Loss of Basic Auth Feature with Vector as Source and Sink\nDescription:\nAnother challenge I'm facing is that when I use the vector type as both the source and sink for these two vectors, I seem to lose the basic authentication feature.\nI would like to understand if there are any updates or improvements planned or any open issue for the addition of basic authentication features when using Vector as both the source and sink.",
        "url": "https://github.com/vectordotdev/vector/discussions/19409",
        "createdAt": "2023-12-18T09:14:04Z",
        "updatedAt": "2023-12-22T19:31:10Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 19434,
        "title": "Converting protobuf bytes to JSON",
        "bodyText": "Hello,\nI'm trying to convert some protobuf messages to JSON for debugging purposes, and I noticed that the resulting JSON contains the literal bytes of bytes fields in protobuf messages.  Is there a way to have protobuf bytes fields be encoded as base64 strings instead?\nMy vector.toml file:\n[sources.http]\ntype = \"http_server\"\naddress = \"0.0.0.0:4318\"\nstrict_path = false\ndecoding.codec = \"protobuf\"\ndecoding.protobuf.desc_file = \"trace.desc\"\ndecoding.protobuf.message_type = \"opentelemetry.proto.trace.v1.TracesData\"\n\n[sinks.console]\ntype = \"console\"\ninputs = [ \"*\" ]\nencoding.codec = \"json\"\n\nJSON output sample (elided and reformatted):\n{\n  \"path\": \"/v1/traces\",\n  \"resource_spans\": [\n    {\n      \"schema_url\": \"\",\n      \"scope_spans\": [\n          \"spans\": [\n            {\n              \"attributes\": [],\n              \"dropped_attributes_count\": 0,\n              \"dropped_events_count\": 0,\n              \"dropped_links_count\": 0,\n              \"end_time_unix_nano\": 1703091504825214464,\n              \"events\": [],\n              \"flags\": 0,\n              \"kind\": \"SPAN_KIND_INTERNAL\",\n              \"links\": [],\n              \"name\": \"kong.header_filter.plugin.opentelemetry\",\n              \"parent_span_id\": \"J@\ufffd\ufffd\ufffd\u0361L\",\n              \"span_id\": \"\ufffdb\ufffd4\\u000f\ufffd\ufffd\\n\",\n              \"start_time_unix_nano\": 1703091504825208064,\n              \"status\": {\n                \"code\": \"STATUS_CODE_UNSET\",\n                \"message\": \"\"\n              },\n              \"trace_id\": \"\u06f8\ufffd\ufffd\u067e_\ufffdy\u011f\ufffdHl\ufffd\ufffd\",\n              \"trace_state\": \"\"\n            }\n          ]\n        }\n      ]\n    }\n  ],\n  \"source_type\": \"http_server\",\n  \"timestamp\": \"2023-12-20T16:58:24.834998494Z\"\n}\n\nNotice the parent_span_id, span_id and trace_id containing the unencoded byte data.\nThank you!\nHans",
        "url": "https://github.com/vectordotdev/vector/discussions/19434",
        "createdAt": "2023-12-20T17:11:34Z",
        "updatedAt": "2023-12-21T08:49:17Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "hanshuebner"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19433,
        "title": "source_lag_time_seconds get large number",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nsource\uff1a kubernetes_logs\nout \uff1akafka\nwe find lag is very big\uff0cI would like to ask, what is the possible reason? thanks\n\nConfiguration\nin_k8s:\n    auto_partial_merge: true\n    data_dir: /var/lib/vector\n    delay_deletion_ms: 60000\n    exclude_paths_glob_patterns:\n      - '**/vector/**'\n      - '**/istio-init/**'\n      - '**/filebeat/**'\n      - '**/vmagent/**'\n      - '**/*.gz'\n      - '**/*.log.*'\n      - '**/vector-logfilter/**'\n    fingerprint_lines: 1\n    glob_minimum_cooldown_ms: 60000\n    max_line_bytes: 32768\n    max_read_bytes: 4096\n    self_node_name: '${VECTOR_SELF_NODE_NAME}'\n    timezone: local\n    type: kubernetes_logs\n    ignore_older_secs: 1800\n    use_apiserver_cache: true\n\nVersion\n0.33.1\nDebug Output\nNo response\nExample Data\nNo response\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/19433",
        "createdAt": "2023-12-13T10:14:51Z",
        "updatedAt": "2023-12-21T02:42:52Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "zzmg"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 19423,
        "title": "Configuration schema",
        "bodyText": "Hey, is there a schema of the Vector config available somewhere?\nWe're using CUE to generate our vector config, and we would like to add typing to it.\nSomething like a Json schema would be useful.",
        "url": "https://github.com/vectordotdev/vector/discussions/19423",
        "createdAt": "2023-12-19T08:32:59Z",
        "updatedAt": "2023-12-19T18:41:05Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "b4nst"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19415,
        "title": "Vector Elasticsearch Sink variable index",
        "bodyText": "I'm currently trying to index into different indices on my configuration when using the Elasticsearch Sink. Like documented, I'm using a variable there:\n[sinks.elasticsearch.bulk]\nindex = \"{{elastic_index}}-%Y-%m-%d-%H\"\nThe elastic_index is set here in the VRL before:\n[transforms.add-elastic-index]\ninputs        = [ \"remote-vector\" ]\ntype          = \"remap\"\ndrop_on_error = false\n\nsource        = '''\nif .msg_type != null {\n\telastic_index = .msg_type\n\tdel(.msg_type)\n} else {\n        elastic_index = \"cache-accesslogs\"\n}\n'''\nI'm receiving this in the vector error log:\nDec 18 18:00:08 nbg-stats-01 vector[295315]: 2023-12-18T18:00:08.921786Z ERROR sink{component_kind=\"sink\" component_id=elasticsearch component_type=elasticsearch}: vector::internal_events::template: Failed to render template for \"index\". error=Missing fields on event: [\"elastic_index\"] error_type=\"template_failed\" stage=\"processing\" internal_log_rate_limit=true\n\nApparently elastic_index is not available as a variable anymore.\nHow can I pass the variable elastic_index? I assume, I have to use .elastic_index in the VRL. But this will push the variable unnecessarily onto the index, too.\nIs it possible to pass variables between the different stages, which are not part of the event?",
        "url": "https://github.com/vectordotdev/vector/discussions/19415",
        "createdAt": "2023-12-18T18:06:01Z",
        "updatedAt": "2023-12-18T19:17:21Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "bebehei"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19412,
        "title": "ransforms resolves nested arrays by remap",
        "bodyText": "Received json data using udp mode of socket, where locs is nested array, The following \"locs\" : [[[1277579919065406, 1170448210, 00,14,109,2,9,1702448920147], [460,1,265747202,64839], 'null', [], [\" move \"]]]. transforms is remap, which transforms me into a nested array of locs by splitting it into a new array like [\"status\":1,\"nat\":2776579......]",
        "url": "https://github.com/vectordotdev/vector/discussions/19412",
        "createdAt": "2023-12-16T02:58:43Z",
        "updatedAt": "2023-12-18T15:55:23Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "850259571"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 19362,
        "title": "How do you process an entire CSV file all at one time",
        "bodyText": "The complete csv file arrives in source folder and I am using lua to load the entire csv into an array then I emit each cell that matches of each line that matches my criteria until EOF. The issue is that vector reprocesses the source csv file for every row in the csv file.\nI have attempted to use multiline from BOF to EOF and it still processes line by line.\nI have also tried to remove file using remove_after_secs and that didn't work either.",
        "url": "https://github.com/vectordotdev/vector/discussions/19362",
        "createdAt": "2023-12-12T00:04:34Z",
        "updatedAt": "2023-12-15T17:59:59Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Manniac07"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19374,
        "title": "Kubernetes source",
        "bodyText": "Hi! I'm wondering if Kubernetes source (https://vector.dev/docs/reference/configuration/sources/kubernetes_logs/) also supports Kubernetes audit logs gathering (if audit enabled)? I found #1424 but didn't get if it was implemented or not.\nAnd if audit logs supported what is the structure of such events that exposed by Vector.dev Kubernetes source?",
        "url": "https://github.com/vectordotdev/vector/discussions/19374",
        "createdAt": "2023-12-13T09:07:46Z",
        "updatedAt": "2023-12-15T10:06:07Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "inatale"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 19396,
        "title": "Kubernetes source: stderr instead of stdout for all logs",
        "bodyText": "Hi al!!\nWe use Kubernetes source with the following config:\nrole: Agent\n\nlogLevel: \"warn\"\n\nresources:\n  requests:\n    cpu: 200m\n    memory: 256Mi\n  limits:\n    cpu: 200m\n    memory: 256Mi\n\ntolerations:\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/master\n    operator: Exists\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/control-plane\n    operator: Exists\n\ncustomConfig:\n  data_dir: \"/var/lib/vector\"\n  \n  api:\n    enabled: true\n    address: 127.0.0.1:8686\n    playground: false\n  \n  log_schema:\n    timestamp_key: \"ts_collected\"\n\n  sources:\n    kuberentes_logs:\n      type: kubernetes_logs\n    kubernetes_audit_logs:\n      type: file\n      include:\n        - /var/log/kubernetes/audit/*.log\n    vector_internal_logs:\n      type: internal_logs  \n\n  transforms:\n    vector_internal_logs_transformed:\n      type: remap\n      inputs: \n      - vector_internal_logs\n      source: |-\n        .message = encode_json(.)\n        .labels.product = \"<name>\"\n        .labels.component = \"Vector.dev\"\n        .severity = upcase!(.metadata.level)\n    kuberentes_logs_transformed:\n      type: remap\n      inputs: \n      - kuberentes_logs\n      source: |-\n        if exists(.pod_labels.\"controller-revision-hash\") {\n          del(.pod_labels.\"controller-revision-hash\")\n        }\n        if exists(.pod_labels.\"pod-template-hash\") {\n          del(.pod_labels.\"pod-template-hash\")\n        }\n        # if (.stream == \"stderr\") {\n        #   .severity = \"ERROR\"\n        # } else {\n        #   .severity = \"INFO\"\n        # }\n        .host = .kubernetes.pod_node_name\n        .timestamp = .ts_collected\n        .labels.product = \"<name>\"\n        .labels.component = \"Kubernetes\"\n        .labels = merge(.labels, map_keys(object!(.kubernetes.pod_labels), recursive: true) -> |key| {  \"pod_labels:\" + key })\n        .labels = merge(.labels, map_keys(object!(.kubernetes.namespace_labels), recursive: true) -> |key| {  \"namespace_labels:\" + key })\n        .labels = merge(.labels, map_keys(object!(.kubernetes.node_labels), recursive: true) -> |key| {  \"node_labels:\" + key })\n        .labels.container_id = .kubernetes.container_id\n        .labels.container_image = .kubernetes.container_image\n        .labels.container_name = .kubernetes.container_name\n        .labels.pod_ip = .kubernetes.pod_ip\n        .labels.pod_name = .kubernetes.pod_name\n        .labels.pod_namespace = .kubernetes.pod_namespace\n        .labels.pod_node_name = .kubernetes.pod_node_name\n        .labels.pod_owner = .kubernetes.pod_owner\n        .labels.pod_uid = .kubernetes.pod_uid\n        .labels.file = .file \n        del(.kubernetes)\n        del(.file)\n  sinks:\n    vector_cached:\n      inputs:\n      - kuberentes_logs_transformed\n      - vector_internal_logs_transformed\n      type: vector\n      address: \"{{ vector.upstream.endpoint_dev }}\"\n      compression: false\n\nWe always get logs with stream set to stderr for our own components in k8s cluster (workload pods) and for Kubernetes kube-system pods. It's very strange that it's always stderr. Of course it depends on app but we suppose that all apps could not write to stderr instaed of stdout.\nSmaple output from logs:\n{\n   \"file\":\"/var/log/pods/kube-system_cilium-5skkj_3ffdbda7-7a4b-43f2-b5f8-5e22f5b15f57/cilium-agent/4.log\",\n   \"kubernetes\":{\n      \"container_id\":\"cri-o://045b45e6b378ef7fdf622c87d7bf61105df7ffdb4db14f4afd1b3af68e118764\",\n      \"container_image\":\"<registry>/cilium/cilium:v1.13.4\",\n      \"container_image_id\":\"<registry>/cilium/cilium@sha256:27684c92eae4f1d870182293767daf4d40db726185e5b87a1f16390d7215f88a\",\n      \"container_name\":\"cilium-agent\",\n      \"namespace_labels\":{\n         \"kubernetes.io/metadata.name\":\"kube-system\"\n      },\n      \"node_labels\":{\n         \"beta.kubernetes.io/arch\":\"amd64\",\n         \"beta.kubernetes.io/os\":\"linux\",\n         \"kubernetes.io/arch\":\"amd64\",\n         \"kubernetes.io/hostname\":\"tr-monitoring-4\",\n         \"kubernetes.io/os\":\"linux\"\n      },\n      \"pod_annotations\":{\n         \"scheduler.alpha.kubernetes.io/tolerations\":\"[{\\\"key\\\":\\\"dedicated\\\",\\\"operator\\\":\\\"Equal\\\",\\\"value\\\":\\\"master\\\",\\\"effect\\\":\\\"NoSchedule\\\"}]\"\n      },\n      \"pod_ip\":\"10.177.104.64\",\n      \"pod_ips\":[\n         \"10.177.104.64\"\n      ],\n      \"pod_labels\":{\n         \"controller-revision-hash\":\"65994d68df\",\n         \"k8s-app\":\"cilium\",\n         \"pod-template-generation\":\"1\"\n      },\n      \"pod_name\":\"cilium-5skkj\",\n      \"pod_namespace\":\"kube-system\",\n      \"pod_node_name\":\"tr-monitoring-4\",\n      \"pod_owner\":\"DaemonSet/cilium\",\n      \"pod_uid\":\"3ffdbda7-7a4b-43f2-b5f8-5e22f5b15f57\"\n   },\n   \"labels\":{\n      \"component\":\"Kubernetes\",\n      \"container_id\":\"cri-o://045b45e6b378ef7fdf622c87d7bf61105df7ffdb4db14f4afd1b3af68e118764\",\n      \"container_image\":\"registry.trlinux.ru/rubik/cilium/cilium:v1.13.4\",\n      \"container_name\":\"cilium-agent\",\n      \"namespace_labels:kubernetes.io/metadata.name\":\"kube-system\",\n      \"node_labels:beta.kubernetes.io/arch\":\"amd64\",\n      \"node_labels:beta.kubernetes.io/os\":\"linux\",\n      \"node_labels:kubernetes.io/arch\":\"amd64\",\n      \"node_labels:kubernetes.io/hostname\":\"tr-monitoring-4\",\n      \"node_labels:kubernetes.io/os\":\"linux\",\n      \"pod_ip\":\"10.177.104.64\",\n      \"pod_labels:controller-revision-hash\":\"65994d68df\",\n      \"pod_labels:k8s-app\":\"cilium\",\n      \"pod_labels:pod-template-generation\":\"1\",\n      \"pod_name\":\"cilium-5skkj\",\n      \"pod_namespace\":\"kube-system\",\n      \"pod_node_name\":\"tr-monitoring-4\",\n      \"pod_owner\":\"DaemonSet/cilium\",\n      \"pod_uid\":\"3ffdbda7-7a4b-43f2-b5f8-5e22f5b15f57\",\n      \"product\":\"<Product>\"\n   },\n   \"message\":\"level=info msg=\\\"Successful endpoint creation\\\" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=582 identity=8925 ipv4= ipv6= k8sPodName=/ subsys=daemon\",\n   \"severity\":\"ERROR\",\n   \"source_type\":\"kubernetes_logs\",\n   \"stream\":\"stderr\",\n   \"ts_collected\":\"2023-12-12T07:06:40.788328367Z\"\n}\n\n\nWe were going to use stream to split between info and critical logs but now it seems not possible. Is it expected Kubernetes source Vector.dev behavior or do we misconfigure something? The config was tested on 2 different clusters (on prem and cloud). We use 0.34.1 Vector.dev version and 0.29.0 Vector.dev chart.\nMaybe it's related somehow relate to future enhancement in k8s (kubernetes/enhancements#3288).\nAny suggestion would be appriciated.",
        "url": "https://github.com/vectordotdev/vector/discussions/19396",
        "createdAt": "2023-12-15T10:05:35Z",
        "updatedAt": "2023-12-15T10:05:36Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "inatale"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 19278,
        "title": "Benchmarking multiple source/sink combinations",
        "bodyText": "Benchmark\nAMD Epyc 9554 (128 core)\nTCP inputs are fed with 50GB file containing 8M log events in syslog RFC5524 format\nEach input is being sent the same 50GB file, so 4 inputs will process total of 200GB\nVRL remap is CPU intensive 250 lines of code.\nAt first we were assuming that CPU will be bottleneck for our pipelines because of\nintensive VRL remapping, but it turned out that with single source/sink we simply can't\nsaturate more than 20 CPU cores.\nIn production we pull data from Kafka and sink it into Elasticsearch. Issue is that with\none source and sink per vector server we can't utilize more than 20 cores.\nWhat we do now is duplicate sources and sinks in order to utilize CPU more. This is working\nfine but we'd like to get to bottom of this and hopefuly be able to run single source/sink\nper server.\nIssue #18164 prompted me to run these tests and see if we can avoid running multiple inputs/sinks.\nResults\n4in,4vrl,4s combination (4 inputs, 4 VRL remaps and 4 sinks) is winning combination for throughput,\nand seems like only one that can scale verticaly to use most of the CPU available\nSeems like sink is limiting factor, even though blackhole should not have any limitations.\nThis can be seen with 1in,1vrl,4s and 1in,4vrl,4s which both utilize CPU much better than any\nof other combinations that have just single sink.\nInsights and comments from someone with more vector.dev exp would be really appriciated here.\n\n\n\n\n4in,4vrl,4s\n4in,4vrl,1s\n4in,1vrl,1s\n1in,1vrl,1s\n1in,1vrl,4s\n1in,4vrl,4s\n\n\n\n\ninput\n45k/s\n16k/s\n14k/s\n60k/s\n80k/s\n35k/s\n\n\ninput total\n180k/s\n64k/s\n55k/s\n60k/s\n80k/s\n140k/s\n\n\nsink\n45k/s\n64k/s\n55k/s\n60k/s\n20k/s\n35k/s\n\n\nsink total\n180k/s\n64k/s\n55k/s\n60k/s\n80k/s\n140k/s\n\n\ncpu\n6800%\n2300%\n2200%\n1900%\n3400%\n5000%\n\n\nvrl util\n0.15\n0.05\n0.16\n0.16\n0.25\n0.4\n\n\nsink util\n0.75\n0.95\n0.77\n0.74\n0.38\n0.55\n\n\ntime\n2m52.244s\n7m46.586s\n8m55.302s\n2m14.218s\n1m40.827s\n0m55.454s\n\n\n\nBellow are configs used for testing\n4 TCP sources into 4 VRL into 4 sinks\n[sources.tcp-1]\n    type = \"socket\"\n    address = \"0.0.0.0:9001\"\n    mode = \"tcp\"\n\n[sources.tcp-2]\n    type = \"socket\"\n    address = \"0.0.0.0:9002\"\n    mode = \"tcp\"\n\n[sources.tcp-3]\n    type = \"socket\"\n    address = \"0.0.0.0:9003\"\n    mode = \"tcp\"\n\n[sources.tcp-4]\n    type = \"socket\"\n    address = \"0.0.0.0:9004\"\n    mode = \"tcp\"\n\n[transforms.parse-1]\n    inputs = [ \"tcp-1\" ]\n    type = \"remap\"\n    file = \"adn.vrl\"\n\n[transforms.parse-2]\n    inputs = [ \"tcp-2\" ]\n    type = \"remap\"\n    file = \"adn.vrl\"\n\n[transforms.parse-3]\n    inputs = [ \"tcp-3\" ]\n    type = \"remap\"\n    file = \"adn.vrl\"\n\n[transforms.parse-4]\n    inputs = [ \"tcp-4\" ]\n    type = \"remap\"\n    file = \"adn.vrl\"\n\n[sinks.blackhole-1]\n    type = \"blackhole\"\n    inputs = [ \"parse-1\" ]\n\n[sinks.blackhole-2]\n    type = \"blackhole\"\n    inputs = [ \"parse-2\" ]\n\n[sinks.blackhole-3]\n    type = \"blackhole\"\n    inputs = [ \"parse-3\" ]\n\n[sinks.blackhole-4]\n    type = \"blackhole\"\n    inputs = [ \"parse-4\" ]\n4 TCP sources into 4 VRL into 1 sink\n[sources.tcp-1]\n    type = \"socket\"\n    address = \"0.0.0.0:9001\"\n    mode = \"tcp\"\n\n[sources.tcp-2]\n    type = \"socket\"\n    address = \"0.0.0.0:9002\"\n    mode = \"tcp\"\n\n[sources.tcp-3]\n    type = \"socket\"\n    address = \"0.0.0.0:9003\"\n    mode = \"tcp\"\n\n[sources.tcp-4]\n    type = \"socket\"\n    address = \"0.0.0.0:9004\"\n    mode = \"tcp\"\n\n[transforms.parse-1]\n    inputs = [ \"tcp-1\" ]\n    type = \"remap\"\n    file = \"adn.vrl\"\n\n[transforms.parse-2]\n    inputs = [ \"tcp-2\" ]\n    type = \"remap\"\n    file = \"adn.vrl\"\n\n[transforms.parse-3]\n    inputs = [ \"tcp-3\" ]\n    type = \"remap\"\n    file = \"adn.vrl\"\n\n[transforms.parse-4]\n    inputs = [ \"tcp-4\" ]\n    type = \"remap\"\n    file = \"adn.vrl\"\n\n[sinks.blackhole]\n    type = \"blackhole\"\n    inputs = [ \"parse-*\" ]\n4 TCP sources into 1 VRL into 1 sink\n[sources.tcp-1]\n    type = \"socket\"\n    address = \"0.0.0.0:9001\"\n    mode = \"tcp\"\n\n[sources.tcp-2]\n    type = \"socket\"\n    address = \"0.0.0.0:9002\"\n    mode = \"tcp\"\n\n[sources.tcp-3]\n    type = \"socket\"\n    address = \"0.0.0.0:9003\"\n    mode = \"tcp\"\n\n[sources.tcp-4]\n    type = \"socket\"\n    address = \"0.0.0.0:9004\"\n    mode = \"tcp\"\n\n[transforms.parse]\n    inputs = [ \"tcp-*\" ]\n    type = \"remap\"\n    file = \"adn.vrl\"\n\n[sinks.blackhole]\n    type = \"blackhole\"\n    inputs = [ \"parse\" ]\n1 TCP source into 1 VRL into 1 sink\n[sources.tcp]\n    type = \"socket\"\n    address = \"0.0.0.0:9001\"\n    mode = \"tcp\"\n\n[transforms.parse]\n    inputs = [ \"tcp\" ]\n    type = \"remap\"\n    file = \"adn.vrl\"\n\n[sinks.blackhole]\n    type = \"blackhole\"\n    inputs = [ \"parse\" ]\n1 TCP source into 1 VRL into 4 sink\n[sources.tcp]\n    type = \"socket\"\n    address = \"0.0.0.0:9001\"\n    mode = \"tcp\"\n\n[transforms.parse]\n    inputs = [ \"tcp\" ]\n    type = \"remap\"\n    file = \"adn.vrl\"\n\n[transforms.lb]\n    type = \"remap\"\n    inputs = [\"parse\"]\n    drop_on_error = true\n    source = \"\"\"\n      .partition = mod(to_unix_timestamp!(.timestamp, unit: \"milliseconds\"), 4)\n    \"\"\"\n\n [transforms.route]\n    type = \"route\"\n    inputs = [\"lb\"]\n    [transforms.route.route]\n        sink1 = '.partition == 0'\n        sink2 = '.partition == 1'\n        sink3 = '.partition == 2'\n        sink4 = '.partition == 3'\n        \n[sinks.blackhole-1]\n    type = \"blackhole\"\n    inputs = [ \"route.sink1\" ]\n\n[sinks.blackhole-2]\n    type = \"blackhole\"\n    inputs = [ \"route.sink2\" ]\n\n[sinks.blackhole-3]\n    type = \"blackhole\"\n    inputs = [ \"route.sink3\" ]\n\n[sinks.blackhole-4]\n    type = \"blackhole\"\n    inputs = [ \"route.sink4\" ]\n1 TCP source into 4 VRL into 4 sink\n[sources.tcp]\n    type = \"socket\"\n    address = \"0.0.0.0:9001\"\n    mode = \"tcp\"\n\n[transforms.lb]\n    type = \"remap\"\n    inputs = [\"tcp\"]\n    drop_on_error = true\n    source = \"\"\"\n      .partition = mod(to_unix_timestamp!(.timestamp, unit: \"milliseconds\"), 4)\n    \"\"\"\n\n[transforms.route]\n    type = \"route\"\n    inputs = [\"lb\"]\n    [transforms.route.route]\n        vrl-1 = '.partition == 0'\n        vrl-2 = '.partition == 1'\n        vrl-3 = '.partition == 2'\n        vrl-4 = '.partition == 3'\n\n[transforms.parse-1]\n    inputs = [ \"route.vrl-1\" ]\n    type = \"remap\"\n    file = \"adn.vrl\"\n\n[transforms.parse-2]\n    inputs = [ \"route.vrl-2\" ]\n    type = \"remap\"\n    file = \"adn.vrl\"\n\n[transforms.parse-3]\n    inputs = [ \"route.vrl-3\" ]\n    type = \"remap\"\n    file = \"adn.vrl\"\n\n[transforms.parse-4]\n    inputs = [ \"route.vrl-4\" ]\n    type = \"remap\"\n    file = \"adn.vrl\"\n        \n[sinks.blackhole-1]\n    type = \"blackhole\"\n    inputs = [ \"parse-1\" ]\n\n[sinks.blackhole-2]\n    type = \"blackhole\"\n    inputs = [ \"parse-2\" ]\n\n[sinks.blackhole-3]\n    type = \"blackhole\"\n    inputs = [ \"parse-3\" ]\n\n[sinks.blackhole-4]\n    type = \"blackhole\"\n    inputs = [ \"parse-4\" ]",
        "url": "https://github.com/vectordotdev/vector/discussions/19278",
        "createdAt": "2023-12-01T14:00:30Z",
        "updatedAt": "2023-12-14T15:11:18Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "jlazic"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19380,
        "title": "Run transform conditionally in aggregator model",
        "bodyText": "We're running Vector as a sidecar in AWS Fargate/ECS to collect logs from containers and then send them via a Vector aggregator to Elasticsearch.\nI need to run a transform to amend the log structure from some of our containers and currently do this in the agent/sidecar config.toml. However, its not clear whether this is the correct approach in the aggregator model and if it would instead be better to do this in the aggregator config.\nSo is it possible to move the transform into the aggregator config with an if statement? We would be sending logs from many containers but only need to run the transform on logs from some specific containers so I wondered if there might be some way to say \"if log contains this thing, run this transform\". The only place I've seen in the docs is in a filter, but we don't want to drop the logs but run a transform instead.\nOr otherwise is it preferred to transform the data in the agent before it is forwarded to the aggregator?\nFor reference:\n\nagent.toml\naggregator.toml",
        "url": "https://github.com/vectordotdev/vector/discussions/19380",
        "createdAt": "2023-12-13T16:27:12Z",
        "updatedAt": "2023-12-13T17:39:32Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "olivermussell"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19372,
        "title": "This should be an easy csv-lua question",
        "bodyText": "I don't know why but I can't find any examples of how to pass the header = true parameter to csv-lua.  Here is my line of code and I just need to know how to add the headers = true parameter to it: remember I am not a programmer I do everything off examples smh\n csvFile = event.log.message\n fields = csv.openstring(event.log.message):lines()()",
        "url": "https://github.com/vectordotdev/vector/discussions/19372",
        "createdAt": "2023-12-12T23:43:01Z",
        "updatedAt": "2023-12-13T15:57:43Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Manniac07"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 19316,
        "title": "Counters increase return no value",
        "bodyText": "Hi everyone,\nI have a metric counter which represents the number of calls, and for some labels combination, it generates a single sample data only from time to time.\nThe problem is that in the dashboard with the following promql these samples are not considered. I know this is a known architecture characteristic for rate/increase functions... but is there some workaround to consider these metrics that do not have the first value as zero? I am using vector to create this metrics from a logs stream and the combination of labels is not predictable.\nPromql:\nsum(increase(number_of_calls{job=\"$job\",foo=\"$foo\"}[$__interval]))\nAlso, as soon as the metric is generated it expires after 60s (flush_period_secs) and if the same metric has value again after 5minutes for instance, the increase will be zero again, so I am wondering to increase the flush_period_secs to a very high value, or is there some way to disable it (set it to 0 disable?!) ? Unfortunately even this way, the first value of each counter will be always discarded when applying a increase/rate function.\nI really appreciate some guidance on this.\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/19316",
        "createdAt": "2023-12-05T22:34:55Z",
        "updatedAt": "2023-12-12T17:16:45Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bmgante"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19357,
        "title": "reading 2 inputs",
        "bodyText": "when I read in 2 different input files and I have logic in the transform to know the difference, it is still giving all the events to metrics names. is this by design or can I separate the metrics. Do you have any examples?\nsources:\n[sources.vectorsiprealmmethod]\n type = \"file\"\n fingerprint.lines = 50\n fingerprint.strategy = \"checksum\"\n fingerprint.ignored_header_bytes = 0\n glob_minimum_cooldown_ms = 10\n include = [\"/home/zuhdrt/**/sip-realm-method/*.csv\"]\n oldest_first = true\n read_from = \"beginning\"\n\n[sources.vectorsipinvites]\n type = \"file\"\n fingerprint.lines = 2\n fingerprint.strategy = \"checksum\"\n fingerprint.ignored_header_bytes = 0\n glob_minimum_cooldown_ms = 10 \n include = [\"/home/zuhdrt/**/sip-invites/*.csv\"]\n oldest_first = true\n read_from = \"beginning\"\n\ntransform:\npath = string.gsub(path, \"-\", \"_\")\n    if string.find(path, \"sip_realm_method\") then\n...\n    elseif string.find(path, \"sip_invites\") then\n...\n\nsink:\n[sinks.vectorsiprealmmethodConsole]\n   type = \"prometheus_exporter\"\n   inputs = [\"vectorsiprealmmethodmetric\"]\n   address = \"0.0.0.0:1234\"\n   suppress_timestamp = true\n   flush_period_secs = 86400",
        "url": "https://github.com/vectordotdev/vector/discussions/19357",
        "createdAt": "2023-12-11T16:24:02Z",
        "updatedAt": "2023-12-11T16:24:03Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Manniac07"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 19322,
        "title": "Running 2 instances of vector under different linux users",
        "bodyText": "Hello,\nI have a configuration [v32] where in , there is a first instance which exposes http, file and unix as source and then using vector as a sink forwards it to another instance (Agent - Collector model), where these received events are stored to a file.\nTill this point in time, I had them both started by the same linux user and had no issues.\nRecently, I had to migrate the collector side to a new user with a larger disk quota associated to it. I am now seeing that most of the times the events/logs are not being forwarded from the agent to the collector. Are there any additional settings on the user that might help? Or are there restrictions preventing it.\nThanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/19322",
        "createdAt": "2023-12-06T12:26:00Z",
        "updatedAt": "2023-12-08T15:38:04Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bmallya"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 19301,
        "title": "File Name from source to sink",
        "bodyText": "Is it possible to pass the file name from the source include? to a label in the sink?\nI know this is probably simple if its possible I just can't find where to do it. my sink is Prometheus Remote Write",
        "url": "https://github.com/vectordotdev/vector/discussions/19301",
        "createdAt": "2023-12-04T18:34:21Z",
        "updatedAt": "2023-12-07T20:08:13Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Manniac07"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 19303,
        "title": "source socket for syslog",
        "bodyText": "Hi All,\nI need to receive syslog messages that do not confirm to any the standards (yay!). I used UDP socket and send onto kafka for further processing via another vector process. I have noticed that i cannot see the message 'Facility' or 'Severity' when using socket. Do i have any options to pull this out of the packet when converting to json?\nDo we have a function to convert the priority to facility and severity?\nthanks in advance!",
        "url": "https://github.com/vectordotdev/vector/discussions/19303",
        "createdAt": "2023-12-04T21:38:05Z",
        "updatedAt": "2023-12-05T17:13:55Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "coredump17"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 19310,
        "title": "Events dropping when webhook is off",
        "bodyText": "Hi guys!\nLately, my vector instance is having a specific problem that consists of events crashing when I turn off my webhook.\nSo, when my webhook is disabled, I lose all events.\nI'm using the http type in my sink and it's configured to get the path from my webhook. When the webhook is activated, it works fine, but when the webhook is deactivated, all the events coming from my source are discarded.\nhere is a snippet of my configuration file:\napi:\nenabled: true\naddress: xxxxxxxxxxxxxxxxxxxxxx\nsources:\nsiem_webhook:\ntype: http_server\naddress: xxxxxxxxxxxxxx\ndecoding:\ncodec: json\nheaders:\n- 'content-type: application/json'\nmethod: POST\npath: '/webhook'\npath_key: path\nquery_parameters:\n- application\nresponse_code: 200\nstrict_path: true\nsinks:\nxxxxxx_n8n:\ntype: http\ninputs:\n- siem_webhook\ncompression: none\nmethod: post\nuri: XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nencoding:\ncodec: json\ntls:\nverify_certificate: false\nbuffer:\ntype: memory\nmax_events: 1000\nwhen_full: block\nacknowledgements:\nenabled: true\nconsole:\ntype: console\nencoding:\ncodec: json\ninputs:\n- siem_webhook\nHere are the logs:\n2023-11-29T18:08:11.021363Z ERROR sink{component_kind=\"sink\" component_id=xxxxx_n8n component_type=http}:request{request_id=19}: vector::sinks::util::retries: Not retriable; dropping the request. reason=\"Http status: 404 Not Found\" internal_log_rat\ne_limit=true\n2023-11-29T18:08:11.021406Z ERROR sink{component_kind=\"sink\" component_id=xxxxxxx_n8n component_type=http}:request{request_id=19}: vector_common::internal_event::service: Service call failed. No retries or retries exhausted. error=None request_id=19\nerror_type=\"request_failed\" stage=\"sending\" internal_log_rate_limit=true\n2023-11-29T18:08:11.021446Z ERROR sink{component_kind=\"sink\" component_id=xxxxx_n8n component_type=http}:request{request_id=19}: vector_common::internal_event::component_events_dropped: Events dropped intentional=false count=1 reason=\"Service call\nfailed. No retries or retries exhausted.\" internal_log_rate_limit=true\nI'd be very grateful if you could help me find out what's going on!",
        "url": "https://github.com/vectordotdev/vector/discussions/19310",
        "createdAt": "2023-12-05T14:38:27Z",
        "updatedAt": "2023-12-05T16:45:15Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "RafaelLobox"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19311,
        "title": "Unexpected Behavior of Histogram Metrics in Vector",
        "bodyText": "I am using Vector for metric collection and have been observing an unexpected behavior with the Histogram metric type. Each time a new metric event is created, it appears to clear the old histogram metrics and adds the new metric event to the set of metrics. This behavior is unexpected as per my understanding of how Vector handles metrics.\nIn Vector, each time a new metric event is created, it should add the new metric event to the existing set of metrics, without clearing the old histogram metrics. However, I am observing that the old histogram metrics are being cleared each time a new metric is added.\nThis behavior is causing issues in my monitoring setup as I am losing the historical data of my histogram metrics. I have checked my Vector configuration and the documentation of the downstream service I am using, but I haven\u2019t found a solution to this issue.\nI am looking for guidance on why this behavior is occurring and how I can prevent the old histogram metrics from being cleared when a new metric is added. Any help would be greatly appreciated. Thank you.",
        "url": "https://github.com/vectordotdev/vector/discussions/19311",
        "createdAt": "2023-12-05T14:51:13Z",
        "updatedAt": "2023-12-05T15:59:30Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "veer123prathap"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19302,
        "title": "Finally got an error for my previous question",
        "bodyText": "this is in regards to my previous question\nI finally got an error I am hoping  someone might be able to respond to this now that I have an error\n2023-12-04T19:47:31.247430Z  WARN sink{component_kind=\"sink\" component_id=mysink_id_writer component_type=prometheus_remote_write component_name=mysink_id_writer}:request{request_id=51614}:http: vector::internal_events::http_client: HTTP error. error=connection closed before message completed error_type=\"request_failed\" stage=\"processing\" internal_log_rate_limit=true 2023-12-04T19:47:31.254703Z  WARN sink{component_kind=\"sink\" component_id=mysink_id_writer component_type=prometheus_remote_write component_name=mysink_id_writer}:request{request_id=51614}: vector::sinks::util::retries: Retrying after error. error=Failed to make HTTP(S) request: connection closed before message completed internal_log_rate_limit=true",
        "url": "https://github.com/vectordotdev/vector/discussions/19302",
        "createdAt": "2023-12-04T20:13:32Z",
        "updatedAt": "2023-12-04T21:55:16Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Manniac07"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19295,
        "title": "Scrap all /metrics endpoints pods from DaemonSet, like Prometheus (Pod&ServiceMonitor)",
        "bodyText": "Hello Guys\nI have this configuration defined in my image :\nMultiple nodes which run in each :\n\nOne vector agent\nOne instance of cAdvisor\n\ncAdvisor is like kube-state-metrics, expose a lot of metrics on 8080/metrics. I can scrap directly KSM via prometheus_scrape Vector Datasource which work nicely, but this is a replicatSet.\nBut cAdvisor run in daemonset to have node base metrics, not like KSM. I thought that i could make the same with cAdvisor, to call directly the pod and do not pass to the service :\n     sources:\n      cadvisor:\n        endpoints:\n        - http://cadvisor:8080/metrics\n        scrape_interval_secs: 60\n        type: prometheus_scrape\n\nBut this seems to pass thought the service, because i have a lot of metrics, but not 100 % from all my nodes :wat:\nI'm a bit lost because i'm not expert in Kube & Vector, my apologies for this idiot question.\nDo i have to use a Prometheus agent, and scrap my pods with Pod&Service monitor, and then be scrapped from Vector with prometheus exporter source ? Or is this possible from Vector to scrap this kind of stuff directly without to use Prometheus ?\nThanks a lots guys\nPs: This is what i try to achieve",
        "url": "https://github.com/vectordotdev/vector/discussions/19295",
        "createdAt": "2023-12-04T14:24:41Z",
        "updatedAt": "2023-12-04T15:24:05Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Momotoculteur"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 3
    },
    {
        "number": 19219,
        "title": "Support fields/VRL for setting sample rate",
        "bodyText": "I asked this in another question but experiemented to get the answer.\nCurrently with the sample filter, it only accepts an int as the input for the sample rate. It would be really good if we could set this via a field or VRL snippet.\nThe use case I have is that my event messages contains a field denoting the service that the log belongs to. I'm using this as the key field for hashing. However as event volume varies a lot, a fixed sample rate doesn't fit my use case.\nWhat I would like to do is be able to set the sample rate based on a condition, in this case the value of the service field or even a event metadata value (if the sample rate is pre-caclulated and added in a previous transform).\nFor example:\n[transforms.my_sampler]\ntype = \"sample\"\ninputs = [ \"upstream_source\"]\nrate = '''\nint(get!({\"serviceA\": 1, \"serviceB\": 1, \"serviceC\": 1000}, [.service_name])) ?? 3\n'''\nI'm not a Rust programmer but I noticed that you have a condition input that supports VRL that supports a boolean response. Could one be added for Int to support this?\nAnother idea would be to have a secondary parameter rate_calculated for this value so that either can be used where one has priority.\nCurrently the way I've had to do it is use a route condition to match the service name and send it to numerous samplers pre-configured for set rates.",
        "url": "https://github.com/vectordotdev/vector/discussions/19219",
        "createdAt": "2023-11-22T13:29:52Z",
        "updatedAt": "2023-12-04T14:59:50Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "NeilJed"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17522,
        "title": "Do retrying requests require lots of memory?",
        "bodyText": "Our team has encountered OOM crashed of vector running on k8s several times,\nafter reading the document about buffering model,\nwe got confused because we have all the buffer settings to be default\nbuffer.type = \"memory\"\nbuffer.max_events = 500\nbuffer.when_full = \"block\"\n\n(we didn't actually write these in the config)\nWith a Kafka source which has average event size to be ~300 bytes, we expected the max memory usage of vector should be around\n300 bytes * 500 # = 150 kb\nand when buffer is full, source should stop consuming message from Kafka\nbut the memory usage reached over 1Gb, and messages are still consumed from Kafka until the pod was OOM killed instead\nSo we did a experiment, set buffer.max_events to 1 and have a http sink that does not exist, and vector still consumed more than 10000 events from Kafka\nafter digging into the source code, I found that sender has its own memory buffer:\n\n  \n    \n      vector/src/topology/builder.rs\n    \n    \n         Line 61\n      in\n      98c54ad\n    \n  \n  \n    \n\n        \n          \n           pub(crate) static SOURCE_SENDER_BUFFER_SIZE: Lazy<usize> = \n        \n    \n  \n\n\nit can buffer thousands of events before sending to sink memory buffer, and this buffer size seems to be unconfigurable\nWe also got many logs like this\nWARN sink{component_kind=\"sink\" component_id=sink component_type=aws_kinesis_streams component_name=sink}:request{request_id=1977859}: vector::sinks::util::retries: Retrying after error. error=TransientError: connection closed before message completed internal_log_rate_limit=true\nSeems that requests were retried over and over again while events were still consumed from the Kafka source\nWould this be the reason that we got such high memory usage?\nAs my current understanding there are at least 3 buffers for a event to go through before sending out\nsource buffer => sink buffer => sink batching buffer\nwhere do the events waited for retrying stand?\nAnd it would be really helpful if there are a page describes all the settings that could affect the memory usage of a vector process, thanks\ud83d\ude4f",
        "url": "https://github.com/vectordotdev/vector/discussions/17522",
        "createdAt": "2023-05-28T07:36:31Z",
        "updatedAt": "2023-12-04T08:59:52Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "LilTwo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17779,
        "title": "How much memory does vector required?",
        "bodyText": "I found that our vector process often has OOM, using kafka as the source and elasticsearch as the sink.\nI'm not sure if it's caused by a memory leak or a large amount of data.",
        "url": "https://github.com/vectordotdev/vector/discussions/17779",
        "createdAt": "2023-06-28T09:48:50Z",
        "updatedAt": "2023-12-04T08:56:57Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "jmjoy"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19265,
        "title": "SASL_SSL security.protocol for kafka source",
        "bodyText": "We're trying to export a kafka topic to datadog for log retention. In the kafka documentation the settings required for the topic are:\nbootstrap.servers={{ BOOTSTRAP SERVER }}\nsecurity.protocol=SASL_SSL\nsasl.mechanisms=PLAIN\nsasl.username={{ CLUSTER_API_KEY }}\nsasl.password={{ CLUSTER_API_SECRET }}\n\n# Best practice for higher availability in librdkafka clients prior to 1.7\nsession.timeout.ms=45000\n\nI have attempted to configure this in vector as follows:\n      kafka:\n        type: kafka\n        bootstrap_servers: \"<bootstrap-server>\"\n        group_id: <group-id>\n        librdkafka_options.\"security.protocol\": SASL_SSL\n        topics:\n          - <topic-name>\n        sasl:\n          enabled: true\n          mechanism: PLAIN\n          username: \"${CLUSTER_API_KEY}\"\n          password: \"${CLUSTER_API_SECRET}\"\n\nSetting librdkafka_options.\"security.protocol\": gives the error:\nConfiguration error. error=unknown field `librdkafka_options.\"security.protocol\"` in `sources.kafka`\n\nWhereas without librdkafka_options.\"security.protocol set there is an error broker transport failure indicating that the wrong security.protocol is being used.\nI believe that this issue is caused by SASL_SSL not being included in the active listeners here: https://github.com/vectordotdev/vector/blob/master/scripts/integration/kafka/compose.yaml#L16.\nHoping however that there is something wrong with the configuration and would really appreciate any help from the community \ud83d\ude4f",
        "url": "https://github.com/vectordotdev/vector/discussions/19265",
        "createdAt": "2023-11-30T09:54:40Z",
        "updatedAt": "2023-11-30T16:58:28Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "jamhamblin"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19043,
        "title": "Inquiry Regarding Testing Methodology for Azure Monitor Integration in Vector",
        "bodyText": "Hello, Vector dev community\nI am currently exploring the integration capability of Vector for forwarding log events to Azure Monitor. In my initial tests, the forwarding functionality appeared to work correctly, providing me with confidence in the basic functionality. As I prepare for the next phase, I am particularly interested in understanding the approach to integration testing within Vector, specifically for Azure Monitor Sink.\nI noticed that there is currently no integration test available for Azure Monitor Sink in your code base. Could you please provide more information about your testing methodology for validating the integration of Vector with Azure Monitor? I am keen to learn about your strategies for testing log event transmission, interaction handling, and data validation within Azure Monitor. Any insights into the tools, frameworks, or techniques utilized for testing the interaction with Azure Monitor would be greatly appreciated.\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/19043",
        "createdAt": "2023-11-03T13:48:37Z",
        "updatedAt": "2024-03-22T10:54:35Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "vparfonov"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19261,
        "title": "Vector stops writing and when Tracing is enabled Vector crashes after about an hour",
        "bodyText": "I am sorry for constant questions but if it isn't obvious I am not a developer and was assigned this anyways.\nfollow up to yesterdays question \n2 issues:\n\nVector will work from anywhere from 1 hr to 2days and then will stop sending data to Victoria Metrics (using Prometheus remote write) without any error logs, all logs say it is still processing. if I restart it starts sending again and will work for another period of time then stops sending again.\nto troubleshoot I enabled trace this caused a second issue. Now the vector service shuts down after an hour without a previous error it just shuts down saying:\nERROR vector::topology::running: Failed to gracefully shut down in time. Killing components. components=\"vectorsiprealmmethodConsole\"\n\n`Source:\nChange this to use a non-default directory for Vector data storage:\ndata_dir = \"/.../.../vector\"\nRandom Syslog-formatted logs\n[sources.vectorsiprealmmethod]\ntype = \"file\"\nfingerprint.lines = 50\nfingerprint.strategy = \"checksum\"\nfingerprint.ignored_header_bytes = 0\nglob_minimum_cooldown_ms = 60000\ninclude = [\"/.../.../.../sip-realm-method/*.csv\"]\noldest_first = true\nread_from = \"beginning\"\nSink:\n[sinks.vectorsiprealmmethodConsole]\ntype = \"prometheus_remote_write\"\ninputs = [\"vectorsiprealmmethodmetric\"]\nendpoint = \"https://.../insert/\"\nhealthcheck.enabled = false\nbuffer.type = \"disk\"\nbuffer.max_size = 1073741824\nbuffer.when_full = \"block\"\n[sinks.vectorsiprealmmethodConsole.request]\nconcurrency = \"adaptive\"`",
        "url": "https://github.com/vectordotdev/vector/discussions/19261",
        "createdAt": "2023-11-29T20:55:27Z",
        "updatedAt": "2023-11-29T20:55:28Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Manniac07"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 3
    },
    {
        "number": 19245,
        "title": "csv parsing help please",
        "bodyText": "What is the best way to parse a multiline csv into metrics? The first row is header row, 1st column is to be ignored, metrics are nested 2nd column then 3rd column. 4th-115th columns are the actual metrics values.\n(Not sure if it makes a difference but each csv will have a different name based on timestamp)\nNew csv will come in every minute.\nhere is a sample of the the csv:\n\n\n\n\n\n\n<style>\n\n</style>\n\n\n\n\n\nTimeStamp\nRealm\nMessage/Event\nRequest Server Total\nRequest Client Total\nRetransmissions Server Total\nRetransmissions Client Total\n100 Trying Server Total\n100 Trying Client Total\n180 Ringing Server Total\n180 Ringing Client Total\n181 Forward Server Total\n181 Forward Client Total\n182 Queued Server Total\n182 Queued Client Total\n183 Progress Server Total\n183 Progress Client Total\n1xx Server Total\n1xx Client Total\n200OK Server Total\n200OK Client Total\n202 Accepted Server Total\n202 Accepted Client Total\n2xx Success Server Total\n2xx Success Client Total\n30x Moved Server Total\n30x Moved Client Total\n305 Use ProxyServer Total\n305 Use ProxyClient Total\n380 Alternative Server Total\n380 Alternative Client Total\n3xx Redirect Server Total\n3xx Redirect Client Total\n400 Bad Request Server Total\n400 Bad Request Client Total\n401 Unauthorized Server Total\n401 Unauthorized Client Total\n403 Forbidden Server Total\n403 Forbidden Client Total\n404 Not Found Server Total\n404 Not Found Client Total\n405 Not Allowed Server Total\n405 Not Allowed Client Total\n406 Not Acceptable Server Total\n406 Not Acceptable Client Total\n407 Proxy Auth Req Server Total\n407 Proxy Auth Req Client Total\n408 Request Timeout Server Total\n408 Request Timeout Client Total\n415 Bad Media Type Server Total\n415 Bad Media Type Client Total\n\n\n\n\n1.66E+09\nAttFlexInt\nINVITE\n6394898\n820302\n10254\n129\n3542599\n818618\n3065044\n15856\n0\n0\n11\n0\n2145226\n0\n0\n0\n6164120\n807437\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n13\n0\n0\n0\n198\n0\n8936\n0\n0\n0\n0\n0\n0\n11281\n0\n0\n2\n\n\n1.66E+09\nAttFlexInt\nACK\n6381923\n819511\n790\n261\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1.66E+09\nAttFlexInt\nBYE\n2352087\n1289229\n9\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2351273\n1288542\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1.66E+09\nAttPriFlexExt\nINVITE\n427360\n3235672\n3663\n64\n157319\n3235703\n7917\n1621092\n0\n0\n0\n4\n0\n1123035\n0\n0\n419232\n3116994\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n38\n0\n0\n16\n0\n1122\n0\n0\n0\n0\n0\n0\n0\n326\n22751\n1\n0\n\n\n1.66E+09\nAttPriFlexExt\nACK\n426927\n3229761\n119\n1419\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1.66E+09\nAttPriFlexExt\nBYE\n615533\n1182702\n1518\n137\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n615270\n1182681\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0",
        "url": "https://github.com/vectordotdev/vector/discussions/19245",
        "createdAt": "2023-11-27T22:36:18Z",
        "updatedAt": "2023-11-28T23:19:24Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Manniac07"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 2
    },
    {
        "number": 19176,
        "title": "parse syslog date without year",
        "bodyText": "Hi All,\nI am new to vector. I am trying to parse network syslogs but cannot use parse_syslog as some logs have an extra field,  not standard format. I can parse my logs with regex but i want to use the timestamp of the actual message, not when it arrived.  The time/date format is \"Nov 15 19:45:00\" - without the year ;(.\n$ parse_timestamp!(\"Nov 15 19:45:00\", format: \"%h %e %T\")\nfunction call error for \"parse_timestamp\" at (0:55): Invalid timestamp \"Nov 15 19:45:00\": input is not enough for unique date and time.\ntesting in vrl, it looks like parse_timestamp requires a year and does not default to 'this year'. Can someone help me overcome this?  Probably easy when you know how ;).",
        "url": "https://github.com/vectordotdev/vector/discussions/19176",
        "createdAt": "2023-11-16T21:22:21Z",
        "updatedAt": "2023-11-28T23:04:39Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "coredump17"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19221,
        "title": "Vector VRL Script Compiler Performance and Architecture",
        "bodyText": "How does Vector's compiler of VRL scripts function entirely? For example, does Vector perform some kind of caching of compiled scripts in its data directory for quicker reloads, and is this sort of compiler functionality documented anywhere? I do understand that vector builds its topology based on the entirety of the configuration, but it seems like the builders performance diminishes when it has to compile VRL scripts.",
        "url": "https://github.com/vectordotdev/vector/discussions/19221",
        "createdAt": "2023-11-22T18:15:28Z",
        "updatedAt": "2023-11-28T20:49:10Z",
        "isAnswered": false,
        "locked": true,
        "author": {
            "login": "wmyre"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19212,
        "title": "Access Metadata",
        "bodyText": "Hi All,\nI have a socket\nsources:\n  src_syslog:\n    type: socket\n    address: 0.0.0.0:514\n    mode: udp\n\n\ntransforms:\n  add_tags:\n    type: remap\n    inputs:\n      - src_syslog\n    source: |-\n       .meta.listener = \"src_syslog\"\nWhen i tap the transform 'add_tags' with -m (meta) i see component_id. I want to use the vrl to .meta.listener = %component_id instead of hard coding it but it shows as null.\ni tried  .meta.listener = %component_id  and .meta.listener = .component_id\n  vector tap -m  --outputs-of add_tags\n{\"component_id\":\"src_syslog\",\"component_kind\":\"source\",\"component_type\":\"socket\",\"event\"...blah}\n\nany help would be greatly appreciated.",
        "url": "https://github.com/vectordotdev/vector/discussions/19212",
        "createdAt": "2023-11-21T20:44:45Z",
        "updatedAt": "2023-11-28T18:47:42Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "coredump17"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19166,
        "title": "Indexed Fields - Splunk HEC integration - Event Endpoint Target",
        "bodyText": "Hoping someone can help because for the life of me I cannot get Vector to send Indexed Fields to Splunk and NOT have to include the fields in the Event message payload, which renders the exercise of indexing the fields futile.\nExample given the following log payload,\n{\n    \"event\" : \"{\\r\\n    \\\"message\\\": \\\"hello world!\\\",\\r\\n    \\\"timestamp\\\" : \\\"2023-11-16T02:49:38Z\\\",\\r\\n    \\\"level\\\": \\\"info\\\"\\r\\n}\",\n    \"fields\": {\n        \"zone\" : \"a\",\n        \"instance\" : \"id-123\"\n    }\n}\n\nWith fluentbit it is trivial to log the event property as the event in Splunk and index fields out of the fields property as fluent provides a event_key property. Once event_key is set you are guaranteed that is all that will be sent to Splunk. https://docs.fluentbit.io/manual/pipeline/outputs/splunk#configuration-parameters For the life of me I cannot see how this is possible with the Splunk sink in vector. Given in vector there is no way to specify the property you want posted as the event payload.\nI have tried all sorts of things like nesting the fields property in the event and then removing it but nothing seems to work. Any suggestions would be much appreciated.\ntransforms:\n    events_json_remap:\n      inputs:\n        - automat_events_json\n      type: \"remap\"\n      source: |\n      .event = parse_json!(.event)\n      .event.fields = del(.fields)\n      . = .event\nsinks:\n    splunk_sink_json:\n      type: splunk_hec_logs\n      inputs:\n        - events_json_remap\n      compression: gzip\n      endpoint: \"https://http-inputs-acme.splunkcloud.com\"\n      endpoint_target: event\n      index: \"main\"\n      source: \"splunk_sink_event\"\n      auto_extract_timestamp: true\n      timestamp_key: \"noname\"\n      sourcetype: \"_json\"\n      default_token: \"sssshhhh\"\n      encoding:\n        codec: json\n        except_fields: [\"fields\"]\n      indexed_fields: ['\"fields.zone\"']",
        "url": "https://github.com/vectordotdev/vector/discussions/19166",
        "createdAt": "2023-11-16T04:55:26Z",
        "updatedAt": "2023-11-24T19:46:57Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tbenade"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 19172,
        "title": "is there interest in distributing sbom file(s)",
        "bodyText": "With the release of a usable cargo-cyclonedx crate (see https://www.reddit.com/r/rust/comments/17ud9it/cargocyclonedx_v040_now_ready_for_production), I would like to check if the project is interested in distributing SBOM files for Vector. I would do the work required to get it on CI, so that it becomes available on GitHub Releases page for those who would make use of it (and I know at least one organisation that would).",
        "url": "https://github.com/vectordotdev/vector/discussions/19172",
        "createdAt": "2023-11-16T09:47:18Z",
        "updatedAt": "2023-11-28T19:14:12Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tshepang"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 19229,
        "title": "Upgrade failed on vector helm chart",
        "bodyText": "Hey!\nNew to vector and I want to try it out. I installed the vector helm chart with the Agent role and everything was fine. However when I changed the customConfig: {} so it could send the logs to Loki I got:\nError: UPGRADE FAILED: cannot patch \"vector\" with kind Service: Service \"vector\" is invalid: spec.ports: Required value\ncustomConfig look like bellow:\ncustomConfig:\n  sinks:\n    loki:\n      inputs:\n        - logs_json # Use the Kubernetes logs as input\n      type: loki\n      endpoint: http://<LOKI-IP>:3100 # Loki service address\n      encoding:\n        codec: json\n      labels:\n        - app: \"web\"\n        - env: \"test\"\n\n  sources:\n    k8s_logs:\n      type: kubernetes_logs\nShould I change the vector service as well?\n/Angelos",
        "url": "https://github.com/vectordotdev/vector/discussions/19229",
        "createdAt": "2023-11-23T09:21:27Z",
        "updatedAt": "2023-11-23T09:21:28Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Angel0r"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 19175,
        "title": "Set sampling rate based on map lookup?",
        "bodyText": "I've got a scenario where I need to sample different events at different rates depending on their source service. The service name is stored in the field meta.service.\nI'm wondering if it's possible to set the value of the rate parameter via code using a map lookup? My thought is I can allow rates for specific services to be defined and then have a default for anything else.\nNot sure if VRL is allowed with the parameter.",
        "url": "https://github.com/vectordotdev/vector/discussions/19175",
        "createdAt": "2023-11-16T16:47:06Z",
        "updatedAt": "2023-11-23T09:01:55Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "NeilJed"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 19193,
        "title": "Why Vector VRL remap can't del field of JSON file?",
        "bodyText": "test json data file - input.json:\n{\n    \"file\": \"/var/log/logify/app666788eedd77.log\",\n    \"host\": \"vector-test\",\n    \"message\": \"{\\\"status\\\": 50e, \\\"ip\\\": \\\"127.0.0.1\\\", \\\"level\\\": 30, \\\"emailAddress\\\": \\\"user@mail.com\\\", \\\"msg\\\": \\\"Task completed successfully\\\", \\\"pid\\\": 12655, \\\"ssn\\\": \\\"407-01-2433\\\", \\\"time\\\": 1694551048}\",\n    \"source_type\": \"file\",\n    \"timestamp\": \"2023-09-12T20:40:21.582883690Z\"\n}\n\nmy Vector toml configure file:\n[sources.json_file]\ntype = \"file\"\ninclude = [\"c:\\\\config\\\\input.json\"]\ndata_dir = \"c:\\\\config\"\n\n[transforms.tf01]\ntype = \"remap\"\n# drop_on_abort = true\ntimezone = \"local\"\ninputs = [\"json_file\"]\nsource = \"\"\". , err = parse_json(.message)\n    del(.ip)\n    del(.status)\n    .newID = \"new IO\"\n\n\"\"\"\n\n\n[sinks.out]\ninputs = [\"tf01\"]\ntype = \"console\"\nencoding.codec = \"text\"\nTest Case command:\nOS : windows 10\nvector -c C:\\config\\test.toml -q\nScreen output:\nPS C:\\config> vector -c C:\\config\\test.toml\n2023-11-20T01:06:25.724598Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=info,rdkafka=info,buffers=info,lapin=info,kube=info\"\n2023-11-20T01:06:25.726452Z  INFO vector::app: Loading configs. paths=[\"C:\\\\config\\\\test.toml\"]\n2023-11-20T01:06:25.730280Z  INFO vector::topology::running: Running healthchecks.2023-11-20T01:06:25.730627Z  INFO vector::topology::builder: Healthcheck passed.  \n2023-11-20T01:06:25.730730Z  INFO source{component_kind=\"source\" component_id=json_file component_type=file}: vector::sources::file: Starting file server. include=[\"c:\\\\config\\\\input.json\"] exclude=[]\n2023-11-20T01:06:25.730965Z  INFO vector: Vector has started. debug=\"false\" version=\"0.34.0\" arch=\"x86_64\" revision=\"c909b66 2023-11-07 15:07:26.748571656\"\n2023-11-20T01:06:25.731485Z  INFO vector::app: API is disabled, enable by setting \n`api.enabled` to `true` and use commands like `vector top`.\n2023-11-20T01:06:25.731782Z  INFO source{component_kind=\"source\" component_id=json_file component_type=file}:file_server: file_source::checkpointer: Loaded checkpoint data.\n2023-11-20T01:06:25.733116Z  INFO source{component_kind=\"source\" component_id=json_file component_type=file}:file_server: vector::internal_events::file::source: Resuming to watch file. file=c:\\config\\input.json file_position=346\n2023-11-20T01:06:38.226094Z  INFO source{component_kind=\"source\" component_id=json_file component_type=file}:file_server: vector::internal_events::file::source: Found new file to watch. file=c:\\config\\input.json\n{\"status\": 50e, \"ip\": \"127.0.0.1\", \"level\": 30, \"emailAddress\": \"user@mail.com\", \"msg\": \"Task completed successfully\", \"pid\": 12655, \"ssn\": \"407-01-2433\", \"time\": 1694551048}\n2023-11-20T01:06:46.559949Z  INFO source{component_kind=\"source\" component_id=json_file component_type=file}:file_server: vector::internal_events::file::source: Stopped watching file. file=c:\\config\\input.json\nwhy .ip and .status is not deleted.?\nthanks.\nAndy",
        "url": "https://github.com/vectordotdev/vector/discussions/19193",
        "createdAt": "2023-11-20T01:10:52Z",
        "updatedAt": "2023-11-23T05:41:58Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "epmpub"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 19124,
        "title": "Missing enrichment dependency and Type Mismatch Error with git in Cargo.toml",
        "bodyText": "Hi,\nDescription:\nI have encountered two issues while working with the web-playground or vrl-server:\nMissing enrichment Dependency:\nThe Cargo.toml file of the main vector.dev repo does not include the enrichment dependency. As a result, attempting to use enrichment::vrl_functions() in the vrl-server codebase leads to unresolved types and potential build errors.\nType Mismatch Error with git in Cargo.toml:\nWhen attempting to use the git feature in our Cargo.toml to reference remote dependencies , a type mismatch error occurs during the build process. The error is as follows:\nerror[E0271]: type mismatch resolving `<Vec<Box<dyn Function>> as IntoIterator>::Item == Box<dyn Function>`\n  --> src/resolve.rs:79:22\n   |\n79 |     functions.extend(vector_vrl_functions::all());\n   |               ------ ^^^^^^^^^^^^^^^^^^^^^^^^^^^ expected trait `vrl::compiler::Function`, found trait `vrl::compiler::function::Function`\n   |               |\n   |               required by a bound introduced by this call\n\n\nvector version:  0.34.0\nvrl version: 0.8.1\nmy Cargo.toml configuration:\n[dependencies]\nvrl.workspace = true\nvector-vrl-functions = { package = \"vector-vrl-functions\" , git = \"https://github.com/vectordotdev/vector\", tag = \"v0.34.0\"}\nenrichment = { git = \"https://github.com/vectordotdev/vector\", tag = \"v0.34.0\"}\nvector-common = { git = \"https://github.com/vectordotdev/vector\", tag = \"v0.34.0\"}",
        "url": "https://github.com/vectordotdev/vector/discussions/19124",
        "createdAt": "2023-11-12T07:39:04Z",
        "updatedAt": "2023-11-20T14:43:34Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19164,
        "title": "Are you changing https://hub.docker.com/r/timberio/vector like you're moving to apt.vector.dev and yum.vector.dev",
        "bodyText": "With the apt and yum/dnf repositories moving to apt.vector.dev and yum.vector.dev, are you also moving the DockerHub repository (https://hub.docker.com/r/timberio/vector) too?",
        "url": "https://github.com/vectordotdev/vector/discussions/19164",
        "createdAt": "2023-11-15T23:52:23Z",
        "updatedAt": "2023-11-16T23:57:02Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "EdN-terascope"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19173,
        "title": "Help me write a configuration for using vectors to ssh to the server and retrieve log files from the server.",
        "bodyText": "Help me write a configuration for using vectors to ssh to the server and retrieve log files from the server.\ndetails\n\nSet the source to ssh to the destination server.\nSet the path to the log file on the destination machine.\nSet the sink to loki.",
        "url": "https://github.com/vectordotdev/vector/discussions/19173",
        "createdAt": "2023-11-16T09:41:34Z",
        "updatedAt": "2023-11-16T15:54:22Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "fam11251"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 19123,
        "title": "What is data_dir? Should the user create it?",
        "bodyText": "[sources.nginx_logs]\ntype = \"file\"\ninclude = [ \"/var/log/nginx/access.log\" ]\nread_from = \"end\"\n\n[sinks.my_sink_id]\ntype = \"console\"\ninputs = [\"nginx_logs\"]\nencoding.codec = \"json\"\nWhen trying to run the above on a fresh install of Vector from the downloaded bash script (not via apt) I encountered the same issue as this discussion:\n#11390\n2023-11-12T03:46:29.890581Z ERROR vector::topology::builder: Configuration error. error=Source \"nginx_logs\": data_dir \"/var/lib/vector/\" does not exist\nBased on the above discussion, I tried creating fixing by creating sudo mkdir /var/lib/vector but that led to the next error:\nConfiguration error. error=Source \"nginx_logs\": Could not create subdirectory \"nginx_logs\" inside of data dir \"/var/lib/vector/\": Permission denied (os error 13)\nThis feels like I'm going the wrong direction due to the permissions. Instead what should I be doing?",
        "url": "https://github.com/vectordotdev/vector/discussions/19123",
        "createdAt": "2023-11-12T04:15:14Z",
        "updatedAt": "2023-11-14T12:53:50Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ddxv"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19126,
        "title": "Clarification on Vector's Reaction to Loki Ingestion Rate Limit Exceeded",
        "bodyText": "Hi,\nWhen i using the Loki sink in Vector, it appears that if the ingestion_rate_mb or ingestion_burst_size_mb rate limit is exceeded for a tenant, Loki drops the logs with a 429 Too Many Requests HTTP status code. This raises a question about how Vector, specifically through vector, reacts in such a scenario.\nRetry Mechanism in Vector.dev:\nIn the event of a Loki ingestion rate limit being exceeded, does Vector have a built-in retry mechanism? If so, how does it operate, and are there any configurable parameters related to retry behavior?\nEffect of Acknowledgment Setting:\nWhen the ack setting is set to true, and if there's a Kafka as a vector source, my understanding is that Vector will continue to retry until it receives a 2xx HTTP status code and continues consume for example. Is this correct? Could you provide additional details on how the retry mechanism works in this specific context?\nthanks",
        "url": "https://github.com/vectordotdev/vector/discussions/19126",
        "createdAt": "2023-11-12T16:40:51Z",
        "updatedAt": "2023-11-14T18:43:49Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19100,
        "title": "log_to_metric template tags",
        "bodyText": "Hello, I would like to reuse one log_to_metric transform for metrics with different tags. I don't know the names of the tags in advance but I know that they are defined under the .labels path.\nLet's say I have the following events:\n{\"name\": \"apple_metric\", \"labels\": {\"color\": \"red\"}},\n{\"name\": \"car_metric\", \"labels\": {\"manufacturer\": \"Audi\"}}\nand the following config\n    [transforms.special_count_metrics]\n      type = \"log_to_metric\"\n      inputs = [ \"...\" ]\n\n      [[transforms.special_count_metrics.metrics]]\n        type = \"counter\"\n        field = \"...\"\n        name = \"{{ name }}\"\n        namespace = \"vector\"\n        increment_by_value = false\n\n        [transforms.special_count_metrics.metrics.tags]\n          # This would be the common tags\n          env = \"test\"\n          region = \"cmh-1\"\n\n          # I need to template color and/or manufacturer here\nHow would I declare all fields defined under the .labels path as tags, Is this possible?",
        "url": "https://github.com/vectordotdev/vector/discussions/19100",
        "createdAt": "2023-11-09T15:17:08Z",
        "updatedAt": "2023-11-17T16:18:13Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "arch-xtof"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19072,
        "title": "Relaying from Docker to GELF requires except_fields",
        "bodyText": "Hi!\nI'm trying to use Vector to relay log messages from my local Docker installation to a Graylog server.\nI know Docker has native GELF logging capability, but when the Graylog server is unreachable for whatever reason the GELF logging driver prevents Docker from starting containers, unfortunately.\nMy solution is to scrape the Docker logs with Vector and send them to the Graylog server.\nSo this is my Vector config:\nsources:\n  dockerd:\n    type: docker_logs\n\nsinks:\n  graylog:\n    type: \"socket\"\n    inputs: [\"dockerd\"]\n    address: \"graylog.example.com:12201\"\n    mode: \"tcp\"\n    encoding:\n      codec: \"gelf\"\n\nUsing it somewhat works, but there are frequent errors like this in the vector log:\nvector[19725]: 2023-11-06T15:32:32.746403Z ERROR sink{component_kind=\"sink\" component_id=graylog component_type=socket component_name=graylog}: vector::internal_events::codecs: Failed serializing frame. error=LogEvent contains a value with an invalid type. field = \"container_created_at\" type = \"timestamp\" expected type = \"string or number\" error_type=\"encoder_failed\" stage=\"sending\" internal_log_rate_limit=true\n\nThese errors lead to a lot of missing messages in Graylog.\nThe errors go away when adding except_fields: [\"label\", \"container_created_at\"] to the encoding section of the sink, and it doesn't seem like there is anything missing on the Graylog side when omitting these fields.\nBut I don't fully understand the ramifications.\nShould I file a Github issue about this?",
        "url": "https://github.com/vectordotdev/vector/discussions/19072",
        "createdAt": "2023-11-07T08:58:44Z",
        "updatedAt": "2023-11-10T08:53:15Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "avollmerhaus"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 19010,
        "title": "Seeking Recommendations for Alerting Based on Vector Metrics in Kubernetes with Prometheus",
        "bodyText": "ello Vector maintainers and community,\nI've been using Vector within my Kubernetes cluster for log processing and forwarding. I have Prometheus set up to collect metrics from Vector. While I have a basic understanding of the metrics provided, I'm looking for recommendations or best practices when it comes to alerting based on these metrics to ensure the health and performance of Vector.\nHere are some of the metrics I've been collecting:\nvector_buffer_byte_size\nvector_buffer_events\nvector_buffer_max_byte_size\nvector_buffer_max_event_size\nvector_buffer_received_bytes_total\nCould anyone share their experience or recommendations on setting up alerts for these (or other relevant) Vector metrics?\nAre there any specific thresholds or patterns that the community has found to be particularly indicative of issues or noteworthy situations?\nAny advice, reference configurations, or links to best practices would be greatly appreciated. Thank you in advance!",
        "url": "https://github.com/vectordotdev/vector/discussions/19010",
        "createdAt": "2023-10-31T12:00:09Z",
        "updatedAt": "2023-11-10T08:48:49Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "nikolasj"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18712,
        "title": "Parsing auditd logs has issues due to a 0x1D character",
        "bodyText": "When using auditd set to log_format = ENRICHED there is a \"group separator\" character between the existing fields and unenriched fields.\nThis causes Vector to not split on this field when using parse_key_value.\nWhen attempting to use strip_ansi_escape_codes, this only removes the value, but we need to split on it.\nWhen attempting to use replace, we get errors for unexpected error: invalid escape character: \\u\nAn example log line would be:\ntype=SYSCALL msg=audit(1695946364.453:9863): arch=c000003e syscall=257 success=yes exit=8 a0=ffffff9c a1=7f8f9b31cd80 a2=80002 a3=0 items=1 ppid=8221 pid=9287 auid=4294967295 uid=0 gid=1002 euid=0 suid=0 fsuid=0 egid=1002 sgid=1002 fsgid=1002 tty=pts0 ses=4294967295 comm=\"sudo\" exe=\"/usr/bin/sudo\" key=\"logins\"ARCH=x86_64 SYSCALL=openat AUID=\"unset\" UID=\"root\" GID=\"ssm-user\" EUID=\"root\" SUID=\"root\" FSUID=\"root\" EGID=\"ssm-user\" SGID=\"ssm-user\" FSGID=\"ssm-user\"\n\nAnd key=\"logins\"ARCH=x86_64 are shown as joined when viewing using cat or tail etc.\nWhen using the transform .message = parse_key_value!(string!(.message)) we get the following:\n\"key\": \"(null)\\u001dARCH=x86_64\",\n\nIs there a recommended way we can replace \\u001d with a whitespace before we send it to parse_key_value so that it properly gets picked up a two fields, not one?\nA separate question may be the possibility of a parse_auditd function.\nExtra information here:\n\nlinux-audit/audit-userspace#140\nhttps://github.com/linux-audit/audit-documentation/wiki/SPEC-Audit-Event-Enrichment",
        "url": "https://github.com/vectordotdev/vector/discussions/18712",
        "createdAt": "2023-09-29T00:20:27Z",
        "updatedAt": "2023-11-07T22:16:21Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "alex-rowe"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 19076,
        "title": "vector stops scraping after a random amount of time",
        "bodyText": "1st let me say I am sorry if this is a dumb question.\nI am trying to send Metrics from a Linux server (csv file) to Victoria metrics.\nchallenges:\n\nI don't have access to the server I only built out toml on test server that I did have access to.\nI am reliant on the team that owns the server to send me the errors log\nfiles are received every minute\npurge will only keep last 5 minutes' worth of files\n\nIssue:\nthe service will successfully scape metrics for a few hours and then it just stops scraping. the only error message they found and sent to me says.\nWARN source(component_kind=\"source\" component_id=(toml name) component_type=file component_name=(toml name)):file_server: vector::internal_events::file::source: Currently ignoring file too small to fingerprint. (file address)\nmy source has the following fingerprint set:\nfingerprint.lines=2 fingerprint.strategy=\"checksum\" fingerprint.ignored_header_bytes = 0\nThe header will be exactly the same for each csv it is in the second row starting in 4th column that we start seeing metrics values.\nif we receive an empty file will that cause the error above and will that also cause Vector to stop scraping?",
        "url": "https://github.com/vectordotdev/vector/discussions/19076",
        "createdAt": "2023-11-07T16:03:21Z",
        "updatedAt": "2023-11-07T16:03:52Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Manniac07"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 19073,
        "title": "What is the exact, complete behaviour of health checks?",
        "bodyText": "Vector supports various methods to define which health checks to run; but the documentation doesn't describe the actual behaviour when such health checks fail.\nConfiguring health checks\nThere are a several methods for configuring health checks execution:\n\nA per-sink healthcheck.enabled boolean flag;\nA global healthchecks section, with two options:\n\nenabled to enable/disable them globally;\nrequire_healthy (or --require-healthy) to refuse starting when any health check fails\n\n\nThe --no-environment flag for vector validate disables health checks execution on validation\n\nHealth check impacts\nWhat happens when a sink health check is enabled and fails?\n\nUnreachable at startup:\n\nIf vector validate is part of the startup sequence (ExecStartPre = vector validate in a SystemD unit), the unit won't start (unless --no-environment is set);\nIf healthchecks.require_healthy is configured, vector will fail to start\nIf none of those checks are performed (either no vector validate or with --no-environment; no require_healthy), what happens? Will vector retry the health check until it succeeds? See #16810\n\n\nTurns unreachable while running:\n\nAre those health checks even run once vector has started?\nDoes Vector retry the health check until it succeeds? How often?\n\n\nUnreachable at reload:\n\nIf a health check fails when the process reloads (newly added sink or existing one), will the vector instance stop?",
        "url": "https://github.com/vectordotdev/vector/discussions/19073",
        "createdAt": "2023-11-07T11:12:36Z",
        "updatedAt": "2023-11-07T13:15:13Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "rbarrois"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19034,
        "title": "\"execute shell\" function in Rancher doesn't work",
        "bodyText": "A note for the community\nNo response\nProblem\nI deployed Vector on our cluster using the helm chart according to the guide below.\nhttps://vector.dev/docs/setup/installation/package-managers/helm/\nBut when I click \"execute shell\" on Rancher dashboard, it doesn't work(Disconnected). While \"view log\" is working well.\nAre there any values in helm chart template that have influence on it?\nConfiguration\nNo response\nVersion\n0.33.0\nDebug Output\nNo response\nExample Data\nNo response\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/19034",
        "createdAt": "2023-11-02T16:07:54Z",
        "updatedAt": "2023-11-07T13:25:47Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "xjtupdy"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19030,
        "title": "Vector agent could not find file to watch after running sudo systemctl start vector",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nHi,\nI got a problem that when I want to use 'file' source to monitor logs files, it worked with vector executable:\nvector --config vector.toml \n\nwith logs:\nsource{component_kind=\"source\" component_id=log component_type=file component_name=log}:file_server: vector::internal_events::file::source: Found new file to watch. file=/home/ubuntu/logs/canary/1/2/3/4/5.log\n2023-11-02T11:43:58.933251Z  INFO source{component_kind=\"source\" component_id=log component_type=file component_name=log}:file_server: vector::internal_events::file::source: Found new file to watch. file=/home/ubuntu/logs/canary/1/2/3/4/test.log\n\nWhich looks good to me.\nHowever, it won't be able to find the file to watch when I ran\nsudo systemctl start vector\n\nlogs:\nNov 02 11:35:14 ip-10-4-1-129 systemd[1]: Started Vector.\nNov 02 11:35:14 ip-10-4-1-129 vector[306863]: 2023-11-02T11:35:14.618550Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=info,rdkafka=info,buffers=info,lapin=info,kube=info\"\nNov 02 11:35:14 ip-10-4-1-129 vector[306863]: 2023-11-02T11:35:14.619429Z  WARN vector::config::loading: DEPRECATED Using the deprecated \"/etc/vector/vector.toml\" config path as the default config location. Vector is migrating to YAML as the default config format. Future>\nNov 02 11:35:14 ip-10-4-1-129 vector[306863]: 2023-11-02T11:35:14.619532Z  INFO vector::app: Loading configs. paths=[\"/etc/vector/vector.toml\"]\nNov 02 11:35:14 ip-10-4-1-129 vector[306863]: 2023-11-02T11:35:14.775263Z  INFO vector::topology::running: Running healthchecks.\nNov 02 11:35:14 ip-10-4-1-129 vector[306863]: 2023-11-02T11:35:14.775387Z  INFO vector: Vector has started. debug=\"false\" version=\"0.33.0\" arch=\"x86_64\" revision=\"89605fb 2023-09-27 14:18:24.180809939\"\nNov 02 11:35:14 ip-10-4-1-129 vector[306863]: 2023-11-02T11:35:14.775403Z  INFO vector::app: API is disabled, enable by setting `api.enabled` to `true` and use commands like `vector top`.\nNov 02 11:35:14 ip-10-4-1-129 vector[306863]: 2023-11-02T11:35:14.775765Z  INFO source{component_kind=\"source\" component_id=log component_type=file component_name=log}: vector::sources::file: Starting file server. include=[\"/home/*/logs/**/*.log\"] exclude=[]\nNov 02 11:35:14 ip-10-4-1-129 vector[306863]: 2023-11-02T11:35:14.780520Z  INFO source{component_kind=\"source\" component_id=log component_type=file component_name=log}:file_server: file_source::checkpointer: Loaded checkpoint data.\nNov 02 11:35:15 ip-10-4-1-129 vector[306863]: 2023-11-02T11:35:15.291235Z  INFO vector::topology::builder: Healthcheck passed.\n\nwhere the above logs are missing, which confused me because  both of them are running with the same config file.\nRelated config:\n[sources.log]\ntype = \"file\"\ninclude = [\"/home/*/logs/**/*.log\"]\nignore_checkpoints = true\nread_from = \"beginning\"\nfingerprinting.strategy = \"device_and_inode\"\n\nAnyone could help?\nThanks!\nConfiguration\nNo response\nVersion\nvector 0.33.0 (x86_64-unknown-linux-gnu 89605fb 2023-09-27 14:18:24.180809939)\nDebug Output\nsource{component_kind=\"source\" component_id=log component_type=file component_name=log}:file_server: vector::internal_events::file::source: Found new file to watch. file=/home/ubuntu/logs/canary/1/2/3/4/5.log\n2023-11-02T11:43:58.933251Z  INFO source{component_kind=\"source\" component_id=log component_type=file component_name=log}:file_server: vector::internal_events::file::source: Found new file to watch. file=/home/ubuntu/logs/canary/1/2/3/4/test.log\n\nExample Data\nNo response\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/19030",
        "createdAt": "2023-11-02T11:56:42Z",
        "updatedAt": "2023-11-02T13:27:18Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Joshualy94"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18926,
        "title": "how can I use prometheus_exporter with ipv6?",
        "bodyText": "I have a k8s cluster on IPv6, and the Vector pod works on IPv6.\nI use the Vector prometheus exporter sink, and I know the address option.\nI hope the prometheus exporter listens to the IPv6 address. How should I do it?\nMy IPv4 setting is 0.0.0.0:9090.\nsetting:\n      prom_exporter:\n        type: prometheus_exporter\n        inputs: [internal_metrics]\n        address: 0.0.0.0:9090",
        "url": "https://github.com/vectordotdev/vector/discussions/18926",
        "createdAt": "2023-10-25T01:24:15Z",
        "updatedAt": "2023-11-02T07:04:26Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "zey1996"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 19007,
        "title": "Unable to send data to aws cloudwatch log.",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nI am trying to send the data to AWS cloudwatch log. So firstly I have tried sending data from stdin (configured my source as stdin), and I am able to send the data after executing command\necho \"test message\" | vector\nFollowing is the logs for above command\n2023-11-01T08:23:16.344503Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=info,rdkafka=info,buffers=info,lapin=info,kube=info\"\n2023-11-01T08:23:16.345115Z  WARN vector::config::loading: DEPRECATED Using the deprecated \"/etc/vector/vector.toml\" config path as the default config location. Vector is migrating to YAML as the default config format. Future Vector versions will use \"/etc/vector/vector.yaml\" as the default config location.\n2023-11-01T08:23:16.345169Z  INFO vector::app: Loading configs. paths=[\"/etc/vector/vector.toml\"]\n2023-11-01T08:23:16.346780Z  INFO vector::sources::file_descriptors: Capturing stdin.\n2023-11-01T08:23:16.417517Z  INFO vector::topology::running: Running healthchecks.\n2023-11-01T08:23:16.417612Z  INFO vector: Vector has started. debug=\"false\" version=\"0.33.0\" arch=\"x86_64\" revision=\"89605fb 2023-09-27 14:18:24.180809939\"\n2023-11-01T08:23:16.417627Z  INFO vector::app: API is disabled, enable by setting `api.enabled` to `true` and use commands like `vector top`.\n2023-11-01T08:23:16.417964Z  INFO vector_common::shutdown: All sources have finished.\n2023-11-01T08:23:16.417981Z  INFO vector_common::shutdown: All sources have finished.\n2023-11-01T08:23:16.419058Z  INFO vector::app: All sources have finished.\n2023-11-01T08:23:16.419077Z  INFO vector: Vector has stopped.\n2023-11-01T08:23:16.419284Z  INFO sink{component_kind=\"sink\" component_id=aws_cloudwatch_logs component_type=aws_cloudwatch_logs component_name=aws_cloudwatch_logs}:request{request_id=1}: vector::sinks::aws_cloudwatch_logs::service: Sending events. events=1\n2023-11-01T08:23:16.420619Z  INFO vector::topology::running: Shutting down... Waiting on running components. remaining_components=\"aws_cloudwatch_logs\" time_remaining=\"59 seconds left\"\n2023-11-01T08:23:17.010537Z ERROR vector::topology::builder: msg=\"Healthcheck failed.\" error=DescribeLogGroups failed: service error component_kind=\"sink\" component_type=\"aws_cloudwatch_logs\" component_id=aws_cloudwatch_logs component_name=aws_cloudwatch_logs\n2023-11-01T08:23:17.058468Z  INFO sink{component_kind=\"sink\" component_id=aws_cloudwatch_logs component_type=aws_cloudwatch_logs component_name=aws_cloudwatch_logs}:request{request_id=1}: vector::sinks::aws_cloudwatch_logs::request: Putting logs. token=Some(\"49039859565611948875833551386623940985914745056819434304\")\n2023-11-01T08:23:17.493723Z  INFO sink{component_kind=\"sink\" component_id=aws_cloudwatch_logs component_type=aws_cloudwatch_logs component_name=aws_cloudwatch_logs}:request{request_id=1}: vector::sinks::aws_cloudwatch_logs::request: Putting logs was successful. next_token=Some(\"49645815922851066143980076859732739576820567984046080434\")\n\nNo when I try to send the data from http_server with following configuration file\n[sources.http]\ntype = \"http_server\"\naddress = \"0.0.0.0:8080\"\nencoding = \"json\"\nhealthcheck = true\n\n\n[sinks.aws_cloudwatch_logs]\ntype = \"aws_cloudwatch_logs\"\ninputs = [\"http\"]\ncompression = \"none\"\ncreate_missing_group = true\ncreate_missing_stream = true\ngroup_name = \"test_opentelemetry\"\nregion = \"ap-south-1\"\nstream_name = \"test\"\nencoding.codec = \"json\"\n\nI am getting following error\nnuc2kor@PUA-V-0001T:~$ sudo RUST_LOG=aws_smithy_http=trace vector -c /etc/vector/vector.toml \n2023-11-01T08:26:26.497705Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=info,rdkafka=info,buffers=info,lapin=info,kube=info\"\n2023-11-01T08:26:26.499104Z  INFO vector::app: Loading configs. paths=[\"/etc/vector/vector.toml\"]\n2023-11-01T08:26:26.571570Z  INFO vector::topology::running: Running healthchecks.\n2023-11-01T08:26:26.571650Z  INFO vector: Vector has started. debug=\"false\" version=\"0.33.0\" arch=\"x86_64\" revision=\"89605fb 2023-09-27 14:18:24.180809939\"\n2023-11-01T08:26:26.571659Z  INFO vector::app: API is disabled, enable by setting `api.enabled` to `true` and use commands like `vector top`.\n2023-11-01T08:26:26.571872Z  INFO source{component_kind=\"source\" component_id=http component_type=http_server component_name=http}: vector::sources::util::http::prelude: Building HTTP server. address=0.0.0.0:8080\n2023-11-01T08:26:27.573499Z ERROR vector::topology::builder: msg=\"Healthcheck failed.\" error=DescribeLogGroups failed: failed to construct request component_kind=\"sink\" component_type=\"aws_cloudwatch_logs\" component_id=aws_cloudwatch_logs component_name=aws_cloudwatch_logs\n2023-11-01T08:26:45.710592Z  INFO sink{component_kind=\"sink\" component_id=aws_cloudwatch_logs component_type=aws_cloudwatch_logs component_name=aws_cloudwatch_logs}:request{request_id=1}: vector::sinks::aws_cloudwatch_logs::service: Sending events. events=2\n2023-11-01T08:26:46.711527Z ERROR sink{component_kind=\"sink\" component_id=aws_cloudwatch_logs component_type=aws_cloudwatch_logs component_name=aws_cloudwatch_logs}:request{request_id=1}: vector::sinks::util::retries: Non-retriable error; dropping the request. error=CloudwatchError::Describe: failed to construct request internal_log_rate_limit=true\n2023-11-01T08:26:46.711549Z ERROR sink{component_kind=\"sink\" component_id=aws_cloudwatch_logs component_type=aws_cloudwatch_logs component_name=aws_cloudwatch_logs}:request{request_id=1}: vector::sinks::util::retries: Internal log [Non-retriable error; dropping the request.] is being suppressed to avoid flooding.\n2023-11-01T08:26:46.711583Z ERROR sink{component_kind=\"sink\" component_id=aws_cloudwatch_logs component_type=aws_cloudwatch_logs component_name=aws_cloudwatch_logs}:request{request_id=1}: vector_common::internal_event::service: Service call failed. No retries or retries exhausted. error=Some(Describe(ConstructionFailure(ConstructionFailure { source: SigningStageError { kind: MissingCredentials } }))) request_id=1 error_type=\"request_failed\" stage=\"sending\" internal_log_rate_limit=true\n2023-11-01T08:26:46.711602Z ERROR sink{component_kind=\"sink\" component_id=aws_cloudwatch_logs component_type=aws_cloudwatch_logs component_name=aws_cloudwatch_logs}:request{request_id=1}: vector_common::internal_event::component_events_dropped: Events dropped intentional=false count=2 reason=\"Service call failed. No retries or retries exhausted.\" internal_log_rate_limit=true\n\nThe sink I have configured is same in both the cases, but I am not able to send the data to the cloud watch using http)server or file as source.\nConfiguration\n[sources.http]\ntype = \"http_server\"\naddress = \"0.0.0.0:8080\"\nencoding = \"json\"\nhealthcheck = true\n\n\n[sinks.aws_cloudwatch_logs]\ntype = \"aws_cloudwatch_logs\"\ninputs = [\"http\"]\ncompression = \"none\"\ncreate_missing_group = true\ncreate_missing_stream = true\ngroup_name = \"test_opentelemetry\"\nregion = \"ap-south-1\"\nstream_name = \"test\"\nencoding.codec = \"json\"\n\nVersion\n0.33.0\nDebug Output\nnuc2kor@PUA-V-0001T:~$ sudo RUST_LOG=aws_smithy_http=trace vector -c /etc/vector/vector.toml \n2023-11-01T08:26:26.497705Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=info,rdkafka=info,buffers=info,lapin=info,kube=info\"\n2023-11-01T08:26:26.499104Z  INFO vector::app: Loading configs. paths=[\"/etc/vector/vector.toml\"]\n2023-11-01T08:26:26.571570Z  INFO vector::topology::running: Running healthchecks.\n2023-11-01T08:26:26.571650Z  INFO vector: Vector has started. debug=\"false\" version=\"0.33.0\" arch=\"x86_64\" revision=\"89605fb 2023-09-27 14:18:24.180809939\"\n2023-11-01T08:26:26.571659Z  INFO vector::app: API is disabled, enable by setting `api.enabled` to `true` and use commands like `vector top`.\n2023-11-01T08:26:26.571872Z  INFO source{component_kind=\"source\" component_id=http component_type=http_server component_name=http}: vector::sources::util::http::prelude: Building HTTP server. address=0.0.0.0:8080\n2023-11-01T08:26:27.573499Z ERROR vector::topology::builder: msg=\"Healthcheck failed.\" error=DescribeLogGroups failed: failed to construct request component_kind=\"sink\" component_type=\"aws_cloudwatch_logs\" component_id=aws_cloudwatch_logs component_name=aws_cloudwatch_logs\n2023-11-01T08:26:45.710592Z  INFO sink{component_kind=\"sink\" component_id=aws_cloudwatch_logs component_type=aws_cloudwatch_logs component_name=aws_cloudwatch_logs}:request{request_id=1}: vector::sinks::aws_cloudwatch_logs::service: Sending events. events=2\n2023-11-01T08:26:46.711527Z ERROR sink{component_kind=\"sink\" component_id=aws_cloudwatch_logs component_type=aws_cloudwatch_logs component_name=aws_cloudwatch_logs}:request{request_id=1}: vector::sinks::util::retries: Non-retriable error; dropping the request. error=CloudwatchError::Describe: failed to construct request internal_log_rate_limit=true\n2023-11-01T08:26:46.711549Z ERROR sink{component_kind=\"sink\" component_id=aws_cloudwatch_logs component_type=aws_cloudwatch_logs component_name=aws_cloudwatch_logs}:request{request_id=1}: vector::sinks::util::retries: Internal log [Non-retriable error; dropping the request.] is being suppressed to avoid flooding.\n2023-11-01T08:26:46.711583Z ERROR sink{component_kind=\"sink\" component_id=aws_cloudwatch_logs component_type=aws_cloudwatch_logs component_name=aws_cloudwatch_logs}:request{request_id=1}: vector_common::internal_event::service: Service call failed. No retries or retries exhausted. error=Some(Describe(ConstructionFailure(ConstructionFailure { source: SigningStageError { kind: MissingCredentials } }))) request_id=1 error_type=\"request_failed\" stage=\"sending\" internal_log_rate_limit=true\n2023-11-01T08:26:46.711602Z ERROR sink{component_kind=\"sink\" component_id=aws_cloudwatch_logs component_type=aws_cloudwatch_logs component_name=aws_cloudwatch_logs}:request{request_id=1}: vector_common::internal_event::component_events_dropped: Events dropped intentional=false count=2 reason=\"Service call failed. No retries or retries exhausted.\" internal_log_rate_limit=true\n\nExample Data\nNo response\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/19007",
        "createdAt": "2023-11-01T08:29:27Z",
        "updatedAt": "2023-11-01T10:19:23Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Strange21"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18943,
        "title": "The right way to profile vector",
        "bodyText": "Hello, Vector team!\nIn Kubernetes clusters under my control, some installations of Vector consume a lot of CPU and memory (3 cores and 7Gb RAM top). Probably, the problem is with the configuration, but I want to find exactly where.\nDo you have any tips or tricks how I can shed more light on why vector has such an appetite? It would be nice if there is the way to extract the date from a running process, but not required.",
        "url": "https://github.com/vectordotdev/vector/discussions/18943",
        "createdAt": "2023-10-25T21:26:30Z",
        "updatedAt": "2023-10-25T21:26:31Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "nabokihms"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 17677,
        "title": "Source vector under the hood",
        "bodyText": "Hello,\nWhat does \"vector\" when used as a sink and source combination use under the hood? I am assuming it is some sort of a tcp connection?\nAnd is there a provision to say sink to multiple sources using some sort of multicast.\nThanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/17677",
        "createdAt": "2023-06-13T13:21:09Z",
        "updatedAt": "2023-10-25T06:15:29Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bmallya"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 18920,
        "title": "Can Vector processing metrics data from Kafka to Prometheus directly",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nI am trying to find a tool which can support:\nprometheus -(1)-> kafka -(2)-> VictoriaMetrics\n(1) prometheus -vector-> kafka Succeed\nvector's prometheus_remote_write source and kafka sink help me send data from prometheus to kafka already. The data in kafka is like:\n{\"name\":\"up\",\"tags\":{\"instance\":\"10.32.140.2:9090\",\"job\":\"prometheus\",\"usage\":\"test-vector\"},\"timestamp\":\"2023-10-24T11:27:07.024Z\",\"kind\":\"absolute\",\"gauge\":{\"value\":1.0}}\n(2) kafka --> VictoriaMetrics Failed\nI got error when I use kafka source and prometheus_remote_write sink:\nERROR vector::cli: Configuration error. error=Data type mismatch between prome_kafka_in (Log) and out_to_prome (Metric)\n\nI noticed that there is a transform named log_to_metric. I tried but it did not work as expected.\n\nConfiguration\n[sources.prome_kafka_in]\ntype = \"kafka\"\nbootstrap_servers = \"kafka_addr:9092\"\ntopics = [\"sre_prometheus_remote_write\"]\ngroup_id =\"vector_prome_kafka\"\n\n#[transforms.log_to_metric]\n#type = \"log_to_metric\"\n#inputs = [ \"prome_kafka_in\" ]\n#\n#  [[transforms.log_to_metric.metrics]]\n#  field = \"gauge.value\"\n#  name = \"up\"\n#  type = \"gauge\"\n\n[sinks.out_to_prome]\ntype = \"prometheus_remote_write\"\ninputs = [ \"prome_kafka_in\" ]\nendpoint = \"http://remote_write_addr\"\n\nVersion\ntimberio/vector:0.33.0-debian\nDebug Output\nNo response\nExample Data\nNo response\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/18920",
        "createdAt": "2023-10-24T11:58:49Z",
        "updatedAt": "2023-10-24T14:27:23Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "u-kyou"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18912,
        "title": "KAFKA SINK encoding error",
        "bodyText": "Hi All, I am trying to receive logs from syslog and forward to kafka.\nOct 23 18:04:51 xxxvector[594955]: 2023-10-23T17:04:51.247550Z DEBUG vector::app: messaged=\"Building runtime.\" worker_threads=80\nOct 23 18:04:51 xxxvector[594955]: 2023-10-23T17:04:51.303469Z  INFO vector::app: Loading configs. paths=[\"/conf/config.json\"]\nOct 23 18:04:51 xxxvector[594955]: 2023-10-23T17:04:51.305713Z ERROR vector::cli: Configuration error. error=missing field `encoding'\nmy  configuration:\n\"sinks\": {\n\"to_kafka\": {\n\"type\": \"kafka\",\n\"inputs\": [\n\"json_src\"\n],\n\"bootstrap_servers\": \"10.14.22.123:9092,10.14.23.332:9092\",\n\"topic\": \"test\",\n\"encoding.codec\": \"raw_message\",\n\"compression\": \"none\",\n\"key_field\": \"hostname\",\n\"librdkafka_options\": {\n\"client.id\": \"xxx\",\n\"ssl.key.location\": \"config/certs/cxxx.key\",\n\"ssl.certificate.location\": \"config/certs/xxx.crt\",\n\"ssl.ca.location\": \"config/certs/truststore.pem\"\n}\n}\n}\n}\nWhat should the encoding be? i tried encoding.codec json and raw message but it will wants 'encoding' key.\nthanks in advance!",
        "url": "https://github.com/vectordotdev/vector/discussions/18912",
        "createdAt": "2023-10-23T17:08:44Z",
        "updatedAt": "2023-10-23T18:29:55Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "coredump17"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 18862,
        "title": "Traces information from Vector to AWS x-ray",
        "bodyText": "Hello.\nI see that Vector supports AWS CloudWatch logs and metrics. Does X-ray traces are being send as part of this data?\nIf not what is the solution to send the traces to AWS X-ray?",
        "url": "https://github.com/vectordotdev/vector/discussions/18862",
        "createdAt": "2023-10-17T13:43:32Z",
        "updatedAt": "2023-10-22T07:44:32Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "MichaelShapira"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18891,
        "title": "Let me try one more question",
        "bodyText": "I know my questions don't gain much traction but let me try one more time. And this might be a very easy one but I can't find it.\nIn reference to my earlier question about labels nested labels.\nI am now putting both labels into their own arrays dynamically. Is there a simple way to return the size of each array? Lua documentation says to add a # in front of the array but that is used in vector to signal the start of a comment.",
        "url": "https://github.com/vectordotdev/vector/discussions/18891",
        "createdAt": "2023-10-19T22:24:21Z",
        "updatedAt": "2023-10-20T12:36:25Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Manniac07"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18836,
        "title": "How to obtain multi-line logs from syslog",
        "bodyText": "How to obtain multi-line logs of type syslog in sources. It is currently known that multi-line logs of file and docker can be obtained. There is no problem in actual testing. If the input source is syslog, how should it be obtained?\nThe following is an example of how I can obtain a multi line log from a file. How can I modify it to obtain a multi line log from a syslog\n[sources.multiline_logs]\ntype = \"file\"\ninclude = [ \"/opt/multiline.log\" ]\n[sources.multiline_logs.multiline]\nstart_pattern = '^<\\d+>'\nmode = \"halt_before\"\ncondition_pattern = '<\\d+>'\ntimeout_ms = 1000",
        "url": "https://github.com/vectordotdev/vector/discussions/18836",
        "createdAt": "2023-10-13T10:14:18Z",
        "updatedAt": "2023-10-19T17:37:45Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "XSWClevo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18885,
        "title": "Is it possible to set the StorageResolution of a CloudWatch Metric?",
        "bodyText": "Is it possible to set the StorageResolution of a CloudWatch Metric?  I have some metrics that I need to see at a higher resolution than 60 seconds.  I configured my source (type: exec) to with a exec_interval_secs of 20 seconds and my sink as a CloudWatch Metric (type: aws_cloudwatch_metrics).  However, when I check the metrics in AWS they only seem to be at the 60 second resolution.\nLinks\n\nRust SDK: https://github.com/vectordotdev/aws-sdk-rust/blob/main/sdk/cloudwatch/src/types/_metric_datum.rs#L79\nSlightly Related Issues:\n\n#6940\n#856",
        "url": "https://github.com/vectordotdev/vector/discussions/18885",
        "createdAt": "2023-10-19T14:01:13Z",
        "updatedAt": "2023-10-19T14:01:14Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "rnhurt"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 2
    },
    {
        "number": 18577,
        "title": "Getting Attributes from SQS messages",
        "bodyText": "Right now I'm working with SQS sink and it all work fine but I only get the body of the SQS message, not the attributes\n\nThis is what the output of the sqs sink right now is\n\nI wonder if it's possible to also get that before we go on ahead to start rewriting things all over",
        "url": "https://github.com/vectordotdev/vector/discussions/18577",
        "createdAt": "2023-09-17T12:14:33Z",
        "updatedAt": "2023-10-18T12:18:58Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "johnxy84"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18860,
        "title": "VRL \"abort\" when end to end acknowledgement is enabled",
        "bodyText": "https://vector.dev/docs/about/under-the-hood/guarantees/#acknowledgement-guarantees states\n\nSome transforms will drop events as part of their normal operation. For example, the dedupe transform will drop events that duplicate another recent event in order to reduce data volume. When this happens, the event will be considered as having been delivered with no error.\n\nHow is the acknowledgement status handled when the remap transform intentionally \"aborts\" some of the events ?",
        "url": "https://github.com/vectordotdev/vector/discussions/18860",
        "createdAt": "2023-10-17T11:57:07Z",
        "updatedAt": "2023-10-17T15:43:16Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "atibdialpad"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18835,
        "title": "AWS Opensearch(es) assume_role auth not working",
        "bodyText": "Hi,\nI've created AWS ECS cluster of vector applications that basically takes log data via http_server and reroutes the data to AWS Opensearch, but the aws authentication assume_role option is giving me a terrible time and it'd be lovely if I can get any help or insight regarding this issue.\nBelow are the config specifications that I've used:\nI've created a task role(vector-task-role) with appropriate trusted relationship settings\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"ecs-tasks.amazonaws.com\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\nand set in my vector config to assume the same iam role as the one I've given the task:\n# input sources to vector\nsources:\n  logs_http:\n    type: http_server\n    method: \"POST\"\n    address: \"0.0.0.0:80\"\n    path: \"/logs/http\"\n    encoding: \"json\"\n\n# output location from vector\nsinks:\n  output_opensearch:\n    type: elasticsearch\n    inputs:\n      - logs_http\n    endpoints:\n      - [redacted_es_endpoint]\n    aws:\n      region: \"ap-northeast-2\"\n    bulk:\n      index: \"{{ .application_name }}-%Y%m%d\"\n    # ES health check\n    healthcheck:\n      enabled: false\n    auth:\n      strategy: \"aws\"\n      assume_role: \"arn:aws:iam::[redacted_account_no]:role/vector-task-role\"\n      region: \"ap-northeast-2\"\nThis is the error message I get:\n<Code>AccessDenied</Code>\\n    <Message>User: arn:aws:sts::[redacted_account_no]:assumed-role/vector-task-role/bd70121139e84e788534139edd9e4e4c is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::[redacted_account_no]:role/vector-task-role</Message>\n\nI've went through all the permissions in the vector-task-role and event gave it all permission just to see if there was something wrong on my end, but it still wouldn't work.\nDoes this error have to do with the aws sdk vector uses?\nAny help would be delightful.\nThanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/18835",
        "createdAt": "2023-10-13T09:51:23Z",
        "updatedAt": "2023-10-17T02:03:22Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "nic-igaw"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18841,
        "title": "Does vector continue reading from rotated files if rotation happens while it's not running?",
        "bodyText": "I see in the docs that vector continues reading until EOF for any files that rotate while it's running. But will it do this even if the file rotates while it's not running?\nE.g. with this config,\nsources:\n  test_source:\n    type: file\n    exclude:\n      - /var/log/*.[0-9]*.log\n    include:\n      - /var/log/*.log\n\nwhat would happen in this scenario?\n\nvector is started\nFile /var/log/application.log is created; vector starts tailing the file\nvector is stopped\napplication.log fills up and is rotated to application.1.log\nA new application.log is created\nvector is started\n\nWill vector read any logs that it missed from application.1.log on startup?",
        "url": "https://github.com/vectordotdev/vector/discussions/18841",
        "createdAt": "2023-10-14T00:02:14Z",
        "updatedAt": "2023-10-16T17:35:03Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "precisionconage"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18851,
        "title": "Open Telemetry",
        "bodyText": "Hello,\nCan vector support forwarding logs/events to an open telemetry collector? If so, is there an example ?\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/18851",
        "createdAt": "2023-10-16T08:53:43Z",
        "updatedAt": "2023-10-16T16:31:56Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bmallya"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18821,
        "title": "Multi line multi key label in header csv",
        "bodyText": "I need to find a way to parse a csv file with metrics values based off of 2 headers.\nExample:\ncsv file\nTimeStamp,KeyLabel,SubKeyLabel,Metrics 1,Metrics 2,3 Metrics\n1234567890,ABC,404 Not Found, 22,11,123\n1234567890,ABC,ALL Good,33,12,321\n1234567890,DEF,404 Not Found,44,18,456\n1234567890,DEF,ALL Good,27,17,654\nResults should be:\nKeyLabel-\"ABC\",SubKeyLabel=\"404 Not Found\", \"Metrics 1 Value\": \"22\"\nKevLabel=\"ABC\"\nSubKeyLabel=\"404 Not Found'\n\"Metrics 2 Value\": \"11\"\nKeyLabel-\"ABC\", SubKeyLabel=\"404 Not Found\"\n1, \"3\nMetrics Value\": 123\"\nKeyLabel-\"ABC\" ,SubKeyLabel-\"ALL Good\", \"Metrics 1 Value\u2122: 33\" KeyLabel-\"ABC\", SubKeyLabel=\"ALL Good\", \"Metrics 2 Value\": \"12\"\nKeyLabel=\"ABC\", SubKeyLabel-\"ALL Good\"\n\"3 Metrics Value\": \"321\nKeyLabel-\"DEF\", SubKeyLabel=\"404 Not Found\"\n\"Metrics\n1 Value\"\u00ab \"\u0434\u0434\nKeyLabel=\"DEF\", SubKeyLabel=\"404 Not Found\"\n'Metrics 2 Value\". 18\nKeyLabel-\"DEF\", SubKeyLabel-\"404 Not Found\", \"3 Metrics Value*: \"456\"\nKeyLabel-\"DEF\", SubKeyLabel-\"ALL Good\"\n\"Metrics 1 Value\": \"27\"\nKeyLabel-\"DEF',SubKeyLabel-\"ALL Good\", \"Metrics 2 Value': 17\" KeyLabel-\"DEF\", SubKeyLabel-\"ALL Good\", \"3 Metrics Value: \"654*",
        "url": "https://github.com/vectordotdev/vector/discussions/18821",
        "createdAt": "2023-10-11T16:04:22Z",
        "updatedAt": "2023-10-11T16:04:23Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Manniac07"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 18691,
        "title": "Multi-tenancy support",
        "bodyText": "Many thanks for a great project!\nWhat's the best way to allow dynamic sources and sinks to be defined? We would like to avoid having to orchestrate multiple instances of Vector if we can. I believe there are no APIs to manage sources and sinks on a running Vector instance. What are the possible ways to go about this?",
        "url": "https://github.com/vectordotdev/vector/discussions/18691",
        "createdAt": "2023-09-27T14:31:59Z",
        "updatedAt": "2023-10-10T08:18:00Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "deepakprabhakara"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 2
    },
    {
        "number": 18772,
        "title": "Convert hostname/fqdn to IP address",
        "bodyText": "Is there a way to convert hostname / FQDN to IP address as part of remap?\nI noticed reverse_dns, but that effectively gets to the FQDN, but looking for IP address.\nTIA.",
        "url": "https://github.com/vectordotdev/vector/discussions/18772",
        "createdAt": "2023-10-04T22:53:52Z",
        "updatedAt": "2023-10-05T09:12:27Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "amarnathpv"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 18756,
        "title": "Vector as a receiver for td-agent",
        "bodyText": "Hi all! I'm trying to setup integration with td-agent, since vector supported fluent as a source, but as a result I'm having an empty messages.\nVersions:\n\nvector: 0.29.1\ntd-agent: 4.3.0\n\nMy vector config:\nsources:\n  src_fluent:\n    type: fluent\n    address: 0.0.0.0:19020\n    keepalive:\n      time_secs: 10\nsinks:\n  snk_fluent:\n    type: file\n    inputs:\n    - src_fluent\n    path: \"/srv/vector/output/fluent.out\"\n    encoding:\n      codec: raw_message\n\nAnd here is config for td-agent:\n<source>\n  @type tail\n  tag vectortest\n  path /srv/vectortest/vector.log\n  <parse>\n   @type none\n  </parse>\n  pos_file /srv/vectortest/vector.log.pos\n</source>\n\n<match vectortest>\n  @type copy\n\n  <store>\n      @type forward\n      send_timeout 60s\n      compress gzip\n      <server>\n        name vector\n        host <vector host ip address>\n        port 19020\n      </server>\n  </store>\n\n</match>\nI've tried it with and without compression from td-agent side, different kind of parsers but no luck at all. I understand that support is in beta status now, but wonder maybe there's something obvious I'm missing. The fact that I'm receiving empty messages means the issue should be on a side of source, while parsing message. I'll really appreciate any help here.",
        "url": "https://github.com/vectordotdev/vector/discussions/18756",
        "createdAt": "2023-10-03T19:18:28Z",
        "updatedAt": "2023-10-03T21:22:58Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "gl1ridae"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18743,
        "title": "Internal_log_rate_limit=true",
        "bodyText": "Please,any help would be appreciated:\nSo I am running vector on a test server pulling data metrics from a csv file that gets loaded with a different file name every minute. When the file arrives, we transform it and Prometheus remote write it to Victoria metrics. (Note: we have 4 different tomls watching 4 different directories)(also given that it is a test server there will be instances that I will receive new files with different file names but exact same file values)\nWe set the remove_after_secs to 10, so the directories don't get full. This process will run for about an hour, then will error out with the following error:\n2023-09-29T23:47:52.067020Z ERROR source{component_kind=\"source\" component_ id=id component_type=file component_name=name}:file_ser ver:\nvector::internal events::file: :sourc\ne: Failed in deleting file. file=/ home/user name/directory path/file.csv error=Operation not permitted (os error 1)\nerror code=\"deletion failed' error type=\"command failed\" stage=\"receiving\" internal_log_rate_limit=true",
        "url": "https://github.com/vectordotdev/vector/discussions/18743",
        "createdAt": "2023-10-02T21:19:04Z",
        "updatedAt": "2023-10-03T14:59:06Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Manniac07"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18734,
        "title": "Mapping Prometheus into Datadog metrics",
        "bodyText": "Hi Team. Based on this https://docs.datadoghq.com/integrations/guide/prometheus-metrics/?tab=latestversion all metrics coming into Vector from any prometheus source whatever is scrape or remote_write metrics should be mapped probably in Datadog Metrics sink to achieve same results as in data coming for example from Datadog-Agent. Is there any solution planed do such thing?\nProbably any format supporting count, gauge, histogram or distribution like metrics should be at the end converted into format that will use all Datadog features and not be only 1:1 metrics store from source to land in DD which is not what we expect here. This also may be done in aggregation transform or something similar.",
        "url": "https://github.com/vectordotdev/vector/discussions/18734",
        "createdAt": "2023-09-30T21:29:11Z",
        "updatedAt": "2023-10-02T23:18:50Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "szibis"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 0
    },
    {
        "number": 18720,
        "title": "How to remove \".\": {}, from JSON?",
        "bodyText": "The program that generate the json produce something like this:\n(extracted)\n                          \"k:{\\\"name\\\":\\\"HOSTNAME\\\"}\": {\n                            \".\": {},\n                            \"f:name\": {},\n                            \"f:valueFrom\": {\n                              \".\": {},\n                              \"f:fieldRef\": {}\n                            }\n                          },\n\nI would like to remove these \".\": {},\nHow to write it in VRL in Vector?\nnew_message = replace(string!(.message), \"\\\".\\\": {},\", \"\")\nbut it complain about the period",
        "url": "https://github.com/vectordotdev/vector/discussions/18720",
        "createdAt": "2023-09-29T15:00:14Z",
        "updatedAt": "2023-10-02T14:44:22Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "patrickdung"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18689,
        "title": "Poblems trying to use AWS sink with assume_role",
        "bodyText": "Hello,\nOn our setup in order to access S3 bucket placed on another account we use sts assume role with external-id and role-session-name, we have a provided credentials file and we are able to use sts assume role, then access the bucket.\nI am trying to setup this using AWS S3 sink, having this:\nCredentials file set in /etc/aws/credentials and environment variable AWS_CONFIG_FILE set to use that file\nEnvironment variables ARN_EXTERNAL_ID and AWS_ROLE_SESSION_NAME\nThe sink section:\nsinks:\n  audit_bucket:\n    type: aws_s3\n    inputs:\n      - squid-logs.shadow\n    acknowledgements:\n      enabled: false\n    auth:\n      assume_role: ${ARN}\n      external_id: ${ARN_EXTERNAL_ID}\n    bucket: s3sinkbucket\n    encoding:\n      codec: json\n    healthcheck:\n      enabled: false\n    region: eu-central-1\n    storage_class: STANDARD_IA\n    filename_append_uuid: true\n    filename_time_format: \"%Y/%m/%d\"\n    key_prefix: \"squid_log/maint5.eu-central-1.aws.openpaas-maint.axa-cloud.com/%F/\"\n\nWhen I start vector it is stopping with following error:\n2023-09-27T08:17:34.874299Z ERROR vector::cli: Configuration error. error=data did not match any variant of untagged enum AwsAuthentication\nIf I remove the external_id, then it starts but it is not able to perform sts as the user is not allowed.\nIs there any way to place together assume_role and external_id ??",
        "url": "https://github.com/vectordotdev/vector/discussions/18689",
        "createdAt": "2023-09-27T08:32:26Z",
        "updatedAt": "2023-09-29T06:11:23Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "agustin-munoz"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 1
    },
    {
        "number": 18642,
        "title": "Can metrics be remapped to send multiple metrics?",
        "bodyText": "In the remap transform there is the option to split the log and send multiple by overriding the top level . to send an array instead of an object, does this also exist for metrics?\n/transforms/remap/#emitting-multiple-log-events\nFor example:\n[sources.internal_metrics]\ntype = \"internal_metrics\"\n\n[transforms.filter_metrics]\ntype = \"remap\"\ninputs = [\"internal_metrics\"]\nsource = '''\n  # code removed to filter on just the metrics we want\n  metric_one = .\n  metric_two = .\n  metric_one.tags.first_tag = \"string\"\n  metric_two.tags.second_tag = \"string\"\n  . = [metric_one, metric_two]\n\nThe use case for this would be for sending metrics with two different single dimensions, rather than multiple dimensions.\nAn option may be two create three transforms, so filter_metrics to just remove metrics we don't want, then use it as the input for two other transforms where they set their tag. Those two transforms are then inputs to the sink.",
        "url": "https://github.com/vectordotdev/vector/discussions/18642",
        "createdAt": "2023-09-21T23:57:30Z",
        "updatedAt": "2023-09-25T21:32:01Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "alex-rowe"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18587,
        "title": "Correlation between RAM, buffer size and batch size?",
        "bodyText": "Hi All,\nIn my current project we recently decided to set-up disk-buffer to increase durability.\nBefore that we had vector deployed in k8s with 4Gib RAM for every pod, batch.max_bytes ~5Mib and everything worked fine.\nThan, we set up disk buffer with buffer.max_size something around size of the RAM. From this moment, seems like vector started leaking memory, i.e. RAM usage started to grow until OOM killer does his job. I've played with configuration and fixed this problem with lowering  batch.max_bytes or with lowering buffer.max_size (in my case to 62% of RAM).\nWhile the problem seems to be solved, it's unclear to me, how these values correlate.\nSo, how do they correlate?\nHow to calculate batch and buffer sizes so no to do a lot of experiments and not to break anything often?",
        "url": "https://github.com/vectordotdev/vector/discussions/18587",
        "createdAt": "2023-09-18T12:00:53Z",
        "updatedAt": "2023-10-10T13:30:41Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "gpu-pug"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18578,
        "title": "Vector unable to slow down the source when buffer is full",
        "bodyText": "In our configuration, we have deployed vector in an AKS cluster as Daemonset. The agent collect logs using kubernetes source connect. There is transformation we do using remap. After that its piped to sink. In Sink, we used the kafka connector  which published the logs to Azure Eventhub.\nThe set up was working perfectly until we got a big surge in amount of logs being generated. We found out during high load vector was shipping all events to Azure Eventhub which was throttling the requests.This was causing the vector pods to get into a state where it was getting OOM Killed.\nWe then read about buffering model and thought that it could be used to slow down the source when Azure Eventhub is unable to catchup the the pace of logs.\nInitially we started with memory based buffering with a maximum events size of 2000. We expected that the behaviour will be when 2000 events are in the buffer, the source will be slowed down. However that didn't happen. It was still hammering azure eventhub with many requests and azure eventhub is rejecting those requests due to throttling.\nThe configuration is as follows\nsources:\n  aks_application_logs:\n    type: kubernetes_logs\n    extra_namespace_label_selector: \"collectlogs=yes\"\n    delay_deletion_ms: 300000\n    max_line_bytes: 64000\n    max_read_bytes: 1024000\n    glob_minimum_cooldown_ms: 30000\n  aks_ingress_logs:\n    type: kubernetes_logs    \n    extra_namespace_label_selector: \"capturelogstostorageAcc=yes\"\n\n  transforms:\n    aks_application_logs_transform:\n      type: remap\n      inputs:\n        - aks_application_logs\n      source: |-\n        if is_json(string!(.message)) {\n            message_json = object!(parse_json(string!(.message)) ?? {})\n            del(.message)\n            . = merge(.,message_json)\n        }\n  sinks:\n    az_storage_acc:\n      type: azure_blob\n      inputs :\n        - aks_ingress_logs\n      container_name: logs\n      storage_account: accountname\n      blob_prefix: \"date/%F/hour/%H/\"\n      healthcheck:\n        enabled: false\n      encoding:\n        codec: json\n    event_hub:\n      type: kafka\n      acknowledgements:\n        enabled: true\n      inputs:\n        - aks_application_logs_transform\n      bootstrap_servers: eventhub_instance:9093\n      topic: applogs\n      encoding:\n        codec: \"json\"\n      healthcheck:\n        enabled: true\n      batch:\n        max_events: 10\n      buffer:\n        type: \"memory\"\n        max_events: 2000\n        when_full: \"block\"\n      librdkafka_options:\n        \"retries\": \"2\"\n      sasl:\n        enabled: true\n        mechanism: PLAIN\n        username: \"$$$ConnectionString\"\n        password: \"$${EH_CONNECTION_STRING}\"      \n\nThe pod was continuously OOM Killed as well during this time.\nLater we switch to disk based buffering to see if that helps. However it was still hammering the azure eventhub with many requests. The only way we were able to handle the situation at the moment is increasing the azure eventhub capacity.\nSo we would like to know, if we are doing any mistake in terms of buffer configuration due to which the source is not slowing down when sink is unable to catch up the with pace.  Any other suggestion is also appreciated.",
        "url": "https://github.com/vectordotdev/vector/discussions/18578",
        "createdAt": "2023-09-17T21:20:15Z",
        "updatedAt": "2023-09-21T18:00:13Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "anirbansingharoy"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 18592,
        "title": "Removed tags still appearing in datadog",
        "bodyText": "When removing tags from within our helm config we are still seeing tags appear within datadog. Is there something obvious I'm missing or are we coming across an error.\nHelm config:\n      remove_unwanted_tags:\n        type: remap\n        inputs:\n          - datadog_agent\n        # https://vector.dev/docs/reference/vrl/expressions/#path-example-quoted-path\n        source: |-\n          del(.tags.aws_autoscaling_groupname)\n          del(.tags.aws_ec2launchtemplate_id)\n          del(.tags.aws_ec2launchtemplate_version)",
        "url": "https://github.com/vectordotdev/vector/discussions/18592",
        "createdAt": "2023-09-18T15:17:06Z",
        "updatedAt": "2023-09-21T16:10:40Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "jamhamblin"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 18624,
        "title": "Only read specific lines when using file source",
        "bodyText": "Hello! I have files containing logs similar to this:\n| 105537091 | x.x.x.x | 3306 | 0    | 1000   | 0      | 0   | 100       | 5       | 0   | 0       |                                 | 140322404325440 |\n| 105536652 | x.x.x.x | 3306 | 0    | 1000   | 0      | 0   | 100       | 5       | 0   | 0       |                                 | 140322404322560 |\n+-----------+---------+------+------+--------+--------+-----+-----------+---------+-----+---------+---------------------------------+-----------------+\n2023-09-21 11:03:10 MySQL_Thread.cpp:3910:process_all_sessions(): [WARNING] Closing unhealthy client connection x.x.x.x:yyyy\n2023-09-21 11:03:14 MySQL_Thread.cpp:3910:process_all_sessions(): [WARNING] Closing unhealthy client connection x.x.x.x:yyyy\n\nI am using a file source and wondering if a vector can only read lines containing the [WARNING] string. I know I can use a filter or transform remap to achieve this, but I am interested if I can ignore other lines at the very source.\nThanks for looking into this in advance!",
        "url": "https://github.com/vectordotdev/vector/discussions/18624",
        "createdAt": "2023-09-21T11:35:09Z",
        "updatedAt": "2023-09-21T13:22:25Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "arch-xtof"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18597,
        "title": "Multiple files at once",
        "bodyText": "Hopefully this is something easy to fix. I am reading files off a directory and have tried using both Prometheus exporter and Prometheus remote write to send my metrics to Victoria metrics. Everything works good until I send 3 or more files at the same time. In this scenario, I am only able to see metrics from 2 files values come across and display in Grafana, I tried 6 files and only saw metrics for 3 files come across in Victoria Metrics.",
        "url": "https://github.com/vectordotdev/vector/discussions/18597",
        "createdAt": "2023-09-18T18:14:13Z",
        "updatedAt": "2023-09-21T12:54:45Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Manniac07"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18614,
        "title": "Prometheus Remote Write to Victoria Metrics",
        "bodyText": "I am working on reading csv's from a directory, transforming them into metrics, and sending them to Victoria Metrics. Everything works fine until I need to drop multiple csv files into that directory. (This would be a valid scenario since the files are being ftp'd from an Oracle Database and there is always a possibility of files getting backed up.) When I try to drop multiple files, I am getting duplicate values and less than the number of files dropped. For instance, I tried with 10 files, I see 5 unique values and 3 duplicates. In Victoria Metrics, the values I am expecting from the 10 csv's are:\nClient - 953\nClient - 1401\nClient - 1158\nClient - 1278...\nSource:\n[source.files]\n type = \"file\"\n glob_minimum_cooldown_ms = 45000\n include = [\"directory/*csv\"]\n oldest_first = true\n read_from = \"beginning\"\n\n\nSink:\n[sink.console]\n type = \"prometheus_remote_write\"\n inputs = [\"my input from transform\"]\n endpoints = \"https://hkljlkjh.com/insert/\"\n [sinks.console.request]\n   rate_limit_duration_sec = 10\n   rate_limit_num = 15\n [sinks.console.buffer]\n    type = \"memory\"\n    max_events = 10000",
        "url": "https://github.com/vectordotdev/vector/discussions/18614",
        "createdAt": "2023-09-20T01:47:20Z",
        "updatedAt": "2023-09-21T12:52:15Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Manniac07"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 18615,
        "title": "What is the difference between the buffer_ and component_ metrics/telemetry?",
        "bodyText": "We are looking to monitor our sinks for any potential errors but I can't work out the exact difference between the buffer_ and component_ telemetry metrics.\nDo the buffer metrics roll up into the component metrics?\nFor example, if we have a buffer configured and set to drop_newest, what metric does that show under? does it show under buffer_discarded_events_total or component_discarded_events_total or both? For this example, we'd want to alert on any events being dropped by the sink.\nSimilarly, if we wanted to monitor a kind of throughput, would we use buffer_sent_events_total or component_sent_events_total? For this example, we'd want to alert on something like \"is sent volume over X less than the average volume\"",
        "url": "https://github.com/vectordotdev/vector/discussions/18615",
        "createdAt": "2023-09-20T06:43:55Z",
        "updatedAt": "2023-09-21T01:46:24Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "alex-rowe"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18601,
        "title": "Events dropped and Error logs when using unix socket as a source",
        "bodyText": "Hello,\nThe vector agent that I am using is doing a couple of things. It has HTTP, TCP, Unix and file as sources. The events that get in through these sources are routed through a transform (basic validation and appending additional metadata) before being sent to another instance of vector on an upstream device which will act as a collector.  The version of vector that I am currently using is 0.30 and here is the config file\ntype = \"http\"\naddress = \"0.0.0.0:50002\"\nencoding = \"text\"\npath = \"/ot/log\"\nstrict_path = true\n\n[sources.tcp_source]\ntype = \"socket\"\nmode = \"tcp\"\naddress = \"0.0.0.0:50003\"\n\n[sources.udp_source]\ntype = \"socket\"\nmode = \"unix_stream\"\npath = \"/tmp/unix_sock\"\n\n[sources.file_source]\ntype = \"file\"\nread_from = \"beginning\"\ninclude = [\"/var/log/**\"]\n\n[transforms.format]\ntype = \"remap\"\ninputs = [\"http_source\",\"tcp_source\",\"udp_source\"]\nfile = \"/logEventValidator.vrl\"\ndrop_on_error = true\ndrop_on_abort = true\n\n\n[sinks.forward]\ntype = \"vector\"\ninputs = [\"format\",\"nonFormat\"]\naddress = \"upstream:50335\"\nversion = \"2\"\n\nWith this running configuration, I have so far been able to acquire and forward all the events. But recently ran into an issue with the unix socket.\nThe events that are being sent to the agent are quite big ~8KB. I was told by the person utilising this configuration that they are seeing these error memory allocation of 4194304 bytes failed error ( I did not get a chance to look at the exact message and where it was popping up).\nTo recreate this, I started pumping in 8KB sized event every 10 ms to the vector agent. I was not able to see any issues with it. Then when I brought this time gap to 1 ms. ie. send 8KB sized event every 1 ms. With these after some time, I am seeing these errors being logged.\n2023-09-15T08:40:47.662096Z ERROR sink{component_kind=\"sink\" component_id=forward component_type=vector component_name=forward}:request{request_id=56469}: vector::sinks::util::retries: Internal log [Non-retriable error; dropping the request.] has been suppressed 6 times.\n2023-09-15T08:40:47.662156Z ERROR sink{component_kind=\"sink\" component_id=forward component_type=vector component_name=forward}:request{request_id=56469}: vector::sinks::util::retries: Non-retriable error; dropping the request. error=Request failed: status: OutOfRange, message: \"Error, message length too large: found 8108587 bytes, the limit is: 4194304 bytes\", details: [], metadata: MetadataMap { headers: {\"content-type\": \"application/grpc\", \"date\": \"Fri, 15 Sep 2023 08:40:47 GMT\", \"content-length\": \"0\"} } internal_log_rate_limit=true\n\nOnce we get into this situation, no more events are sent through to the collector unless we restart the vector agent.\nI wanted to understand a bit more about this error and see why this is happening?  would anyone know if memory allocation error has ever been seen and if the memory allocation error and what I have put in above are the same?",
        "url": "https://github.com/vectordotdev/vector/discussions/18601",
        "createdAt": "2023-09-19T06:24:30Z",
        "updatedAt": "2023-09-20T05:59:30Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bmallya"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18588,
        "title": "Elasticsearch Sink Stats",
        "bodyText": "Hey Team,\nBeen fiddling with the elasticsearch sink over last couple of days, having an issue with the stats reporting incorrectly.  Basically the scenario is as follows,\n\nBulk Post 3000 events\n1 of those events fails to map to a template correctly so elasticsearch rejects it but accepts the other 29999 events\nVector reports dropping 3000 events from internal logging\nVector top reports error on the sink and the stats show that 3000 less events went in\n\nHopefully that makes sense, basically it gets to a point where vector top reports the sink as say 2m events in and on the out queue it reports 1m events out.\nNot sure if someone else can confirm this behavior\nThanks\nTasty",
        "url": "https://github.com/vectordotdev/vector/discussions/18588",
        "createdAt": "2023-09-18T13:37:07Z",
        "updatedAt": "2023-09-20T01:53:07Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tastyfrankfurt"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18557,
        "title": "Solution for vector to vector metrics data pipeline using http",
        "bodyText": "Hi Dear Vector community,\nI'm recently doing a project plan to use vector for both agent and aggregator role for collecting the metrics data. The vector agent will working in the data plane where our clients generate some metrics data, on our control plane we have another vector aggregator cluster collecting the data received from the data plane then forward to the centralized prometheus server for monitoring.\nHere I runs into the trouble for the metrics data, because #17056, I can't use the native_json as the codec for http sink and source, so I use json as the codec. Then the http source in the control plane got the data:\n{\n\t\"aggregated_summary\":{\n\t\t\"count\":2502,\n\t\t\"quantiles\":[{\"quantile\":0.0,\"value\":0.000031142},{\"quantile\":0.25,\"value\":0.000038841},{\"quantile\":0.5,\"value\":0.000045922},{\"quantile\":0.75,\"value\":0.000050285},{\"quantile\":1.0,\"value\":0.00008474}],\n\t\t\"sum\":0.114286825},\n\t\"kind\":\"absolute\",\n\t\"name\":\"go_gc_duration_seconds\",\n\t\"path\":\"/\",\n\t\"source_type\":\"http_server\",\n\t\"tags\":{\"nodename\":\"ip-\",\n\t\"service_name\":\"nodeExporter\",\n\t\"source_type\":\"nodeExporter\"},\n\t\"timestamp\":\"2023-09-12T19:08:20.427244440Z\"\n}\n\nBut this data is in log format so actually it is:\n{\"log\": \"{{above_data}}\"}\n\ndirectly connect this source to prometheus_exporter won't work, because prometheus_exporter sink looking for metric input, so I have to add the process for translate the data, I tried some solution as I saw in the discussions and issues, for example:\n  transforms:\n    parse_http:\n      type: remap\n      inputs:\n      - http_server\n      source: |\n        .metric = .\n        .metric.timestamp = to_timestamp!(.metric.timestamp)\n     to_metric:\n      type: lua\n      inputs:\n      - parse_http\n      version: '2'\n      hooks:\n        function (event, emit)\n            event.metric = event.log.metric\n            event.log = nil\n            emit(event)\n        end\n\nThis will work for gauge and counter type metrics, but not for the histgram and summary, the reason I find from the source code: https://github.com/vectordotdev/vector/blob/master/lib/vector-core/src/event/lua/metric.rs#L781, vector seems think the histgram and summary type data in lua using different schema from the vector metrics. So if I want to use the lua transform the \"metrics-log\" to metrics, I need add the steps to transform the data which already in the vector metrics format to lua format, this will downgrade the performance a lot(need 5x resource). I'm wandering before the fix of the native type on the http source/sink, what is the solution we are using for the vector to vector metrics data pipeline using http source and sink.",
        "url": "https://github.com/vectordotdev/vector/discussions/18557",
        "createdAt": "2023-09-13T20:11:21Z",
        "updatedAt": "2023-09-19T17:28:20Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ee07b415"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18428,
        "title": "What is the recommended setup for running vector.dev in a docker-compose?",
        "bodyText": "I'm using docker-compose with a client + server container. What is the recommended setup for having vector.dev forward logs from both the client and server container to a sink?",
        "url": "https://github.com/vectordotdev/vector/discussions/18428",
        "createdAt": "2023-08-30T01:25:41Z",
        "updatedAt": "2023-09-18T21:32:12Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "vedantroy"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18542,
        "title": "[sink elasticsearch] mapper_parsing_exception, as object, but found a concrete value",
        "bodyText": "Hi there,\nI am testing vector after a major update (0.25.2 to latest) and see these error messages:\n2023-09-12T10:53:25.601898Z ERROR sink{component_kind=\"sink\" component_id=out_sophos-xg component_type=elasticsearch component_name=out_sophos-xg}:request{request_id=3}: vector::sinks::util::retries: Not retriable; dropping the request. reason=\"error type: mapper_parsing_exception, reason: object mapping for [destination.geo] tried to parse field [geo] as object, but found a concrete value\" internal_log_rate_limit=true\n\n2023-09-12T11:53:13.690820Z ERROR sink{component_kind=\"sink\" component_id=out_sophos-xg component_type=elasticsearch component_name=out_sophos-xg}: vector::sinks::util::retries: Internal log [Not retriable; dropping the request.] is being suppressed to avoid flooding.\n2023-09-12T11:53:13.690855Z ERROR sink{component_kind=\"sink\" component_id=out_sophos-xg component_type=elasticsearch component_name=out_sophos-xg}: vector_common::internal_event::service: Internal log [Service call failed. No retries or retries exhausted.] is being suppressed to avoid flooding.\n2023-09-12T11:53:13.690986Z ERROR sink{component_kind=\"sink\" component_id=out_sophos-xg component_type=elasticsearch component_name=out_sophos-xg}: vector_common::internal_event::component_events_dropped: Internal log [Events dropped] is being suppressed to avoid flooding.\n2023-09-12T11:53:22.894776Z ERROR sink{component_kind=\"sink\" component_id=out_sophos-xg component_type=elasticsearch component_name=out_sophos-xg}: vector::sinks::util::retries: Internal log [Not retriable; dropping the request.] has been suppressed 9 times.\n2023-09-12T11:53:22.894800Z ERROR sink{component_kind=\"sink\" component_id=out_sophos-xg component_type=elasticsearch component_name=out_sophos-xg}: vector::sinks::util::retries: Not retriable; dropping the request. reason=\"error type: mapper_parsing_exception, reason: object mapping for [source.as] tried to parse field [as] as object, but found a concrete value\" internal_log_rate_limit=true\n2023-09-12T11:53:22.894828Z ERROR sink{component_kind=\"sink\" component_id=out_sophos-xg component_type=elasticsearch component_name=out_sophos-xg}: vector_common::internal_event::service: Internal log [Service call failed. No retries or retries exhausted.] has been suppressed 9 times.\n2023-09-12T11:53:22.894838Z ERROR sink{component_kind=\"sink\" component_id=out_sophos-xg component_type=elasticsearch component_name=out_sophos-xg}: vector_common::internal_event::service: Service call failed. No retries or retries exhausted. error=None request_id=23 error_type=\"request_failed\" stage=\"sending\" internal_log_rate_limit=true\n2023-09-12T11:53:22.894862Z ERROR sink{component_kind=\"sink\" component_id=out_sophos-xg component_type=elasticsearch component_name=out_sophos-xg}: vector_common::internal_event::component_events_dropped: Internal log [Events dropped] has been suppressed 9 times.\n2023-09-12T11:53:22.894872Z ERROR sink{component_kind=\"sink\" component_id=out_sophos-xg component_type=elasticsearch component_name=out_sophos-xg}: vector_common::internal_event::component_events_dropped: Events dropped intentional=false count=406 reason=\"Service call failed. No retries or retries exhausted.\" internal_log_rate_limit=true\n2023-09-12T11:53:23.894770Z ERROR sink{component_kind=\"sink\" component_id=out_sophos-xg component_type=elasticsearch component_name=out_sophos-xg}: vector::sinks::util::retries: Internal log [Not retriable; dropping the request.] is being suppressed to avoid flooding.\n2023-09-12T11:53:23.894808Z ERROR sink{component_kind=\"sink\" component_id=out_sophos-xg component_type=elasticsearch component_name=out_sophos-xg}: vector_common::internal_event::service: Internal log [Service call failed. No retries or retries exhausted.] is being suppressed to avoid flooding.\n2023-09-12T11:53:23.894847Z ERROR sink{component_kind=\"sink\" component_id=out_sophos-xg component_type=elasticsearch component_name=out_sophos-xg}: vector_common::internal_event::component_events_dropped: Internal log [Events dropped] is being suppressed to avoid flooding.\n\nas a sink is used opensearch v2.9\ni did not see these problems with the previous vector version 0.25.2\nThe biggest changes in the migration were related to \"geoip\".\nAny idea ?\nThanks for any help",
        "url": "https://github.com/vectordotdev/vector/discussions/18542",
        "createdAt": "2023-09-12T12:38:58Z",
        "updatedAt": "2023-09-13T09:32:26Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "StefanSa"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 18479,
        "title": "About Message Sorting\uff08source : kafka, sinks: clickhouse\uff09",
        "bodyText": "Hello,\nI recently encountered a problem\uff0cData update failed\u3002\nHere is my data link\uff1akafka <-- vector -->  clickhouse\nin my vector sink_config\uff0cI added the following parameters\uff1a\nbuffer.max_events = 100000\nbatch.max_bytes = 61440000\nbatch.timeout_secs = 30\nmy clickhouse table engine (Because my data may be updated. I need to keep the final insertion record\uff0cI used the clickHouse replacingmergetree feature to change the update to insert\uff0c):\ncreate table xx (\nid Int64,\nname String\n)ENGINE=ReplacingMergeTree()\norder by id\nWhen my writes and updates are almost at the same time\uff08For the same id\uff09\uff0cI found that some data is in a saved insert state,\nNormally, it should be like this\uff1a\ninsert table xx (id,name )values (1,'a')(2,'b')(3,'c')(1,'e')\nBut this question seems to be like this\uff1a\ninsert table xx (id,name )values (1,'e')(2,'b')(3,'c')(1,'a')\nthe record \uff081,'a'\uff09was placed in the back on  the same batch\nBy the way, I have confirmed that the data is stored in the order of events in Kafka\nso, I want to know how the data is sorted in the vector  buffer or batch\u3002",
        "url": "https://github.com/vectordotdev/vector/discussions/18479",
        "createdAt": "2023-09-05T10:06:18Z",
        "updatedAt": "2023-09-12T10:37:19Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "yangshike"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 18504,
        "title": "Is it possible to have dynamic keys with VRL?",
        "bodyText": "For example I have this code where I would like the value of the variable field_name to be the key name to be used to tag my logs.\nfield_name = \"something\"\n\n.fields[field_name] = \"this is a test\"\n\nThis would result to a log that looks like this\n{\n    \"fields\": {\n          \"something\": \"this is a test\"\n     }\n    ......\n}\nThanks in advance!",
        "url": "https://github.com/vectordotdev/vector/discussions/18504",
        "createdAt": "2023-09-07T22:45:00Z",
        "updatedAt": "2023-09-08T22:32:27Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "camilisette"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18467,
        "title": "Allow weak key for openssl v3 (tls-enabled kafka)",
        "bodyText": "Since upgrading to vector 0.32 with bundled openssl from v1.1 to v3.0, it starts to refuse weak encryption algorithms for safety reasons (that's good). Specifically we are trying to sink messages to a tls-enabled kafka, which is unfortunately self-signed with weak key, thus, vector fails to verify certification during ssl handshake.\nUpgrading sever side cert is not an option at the moment, so we are trying to figure out a way to allow weak key in vector. So could there be workaround to allow weak key for ssl to work? (Yes, we understand compromised security risks.) Probably by tweaking openssl config outlined here at librdkafka? Which I did try but it seems not work yet.\nSetup\nMy setup is using docker image (timberio/vector:0.32.0-debian) with following vector config:\n[sources.bind]\ntype = 'stdin'\n\n[transforms.bind_decode]\n  type = \"remap\"\n  inputs = [\"bind\"]\n  source = '''\n  . = parse_json!(.message)\n  '''\n\n[sinks.bind_next_kafka]\n  type = \"kafka\"\n  inputs = [ \"bind_decode\" ]\n  bootstrap_servers = \"my-broker:9093\"\n  topic = \"user-event\"\n  sasl.enabled = true\n  sasl.mechanism = \"SCRAM-SHA-256\"\n  sasl.username = \"redacted\"\n  sasl.password = \"redacted\"\n  encoding.codec = \"json\"\n  tls.enabled = true\n  tls.ca_file = \"/etc/vector/my.pem\"\nvector fails to do ssl handshake during healthcheck.\n2023-09-01T11:37:46.556755Z ERROR rdkafka::client: librdkafka: Global error: SSL (Local: SSL error): sasl_ssl://my-broker:9093/bootstrap: SSL handshake failed: ssl/statem/statem_clnt.c:1890: error:0A000086:SSL routines::certificate verify failed (after 26ms in state SSL_HANDSHAKE)\n2023-09-01T11:37:46.556854Z ERROR rdkafka::client: librdkafka: Global error: AllBrokersDown (Local: All broker connections are down): 1/1 brokers are down\n2023-09-01T11:37:47.538890Z ERROR rdkafka::client: librdkafka: Global error: SSL (Local: SSL error): sasl_ssl://my-broker:9093/bootstrap: SSL handshake failed: ssl/statem/statem_clnt.c:1890: error:0A000086:SSL routines::certificate verify failed (after\n18ms in state SSL_HANDSHAKE, 1 identical error(s) suppressed)\n\nDebug\nWith the vector container, there is openssl v3 bundled in. Using it to test revealed that CA certificate key too weak\nopenssl s_client -connect\u00a0 my-broker:9093\u00a0 -CAfile /etc/vector/my.pem\n\nPeer signing digest: SHA256\nPeer signature type: RSA\nServer Temp Key: ECDH, prime256v1, 256 bits\n\n---\nSSL handshake has read 2054 bytes and written 484 bytes\nVerification error: CA certificate key too weak\n---\n\nNew, TLSv1.2, Cipher is ECDHE-RSA-AES256-GCM-SHA384\nServer public key is 2048 bit\nSecure Renegotiation IS supported\nCompression: NONE\nExpansion: NONE\nNo ALPN negotiated\nSSL-Session:\n\u00a0 \u00a0 Protocol\u00a0 : TLSv1.2\n\u00a0 \u00a0 Cipher\u00a0 \u00a0 : ECDHE-RSA-AES256-GCM-SHA384\n\nOptions tried\nI tried to follow confluentinc/librdkafka#4204, by appending config to openssl conf\nvim /usr/lib/ssl/openssl.conf\n[default_conf]\nssl_conf = ssl_sect\n\n[ssl_sect]\nsystem_default = system_default_sect\n\n[system_default_sect]\nMinProtocol = TLSv1.2\nCipherString = DEFAULT@SECLEVEL=1\n\nrun  openssl s_connect -client to verify, it still reports key too weak error. I suspect this config snippet may be not correct.",
        "url": "https://github.com/vectordotdev/vector/discussions/18467",
        "createdAt": "2023-09-02T02:54:14Z",
        "updatedAt": "2023-09-08T19:15:21Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "juvenn"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18380,
        "title": "Shared buffer",
        "bodyText": "We have Vector deployed in Kubernetes as a deployment and we wanted to buffer to disk. Is it possible/advisable to have a shared buffer storage between the Vector pods? Any suggestions on how we can keep running them as a deployment and still buffer to a persistent disk?",
        "url": "https://github.com/vectordotdev/vector/discussions/18380",
        "createdAt": "2023-08-24T20:39:18Z",
        "updatedAt": "2023-09-08T19:04:04Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "camilisette"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 18501,
        "title": "Is checkpoints.json need to be mounted to hostPath volume ?",
        "bodyText": "Hi.\nI have a question.\nI saw Checkpoint is located in {DATA-DIR}/??/checkpoints.json .\nIs checkpoint need to be mounted to hostPath volume ?\nThe reason I'm asking is that I'm wondering if Vector Pod reads Checkpoint as it is if it restarts.",
        "url": "https://github.com/vectordotdev/vector/discussions/18501",
        "createdAt": "2023-09-07T14:48:27Z",
        "updatedAt": "2023-09-08T05:37:17Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "pingping95"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18498,
        "title": "Is it possible to have several http_server sources with the same port but different paths?",
        "bodyText": "Hi all! The question is more or less in the title. I wonder if it's possible with 2 or more http_server sources, all listening same port but different paths? Can figure it out myself.\nThank you in advance!",
        "url": "https://github.com/vectordotdev/vector/discussions/18498",
        "createdAt": "2023-09-06T22:35:59Z",
        "updatedAt": "2023-09-07T00:43:31Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "gl1ridae"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18384,
        "title": "Can't run vector on centos 7 arm",
        "bodyText": "I got an error when run vector latest version:\n[root@ecs-be51-0002 bin]# ./vector\n./vector: relocation error: ./vector: symbol __cxa_thread_atexit_impl, version GLIBC_2.18 not defined in file libc.so.6 with link time reference\nEnv Info:\n[root@ecs-be51-0002 bin]# uname -a\nLinux ecs-be51-0002 4.18.0-80.7.2.el7.aarch64 #1 SMP Thu Sep 12 16:13:20 UTC 2019 aarch64 aarch64 aarch64 GNU/Linux\n[root@ecs-be51-0002 bin]# cat /etc/redhat-release\nCentOS Linux release 7.6.1810 (AltArch)\n[root@ecs-be51-0002 bin]# cat /etc/os-release\nNAME=\"CentOS Linux\"\nVERSION=\"7 (AltArch)\"\nID=\"centos\"\nID_LIKE=\"rhel fedora\"\nVERSION_ID=\"7\"\nPRETTY_NAME=\"CentOS Linux 7 (AltArch)\"\nANSI_COLOR=\"0;31\"\nCPE_NAME=\"cpe:/o:centos:centos:7\"\nHOME_URL=\"https://www.centos.org/\"\nBUG_REPORT_URL=\"https://bugs.centos.org/\"\n\nCENTOS_MANTISBT_PROJECT=\"CentOS-7\"\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\nREDHAT_SUPPORT_PRODUCT=\"centos\"\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\nGLIBC Info\nroot@ecs-be51-0002 bin]# strings /usr/lib64/libc.so.6 | grep GLIB\nGLIBC_2.17\nGLIBC_2.18\nGLIBC_PRIVATE\ngethostbyname2_r@@GLIBC_2.17\n_IO_fopen@@GLIBC_2.17\n_IO_file_finish@@GLIBC_2.17\n_dl_starting_up@@GLIBC_PRIVATE\n_IO_fgetpos@@GLIBC_2.17\ngetaliasent_r@@GLIBC_2.17\n_IO_file_seekoff@@GLIBC_2.17\ngetrpcent_r@@GLIBC_2.17\ngetservbyname_r@@GLIBC_2.17\n_rtld_global@@GLIBC_PRIVATE\n_IO_file_init@@GLIBC_2.17\ngetservbyport_r@@GLIBC_2.17\n__libc_enable_secure@@GLIBC_PRIVATE\nposix_spawn@@GLIBC_2.17\ntmpfile@@GLIBC_2.17\nrealpath@@GLIBC_2.17\n_IO_fgetpos64@@GLIBC_2.17\ngetrpcbyname_r@@GLIBC_2.17\n_rtld_global_ro@@GLIBC_PRIVATE\n__pointer_chk_guard@@GLIBC_PRIVATE\ngetspent_r@@GLIBC_2.17\n_IO_file_close_it@@GLIBC_2.17\nsys_sigabbrev@@GLIBC_2.17\n_IO_fsetpos@@GLIBC_2.17\n_sys_nerr@@GLIBC_2.17\npthread_cond_timedwait@@GLIBC_2.17\n_IO_do_write@@GLIBC_2.17\ngetnetbyname_r@@GLIBC_2.17\ngetpwnam_r@@GLIBC_2.17\n_mcount@@GLIBC_2.18\ngethostbyname_r@@GLIBC_2.17\n_IO_file_setbuf@@GLIBC_2.17\npthread_cond_destroy@@GLIBC_2.17\ngetaliasbyname_r@@GLIBC_2.17\ngetprotoent_r@@GLIBC_2.17\ngetspnam_r@@GLIBC_2.17\ngetrpcbynumber_r@@GLIBC_2.17\nnftw64@@GLIBC_2.17\ngetnetent_r@@GLIBC_2.17\n_res@GLIBC_2.17\nposix_spawnp@@GLIBC_2.17\n_IO_popen@@GLIBC_2.17\npclose@@GLIBC_2.17\nnftw@@GLIBC_2.17\n_IO_file_xsputn@@GLIBC_2.17\n_sys_errlist@@GLIBC_2.17\n_IO_proc_open@@GLIBC_2.17\n_IO_file_overflow@@GLIBC_2.17\n_IO_fdopen@@GLIBC_2.17\ngetgrgid_r@@GLIBC_2.17\ngetnetbyaddr_r@@GLIBC_2.17\n__res_maybe_init@GLIBC_PRIVATE\npthread_cond_signal@@GLIBC_2.17\nh_errno@GLIBC_PRIVATE\n_IO_file_attach@@GLIBC_2.17\n_IO_file_sync@@GLIBC_2.17\ngetpwuid_r@@GLIBC_2.17\npthread_cond_wait@@GLIBC_2.17\n_sys_siglist@@GLIBC_2.17\nfopencookie@@GLIBC_2.17\npthread_cond_broadcast@@GLIBC_2.17\ngetpwent_r@@GLIBC_2.17\ngethostbyaddr_r@@GLIBC_2.17\nsched_setaffinity@@GLIBC_2.17\ngetgrent_r@@GLIBC_2.17\nfnmatch@@GLIBC_2.17\ngetgrnam_r@@GLIBC_2.17\n_IO_fclose@@GLIBC_2.17\npthread_cond_init@@GLIBC_2.17\nregexec@@GLIBC_2.17\nlocaleconv@@GLIBC_2.17\n_IO_file_write@@GLIBC_2.17\ngethostent_r@@GLIBC_2.17\n_IO_proc_close@@GLIBC_2.17\ngetservent_r@@GLIBC_2.17\n_IO_file_fopen@@GLIBC_2.17\ngetprotobyname_r@@GLIBC_2.17\nsched_getaffinity@@GLIBC_2.17\n__tls_get_addr@@GLIBC_2.17\ngetprotobynumber_r@@GLIBC_2.17\n_IO_file_underflow@@GLIBC_2.17\n_dl_argv@@GLIBC_PRIVATE\npthread_attr_init@@GLIBC_2.17\n_IO_fsetpos64@@GLIBC_2.17\nI have noticed the issue #13183, but it seems that my problem not due to GLIBC version.\nIs there a solution to this problem? Such as recompile Vector or update libc.so.6?",
        "url": "https://github.com/vectordotdev/vector/discussions/18384",
        "createdAt": "2023-08-25T08:41:17Z",
        "updatedAt": "2023-09-05T18:58:02Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "dev-lake"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18451,
        "title": "sink elasticsearch  Service call failed. No retries or retries exhausted.",
        "bodyText": "When using the sink of the vector to write data to es, a high load on one of the nodes in es caused the request to fail. The vector discarded the log, is there any way to prevent the log from being lost\nLOG \uff1a\n2023-08-31T02:27:05.788372Z  WARN sink{component_kind=\"sink\" component_id=to_es7 component_type=elasticsearch component_name=to_es7}:request{request_id=9893}:http: vector::internal_events::http_client: HTTP error. error=error trying to connect: tcp connect error: Connection refused (os error 111) error_type=\"request_failed\" stage=\"processing\" internal_log_rate_limit=true\n2023-08-31T02:27:05.788447Z  WARN sink{component_kind=\"sink\" component_id=to_es7 component_type=elasticsearch component_name=to_es7}:request{request_id=9893}: vector::sinks::util::retries: Retrying after error. error=Failed to make HTTP(S) request: error trying to connect: tcp connect error: Connection refused (os error 111) internal_log_rate_limit=true\n2023-08-31T02:27:07.871497Z  WARN sink{component_kind=\"sink\" component_id=to_es7 component_type=elasticsearch component_name=to_es7}:request{request_id=9894}:http: vector::internal_events::http_client: Internal log [HTTP error.] is being suppressed to avoid flooding.\n2023-08-31T02:27:07.871570Z  WARN sink{component_kind=\"sink\" component_id=to_es7 component_type=elasticsearch component_name=to_es7}:request{request_id=9894}: vector::sinks::util::retries: Internal log [Retrying after error.] is being suppressed to avoid flooding.\n2023-08-31T03:06:02.326726Z ERROR sink{component_kind=\"sink\" component_id=to_es7 component_type=elasticsearch component_name=to_es7}:request{request_id=10425}: vector::sinks::util::retries: Not retriable; dropping the request. reason=\"error type: es_rejected_execution_exception, reason: rejected execution of processing of [1933846][indices:data/write/bulk[s][p]]: request: BulkShardRequest [[logstash-nginx-access-orouter-2023.08.31.03][0]] containing [1000] requests, target allocation id: q9o1zf2zSCupQm-xobRdpg, primary term: 1 on EsThreadPoolExecutor[name = dx-elk17.dx/write, queue capacity = 1000, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@131730c3[Running, pool size = 40, active threads = 40, queued tasks = 1050, completed tasks = 445142]]\" internal_log_rate_limit=true\n2023-08-31T03:06:02.326838Z ERROR sink{component_kind=\"sink\" component_id=to_es7 component_type=elasticsearch component_name=to_es7}:request{request_id=10425}: vector_common::internal_event::service: Service call failed. No retries or retries exhausted. error=None request_id=10425 error_type=\"request_failed\" stage=\"sending\" internal_log_rate_limit=true\n2023-08-31T03:06:02.326862Z ERROR sink{component_kind=\"sink\" component_id=to_es7 component_type=elasticsearch component_name=to_es7}:request{request_id=10425}: vector_common::internal_event::component_events_dropped: Events dropped intentional=false count=1000 reason=\"Service call failed. No retries or retries exhausted.\" internal_log_rate_limit=true",
        "url": "https://github.com/vectordotdev/vector/discussions/18451",
        "createdAt": "2023-09-01T10:14:00Z",
        "updatedAt": "2023-09-04T02:35:22Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Nobe-yt"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18468,
        "title": "Monitoring any parse error metrics from remap component",
        "bodyText": "Hi,\nIssue Description\nI'm using Vector and have encountered parsing errors in the remap component. I'd like to capture and monitor these parse errors for observability and troubleshooting purposes. Additionally, I've noticed that Vector documentation mentions metrics like timestamp_parse_errors_total and parse_errors_total, but it's not clear how to use them or if they are exposed for monitoring.\nRequest\n\nIs it possible to expose parse errors generated by the remap component as metrics within Vector?\nIf so, how can I configure Vector to capture and expose these parse errors as metrics?\nAre metrics like timestamp_parse_errors_total and parse_errors_total available for monitoring, and if yes, how can I access and utilize them?\n\nEnvironment Details\n\nVector Version: 0.30.0\nConfiguration:\n\nsources:\n  internal_metrics:\n    type: internal_metrics\n  \n\nthanks",
        "url": "https://github.com/vectordotdev/vector/discussions/18468",
        "createdAt": "2023-09-02T13:05:54Z",
        "updatedAt": "2023-09-02T13:05:54Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 18450,
        "title": "Can the vector obtain CPU and memory information",
        "bodyText": "Is there a way to obtain information about the server's CPU, memory, and traffic usage when using Vector? I don't seem to see any way to obtain it in the official documents",
        "url": "https://github.com/vectordotdev/vector/discussions/18450",
        "createdAt": "2023-09-01T10:06:48Z",
        "updatedAt": "2023-09-02T10:08:45Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "XSWClevo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18409,
        "title": "Vector enabled using Systemctl is throwing Request timeout error",
        "bodyText": "Hi,\nI'm running a vector agent in Debian os. Vector pipeline is integrated with prometheus_remote_write sink.\nThe issue is, whenver i try to enable the vector using /usr/bin/vector. It is able to send data to the prometheus Backend. But if the vector is enabled using systemctl it is not able to send metrics and i'm getting request timed out error.\nvector::sinks::util::retries: Request timed out. If this happens often while the events are actually reaching their destination, try decreasing batch.max_bytesand/or usingcompressionif applicable. Alternativelyrequest.timeout_secs can be increased. internal_log_rate_limit=true\nAnyone have any idea on this behaviour?\nPS: I'm using the same vector.toml for both the cases.\nThanks in Advance for your help!!",
        "url": "https://github.com/vectordotdev/vector/discussions/18409",
        "createdAt": "2023-08-28T19:27:36Z",
        "updatedAt": "2023-09-01T14:46:02Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "jeyakumar8"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18368,
        "title": "Is there a way to convert timestamp to local time on output?",
        "bodyText": "The event timestamp can only be the UTC timestamp, I want to be able to output the local timestamp, for example, through the \"internal_logs\" data source, the vector log data is output to a file, the timestamp of each log in the file is still UTC time, is there a way to convert the output local time, match the time of the machine.\nAs follows:\n{\"host\":\"DEV-DFNCFT-AP08\",\"message\":\"Signal received.\",\"metadata\":{\"kind\":\"event\",\"level\":\"INFO\",\"module_path\":\"vector::signal\",\"target\":\"vector::signal\"},\"pid\":46992,\"signal\":\"SIGINT\",\"source_type\":\"internal_logs\",\"timestamp\":\"2023-08-25T07:09:04.787290941Z\"}",
        "url": "https://github.com/vectordotdev/vector/discussions/18368",
        "createdAt": "2023-08-24T01:32:14Z",
        "updatedAt": "2023-09-01T14:27:16Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "41405465"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 18131,
        "title": "Does the File source use poll or notify?",
        "bodyText": "I want to use File source to collect logs with low latency. However there are random several seconds (about 1s ~ 10s) delay on new contents in the log file.\nHere is my configuration file. Is there anything that I miss?\ndata_dir = \".\"\n\n[sources.file]\ntype = \"file\"\ninclude = [ \"tmp/test.log\" ]\nencoding.charset = \"utf8\"\n\n[sinks.print]\ntype = \"console\"\ninputs = [\"file\"]\nencoding.codec = \"text\"\n\nI use Ubuntu 16.04.",
        "url": "https://github.com/vectordotdev/vector/discussions/18131",
        "createdAt": "2023-08-01T14:37:22Z",
        "updatedAt": "2023-08-31T18:54:15Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "WuBingzheng"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 18418,
        "title": "Vector Internal log level change",
        "bodyText": "Hello,\nIs there a way to change the log level of a running instance of vector? I might want to start off with logging only WARN and above, but eventually want to change it to verbose.\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/18418",
        "createdAt": "2023-08-29T06:37:38Z",
        "updatedAt": "2023-08-29T14:33:37Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bmallya"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18351,
        "title": "Observing any event loss with internal vector metrics",
        "bodyText": "Hi.\ni have an question about what are the best metrics that we can rely on for observing vector event log loss at the sink .\nany advice would be appreciated",
        "url": "https://github.com/vectordotdev/vector/discussions/18351",
        "createdAt": "2023-08-23T08:03:42Z",
        "updatedAt": "2023-09-02T12:58:20Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18273,
        "title": "Source: UDP & protobuf",
        "bodyText": "Is it possible to use UDP as source and to decode (juniper) protobuf messages?",
        "url": "https://github.com/vectordotdev/vector/discussions/18273",
        "createdAt": "2023-08-16T11:23:46Z",
        "updatedAt": "2023-08-26T19:08:03Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "lzwaan"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 17889,
        "title": "Share your feedback on the new log event data model",
        "bodyText": "Discussion to collect feedback on the newly released log namespacing feature. See https://vector.dev/blog/log-namespacing/ for the feature announcement. Let us know what you think below!",
        "url": "https://github.com/vectordotdev/vector/discussions/17889",
        "createdAt": "2023-06-28T21:46:00Z",
        "updatedAt": "2024-11-04T17:29:53Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "jszwedko"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 2
    },
    {
        "number": 18232,
        "title": "Not able to read new log lines. but reading when vector restarted in Ubuntu",
        "bodyText": "I'm running vector in ubuntu. I have setup vector to read logs from files.\nMy setup is working and when new file added to the directory it's reading the data from the file.  but when I add new lines to the existing files it's not reading.\nWhen I stop and start vector. the new lines I added are being read. are there any settings I need to do in order to read the new lines.\nMy sample vector.\n`\ndata_dir = \"/home/user/vector/data\"\n#####CHANGETHIS Logs ########\n[sources.test_log]\ntype = \"file\"\ninclude = [\"/home/user/logs/logtest*.log\"]\nread_from = \"end\"\n`\nI even tried adding the line_delimiter but still the same.\nthanks",
        "url": "https://github.com/vectordotdev/vector/discussions/18232",
        "createdAt": "2023-08-14T02:44:36Z",
        "updatedAt": "2023-08-22T21:26:57Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "kbitra"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 18269,
        "title": "Split sinks-elasticsearch feature into sinks-elasticsearch and sinks-elasticsearch-aws",
        "bodyText": "Currently, the sinks-elasticsearch feature in Vector enables AWS support by default, which may not be necessary for all users. This leads to an increase in compiled code size for users who only require the normal Elasticsearch functionality. To address this, I propose splitting the sinks-elasticsearch feature into two separate features: sinks-elasticsearch and sinks-elasticsearch-aws.\nOr we can make sinks-elasticsearch feature disable aws on default and enable aws when aws-core feature enabled?\nAnother idea is add sinks-elasticsearch-common feature for normal Elasticsearch, this won't introduce breakchange. Like the following:\nsinks-elasticsearch-common = [\"transforms-metric_to_log\"] sinks-elasticsearch = [\"sinks-elasticsearch-common\", \"aws-core\"]\nI can help make it if you wish to make a change :)",
        "url": "https://github.com/vectordotdev/vector/discussions/18269",
        "createdAt": "2023-08-16T06:37:10Z",
        "updatedAt": "2023-08-21T02:54:30Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "suikammd"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18314,
        "title": "The consumption speed will be fast for a while, and slow for a while",
        "bodyText": "filebeat -->kafka--->vector--->opensearch\nversion: 0.29.1\n\ndata_dir = \"/data/vector-xxx-nosplit/data\"\n[sources.dummy_logs]\ntype = \"kafka\" # required\nbootstrap_servers = \"xxx:9092,xxx:9092,xxx:9092\" # required\ngroup_id = \"test-xxx-info-group\" # required\nauto_offset_reset = \"latest\"\ncommit_interval_ms = 2000\ntopics = [\"test-xxx-info\"] # required\n\n# Parse Syslog logs\n# See the Vector Remap Language reference for more info: https://vrl.dev\n[transforms.parse_logs]\ntype = \"remap\"\ninputs = [\"dummy_logs\"]\ndrop_on_error = true\nreroute_dropped = false\nsource        = '''\n      . = parse_json!(.message)\n      .datestamp, err = truncate(.message,limit: 23)\n      .@timestamp = parse_timestamp!(.datestamp, format: \"%F %T%.3f\")\n     .timestamp = now()\n     .unix_time,error = to_int(format_timestamp!(.timestamp, format: \"%s\"))\n     .utc8_time = .unix_time + 28800\n     .currentdate = format_timestamp!(to_timestamp!(.utc8_time), format: \"%Y.%m.%d\")\n     del(.timestamp);del(.unix_time);del(.utc8_time);del(.agent);del(.ecs);del(.@metadata);del(.fields);del(.input);del(.log);del(.host);del(.beat);del(.meta);del(.source);del(.prospector);del(.datestamp)\n'''\n\n\n[sinks.elasticsearch]\n type = \"elasticsearch\" # required\n inputs = [\"parse_logs\"] # required\n api_version = \"v8\"\n doc_type = \"_doc\"\n compression = \"none\" # optional, default\n endpoints = [\"https://XXXXXX:443\"] # required\n mode = \"bulk\"\n bulk.action = \"index\"\n bulk.index = \"test-upex-info-{{ .currentdate }}\" # optional, default\n auth.strategy = \"basic\" # required\n auth.user = \"\"\n auth.password = ''",
        "url": "https://github.com/vectordotdev/vector/discussions/18314",
        "createdAt": "2023-08-20T09:08:58Z",
        "updatedAt": "2023-08-20T09:09:26Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "uglyliu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 13490,
        "title": "What's the best way to track implementation of tracing in Vector?",
        "bodyText": "I'm subscribed to several issues that seem like they should be related, but I feel like I might be missing an umbrella issue or milestone or project.\nOnes I'm subscribed to:\n\n#2661\n#3949\n#4063\n#1444",
        "url": "https://github.com/vectordotdev/vector/discussions/13490",
        "createdAt": "2022-07-09T18:20:30Z",
        "updatedAt": "2023-08-16T20:13:29Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "kamalmarhubi"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 18262,
        "title": "Failed to render template for \"index\"",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nHello. I periodically get an error:\n{\n  \"_index\": \"vector-dev-2023-08-15\",\n  \"_id\": \"p8of-4kB8-sMq-fMul89\",\n  \"_version\": 1,\n  \"_score\": null,\n  \"_source\": {\n    \"error\": \"Missing fields on event: [\\\".service_name\\\"]\",\n    \"error_type\": \"template_failed\",\n    \"host\": \"vector-j89tn\",\n    \"internal_log_rate_limit\": true,\n    \"message\": \"Failed to render template for \\\"index\\\".\",\n    \"metadata\": {\n      \"kind\": \"event\",\n      \"level\": \"ERROR\",\n      \"module_path\": \"vector::internal_events::template\",\n      \"target\": \"vector::internal_events::template\"\n    },\n    \"pid\": 1,\n    \"source_type\": \"internal_logs\",\n    \"stage\": \"processing\",\n    \"timestamp\": \"2023-08-15T21:35:06.829736905Z\",\n    \"vector\": {\n      \"component_id\": \"opensearch\",\n      \"component_kind\": \"sink\",\n      \"component_name\": \"opensearch\",\n      \"component_type\": \"elasticsearch\"\n    }\n  },\n  \"fields\": {\n    \"timestamp\": [\n      \"2023-08-15T21:35:06.829Z\"\n    ]\n  },\n  \"sort\": [\n    1692135306829\n  ]\n}\n\nBut I can't catch an event where this field is not present. I catch through type: \"filter\".\nConfiguration\ncustomConfig:\n    data_dir: /var/lib/vector\n    api:\n    enabled: true\n    address: 127.0.0.1:8686\n    playground: false\n    sources:\n    k8s:\n        type: kubernetes_logs\n        exclude_paths_glob_patterns: [\"*kube-system*\"]\n        extra_namespace_label_selector: vector.dev/logging=true\n    transforms:\n    k8s-remap:\n        type: remap\n        inputs: [\"k8s\"]\n        source: |-\n        kubernetes = {\"service_namespace\":(.kubernetes.pod_namespace), \"service_name\":(.kubernetes.container_name), \"pod_name\": (.kubernetes.pod_name), \"@timestamp\": (parse_timestamp!(.timestamp, format: \"MMM D, YYYY @ HH:mm:ss.SSS\"))}\n        ., err = parse_json(.message)\n        ., err = merge(., kubernetes)\n        drop_on_error: true\n        reroute_dropped: true\n    k8s-reforge:\n        type: remap\n        inputs: [\"*dropped\"]\n        source: |-\n        ., err  = {\"message\": (to_string(.message)), \"dropped\": true, \"service_namespace\":(.kubernetes.pod_namespace), \"service_name\":(.kubernetes.container_name), \"pod_name\": (.kubernetes.pod_name), \"metadata\": (.metadata), \"@timestamp\": (parse_timestamp!(.timestamp, format: \"MMM D, YYYY @ HH:mm:ss.SSS\"))}\n    exists-service-name:\n      type: \"filter\"\n      inputs: [\"k8s-remap\",\"k8s-reforge\"]\n      condition: \n        type: \"vrl\"\n        source: |-\n          !exists(.container_name)\n    sinks:\n    opensearch-debug:\n        type: elasticsearch\n        inputs: [\"exists-service-name\"]\n        api_version: v8\n        endpoints: [\"elasticsearch\"]\n        bulk:\n        index: \"vector-debug-%Y-%m-%d\"\n    opensearch:\n        type: elasticsearch\n        inputs: [\"k8s-remap\",\"k8s-reforge\"]\n        api_version: v8\n        endpoints: [\"elasticsearch\"]\n        bulk:\n        index: \"dev-{{ .service_name }}-%Y-%m-%d\" \n\n\n\n### Version\n\n0.31.0\n\n### Debug Output\n\n_No response_\n\n### Example Data\n\n_No response_\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_",
        "url": "https://github.com/vectordotdev/vector/discussions/18262",
        "createdAt": "2023-08-15T21:49:52Z",
        "updatedAt": "2023-08-16T00:46:25Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "gorbunov-di"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 1
    },
    {
        "number": 18177,
        "title": "Trace support for datadog-agent => vector => kafka => vector => datadog",
        "bodyText": "I am setting up a Vector pipeline that receives Traces from datadog-agent and puts them into a Kafka queue, then another Vector pipeline picks up Traces and send them to Datadog. However the Kafka sink only supports Logs and Metrics (https://github.com/vectordotdev/vector/blob/master/src/sinks/kafka/config.rs#L279) and the Kafka source only supports Logs.\nIs there any plan to add Trace support for Kafka source/sink?",
        "url": "https://github.com/vectordotdev/vector/discussions/18177",
        "createdAt": "2023-08-07T21:17:02Z",
        "updatedAt": "2023-08-15T17:41:15Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "hvtuananh"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 18216,
        "title": "How to reshape statsd metrics prior to sending to Clickhouse sink?",
        "bodyText": "I have a vector pipeline working great with log events. I'm now trying to add a pipeline for metrics and I'm getting quite confused about how they work. Here's my pipeline\n[sources.statsd-source]\ntype = \"statsd\"\naddress = \"0.0.0.0:9831\"\nmode = \"udp\"\n\n[transforms.filter_stream_metrics]\ninputs = [\"statsd-source\"]\ntype = \"remap\"\ndrop_on_abort = true\ndrop_on_error = true\nsource = \"\"\"\nlog(\"got stream metrics\" + encode_json(.), \"info\", 0)\nif (!starts_with(string!(.name), \"flink\")) {\n  abort\n}\nlog(\"passed stream metrics start with\" + encode_json(.), \"info\", 0)\n\"\"\"\n\n[sinks.statsd-sink]\ninputs = [\"filter_stream_metrics\"]\ntype = \"console\"\nencoding.codec = \"json\"\ntarget = \"stdout\"\nWhen I pipe a message that I expect to pass the intermediate transform step, here's what I see printed/logged by vector\n2023-08-11T07:07:17.067016Z  INFO transform{component_kind=\"transform\" component_id=filter_stream_metrics component_type=remap component_name=filter_stream_metrics}: vrl_stdlib::log: got stream metrics{\"kind\":\"incremental\",\"name\":\"flink.task.numRecordsIn\",\"tags\":{\"app_name\":\"testing\",\"task_name\":\"also_testing\"},\"type\":\"counter\"} internal_log_rate_secs=0 vrl_position=0\n2023-08-11T07:07:17.067074Z  INFO transform{component_kind=\"transform\" component_id=filter_stream_metrics component_type=remap component_name=filter_stream_metrics}: vrl_stdlib::log: passed stream metrics start with{\"kind\":\"incremental\",\"name\":\"flink.task.numRecordsIn\",\"tags\":{\"app_name\":\"testing\",\"task_name\":\"also_testing\"},\"type\":\"counter\"} internal_log_rate_secs=0 vrl_position=109\n{\"name\":\"flink.task.numRecordsIn\",\"tags\":{\"app_name\":\"testing\",\"task_name\":\"also_testing\"},\"kind\":\"incremental\",\"counter\":{\"value\":1.0}}\n\nWhy is it that the sink has an extra field, counter with some value, while the logs that print the root object in the transform don't have that field? Does vector somehow rollup metrics? If so, do rollups only happen at the sink level?\nMy ultimate goal is to pipe my statsd metrics to a Clickhouse sink but it's not clear to me how to properly shape my metric to conform to my clickhouse table structure. I've tried something like the following\n[transforms.reshape_stream_entities_written_metrics]\ninputs = [\"filter_stream_metrics\"]\ntype = \"remap\"\nsource = \"\"\"\nfinal_fields = {}\nfinal_fields.unix_timestamp = now()\nfinal_fields.app_name = .tags.app_name\nfinal_fields.task_name = .tags.task_name\nfinal_fields.entities_written = .counter.value\nlog(encode_json(final_fields), \"info\", 0)\n. |= final_fields\nlog(encode_json(.), \"info\", 0)\n\"\"\"\nHowever, when I add this transform and use it as the source for the final sink, I see the following get logged\n2023-08-11T07:16:25.888892Z  INFO transform{component_kind=\"transform\" component_id=filter_stream_metrics component_type=remap component_name=filter_stream_metrics}: vrl_stdlib::log: got stream metrics{\"kind\":\"incremental\",\"name\":\"flink.task.numRecordsIn\",\"tags\":{\"app_name\":\"testing\",\"task_name\":\"also_testing\"},\"type\":\"counter\"} internal_log_rate_secs=0 vrl_position=0\n2023-08-11T07:16:25.890325Z  INFO transform{component_kind=\"transform\" component_id=filter_stream_metrics component_type=remap component_name=filter_stream_metrics}: vrl_stdlib::log: passed stream metrics start with{\"kind\":\"incremental\",\"name\":\"flink.task.numRecordsIn\",\"tags\":{\"app_name\":\"testing\",\"task_name\":\"also_testing\"},\"type\":\"counter\"} internal_log_rate_secs=0 vrl_position=109\n2023-08-11T07:16:25.900010Z  INFO transform{component_kind=\"transform\" component_id=reshape_stream_entities_written_metrics component_type=remap component_name=reshape_stream_entities_written_metrics}: vrl_stdlib::log: {\"app_name\":\"testing\",\"entities_written\":null,\"task_name\":\"also_testing\",\"unix_timestamp\":\"2023-08-11T07:16:25.899834507Z\"} internal_log_rate_secs=0 vrl_position=254\n2023-08-11T07:16:25.900087Z  INFO transform{component_kind=\"transform\" component_id=reshape_stream_entities_written_metrics component_type=remap component_name=reshape_stream_entities_written_metrics}: vrl_stdlib::log: {\"kind\":\"incremental\",\"name\":\"flink.task.numRecordsIn\",\"tags\":{\"app_name\":\"testing\",\"task_name\":\"also_testing\"},\"type\":\"counter\"} internal_log_rate_secs=0 vrl_position=314\n{\"name\":\"flink.task.numRecordsIn\",\"tags\":{\"app_name\":\"testing\",\"task_name\":\"also_testing\"},\"kind\":\"incremental\",\"counter\":{\"value\":1.0}}\n\n\nWhich looks like the transform to reshape the metric isn't working.\n\nDoes vector rollup statsd metrics?\nIs it possible to reshape metrics prior to dumping them into a sink like Clickhouse?\nIf so, how? Are there examples I can follow?",
        "url": "https://github.com/vectordotdev/vector/discussions/18216",
        "createdAt": "2023-08-11T07:17:45Z",
        "updatedAt": "2023-08-12T10:58:12Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "paymog"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 4
    },
    {
        "number": 18211,
        "title": "Dynamically add *.toml files at runtime",
        "bodyText": "Hello All,\nWe have a use-case where vector would be running in a k8s cluster and we'd like to update the vector instance with a new toml file under certain circumstances. By adding additional files to the /etc/vector directory will they be magically picked up and used or would we need to edit the /etc/vector/vector.toml file? Does the playground/graphql API support adding additional files or would that be something we needed to code up to get additional entries added in (again, assuming vector would pick those up anyway)?\nThe general idea is to be able to dynamically add new transforms and sinks at runtime as needs arise and how in the world can we do that in a sane way. Any ideas one way or another?",
        "url": "https://github.com/vectordotdev/vector/discussions/18211",
        "createdAt": "2023-08-10T20:09:01Z",
        "updatedAt": "2023-09-08T15:31:43Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "cdancy"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18207,
        "title": "Error use lua to replace log to metric",
        "bodyText": "When I try this code\nmy source is\n{\"content\":{\"amount\":5,\"documentation\":\"this is a summary\",\"labelnames\":[\"label1\",\"label2\"],\"labels\":{\"label1\":\"label1\"},\"name\":\"my_summary\",\"namespace\":\"\",\"subsystem\":\"\",\"unit\":\"\"},\"headers\":{},\"message_key\":null,\"method\":\"observe\",\"offset\":17,\"partition\":0,\"source_type\":\"kafka\",\"timestamp\":\"2023-08-10T09:41:29.836Z\",\"topic\":\"aml-log-collector\",\"type\":\"summary\"}\n  route-logs-by-type:\n    type: route\n    inputs:\n      - log-collecotr\n    route:\n      counter: .type == \"counter\"\n      gauge: .type == \"gauge\"\n      histogram: .type == \"histogram\"\n      summary: .type == \"summary\"\n\n\n  lua-summary-log-to-metric:\n    type: lua\n    inputs:\n      - route-logs-by-type.summary\n    version: \"2\"\n    hooks:\n      process: |-\n        function (event, emit)\n          event.metric = {\n            name = event.log.content.name,\n            timestamp = event.log.timestamp,\n            namespace = event.log.content.namespace,\n            tags = event.log.content.labels,\n            kind = \"incremental\",\n            distribution = {\n              samples = {\n                { value = event.log.content.amount, rate = 1 }\n              },\n              statistic = \"summary\"\n            }\n          }\n          event.log = nil\n          emit(event)\n        end\n2023-08-10T09:41:29.890807Z ERROR transform{component_kind=\"transform\" component_id=lua-summary-log-to-metric component_type=lua component_name=lua-summary-log-to-metric}: vector::internal_events::lua: Error in building lua script. error=RuntimeErrorHooksProcess { source: CallbackError { traceback: \"stack traceback:\\n\\t[C]: in local 'emit'\\n\\t[string \\\"src/transforms/lua/v2/mod.rs:230:9\\\"]:16: in function <[string \\\"src/transforms/lua/v2/mod.rs:230:9\\\"]:1>\", cause: FromLuaConversionError { from: \"nil\", to: \"Vec\", message: Some(\"expected table\") } } } error_type=\"script_failed\" error_code=\"runtime_error_hook_process\" stage=\"processing\" internal_log_rate_limit=true\nError occur",
        "url": "https://github.com/vectordotdev/vector/discussions/18207",
        "createdAt": "2023-08-10T10:00:17Z",
        "updatedAt": "2023-08-11T06:53:49Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "xince-fun"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18205,
        "title": "When the data source is Kafka, can it be automatically created when the Topic does not exist?",
        "bodyText": "When the service starts, the topic was not created. Can it be automatically restored or exited? Now it doesn't work anymore",
        "url": "https://github.com/vectordotdev/vector/discussions/18205",
        "createdAt": "2023-08-10T09:51:19Z",
        "updatedAt": "2023-08-10T10:03:36Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "zhangliang-zl"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 18132,
        "title": "How to fuzzy matching\uff1f",
        "bodyText": "Let's say my data is like this\n{\n\"body\":\"2023-08-02T00:18:34.866228161+08:00 stdout F 2023-08-02 00:18:34.865 INFO 7 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer WebApplicationContext : Tomcat started on port(s): 8080 (http) with context path '...\"\n\"fields\":{\n\"containername\":\"java-demo\"\n\"environment\":\"dev\"\n\"logconfig\":\"java-demo\"\n\"namespace\":\"linuxea-dev\"\n\"nodename\":\"172.16.100.83\"\n\"podname\":\"production-java-demo-5cf5b97645-tsmxx\"\n\"topic\":\"java-demo\"\n}\n}\n\nfile\n[api]\nenabled = true\naddress = \"0.0.0.0:8686\"\n\n[sources.my_source_id]\ntype = \"kafka\"\nbootstrap_servers = \":9092\"\ngroup_id = \"consumer-group-name\"\ntopics = [ \"pod-dev-java-demo\" ]\n[sources.my_source_id.decoding]\ncodec = \"json\"\n\n\n[transforms.ftest]\ntype = \"filter\"\ninputs = [\"my_source_id\"]\ncondition = '.body == \"*WebApplicationContext*\"'\n\n\n[transforms.remap_alert_udev]\ntype = \"remap\"\ninputs = [\"ftest\"]\nsource = '''\n. = {\n    \"startsAt\": .happen_time,\n    \"labels\": {\n        \"hostname\": .hostname,\n        \"device\": .data.pass_through_name,\n        \"created_time\": .timestamp,\n        \"event\": downcase!(.name),\n        \"service\": \"udev\"\n    }\n}\nif (.labels.event == \"add_device\") {\n    .labels.alertname = \"event_add_device\"\n    .labels.severity = \"info\"\n} else if (.labels.event == \"remove_device\") {\n    .labels.alertname = \"event_remove_device\"\n    .labels.severity = \"critical\"\n}\n'''\n\n[sinks.sink0]\ninputs = [\"remap_alert_*\"]\ntarget = \"stdout\"\ntype = \"console\"\n[sinks.sink0.encoding]\ncodec = \"json\"\n\nI'm not going to configure logs as json\nonly want to match everything that contains \"WebApplicationContext\" in the body\nhow should I fuzzy matches to get everything that contains the \"WebApplicationContext\" keyword\nCan you correct my problem\nthanks",
        "url": "https://github.com/vectordotdev/vector/discussions/18132",
        "createdAt": "2023-08-01T17:07:32Z",
        "updatedAt": "2023-08-09T01:24:34Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "marksugar"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 18179,
        "title": "vector send same message repeatedly",
        "bodyText": "I am trying to collect firewall logs using Vector on Windows Server 2019. I am using the file type to collect the content of the C:\\windows\\system32\\logfiles\\firewall\\pfirewall.log file. I am then using the sink to put the source message on my server for storage. However, I found that when the input firewall traffic is large, Vector will repeatedly send the same log to my server. Does anyone have this problem?",
        "url": "https://github.com/vectordotdev/vector/discussions/18179",
        "createdAt": "2023-08-08T09:43:50Z",
        "updatedAt": "2023-08-08T09:43:52Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "chabater"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 18145,
        "title": "Release date of Vector 0.32.0",
        "bodyText": "Hello, may I know the release data of Vector 0.32.0. We are waiting a fix in Vector 0.32.0.\nThanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/18145",
        "createdAt": "2023-08-03T02:22:56Z",
        "updatedAt": "2023-08-03T15:58:42Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ffcactus"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18113,
        "title": "Is there a list of variables for template syntax?",
        "bodyText": "there're some examples over the resources documentation like application_id and host. is there a complete list somewhere? couldn't find any...",
        "url": "https://github.com/vectordotdev/vector/discussions/18113",
        "createdAt": "2023-07-30T08:19:43Z",
        "updatedAt": "2023-08-02T16:31:35Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "AvihaiSam"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 17984,
        "title": "Functions in VRL - Community IDs",
        "bodyText": "Hello, firstly, thanks for making such a good product! \ud83d\ude04\nI have recently implemented community ID hashing in VRL, see the bellow VRL snippet (see here for a full config file with tests):\n  . = parse_json!(.message)\n  saddr = ip_aton!(to_string!(.sourceIP))\n  daddr = ip_aton!(to_string!(.destIP))\n  sport = to_int!(.sourcePort)\n  dport = to_int!(.destPort)\n  proto = to_string!(.proto)\n\n\n  proto_padded = \"\"\n  if proto == \"TCP\" {\n    proto_padded = \"0600\"\n  } else if proto == \"UDP\" {\n    proto_padded = \"1100\"\n  } else {\n    log(\"Only TCP or UDP supported\")\n    abort\n  }\n\n\n  seed_padded = \"0000\"\n  sport_padded = to_string(format_int!(sport, 16))\n  dport_padded = to_string(format_int!(dport, 16))\n  saddr_padded = to_string(format_int!(saddr, 16))\n  daddr_padded = to_string(format_int!(daddr, 16))\n\n\n  if length(sport_padded) < 4 {\n    to_add = \"0\" *  (4 - length(sport_padded))\n    sport_padded = to_add + sport_padded\n  }\n\n  if length(dport_padded) < 4 {\n    to_add = \"0\" *  (4 - length(dport_padded))\n    dport_padded = to_add + dport_padded\n  }\n\n  if length(daddr_padded) < 8 {\n    to_add = \"0\" *  (8 - length(daddr_padded))\n    daddr_padded = to_add + daddr_padded\n  }\n\n  if length(saddr_padded) < 8 {\n    to_add = \"0\" *  (8 - length(saddr_padded))\n    saddr_padded = to_add + saddr_padded\n  }\n\n\n  to_hash_base16 = \"\"\n  if saddr < daddr {\n    to_hash_base16 = seed_padded + saddr_padded + daddr_padded + proto_padded + sport_padded + dport_padded\n  } else {\n    to_hash_base16 = seed_padded + daddr_padded + saddr_padded + proto_padded + dport_padded + sport_padded\n  }\n  to_hash, err = decode_base16(to_hash_base16)\n  if err != null {\n    log(err)\n  }\n  .community_id = \"1:\" + encode_base64(decode_base16!(sha1(to_hash)))\n\nIt's worth noting, the above is not a complete implementation of Community IDs. It has the following issues:\n\nIt only supports UDP + TCP\nIt lacks support for equal IP addresses, where it should fall back to using the ports for ordering\nIt only supports a seed of 0\n\nI've raised this discussion because I was looking to understand:\n\nDoes VRL support user defined functions, for example, I have had to repeat the padding functionality 4 times which could be put into a function.\nWould you be open to me making a community_id VRL function? I have found a Rust implementation.",
        "url": "https://github.com/vectordotdev/vector/discussions/17984",
        "createdAt": "2023-07-16T19:14:08Z",
        "updatedAt": "2023-09-06T07:16:27Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "owen5mith"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18105,
        "title": "Token based auth for Clickhouse using auth.token",
        "bodyText": "Hi,\nI was trying to figure out how to get auth token as mentioned here in docs.\nhttps://vector.dev/docs/reference/configuration/sinks/clickhouse/#auth.token\nThe Clickhouse docs do not mention anything about this method but if Vector supports it, I'd like to know how to set it up.\nCurrently, I am using a user-password-based strategy, but auth token-based method looks like a neater way of pushing data.\nfor more context, we have a simple clickhouseDB server and we are using clickhouse sink to push data.\nAny help on this is appreciated, thanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/18105",
        "createdAt": "2023-07-28T15:14:10Z",
        "updatedAt": "2023-08-07T13:36:32Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "DSdatsme"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 18086,
        "title": "errors with label {{ massage }}",
        "bodyText": "A note for the community\nNo response\nProblem\ni dont understand why i get these error when i add the field message: \"{{ message }}\"\nwithout this field all work correct but at the same time I see this label in loki\nConfiguration\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vector\n  namespace: loki\ndata:\n  vector.yaml: |-\n    api:\n      address: 0.0.0.0:8686\n      enabled: true\n      playground: false\n    data_dir: /var/local/lib/vector/\n    log_schema:\n      host_key: host\n      message_key: message\n      source_type_key: source_type\n      timestamp_key: timestamp\n    sinks:\n      loki:\n        compression: snappy\n        encoding:\n          codec: json\n        endpoint: http://loki-loki-distributed-gateway\n        inputs:\n          - kube_logs\n        labels:\n          message: \"{{ message }}\"\n          file: \"{{ file }}\"\n          stream: \"{{ stream }}\"\n          k8s_pod_namespace: \"{{ kubernetes.pod_namespace }}\"\n          k8s_pod_name: \"{{ kubernetes.pod_name }}\"\n          k8s_pod_uid: \"{{ kubernetes.pod_uid }}\"\n          k8s_namespace_labels_*: \"{{ kubernetes.namespace_labels }}\"\n          k8s_pod_node_name: \"{{ kubernetes.pod_node_name }}\"\n          k8s_pod_annotations_*: \"{{ kubernetes.pod_annotations }}\"\n          k8s_node_labels_*: \"{{ kubernetes.node_labels }}\"\n          k8s_container_image: \"{{ kubernetes.container_image }}\"\n          k8s_pod_owner: \"{{ kubernetes.pod_owner }}\" \n          k8s_container_image_id: \"{{ kubernetes.container_image_id }}\"\n          k8s_container_id: \"{{ kubernetes.container_id }}\"\n          k8s_pod_labels_*: \"{{ kubernetes.pod_labels }}\"\n          k8s_container_name: \"{{ kubernetes.container_name }}\"\n          k8s_pod_ip: \"{{ kubernetes.pod_ip }}\"\n        request:\n          in_flight_limit: 1\n        out_of_order_action: accept\n        path: /loki/api/v1/push\n        remove_timestamp: true\n        type: loki\n    sources:\n      kube_logs:\n        auto_partial_merge: true\n        data_dir: /var/local/lib/vector/\n        delay_deletion_ms: 60000\n        fingerprint_lines: 1\n        glob_minimum_cooldown_ms: 30000\n        ignore_older_secs: 600\n        ingestion_timestamp_field: .ingest_timestamp\n        max_line_bytes: 1000000\n        max_read_bytes: 1000000\n        read_from: beginning\n        self_node_name: ${VECTOR_SELF_NODE_NAME}\n        timezone: Europe/Moscow\n        type: kubernetes_logs\n\n\n\n### Version\n\nlatest\n\n### Debug Output\n\n2023-07-25T23:08:31.619567Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] is being suppressed to avoid flooding.\n2023-07-25T23:08:41.876097Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] has been suppressed 140 times.\n2023-07-25T23:08:41.876141Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\". error=Missing fields on event: [\"massage\"] error_type=\"template_failed\" stage=\"processing\" internal_log_rate_limit=true\n2023-07-25T23:08:41.876261Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] is being suppressed to avoid flooding.\n2023-07-25T23:09:00.353597Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] has been suppressed 4 times.\n2023-07-25T23:09:00.353677Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\". error=Missing fields on event: [\"massage\"] error_type=\"template_failed\" stage=\"processing\" internal_log_rate_limit=true\n2023-07-25T23:09:00.353800Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] is being suppressed to avoid flooding.\n2023-07-25T23:09:31.136773Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] has been suppressed 24 times.\n2023-07-25T23:09:31.136815Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\". error=Missing fields on event: [\"massage\"] error_type=\"template_failed\" stage=\"processing\" internal_log_rate_limit=true\n2023-07-25T23:09:33.209788Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] is being suppressed to avoid flooding.\n2023-07-25T23:09:47.574007Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] has been suppressed 4 times.\n2023-07-25T23:09:47.574059Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\". error=Missing fields on event: [\"massage\"] error_type=\"template_failed\" stage=\"processing\" internal_log_rate_limit=true\n2023-07-25T23:09:49.634743Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] is being suppressed to avoid flooding.\n2023-07-25T23:10:08.121251Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] has been suppressed 25 times.\n2023-07-25T23:10:08.121291Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\". error=Missing fields on event: [\"massage\"] error_type=\"template_failed\" stage=\"processing\" internal_log_rate_limit=true\n2023-07-25T23:10:08.121432Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] is being suppressed to avoid flooding.\n2023-07-25T23:10:30.697554Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] has been suppressed 3 times.\n2023-07-25T23:10:30.697616Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\". error=Missing fields on event: [\"massage\"] error_type=\"template_failed\" stage=\"processing\" internal_log_rate_limit=true\n2023-07-25T23:10:30.697759Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] is being suppressed to avoid flooding.\n2023-07-25T23:10:47.133987Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] has been suppressed 2 times.\n2023-07-25T23:10:47.134024Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\". error=Missing fields on event: [\"massage\"] error_type=\"template_failed\" stage=\"processing\" internal_log_rate_limit=true\n2023-07-25T23:10:59.445844Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\". error=Missing fields on event: [\"massage\"] error_type=\"template_failed\" stage=\"processing\" internal_log_rate_limit=true\n2023-07-25T23:10:59.446036Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] is being suppressed to avoid flooding.\n2023-07-25T23:11:28.200861Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] has been suppressed 27 times.\n2023-07-25T23:11:28.200913Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\". error=Missing fields on event: [\"massage\"] error_type=\"template_failed\" stage=\"processing\" internal_log_rate_limit=true\n2023-07-25T23:11:28.201020Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] is being suppressed to avoid flooding.\n2023-07-25T23:11:54.871740Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] has been suppressed 3 times.\n2023-07-25T23:11:54.871783Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\". error=Missing fields on event: [\"massage\"] error_type=\"template_failed\" stage=\"processing\" internal_log_rate_limit=true\n2023-07-25T23:12:03.086185Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] is being suppressed to avoid flooding.\n2023-07-25T23:12:07.209317Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] has been suppressed 25 times.\n2023-07-25T23:12:07.209348Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\". error=Missing fields on event: [\"massage\"] error_type=\"template_failed\" stage=\"processing\" internal_log_rate_limit=true\n2023-07-25T23:12:07.209452Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] is being suppressed to avoid flooding.\n2023-07-25T23:12:25.669049Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] has been suppressed 6 times.\n2023-07-25T23:12:25.669093Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\". error=Missing fields on event: [\"massage\"] error_type=\"template_failed\" stage=\"processing\" internal_log_rate_limit=true\n2023-07-25T23:12:25.669207Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] is being suppressed to avoid flooding.\n2023-07-25T23:12:44.138845Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] has been suppressed 1 times.\n2023-07-25T23:12:44.138885Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\". error=Missing fields on event: [\"massage\"] error_type=\"template_failed\" stage=\"processing\" internal_log_rate_limit=true\n2023-07-25T23:12:44.138999Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] is being suppressed to avoid flooding.\n2023-07-25T23:13:02.616531Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] has been suppressed 2 times.\n2023-07-25T23:13:02.616570Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\". error=Missing fields on event: [\"massage\"] error_type=\"template_failed\" stage=\"processing\" internal_log_rate_limit=true\n2023-07-25T23:13:02.616688Z  WARN sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{ massage }}\" with label_key \"massage\"\".] is being suppressed to avoid flooding.\n\n\n### Example Data\n\n_No response_\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_",
        "url": "https://github.com/vectordotdev/vector/discussions/18086",
        "createdAt": "2023-07-25T23:05:49Z",
        "updatedAt": "2023-07-26T23:54:04Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "AkakievKD"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 10
        },
        "upvoteCount": 1
    },
    {
        "number": 18087,
        "title": "How to exit/short-circuit pipeline during transform?",
        "bodyText": "I'm writing a transformation, extract-entity-count, which parses logs looking for a specific entry. Downstream of this there will be a postgres sink which writes the results of the transformation into a postgres table. Not all log entries will be relevant to this particular component of my pipeline. Is there a way to for a transformation to exit/short circuit such that none of the downstream components of the pipeline which use extract-entity-count as an input get triggered?\nAs an example, I have the following so far:\n[transforms.extract-entity-count]\ninputs = [\"parse_graph_node_logs\"]\ntype = \"remap\"\nsource = \"\"\"\nfinal_fields = {}\nfinal_fields.subgraph_id = .subgraph_id\nfinal_fields.unix_timestamp = .unix_timestamp\nfinal_fields.cluster_name = .cluster_name\n\n# Extract the entity count from the message\nregexed, err = parse_regex(.message, r'^.*entities: (?P<entity_count>\\\\d+),')\nif err == null {\n  final_fields.entity_count = parse_int!(regexed.entity_count)\n} else {\n  # if we didn't extract an entity count from this message, then set all fields to null, there's nothing to write to postgres\n  final_fields = {}\n  . = {}\n}\n\n. |= final_fields\n\"\"\"\n\n[sinks.stdout-entity-count]\ninputs = [\"extract-entity-count\"]\ntype = \"console\"\nencoding.codec = \"json\"\ntarget = \"stdout\"\nWhen I run this I see many log lines from vector which simply log {}. Ideally I'd like my stdout sink to only get run when extract-entity-count has something useful. Is that possible? I ask because I'd like to reduce ingestion load on my postgres instance (and any other sinks I create) when there's nothing useful for postgres to ingest.\nHere's a sample what I see when I run the above\ne2e-tests-vector-1                 | {}\ne2e-tests-vector-1                 | {}\ne2e-tests-vector-1                 | {}\ne2e-tests-vector-1                 | {}\ne2e-tests-vector-1                 | {}\ne2e-tests-vector-1                 | {}\ne2e-tests-vector-1                 | {}\ne2e-tests-vector-1                 | {}\ne2e-tests-vector-1                 | {}\ne2e-tests-vector-1                 | {}\ne2e-tests-vector-1                 | {}\ne2e-tests-vector-1                 | {}\ne2e-tests-vector-1                 | {}\ne2e-tests-vector-1                 | {}\ne2e-tests-vector-1                 | {}\ne2e-tests-vector-1                 | {\"cluster_name\":\"graph-node\",\"container_created_at\":\"2023-07-26T07:06:51.251591964Z\",\"container_id\":\"154141814d4a109a5ec49924ab834cc74856824e37f8ec463e0f94bd7ef2091d\",\"container_name\":\"e2e-tests-graph-node-1\",\"entity_count\":1652,\"host\":\"31e19d3715f5\",\"image\":\"e2e-tests-graph-node\",\"label\":{\"com.docker.compose.config-hash\":\"9c9c097d3ce45ebaa68b7f2a18e5ef39a68c725eddad5192b3f6093d7b5a33e2\",\"com.docker.compose.container-number\":\"1\",\"com.docker.compose.depends_on\":\"postgres-second:service_started,ipfs:service_started,postgres:service_started\",\"com.docker.compose.image\":\"sha256:9718789d195fa06a59674b20224b370f03a99de9637f57059a45fecd0fd8ac1a\",\"com.docker.compose.oneoff\":\"False\",\"com.docker.compose.project\":\"e2e-tests\",\"com.docker.compose.project.config_files\":\"/Users/paymahn/code/goldsky/graph-tools/packages/e2e-tests/docker-compose.yml\",\"com.docker.compose.project.working_dir\":\"/Users/paymahn/code/goldsky/graph-tools/packages/e2e-tests\",\"com.docker.compose.service\":\"graph-node\",\"com.docker.compose.version\":\"2.10.2\"},\"message\":\"Committed write batch, time_ms: 106, weight: 1686248, entities: 1652, block_count: 826, block_number: 17631324, sgd: 1, subgraph_id: QmThBaPEnXZK9w57bmnaUTe5hqRAfNnA1bfPAG9ZvxgB4H, component: SubgraphInstanceManager\",\"severity\":\"INFO\",\"source_type\":\"docker_logs\",\"stream\":\"stderr\",\"subgraph_id\":\"QmThBaPEnXZK9w57bmnaUTe5hqRAfNnA1bfPAG9ZvxgB4H\",\"timestamp\":\"2023-07-26T07:28:10.541601334Z\",\"unix_timestamp\":1690356490541}",
        "url": "https://github.com/vectordotdev/vector/discussions/18087",
        "createdAt": "2023-07-26T07:34:41Z",
        "updatedAt": "2023-07-26T08:02:30Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "paymog"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 3
    },
    {
        "number": 18077,
        "title": "How to parse a specific key of a json object and store it in a new field using vector remap language",
        "bodyText": "Hi,\nI am new to vector and VRL. I have a metric thath I am trying to send to GCP cloud monitoring sink. We want to process the metric's value from a json and then set it in a new field. Here is a sample input:\n{\"name\":\"some_service.metric_name_sample\",\"tags\":{\"key1\":\"val1\",\"key2\":\"val2\"},\"kind\":\"absolute\",\"gauge\":{\"value\":1.0}}\nI basically need to fetch the value from gauge and set it as one of the fields inside tags. Like expected output:\n{\"name\":\"some_service.metric_name_sample\",\"tags\":{\"key1\":\"val1\",\"key2\":\"val2\", \"value\": 1},\"kind\":\"absolute\",\"gauge\":{\"value\":1.0}}\nIs it possible to do it using remap? I thought it would be as simple as:\n\n[transforms.enrich_metrics]\ntype = \"remap\"\ninputs = [\"new_metrics\"]\nsource = '''\n.tags.value= .gauge.value\n'''\nOutput : It was same as input i.e. {\"name\":\"some_service.metric_name_sample\",\"tags\":{\"key1\":\"val1\",\"key2\":\"val2\"},\"kind\":\"absolute\",\"gauge\":{\"value\":1.0}}\n\nOn trying this:\n\n[transforms.enrich_metrics]\ntype = \"remap\"\ninputs = [\"new_metrics\"]\nsource = '''\n.tags.value= to_string!(.gauge.value)\n'''\nOutput: {\"name\":\"some_service.metric_name_sample\",\"tags\":{\"key1\":\"val1\",\"key2\":\"val2\",\"value\":\"\"},\"kind\":\"absolute\",\"gauge\":{\"value\":1.0}}\n\nIt looks like gauge needs to be parsed somehow, I tried parse_json, but that also kind of didn't work. It looks like .gauge is getting null value somehow.\n\n[transforms.enrich_metrics]\ntype = \"remap\"\ninputs = [\"new_metrics\"]\nsource = '''\n.tags.value= parse_json!(.gauge.value)\n'''\nOutput : It was same as input i.e. {\"name\":\"some_service.metric_name_sample\",\"tags\":{\"key1\":\"val1\",\"key2\":\"val2\"},\"kind\":\"absolute\",\"gauge\":{\"value\":1.0}}\n\nError: 2023-07-25T11:40:46.827680Z ERROR transform{component_kind=\"transform\" component_id=enrich_metrics component_type=remap component_name=enrich_metrics}: vector::internal_events::remap: Mapping failed with event. error=\"function call error for \\\"parse_json\\\" at (346:365): expected string, got null\" error_type=\"conversion_failed\" stage=\"processing\" internal_log_rate_limit=true\nCan somebody help me with this? @jszwedko",
        "url": "https://github.com/vectordotdev/vector/discussions/18077",
        "createdAt": "2023-07-25T11:49:42Z",
        "updatedAt": "2023-07-25T11:56:17Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "rk3094"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18067,
        "title": "Generate Unique ID (uuid) and pass across all remap transforms (parse_regex & parse_json)",
        "bodyText": "Hi\nI want to generate an an uid in the initial transform and have that available across all transforms.  I was able to accomplish this if all transforms uses \"parse_regex\"  but when I combine parse_regex and parse_json the uuid is coming as null..\n**When I print the transform to console.. this transform \"read_log_uid_s\" shows\nid=78209ab9-c5ec-4a1c-8289-d3992e6ad633\nbut when I print the transform \"read_log_process_log_s\"  it show\nid=null**\nHere is my toml\n`\n#####read logs from file  ########\n[sources.read_log]\ntype = \"file\"\ninclude = [\"/home/logs/*.log\"]\nread_from = \"end\"\n####ADD uuid<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n[transforms.read_log_uid]\ntype = \"remap\"\ninputs = [\"read_log\"]\nsource ='''\n. |= parse_regex!(.message, r'(.+)(?P{.+)')\n.sourcedata = .file\n.id=uuid_v4()\n'''\n####<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n####<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n[transforms.read_log_process_log]\ntype = \"remap\"\ndrop_on_error = true\ninputs = [\"read_log_uid\"]\nsource = '''. = parse_json!(.rawlogmessage)\n.sourcedata = .sourcedata\n.sourcetype = \"file\"\n.id=.id\n'''\n####<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n####<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n[sinks.read_log_uid_s]\ntype = \"console\"\ninputs = [\"read_log_uid\"]\nencoding.codec = \"json\"\n[sinks.read_log_process_log_s]\ntype = \"console\"\ninputs = [\"read_log_process_log\"]\nencoding.codec = \"json\"\n`",
        "url": "https://github.com/vectordotdev/vector/discussions/18067",
        "createdAt": "2023-07-23T06:30:56Z",
        "updatedAt": "2023-07-24T15:10:51Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "kbitra"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18038,
        "title": "ECS cluster with Vector agent running to ingest API data and S3 object",
        "bodyText": "I'm currently building a data pipeline that ingest data with the usage of a vector agent running in an ecs cluster that's being managed by Terraform. Also, the docker image read its' configuration file from a s3 bucket.\nSo, right now the vector agent runs perfect in the ecs service that's pulling the data from the sns topic into the s3 bucket.\nHere's the terraform code for it:\nresource \"aws_ecs_task_definition\" \"s3_task_def\" {\n  family                = \"vector-s3-task\"\n  network_mode          = \"awsvpc\"\n  requires_compatibilities = [\"FARGATE\"]\n  cpu                   = \"256\"\n  memory                = \"512\"\n  execution_role_arn    = aws_iam_role.logging_execution_role.arn\n  task_role_arn         = aws_iam_role.logging_role.arn\n\n  container_definitions = jsonencode([\n    {\n      \"name\":      \"infosec-vector-container\",\n      \"image\":     \"${aws_ecr_repository.repository.repository_url}:latest\",\n      \"essential\": true,\n      \"portMappings\": [\n        {\n          \"containerPort\": 8686,\n          \"hostPort\":      8686\n        }\n      ],\n      \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n          \"awslogs-group\":        aws_cloudwatch_log_group.log_group.name,\n          \"awslogs-region\":       \"us-west-2\",\n          \"awslogs-stream-prefix\": aws_cloudwatch_log_stream.log_stream.name\n        }\n      },\n      \"environment\": [\n        {\n          \"name\":  \"VECTOR_FILE\",\n          \"value\": var.vector\n        }\n      ]\n    }\n  ])\n}\n\n\n# creates an ECS service within the ECS cluster for s3 bucket\nresource \"aws_ecs_service\" \"service\" {\n  name            = \"vector-s3-service\"\n  cluster         = aws_ecs_cluster.cluster.id\n  task_definition = aws_ecs_task_definition.s3_task_def.arn\n  desired_count   = var.desired_count\n  launch_type     = \"FARGATE\"\n\n  network_configuration {\n    security_groups = [aws_security_group.sg.id]\n    subnets         = [aws_subnet.private_subnet.id]\n  }\n}\n\nHowever, when i add the ecs service that ingest data via api, i start to receive weird logs into the ecs logs and the logs that's being pulled from the api is not being sent to the s3 bucket.\nHere's the api ecs service terraform code:\nresource \"aws_ecs_task_definition\" \"api_task_def\" {\n  family                = \"vector-api-task\"\n  network_mode          = \"awsvpc\"\n  requires_compatibilities = [\"FARGATE\"]\n  cpu                   = \"512\"\n  memory                = \"1024\"\n  execution_role_arn    = aws_iam_role.logging_execution_role.arn\n  task_role_arn         = aws_iam_role.logging_role.arn\n\n  container_definitions = jsonencode([\n    {\n      \"name\":      \"infosec-vector-container\",\n      \"image\":     \"${aws_ecr_repository.repository.repository_url}:latest\",\n      \"essential\": true,\n      \"portMappings\": [\n        {\n          \"containerPort\": 8686,\n          \"hostPort\":      8686\n        }\n      ],\n      \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n          \"awslogs-group\":        aws_cloudwatch_log_group.log_group.name,\n          \"awslogs-region\":       \"us-west-2\",\n          \"awslogs-stream-prefix\": aws_cloudwatch_log_stream.log_stream.name\n        }\n      },\n      \"environment\": [\n        {\n          \"name\":  \"VECTOR_FILE\",\n          \"value\": var.vector_api\n        },\n        {\n          \"name\":  \"SLACK_KEY\",\n          \"value\": data.aws_secretsmanager_secret_version.slack_secret.secret_string\n        }\n      ]\n    }\n  ])\n}\n\n\n# creates an ECS service within the ECS cluster for s3api polling\nresource \"aws_ecs_service\" \"api_service\" {\n  name            = \"vector-api-service\"\n  cluster         = aws_ecs_cluster.cluster.id\n  task_definition = aws_ecs_task_definition.api_task_def.arn\n  desired_count   = 1\n  launch_type     = \"FARGATE\"\n\n  network_configuration {\n    security_groups = [aws_security_group.sg.id]\n    subnets         = [aws_subnet.private_subnet.id]\n  }\n}\n\nAlso here's my vector.toml file for the api polling config\n###################################################################################################################\n### SLACK AUDIT ###\n###################################################################################################################\n\n[sources.slack_audit]\ntype = \"http_client\"\nendpoint = \"https://api.slack.com/audit/v1/logs\"\nmethod = \"GET\"\nscrape_interval_secs = 900\nauth.strategy = \"bearer\"\nauth.token = \"${SLACK_KEY}\"\n\n    [sources.slack_audit.headers]\n    Accept = [\"application/json\"]\n   \n\n\n[transforms.slack_audit_output]\ntype = \"remap\"\ninputs = [\"slack_audit\"]\nsource = '''\n    .source = \"slack_audit\"\n    .vtime = now()\n    .data = parse_json!(.message)\n    del(.message)\n'''\n\n###################################################################################################################\n### SLACK DATA SOURCE ###\n###################################################################################################################\n[sinks.infosec_log_prod]\ntype = \"aws_s3\"\ninputs = [\"*_output\"]\nbucket = \"EXAMPLE\"\nkey_prefix = \"application={{ source }}/env=prod/year=%Y/month=%m/day=%d/\"\nregion = \"us-west-2\"\ncompression = \"gzip\"\nfilename_extension = \"json\"\nencoding.codec = \"json\"\nencoding.timestamp_format = \"rfc3339\"\n\nThese are the logs that I'm receiving in both ecs service/task logs and I'm not understanding why.\n{\"appname\":\"shaneIxD\",\"facility\":\"local2\",\"hostname\":\"random.org\",\"message\":\"A bug was encountered but not in Vector, which doesn't have bugs\",\"msgid\":\"ID258\",\"procid\":240,\"severity\":\"warning\",\"timestamp\":\"2023-07-20T21:39:06.581Z\",\"version\":1}\n\n{\"appname\":\"meln1ks\",\"facility\":\"syslog\",\"hostname\":\"make.com\",\"message\":\"We're gonna need a bigger boat\",\"msgid\":\"ID452\",\"procid\":4132,\"severity\":\"info\",\"timestamp\":\"2023-07-20T21:39:07.580Z\",\"version\":1}\n\n{\"appname\":\"devankoshal\",\"facility\":\"local5\",\"hostname\":\"we.de\",\"message\":\"Great Scott! We're never gonna reach 88 mph with the flux capacitor in its current state!\",\"msgid\":\"ID517\",\"procid\":8273,\"severity\":\"notice\",\"timestamp\":\"2023-07-20T21:39:08.580Z\",\"version\":1}\n\n{\"appname\":\"ahmadajmi\",\"facility\":\"user\",\"hostname\":\"names.com\",\"message\":\"A bug was encountered but not in Vector, which doesn't have bugs\",\"msgid\":\"ID236\",\"procid\":6192,\"severity\":\"notice\",\"timestamp\":\"2023-07-20T21:39:09.580Z\",\"version\":2}\n\n\n{\"appname\":\"devankoshal\",\"facility\":\"local7\",\"hostname\":\"make.us\",\"message\":\"There's a breach in the warp core, captain\",\"msgid\":\"ID172\",\"procid\":5465,\"severity\":\"info\",\"timestamp\":\"2023-07-20T21:39:10.580Z\",\"version\":1}\n\n\n{\"appname\":\"devankoshal\",\"facility\":\"news\",\"hostname\":\"make.de\",\"message\":\"A bug was encountered but not in Vector, which doesn't have bugs\",\"msgid\":\"ID916\",\"procid\":5888,\"severity\":\"emerg\",\"timestamp\":\"2023-07-20T21:39:11.580Z\",\"version\":1}\n\nEvery time I restart the service, I get the exact same logs. When I remove the API ECS service, my S3 ECS service works fine. Has anyone encountered this before ?\nI've tested the configuration file without sending the data to the s3 bucket and into my terminal instead, it worked as it's supposed to.\nI've tried removing the s3 ecs service to see if that was the problem, I still receive the same error.\nI've tried removing the API s3 ecs service and everything works fine.",
        "url": "https://github.com/vectordotdev/vector/discussions/18038",
        "createdAt": "2023-07-20T22:14:23Z",
        "updatedAt": "2023-07-24T14:37:33Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tayontech"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18029,
        "title": "ERROR: Internal log is being rate limited",
        "bodyText": "ERROR transform{component_kind=\"transform\" component_id=access_logs_remap component_type=remap component_name=access_logs_remap}:\nvector::internal_events::remap: Internal log [Mapping failed with event.] is being rate limited.\nERROR sink{component_kind=\"sink\" component_id=to_es_logs component_type=elasticsearch component_name=to_es_logs}:request{request_id=1762}:\nvector_common::internal_event::service: Internal log [Service call failed. No retries or retries exhausted.] is being rate limited.\nI want to know how to solve the error?\nIf the error is nomal ?",
        "url": "https://github.com/vectordotdev/vector/discussions/18029",
        "createdAt": "2023-07-20T03:15:25Z",
        "updatedAt": "2023-07-24T06:44:30Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "yingzihezhuzhu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 18043,
        "title": "Will Vector 0.32.0 use VRL 0.6.0",
        "bodyText": "We are waiting for a fix in Vector. In fact, it's in VRL.  vectordotdev/vrl#299\nThe fix is merged into VRL and will be included in VRL 0.6.0. I would like to know if the next release of Vector (0.32.0) will use it.\nThanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/18043",
        "createdAt": "2023-07-21T05:17:01Z",
        "updatedAt": "2023-07-24T01:12:01Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ffcactus"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 18041,
        "title": "Prevent Vector from ingesting old log lines upon restart after a maintenance",
        "bodyText": "I have a use case where Vector is tailing logs from a file (file source) on a linux machine. Now, there are times when I transition the machine to under maintenance at which point we stop vector (and many other processes). Once the maintenance is done, Vector is restarted and the machine is marked operational.\nSometimes during the maintenance, there are lot of errors that gets logged in some of the log files vector was tailing, so when Vector comes back up after the maintenance it ingests all those old logs (due to the checkpoint memory) and these old (not important) logs cause false alarms in our monitoring system. The idea is \"errors are expected when in maintenance and we do not want to ingest and alert on them\"\nTo solve this issue, I am thinking of doing the following :\n\nWhen Vector restarts after the maintenance, let it start afresh by deleting the checkpointing file.\nUse https://vector.dev/docs/reference/configuration/sources/file/#read_from so that when the files are re-discovered, only new logs are ingested.\n\nI am yet to test this  but does that sound okay @jszwedko ?",
        "url": "https://github.com/vectordotdev/vector/discussions/18041",
        "createdAt": "2023-07-21T03:33:30Z",
        "updatedAt": "2023-07-22T01:52:31Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "atibdialpad"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 1
    },
    {
        "number": 13680,
        "title": "vector is not a good name",
        "bodyText": "the name vector just too common to search.\nwhy don't use a unique name?\nthanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/13680",
        "createdAt": "2022-07-22T06:08:30Z",
        "updatedAt": "2023-07-20T18:13:03Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ahfuzhang"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 6
    },
    {
        "number": 18033,
        "title": "Compatibility matrix with k8s",
        "bodyText": "Hey guys!\nCan I find a compatibility matrix or all supported k8s versions somewhere in the Docs?\nA compatibility matrix could be essential for users to quickly determine whether the Vector is compatible with their specific Kubernetes version, thus ensuring a smooth integration and usage experience.\nI will upgrade k8s to 1.26, 1.27, respectively, and I don't know if Vector supports these k8s versions.",
        "url": "https://github.com/vectordotdev/vector/discussions/18033",
        "createdAt": "2023-07-20T07:20:42Z",
        "updatedAt": "2023-07-20T15:56:20Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tomas-balaz"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18026,
        "title": "Need Help : Processing a metric timestamp value to unix time epoch nanoseconds",
        "bodyText": "Hi,\nI am new to vector and VRL. I have a metric that I am trying to sent to GCP cloud monitoring sink. There is a requirement from the consumer service to accept metric timestamp in unix epoch time format of nanoseconds. Here is a json of the metric that I have:\n{\"name\":\"some_service.metric_name_sample\",\"tags\":{\"key1\":\"val1\",\"key2\":\"val2\"},\"timestamp\":\"2023-07-19T16:56:42.792764503Z\",\"kind\":\"absolute\",\"gauge\":{\"value\":1.0}}\nI need to convert the timestamp field to unix epoch time format in nanoseconds. I tried to do this using a transform of \"remap\" type in vector remap language using to_unix_timestamp method. But it is not having any effect on the json. Here is the code in config.toml:\n\n[transforms.change_timestamp_format]\ntype = \"remap\"\ninputs = [\"new_metrics\"]\nsource = '''\n.tags.new_time = to_unix_timestamp!(.timestamp, unit: \"nanoseconds\")\n'''\n\nExpectation is to get a new key value pair in the json blob inside tags as \"new_time\": \"1689765925\"\nCan somebody please help me with this?",
        "url": "https://github.com/vectordotdev/vector/discussions/18026",
        "createdAt": "2023-07-19T17:13:04Z",
        "updatedAt": "2023-07-25T11:55:32Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "rk3094"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 18012,
        "title": "Whether the topic field of sinks kafka can be obtained from the source or transforms",
        "bodyText": "deploy vector by helm chart. Vector Helm\n\nQuestion Description\nThe logs of different services are written to different topics, so when writing logs to kafka in the sinks stage, the value of the kafka topic field obtained in the transform stage must be used to determine the topic information written to kafka.\nHowever, after reading the docs and trying to do in some ways, the result did not meet expectations.  So ask it here.\nSome try\ntopic: .k8s_topic\n\ntopic: k8s_topic\n\ntopic: \"{{ .k8s_topic }}\"\n\ntopic: {{ .k8s_topic }}\n\nMy Config of values.yaml\ncustomConfig:\n  data_dir: /data/logs\n  api:\n    enabled: true\n    address: 127.0.0.1:8686\n    playground: false\n  # \u65e5\u5fd7\u6765\u6e90\n  sources:\n    k8s_log:\n      type: kubernetes_logs\n      data_dir: \"/data/logs/\" # The directory used to persist file checkpoint positions. \u5b58\u50a8checkpoint\u4f4d\u7f6e\u7684\u76ee\u5f55\uff0c\u6240\u4ee5\u6307\u5b9a\u6536\u96c6\u76ee\u5f55\u662f\u4ec0\u4e48\uff1f\n      extra_label_selector: \"app=hello-admin-clone-stag\"\n\n  # \u65e5\u5fd7\u8f6c\u6362\n  transforms:\n    trans_log:\n      type: remap\n      inputs:\n        - k8s_log\n      source: |-\n        .node_name = del(.kubernetes.pod_node_name)\n        .pod_name = del(.kubernetes.pod_name)\n        .swimlane = del(.kubernetes.pod_labels.swimlane)\n        .owt = del(.kubernetes.pod_labels.owt)\n        .service = del(.kubernetes.pod_labels.service)\n        .cluster = del(.kubernetes.pod_labels.cluster)\n        .k8s_topic = del(.kubernetes.pod_labels.kafkaTopic)\n        .version = del(.kubernetes.pod_annotations.version)\n        . |= parse_regex!(.file, r'/.*?/.*?/.*?/.*?/(?P<log_path>.*)$')\n        del(.file)\n        del(.kubernetes)\n        .yx_label = \"vector\"\n\n  # \u65e5\u5fd7\u8f93\u51fa\n  sinks:\n    stdout: # \u81ea\u5b9a\u4e49\u540d\u79f0\n      type: console\n      inputs: [trans_log]\n      encoding:\n        codec: json\n    kafka_sink:\n      type: kafka\n      inputs:\n        - trans_log\n      bootstrap_servers: 127.0.0.1:9092\n      encoding:\n        codec: json\n      topic: \"{{ .k8s_topic }}\"\n      compression: gzip\nFinal\nWe look forward to your discussion. Thanks~",
        "url": "https://github.com/vectordotdev/vector/discussions/18012",
        "createdAt": "2023-07-18T11:09:57Z",
        "updatedAt": "2023-07-19T02:21:28Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "YuWan1117"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17873,
        "title": "Vector source/sink not propagating non-retryable failures",
        "bodyText": "I'm still trying to quantify exactly what the bug is, but it seems like the vector source/sink does not propagate back up non-retryable delivery failures from the end sink. In my setup, I have a Vector instance as a sort of post office delivery multiplexer, reading from Kafka and distributing to one or more exporter Vector instances, connected via the vector sink.\n                            / -> (vector sink) -> (vector source) Exporter 1 -> http sink\nKafka -> (kafka source) Mux | -> (vector sink) -> (vector source) Exporter 2 -> loki sink\n                            \\ -> (vector sink) -> (vector source) Exporter 3 -> splunk_hec_logs sink\n\nIn exporters where some failures are non-retryable (e.g. HTTP sink with non-retryable errors like 400), it seems as if at the exporter level the event is appropriately dropped. However, this drop action apparently does not communicate back a hard failure acknowledgement or similar signal through the vector protocol, back to the Mux instance. The vector sink retry on the Mux instance apparently sees a delivery failure (or rather lack of acknowledgement) and repeatedly tries to retry delivery of the message to the exporter. This continues ad nauseam until the retry count is exceeded or (more likely) the exporter's vector sink buffer on the Mux is filled - which then has a follow-on bad behavior of stopping the whole mux in its tracks, stopping delivery of all messages to all destination exporters.",
        "url": "https://github.com/vectordotdev/vector/discussions/17873",
        "createdAt": "2023-07-05T20:25:00Z",
        "updatedAt": "2023-07-18T19:26:02Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "sbalmos"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 18015,
        "title": "Corrupt disk buffer at startup causes crash?",
        "bodyText": "Another one I'm trying to track down what it might be before I file a bug report. Our k8s cluster had a node failure, so I went and restarted our one Vector instance that performs message routing and distribution. The sinks in this instance all have disk buffers on them, backed by a large PVC.\nI've seen this only once before, but never really caught why. I perform a rolling restart, which initiates a normal Vector shutdown. Invariably some of the sinks never finish acknowledgements (Splunk HEC is notorious at this, taking nearly 5 minutes to index-acknowledge), so the instance gets killed after the 60 second timeout. After the instance starts back up, it would immediately crash on startup with the error message below. I have to completely delete the PVC containing all the disk buffers in order to get the instance to start again. Apparently the 60 second timeout, in its processing of killing the shutdown, corrupted the buffer. Any ideas on a workaround? Having to delete the PVC and start the buffers from scratch is... annoying. :) I'm willing to work on a PR if I could figure out what/where the disk buffer at-startup recovery code might be failing.\n2023-07-18T16:09:21.356944Z ERROR vector::topology: Configuration error. error=Sink \"splunk_hec\": error occurred when building buffer: failed to build individual stage 0: failed to seek to position where reader left off: failed to decoded record: InvalidProtobufPayload",
        "url": "https://github.com/vectordotdev/vector/discussions/18015",
        "createdAt": "2023-07-18T16:32:25Z",
        "updatedAt": "2023-07-18T16:32:26Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "sbalmos"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 17942,
        "title": "How to limit vector memory usage",
        "bodyText": "We will use vector as deamonset deployed on each node node for collecting logs.Since the node itself will deploy the service, vector agent will allocate only 500MiB of memory.\nBut when we process the logs in large amount, vector will request more than 500MiB of memory and cause oom kill.We We have activated the sink disk buffer, but the memory still exceeds the limit, how should we configure the memory limit in vector and avoid the oom?",
        "url": "https://github.com/vectordotdev/vector/discussions/17942",
        "createdAt": "2023-07-11T13:27:07Z",
        "updatedAt": "2023-07-15T14:20:53Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "z2665"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17902,
        "title": "Getting Parse error in Source",
        "bodyText": "Hi All ,\nI am getting below error in my fluentd source.\n2023-07-07T10:06:30.135630Z ERROR source{component_kind=\"source\" component_id=fluent component_type=fluent component_name=fluent}:connection{peer_addr=XXXX:62907}: vector::internal_events::codecs: Internal log [Failed framing bytes.] has been rate limited 6 times.\n2023-07-07T10:06:30.135646Z ERROR source{component_kind=\"source\" component_id=fluent component_type=fluent component_name=fluent}:connection{peer_addr=XXXX:62907}: vector::internal_events::codecs: Failed framing bytes. error=bytes remaining on stream error_type=\"parser_failed\" stage=\"processing\" internal_log_rate_limit=true\nAny idea about the issue?",
        "url": "https://github.com/vectordotdev/vector/discussions/17902",
        "createdAt": "2023-07-07T10:15:03Z",
        "updatedAt": "2023-07-14T22:41:28Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "sudhanshud"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 17971,
        "title": "[ASK] Why sources `file` and Sinks `console` generated random log?",
        "bodyText": "Hi all, need your insight and help, not sure below behavior is bug or not.\nusing this vector config :\n    data_dir: /vector-data-dir\n    sources:\n      file_log:\n        type: file\n        include:\n          - \"/var/log/sidecar/*.log\"\n    sinks:\n      stdout:\n        type: console\n        inputs:\n          - file_log\n        encoding:\n          codec: json\nsomehow its generate random log like below,\n\nlooking at the trend, seem its demo_log like in ref: https://vector.dev/guides/level-up/transformation/\nany method to prevent it? since I just want to tailing the file logs, not generate random logs,\nThanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/17971",
        "createdAt": "2023-07-14T07:54:57Z",
        "updatedAt": "2023-07-14T14:39:58Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "kholisrag"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17959,
        "title": "About sinks global config",
        "bodyText": "I need to write many clickhouse tables, but there is only one instance of clickhouse and database. However, now in the configuration file, I have to write the database address, username, and password for each table. Can there be a global configuration?",
        "url": "https://github.com/vectordotdev/vector/discussions/17959",
        "createdAt": "2023-07-13T04:05:42Z",
        "updatedAt": "2023-07-14T07:10:45Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "yangshike"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 17954,
        "title": "Vector fails to resolve metadata.google.internal",
        "bodyText": "Hi Vector team,\nI am using Vector in a custom Kubernetes cluster running on GCP VM instances. I want to push logs to a Google Cloud Storage Bucket but my Vector pods were always failing.\nI have all the correct permissions on my GCP instance VM node (roles: \"Storage Object Creator\" and \" Storage Object Viewer \").\nSo after investigating the issue, I found out that the pod was not resolving the \"metadata.google.internal\" to 169.254.169.254. The GCP VM instance had the \"metadata.google.internal\" entry hardcoded in /etc/hosts but that entry was not being used by K8s containers. I can still reach the endpoint by using the IP. Here is the test on debug pod:\n/ # curl http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token -H \"metadata-flavor: Google\"\ncurl: (6) Could not resolve host: metadata.google.internal\n/ # curl http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token -H \"metadata-flavor: Google\"\n{\"access_token\":\"[redacted]\",\"expires_in\":[redacted],\"token_type\":\"Bearer\"}/ #\n\nThe workaround was to add the hardcoded entry in coreDNS configmap by adding the following and then restarting the vector pods:\nhosts /etc/coredns/customdomains.db google.internal\n    {\n     169.254.169.254 metadata.google.internal   \n  }\n\nIs it possible to fallback to the IP 169.254.169.254 if the DNS \"metadata.google.internal\" fails? Or just use the IP and not use the DNS?\nLogs:\n2023-07-12T16:14:34.824322Z DEBUG vector::app: Internal log rate limit configured. internal_log_rate_secs=10\n2023-07-12T16:14:34.824411Z  INFO vector::app: Log level is enabled. level=\"debug\"\n2023-07-12T16:14:34.826068Z  INFO vector::app: Loading configs. paths=[\"/etc/vector\"]\n2023-07-12T16:14:34.827334Z DEBUG vector::config::loading: No secret placeholder found, skipping secret resolution.\n2023-07-12T16:14:34.828552Z DEBUG vector::topology::builder: Building new source. component=k8s\n2023-07-12T16:14:34.829273Z  INFO source{component_kind=\"source\" component_id=k8s component_type=kubernetes_logs component_name=k8s}: vector::sources::kubernetes_logs: Obtained Kubernetes Node name to collect logs for (self). self_node_name=\"node-4vgd\"\n2023-07-12T16:14:34.842478Z  INFO source{component_kind=\"source\" component_id=k8s component_type=kubernetes_logs component_name=k8s}: vector::sources::kubernetes_logs: Excluding matching files. exclude_paths=[\"**/*.gz\", \"**/*.tmp\"]\n2023-07-12T16:14:34.842597Z DEBUG vector::topology::builder: Building new sink. component=bucket\n2023-07-12T16:14:34.842898Z DEBUG sink{component_kind=\"sink\" component_id=bucket component_type=gcp_cloud_storage component_name=bucket}: vector::gcp: Fetching implicit GCP authentication token.\n2023-07-12T16:14:34.877233Z DEBUG sink{component_kind=\"sink\" component_id=bucket component_type=gcp_cloud_storage component_name=bucket}:http: vector::internal_events::http_client: Sending HTTP request. uri=http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token method=GET version=HTTP/1.1 headers={\"metadata-flavor\": \"Google\", \"user-agent\": \"Vector/0.31.0 (x86_64-unknown-linux-gnu 0f13b22 2023-07-06 13:52:34.591204470)\", \"accept-encoding\": \"identity\"} body=[empty]\n2023-07-12T16:14:34.880664Z DEBUG hyper::client::connect::dns: resolving host=\"metadata.google.internal\"\n2023-07-12T16:14:34.887297Z  WARN sink{component_kind=\"sink\" component_id=bucket component_type=gcp_cloud_storage component_name=bucket}:http: vector::internal_events::http_client: HTTP error. error=error trying to connect: dns error: failed to lookup address information: Name or service not known error_type=\"request_failed\" stage=\"processing\" internal_log_rate_limit=true\n2023-07-12T16:14:34.889769Z ERROR vector::topology: Configuration error. error=Sink \"bucket\": Failed to get implicit GCP token: Failed to make HTTP(S) request: error trying to connect: dns error: failed to lookup address information: Name or service not known\n\nConfiguration vector-values.yaml:\n---\nrole: Agent\nserviceHeadless:\n  enabled: false\nservice:\n  enabled: false\nenv:\n  - name: \"VECTOR_LOG\"\n    value: \"debug\"\ncustomConfig:\n  data_dir: /vector-data-dir\n  sources:\n    k8s:\n      type: kubernetes_logs\n  sinks:\n    bucket:\n      type: gcp_cloud_storage\n      bucket: my_k8s_logs\n      encoding:\n        codec: \"raw_message\"\n      inputs:\n        - k8s",
        "url": "https://github.com/vectordotdev/vector/discussions/17954",
        "createdAt": "2023-07-12T17:32:26Z",
        "updatedAt": "2023-07-13T16:08:57Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "luisloros"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 17949,
        "title": "does vector support database metrics collect like mysql\uff1f",
        "bodyText": "what will be great if vector support auto collect database metrics",
        "url": "https://github.com/vectordotdev/vector/discussions/17949",
        "createdAt": "2023-07-12T08:14:30Z",
        "updatedAt": "2023-07-12T14:55:38Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "lddlww"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10022,
        "title": "How can I retrieve useful fields via GraphQL API?",
        "bodyText": "I am playing with vector's GraphQL endpoint and playground, and I tried to retrieve fields name and other fields like sources for sinks.\nAfter enabling GraphQL and starting up vector, I found I can only retrieve pagination information about the objects like sinks.\n\nHowever, in the public playground, I can retrieve fields like name and sources using the GraphQL API:\nQuery:\n{\n  sinks {\n    name\n    sources {\n      name\n    }\n  }\n}\n\nResponse:\n{\n  \"data\": {\n    \"sinks\": [\n      {\n        \"name\": \"prometheus\",\n        \"sources\": [\n          {\n            \"name\": \"internal_metrics\"\n          }\n        ]\n      },\n      {\n        \"name\": \"blackhole\",\n        \"sources\": []\n      }\n    ]\n  }\n}\n\nI think even for the default vector configuration, it sets up 1 sink (sinks.print)/1 transform (transforms.parse_logs)/1 source (sources.dummy_logs) in vector.toml, so I think there should be some data I can retrieve. Is there anything I am missing here? Please forgive my ignorance since I am pretty new to both vector and GraphQL. Thanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/10022",
        "createdAt": "2021-11-13T01:34:59Z",
        "updatedAt": "2023-07-11T09:09:50Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "niyue"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 2
    },
    {
        "number": 17940,
        "title": "how to use serviceaccount token for authentication?",
        "bodyText": "https://vector.dev/docs/reference/configuration/sinks/pulsar/#auth\naccording to the document, looks like the sink function only supports static token or oauth2.\ncan I use file path for token? because sa token is automatically rolled out after a particular time.",
        "url": "https://github.com/vectordotdev/vector/discussions/17940",
        "createdAt": "2023-07-11T07:35:58Z",
        "updatedAt": "2023-07-11T07:35:59Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "zzzz465"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 2
    },
    {
        "number": 17924,
        "title": "How to implement customized fetch data when remap, similar to geoip",
        "bodyText": "To parse a geoip use the following\n.geoip = get_enrichment_table_record!(\"geoip\", { \"ip\": .xff })\nI have an id data like 10001, 10002, I want to parse the json data and get the result in json when I pass in the ids {\"10001\": {\"region\": \"eu\", \"name\": \"hello world\"}} by using a form similar to geoip. What should I do?",
        "url": "https://github.com/vectordotdev/vector/discussions/17924",
        "createdAt": "2023-07-10T10:08:53Z",
        "updatedAt": "2023-07-11T02:34:29Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tangguangliang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17925,
        "title": "Getting error in filter condition",
        "bodyText": "Hi,\nCould anyone help me with this error. I'm using a \"filter\" transform and getting an error for using >(greater than) operator whereas while using == or != I dont find any error.\nERROR:\n2023-07-10T11:57:40.493859Z  INFO vector::app: Loading configs. paths=[\"/root/.vector/config/vector.toml\"]\n2023-07-10T11:57:40.498064Z ERROR vector::topology: Configuration error. error=Transform \"filter_out\":\nerror[E100]: unhandled error\n\u250c\u2500 :1:1\n\u2502\n1 \u2502 .time_diff > 20\n\u2502 ^^^^^^^^^^^^^^^\n\u2502 \u2502\n\u2502 expression can result in runtime error\n\u2502 handle the error case to ensure runtime success\n\u2502\n= see documentation about error handling at https://errors.vrl.dev/#handling\n= learn more about error code 100 at https://errors.vrl.dev/100\n= see language documentation at https://vrl.dev\n= try your code in the VRL REPL, learn more at https://vrl.dev/examples",
        "url": "https://github.com/vectordotdev/vector/discussions/17925",
        "createdAt": "2023-07-10T12:02:16Z",
        "updatedAt": "2023-07-10T14:12:56Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "manojkdn"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17866,
        "title": "source/address field: hostname not supported ?",
        "bodyText": "It looks like vector does not resolve hostnames for sockets.\nI have a docker network with 2 containers, I wanted to use the docker version of vector with a source port on the other container.\nI can not rely on IPs as docker may change them.\nDo you know a solution ?\nTy !\n[sources.some]\ntype = \"socket\"\naddress = \"somehost.local:30001\"\nConfiguration error. error=Must be a valid IPv4/IPv6 address with port, or start with \"systemd\"",
        "url": "https://github.com/vectordotdev/vector/discussions/17866",
        "createdAt": "2023-07-05T13:24:46Z",
        "updatedAt": "2023-07-06T21:30:46Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "softlion"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 17885,
        "title": "How to parse escaped nginx log line with escaped double quotes using VRL",
        "bodyText": "This is my source data:\n{\n  \"_timestamp\": 1688641704414631,\n  \"kubernetes_annotations_kubernetes_io_psp\": \"eks.privileged\",\n  \"kubernetes_container_hash\": \"registry.k8s.io/ingress-nginx/controller@sha256:4ba73c697770664c1e00e9f968de14e08f606ff961c76e5d7033a4a9c593c629\",\n  \"kubernetes_container_image\": \"registry.k8s.io/ingress-nginx/controller@sha256:4ba73c697770664c1e00e9f968de14e08f606ff961c76e5d7033a4a9c593c629\",\n  \"kubernetes_container_name\": \"controller\",\n  \"kubernetes_labels_app_kubernetes_io_component\": \"controller\",\n  \"kubernetes_labels_app_kubernetes_io_instance\": \"ingress-nginx\",\n  \"kubernetes_labels_app_kubernetes_io_name\": \"ingress-nginx\",\n  \"kubernetes_labels_pod_template_hash\": \"7d97444d5\",\n  \"kubernetes_namespace_name\": \"ingress-nginx\",\n  \"kubernetes_pod_name\": \"ingress-nginx-controller-7d97444d5-l9rfd\",\n  \"log\": \"10.2.81.141 - alice [06/Jul/2023:11:08:24 +0000] \\\"POST /api/path1/prometheus/api/v1/write HTTP/1.1\\\" 200 0 \\\"-\\\" \\\"Prometheus/2.15.2+ds\\\" 468 0.005 [zinc-cp1-zinc-cp-4082] [] 9.9.176.197:4082 0 0.005 200 0ca2f386ce3b27951f36fb1f54af41f5\\n\",\n  \"stream\": \"stdout\"\n}\nlog field is standard nginx log line.\nHow do I parse this using VRL with parse_nginx_log?",
        "url": "https://github.com/vectordotdev/vector/discussions/17885",
        "createdAt": "2023-07-06T11:57:23Z",
        "updatedAt": "2023-07-06T19:10:04Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "prabhatsharma"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 17854,
        "title": "Source file: allow to skip file header (lines)",
        "bodyText": "Hi!\nWe exensivly use Vector.dev to gather logs. File source is one the most useful for us. Some files (like /var/log/dirsrv//errors, /var/log/dirsrv//audit, /var/log/dirsrv/**/access)  from FreeIPA server have the identical header like this:\n         389-Directory/[1.4.3.6](http://1.4.3.6/) B2022.332.1108\n         ipatest.xxx.dcs.lan:636 (/etc/dirsrv/slapd-XXX-DCS-LAN)\n\n\nAnd it may vary on dev\\test\\prod as well as different server. It would be great to have option to skip rows for some files.\nNow we use the following config for files:\nfingerprint.strategy = \"checksum\"\nfingerprint.lines = 5\nfingerprint.ignored_header_bytes = 0\n\nIt allows us to control rotation (fingerprint.ignored_header_bytes couldn't be used because header could vary on different servers) but header lines got captured by Vector.dev and finally ingested in our backend storage (ClickHouse).\nDo we have any other option (execpt some sort of Regex and so on) to skip file header?\nThanks.\nFreeipa.toml\n#\n#  SOURCES\n#\n\n[sources.freeipa_additional_logs]\ntype = \"file\"\ninclude = [ \"/var/log/apache2/*.log\", \"/var/log/ipa/*.log\", \"var/log/pki/pki-ca-spawn.*.log\", \"/var/log/pki/pki-kra-spawn.*.log\", \"/var/log/dirsrv/*/errors\", \"/var/log/sssd/*.log\" ]\nexclude = [ \"/var/log/ipa/*backup.log\"  ]\n\n[sources.freeipa_backup_log]\ntype = \"file\"\ninclude = [ \"/var/log/ipa/*backup.log\" ]\nmultiline.start_pattern = \"^$\"\nmultiline.condition_pattern = \"^$\"\nmultiline.mode = \"halt_before\"\nmultiline.timeout_ms = 1000\n\n[sources.freeipa_audit_log]\ntype = \"file\"\ninclude = [ \"/var/log/dirsrv/*/audit\" ]\nfingerprint.strategy = \"checksum\"\nfingerprint.lines = 5\nfingerprint.ignored_header_bytes = 0\nmultiline.start_pattern = \"^time: (\\\\d{14})\"\nmultiline.condition_pattern = \"^time: (\\\\d{14})\"\nmultiline.mode = \"halt_before\"\nmultiline.timeout_ms = 1000\n\n[sources.freeipa_access_log]\ntype = \"file\"\ninclude = [ \"/var/log/dirsrv/*/access\", \"/var/log/dirsrv/*/access.*\"]\nfingerprint.strategy = \"checksum\"\nfingerprint.lines = 5\nfingerprint.ignored_header_bytes = 0\n\n[sources.freeipa_logs]\ntype = \"file\"\ninclude = [ \"/var/log/auth.log\" ]",
        "url": "https://github.com/vectordotdev/vector/discussions/17854",
        "createdAt": "2023-07-04T13:27:33Z",
        "updatedAt": "2023-07-06T09:07:38Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "inatale"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17830,
        "title": "Mask PII data in ALB logs",
        "bodyText": "I am sending my AWS ALB logs data (dumped at s3) to Elasticsearch using vector. Unfortunately, the request_url parts have some PII data in them. I want to mask it using vector before sending it to elastic search. The PII can be summed up in regex pattern as below:\npii_regex_patterns = {\n\"email\": r\"([a-zA-Z0-9.-]+@[a-zA-Z0-9.-]+.[a-zA-Z0-9_-]+)\",\n\"phone\": r\"(+\\d{1,3}[- ]?)?\\d{10}[\\s , )]\",\n\"pan\": r\"[A-Z]{5}[0-9]{4}[A-Z]{1}\",\n\"aadhar\": r\"[0-9]{4}[ -]?[0-9]{4}[ -]?[0-9]{4}[\" \" , . )]\",\n}\nCan I possibly do that? Currently my vector.toml file is simple with the following transform section:\n[transforms.aws_alb_logs_transform]\ntype = \"remap\"\ninputs = [\"aws_alb_logs\"]\ndrop_on_error = false\nsource = '''\n. = parse_aws_alb_log!(string!(.message))\n.request_url_parts = parse_url!(.request_url)\n'''\nhow can I achieve it? Have trouble with existing syntax.",
        "url": "https://github.com/vectordotdev/vector/discussions/17830",
        "createdAt": "2023-07-01T13:13:08Z",
        "updatedAt": "2023-07-05T17:33:50Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "bhushan-amit"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17839,
        "title": "No. of connections",
        "bodyText": "I'm evaluating Vector as an agent and using Splunk HEC as a Sink.\nI would like to limit the number of connections between the source server to Splunk HEC endpoint due to firewall in between. Don't want to cause any DDoS on Firewall due to limitless connections to flush the data. What are the possibilities to limit?\nAlso, as mentioned in the internal_metrics source documentation, I don't see any connection metrics populated. Are there any specific configurations to be enable to populate connection metrics?",
        "url": "https://github.com/vectordotdev/vector/discussions/17839",
        "createdAt": "2023-07-03T10:03:56Z",
        "updatedAt": "2023-09-08T15:31:28Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "email2vimalraj"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17831,
        "title": "How to get the original message ?",
        "bodyText": "The original message like this:\n{\n     \"abilityState\": \"1\",\n     \"t_servicename\": \"createTask\",\n     \"tag\": \"push-openapi-access\"\n}\n\nI  get message by vector like this:\n{\n\"log\": \"{\\\"abilityState\\\":\\\"1\\\",\\\"t_servicename\\\":\\\"createTask\\\",\\\"tag\\\":\\\"push-openapi-access\\\"}\"\n}\n\nthis is my  transforms  config:\n[sources.a]\n    type = \"fluent\"\n    address = \"0.0.0.0:8225\"\n    receive_buffer_bytes = 65_536\n[transforms.b]\ntype = \"remap\"\ninputs = [\"a\"]\nsource = '''\n  del(.host)\n  del(.source_type)\n  del(.timestamp)\n  del(.tag)\n'''\n[sinks.out]\n    inputs = [\"b\"]\n    type = \"console\"\n    encoding.codec = \"json\"\n\nHow can I get the original message ?\nI used to finish it by  transforms json_parser ,but now it has been deprecated.\n[transforms.a_parse]\ninputs = [\"a_coming\"]\ntype = \"json_parser\"\nfield = \"log\"",
        "url": "https://github.com/vectordotdev/vector/discussions/17831",
        "createdAt": "2023-07-01T14:26:52Z",
        "updatedAt": "2023-07-05T16:18:52Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "loveyang2012"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17817,
        "title": "a question about  json_parser",
        "bodyText": "Discussed in #17801\n\nOriginally posted by loveyang2012 June 29, 2023\nI have gotten message like this:\n{\"log\":\"this is 456\",\"tag\":\"demotag1\",\"timestamp\":\"2023-06-29T22:08:58.758192934Z\"}\n{\"log\":\"this is 123\",\"tag\":\"demotag2\",\"timestamp\":\"2023-06-29T22:08:58.580667480Z\"}\nwhen json_parser transforms  is deprecated,how can I only get message like \"this is 456\" and  \"this is 123\",yes, only get json's value ,\nand  I want sent \"this is 456\" to kafka1 ,\"this is 123\" to kafka2.  may i get a  example,please",
        "url": "https://github.com/vectordotdev/vector/discussions/17817",
        "createdAt": "2023-06-30T00:38:22Z",
        "updatedAt": "2023-09-08T15:31:56Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "loveyang2012"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17561,
        "title": "Elasticsearch bulk update mode?",
        "bodyText": "I was wondering why Vector does not support update action for bulk API? Documentation only mentions that index and create actions are supported, but could not find anything on why update action is not. Only thing that comes to mind is that document ID must be present for update action to work, but that's something we can ensure to have in VRL.\nHaving update bulk action would be very helpful for our use case where we append data to Elasticsearch document.",
        "url": "https://github.com/vectordotdev/vector/discussions/17561",
        "createdAt": "2023-06-01T07:25:01Z",
        "updatedAt": "2023-07-04T08:11:39Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "jlazic"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17813,
        "title": "vector ignores last line of the file",
        "bodyText": "File source doesn't create an event for the last line in the file.\nIn my log files last log line doesn't have a new line at the end, but I still would expect vector to generate an event for this line because it reaches an EOF.\nMy vector source conf:\n[sources.log_source]\n  include = [\"/log/clusterLogs/**/*.log\"]\n  type = \"file\"\n  read_from = \"beginning\"\n  remove_after_secs = 10\n  ignore_checkpoints = true\n  [sources.log_source.fingerprint]\n    strategy = \"device_and_inode\"\n\nAm I missing some configuration that will create an event for last line as well?",
        "url": "https://github.com/vectordotdev/vector/discussions/17813",
        "createdAt": "2023-06-30T11:19:55Z",
        "updatedAt": "2023-07-03T08:12:48Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "akunafin"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17801,
        "title": "a question about  json_parser",
        "bodyText": "I have gotten message like this:\n{\"log\":\"this is 456\",\"tag\":\"demotag1\",\"timestamp\":\"2023-06-29T22:08:58.758192934Z\"}\n{\"log\":\"this is 123\",\"tag\":\"demotag2\",\"timestamp\":\"2023-06-29T22:08:58.580667480Z\"}\nwhen json_parser of transforms  is deprecated,how can I only get message like \"this is 456\" and  \"this is 123\",yes, only get json's value ,\nand  I want sent \"this is 456\" to kafka1 ,\"this is 123\" to kafka2.",
        "url": "https://github.com/vectordotdev/vector/discussions/17801",
        "createdAt": "2023-06-29T14:50:00Z",
        "updatedAt": "2023-06-30T21:28:14Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "loveyang2012"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 17780,
        "title": "Splunk HEC Source Endpoints (re /services/collector/s2s)",
        "bodyText": "Is there a reason why Vector Splunk HEC Source does not implement the /services/collector/s2s endpoint that the Splunk Universal Forwarder 'httpout' (in outputs.conf) uses.\nThe Vector docs DO state this.\nThis source exposes three HTTP endpoints at a configurable address that jointly implement the Splunk HEC API: /services/collector/event, /services/collector/raw, and /services/collector/health.\nBut this more-or-less precludes using the Splunk UF as a source for Vector HEC Source. I wonder why?\nThis can be demonstrated using examples from Splunk. (Vector Splunk HEC is listening on 8080)\nsplunk@:$ curl  -H \"Authorization: Splunk 400c4a05-47ef-45c6-aff5-9d83811f5a30\" http://x.x.x.x:8080/services/collector/event -d '{\"sourcetype\": \"my_sample_data\", \"event\": \"http auth ftw!\"}';echo\n{\"text\":\"Success\",\"code\":0}\nsplunk@$ curl  -H \"Authorization: Splunk 400c4a05-47ef-45c6-aff5-9d83811f5a30\" http://x.x.x.x:8080/services/collector/s2s -d '{\"sourcetype\": \"my_sample_data\", \"event\": \"http auth ftw!\"}';echo\nHTTP method not allowed\nI have suspicions as to why this 'disparity' might exist, but I am sure there must be a technical reason, or perhaps there has not been time to implement it yet?\nPS. Splunk reference for endpoins\nhttps://docs.splunk.com/Documentation/Splunk/9.0.3/Data/HECRESTendpoints",
        "url": "https://github.com/vectordotdev/vector/discussions/17780",
        "createdAt": "2023-06-28T10:16:27Z",
        "updatedAt": "2023-06-28T14:40:14Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "rdpsky"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17769,
        "title": "Read From File Fails",
        "bodyText": "It seems that vector fails to read from a file but I do not get any error message. Can someone help me figure this out?\nMy config file looks like this:\n[sources.file_logs]\ntype         = \"file\"\ndata_dir = \"/etc/vector/var/\"\ninclude      = [\"C:/src/vectorSample/data/*.log\"]\n[sinks.output]\ntype = \"console\"\ninputs = [ 'file_logs' ]\nencoding.codec = \"json\"\nWhen I run vector, it writes the following output and do not make any progress even if run for few hours:\n2023-06-27T12:33:27.720525Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=trace,rdkafka=info,buffers=info,lapin=info,kube=info\"\n2023-06-27T12:33:27.725498Z  INFO vector::app: Loading configs. paths=[\"/etc/vector/vector.toml\"]\n2023-06-27T12:33:27.741504Z  INFO vector::topology::running: Running healthchecks.\n2023-06-27T12:33:27.741622Z  INFO vector: Vector has started. debug=\"false\" version=\"0.30.0\" arch=\"x86_64\" revision=\"38c3f0b 2023-05-22 17:38:48.655488673\"\n2023-06-27T12:33:27.741635Z  INFO vector::app: API is disabled, enable by setting api.enabled to true and use commands like vector top.\n2023-06-27T12:33:27.741623Z  INFO vector::topology::builder: Healthcheck passed.\n2023-06-27T12:33:27.745869Z  INFO source{component_kind=\"source\" component_id=file_logs component_type=file component_name=file_logs}: vector::sources::file: Starting file server. include=[\"C:/src/vectorSample/data/*.log\"] exclude=[]\n2023-06-27T12:33:27.754018Z  INFO source{component_kind=\"source\" component_id=file_logs component_type=file component_name=file_logs}:file_server: file_source::checkpointer: Loaded checkpoint data.",
        "url": "https://github.com/vectordotdev/vector/discussions/17769",
        "createdAt": "2023-06-27T12:34:36Z",
        "updatedAt": "2023-06-27T21:27:14Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ligal2"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 7689,
        "title": "vector=>kafka=>vector=>prometheus",
        "bodyText": "Hello! I need some help with vector data model:\nI have a metrics from prometheus(metric format)\nthen vector takes it and send to kafka(in metric format)\nI read from kafka by another vector in log(text ) format. And i cant't convert thes messge(metric format) to  send to prometheus\nI have the message after kafka:\n{ \"metric\":{ \"gauge\":{ \"value\":23.0 }, \"kind\":\"absolute\", \"name\":\"ingress_nginx_detail_upstream_response_seconds_bucket\", \"tags\":{ \"app\":\"controller-main\", \"content_kind\":\"cache-headers-not-present\", \"controller\":\"main\", \"ingress\":\"akhq\", \"instance\":\"10.244.19.165:10354\", \"job\":\"nginx-ingress-controller\", \"le\":\"1.5\", \"location\":\"/\", \"namespace\":\"preprod\", \"node\":\"s-bd\", \"prometheus\":\"deckhouse\", \"scrape_source\":\"protobuf\", \"service\":\"akhq\", \"service_port\":\"http\", \"tier\":\"cluster\", \"vhost\":\"ak\" }, \"timestamp\":\"2021-05-31T09:31:49.523Z\" } }\nHow can i expose it by sink prometheus exporter",
        "url": "https://github.com/vectordotdev/vector/discussions/7689",
        "createdAt": "2021-05-31T16:16:30Z",
        "updatedAt": "2023-06-26T19:03:43Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "petrxpx"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 0
    },
    {
        "number": 17758,
        "title": "how to get source log when happend error",
        "bodyText": "vector error output:\n2023-06-26T08:15:42.283420Z ERROR sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}:request{request_id=1}: vector::sinks::util::retries: Non-retriable error; dropping the request. error=Server responded with an error: 400 Bad Request internal_log_rate_limit=true\n2023-06-26T08:15:42.300682Z ERROR sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}:request{request_id=1}: vector_common::internal_event::service: Service call failed. No retries or retries exhausted. error=Some(ServerError { code: 400 }) request_id=1 error_type=\"request_failed\" stage=\"sending\" internal_log_rate_limit=true\n2023-06-26T08:15:42.300744Z ERROR sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}:request{request_id=1}: vector_common::internal_event::component_events_dropped: Events dropped intentional=false count=2211 reason=\"Service call failed. No retries or retries exhausted.\" internal_log_rate_limit=true\nnow, i want get the source log before drop.  What should I do",
        "url": "https://github.com/vectordotdev/vector/discussions/17758",
        "createdAt": "2023-06-26T10:25:16Z",
        "updatedAt": "2023-06-27T02:14:35Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ColdV"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17568,
        "title": "Need help: timestamp",
        "bodyText": "Hi.\nAssumed that i have three vector pipeline in different locations in k8s clusters and my logs transferred between them\nvector01 --> vector02 --> vector03 --> Loki\nFinally my logs inserted in Loki data store and i received my logs in Loki with  the timestamp from vector (remove_timestamp: false)\nMy question is this timestamp comes from which vector ? i mean when did this timestamp created on which vector ? is it overwrite  with the newest time at each vector?",
        "url": "https://github.com/vectordotdev/vector/discussions/17568",
        "createdAt": "2023-06-01T18:44:20Z",
        "updatedAt": "2023-06-23T20:14:06Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17727,
        "title": "Multiline file source config for ldif",
        "bodyText": "Hi,\nas the title states I'm looking for an improvement of an existing configuration snippet to parse ldif logs.\nThe current snippet looks like that:\n[sources.source_ldif]\ntype = \"file\"\ninclude = [ \"/tmp/test.ldif\" ]\nignore_older_secs = 600\nread_from = \"beginning\"\nignore_checkpoints = true\n\n    [sources.source_ldif.multiline]\n    start_pattern = \"^#\"\n    mode = \"halt_with\"\n    condition_pattern = \"^#\"\n    timeout_ms = 1000\n\nThis works for most of the \"events\" but it fails for these which have an additional # in the event like this one:\n# modify 1687132883 dc=test,dc=test cn=admin,dc=test,dc=test IP=127.0.0.1:60908 conn=665091\n# realdn: cn=ldap-server,ou=system,dc=test,dc=test\ndn: uid=sys.test,ou=People,dc=test,dc=test\nchangetype: modify\nreplace: pwdLastSuccess\npwdLastSuccess: 20230619000123Z\n-\nreplace: entryCSN\nentryCSN: 20230619000123.158473Z#000000#014#000000\n-\nreplace: modifiersName\nmodifiersName: cn=test,dc=test,dc=test\n-\nreplace: modifyTimestamp\nmodifyTimestamp: 20230619000123Z\n-\n# end modify 1687132883\n\nWhat would I need to change in the config so that only the last # is used or in combination a regex like #\\wend or similar?\nThank you very much and BR,\nJan",
        "url": "https://github.com/vectordotdev/vector/discussions/17727",
        "createdAt": "2023-06-22T06:14:37Z",
        "updatedAt": "2023-06-23T05:57:48Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "janwaush"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17699,
        "title": "source.file multiline generates \\n\\t and How to handle characters like this?",
        "bodyText": "Hi.\nI`m now testing vector for log pipeline at kubernetes environment.\nI tested below config file and input java stack trace log to example log file.\n[sources.file]\n  type = \"file\"\n  include = [\"/Users/gimtaehun/Documents/test/vector/*.log\"]\n\n  [sources.file.multiline]\n    start_pattern = '^\\d+\\-\\d+\\-\\d+'\n    mode = \"halt_before\"\n    condition_pattern = '^\\d+\\-\\d+\\-\\d+'\n    timeout_ms = 1000\n\n[sinks.out]\n  type = \"console\"\n  inputs = [ \"file\" ]\n  target = \"stdout\"\n  encoding.codec = \"json\"\n\n\nThis is succesfully combine multi-line logs well into one log.\nbut multiline generates \\n\\t .\nSo i want to know this is normal ?\nIf so, How to handle this \\t\\n (new line , tab) ?\nmy final goal is to send these multiline logs to Loki.\n\nI just want to send these logs to Loki to make it easier for \ufffddevelopers to read.\nthanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/17699",
        "createdAt": "2023-06-16T03:58:10Z",
        "updatedAt": "2023-06-16T04:06:11Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "pingping95"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17630,
        "title": "Reading JSON events from AWS CloudWatch Logs",
        "bodyText": "When trying to read events from AWS CloudWatch Logs, a CloudWatch Logs subscription filter is required. This filter can send to Kinesis or Kinesis Firehose however the events are batched. Kinesis Firehose allows for the events to be written to objects in S3.\nWhen looking at the objects in S3, the batched events are on a single line without a delimiter. This means that 3 objects of A, B and C will be converted to a single line of {A}{B}{C}. This is causing considerable difficulty in transforming this back to individual events.\nIs there a method to handle JSON streams where multiple JSON objects are sent on the same line. Obviously if these were newline delimited this would be an easy problem to solve however AWS have made this difficult by not providing a customisable delimiter. Also, these are not a valid JSON array, so array conversion is not useful.\nAny help would be greatly appreciated as this has become a blocker to using Vector on a project I am working on.",
        "url": "https://github.com/vectordotdev/vector/discussions/17630",
        "createdAt": "2023-06-08T02:54:28Z",
        "updatedAt": "2023-06-14T23:53:22Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "mikelorant"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 17611,
        "title": "(s3 source) Select only certain objects to process from S3-SQS events?",
        "bodyText": "Asking just to confirm / validate before I go off and start writing another patch. :) In my ongoing work to convert pipelines, I have AWS Cloudtrail logs that are being posted into a shared logging bucket. The S3 source of the current product (which rhymes with tile-neat) allows one to specify a regex to match against the object name in the SQS event, and to ignore (e.g. leave in SQS) non-matching events. I don't see anything particularly like this right now.\nA downright horrid workaround would be to ingest all S3-SQS events with the current S3 source and then immediately repost the non-matching events through an SQS sink. But I have a feeling that would be an infinite loop?",
        "url": "https://github.com/vectordotdev/vector/discussions/17611",
        "createdAt": "2023-06-06T13:52:38Z",
        "updatedAt": "2023-06-14T19:25:19Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "sbalmos"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17691,
        "title": "Regex replace including right square bracket but is ignored?",
        "bodyText": "I tried to replace the name \"PC-TEST in string for several days without luck\nOrginal text:\n\"stations\\\":[[\\\"F4:B5:20:XX:YY:ZZ\\\",\\\"PC-TEST\\\"]]\nVia the following code, ending with a square bracket \"]\" (?P.*?\"])\nreplace(string!(.data.raw_data), r'(?P<a>ZZ(.*?\"){2})([a-zA-Z0-9_-]+)(?P<b>.*?\"\\])', \"$${a}ANONYMIZED$${b}\")\nPreferred text:\n\"stations\\\":[[\\\"f4:b5:20:XX:YY:ZZ\\\",\\\"ANONYMIZED\\\"]]\nWithout the closing ] bracket it will work but then it replaces some other text as well, how can i use a square bracket in this search?\nAlse tried to replace the square bracket in the replace function with [[:punct:]] which will work but also replaces other text character?",
        "url": "https://github.com/vectordotdev/vector/discussions/17691",
        "createdAt": "2023-06-14T10:05:21Z",
        "updatedAt": "2023-06-14T11:16:25Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "lzwaan"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17382,
        "title": "Vector reports fields in GCP service account credentials.json file are unknown",
        "bodyText": "I configured vector to ship logs to gcp_stackdriver(Google operations)\n  sinks:\n    google_stackdriver_sink:\n      type: \"gcp_stackdriver_logs\"\n      credentials_path: \"/path/to/credentials.json\"\n      project_id: \"<project-id>\"\n      inputs:\n        - logs.apache\n      log_id: \"<log_id>\"\n      resource:\n        type: global\n      batch:\n        max_size: 1048576\n        timeout_secs: 1\n\nI downloaded the credentials.json file from Google Cloud but as shown below Vector claims the fields in the credentials.json file are unknown.\n2023-05-11T11:14:29.844667Z ERROR vector::cli: Configuration error. error=unknown field type at line 12 column 3\nThe first standard field in credentials.json is \"type\": \"service_account\". If I put the line \"project_id\" before \"type\", vector reports \"project_id\" is unknown field.",
        "url": "https://github.com/vectordotdev/vector/discussions/17382",
        "createdAt": "2023-05-15T04:25:54Z",
        "updatedAt": "2023-06-13T17:42:19Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "danhul"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 6
        },
        "upvoteCount": 1
    },
    {
        "number": 17661,
        "title": "remap & Lua",
        "bodyText": "I want to use Lua in Transform/Remap. Is it possible?  And how?",
        "url": "https://github.com/vectordotdev/vector/discussions/17661",
        "createdAt": "2023-06-12T04:02:26Z",
        "updatedAt": "2023-06-12T11:12:30Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Lempossible"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17626,
        "title": "Question about extra_label_selector",
        "bodyText": "Hi,\nnot sure if this was already answered. I am new to Vector configuration and I wonder what is the correct way to specify multiple labels in extra_label_selector. For example:\nextra_label_selector = \"app=production,app=staging,app=scheduler-production,app=other-app-production,app=other-app-staging\" does not seem to work, I guess because of duplicated key app?\nThanks,\nLeon",
        "url": "https://github.com/vectordotdev/vector/discussions/17626",
        "createdAt": "2023-06-07T20:56:03Z",
        "updatedAt": "2023-06-08T18:27:12Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "lkananowicz"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 17633,
        "title": "Parsing audit.log*",
        "bodyText": "Hi,\nI want to parse /var/log/audit/audit.log* files but cannot find any predefined parser or examples on how to do this.\nI have created the following transformer, but would like to know if there is a better way to do it?\n# Parse .message into tokens. Regex accepts values with or without quotations\ntokens = parse_regex_all!(.message, r'(?P<key>\\w*)=(?P<value>((\\'[^\\']*\\')|\\\"[^\\\"]*\\\"|(\\S*)))')\n\n# Parse tokens and set each token as a key/value in .message\nmap_values(tokens) -> |token| {\n    if token.key == \"msg\" {\n        if starts_with(to_string(token.value), \"audit(\") {\n            audit = parse_regex!(token.value, r'audit\\((?P<ts>[^:]+):(?P<id>\\d+)\\):')\n            .timestamp = to_timestamp(to_float!(audit.ts)) ?? .timestamp\n            .msg = set!(.msg, [\"timestamp\"], audit.ts)\n            .msg = set!(.msg, [\"id\"], audit.id)\n        } else {\n            # Parse value of \"msg\" field into tokens\n            msg_tokens = parse_regex_all!(token.value, r'(?P<key>\\w*)=(?P<value>((\\'[^\\']*\\')|\\\"[^\\\"]*\\\"|(\\S*)))')\n            map_values(msg_tokens) -> |msg_token| {\n                .msg = set!(.msg, [msg_token.key], msg_token.value)\n            }\n        }\n    } else {\n        .= set!(., [token.key], token.value)\n    }\n}",
        "url": "https://github.com/vectordotdev/vector/discussions/17633",
        "createdAt": "2023-06-08T06:44:13Z",
        "updatedAt": "2023-06-08T13:28:20Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "mbneimann-at-work"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17636,
        "title": "error logs help",
        "bodyText": "I have the log full of these errors, any idea how to fix them?\nSome of them could be because not all the log entries are similar and can be labeled but the others?\n\nMon-Vector   | 2023-06-08T08:17:23.601693Z ERROR sink{component_kind=\"sink\" component_id=loki_container component_type=loki component_name=loki_container}: vector::internal_events::template: Failed to render template for \"label_value \"{{label.\"com.docker.compose.project\" }}\" with label_key \"compose_proyect\"\". error=Missing fields on event: [\"label.\\\"com.docker.compose.project\\\"\"] error_type=\"template_failed\" stage=\"processing\" internal_log_rate_limit=true\nMon-Vector   | 2023-06-08T08:17:24.070387Z ERROR sink{component_kind=\"sink\" component_id=loki_container component_type=loki component_name=loki_container}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{label.\"net.unraid.docker.managed\" }}\" with label_key \"docker_managed\"\".] is being suppressed to avoid flooding.\nMon-Vector   | 2023-06-08T08:17:24.363150Z ERROR source{component_kind=\"source\" component_id=container_logs component_type=docker_logs component_name=container_logs}: vector::internal_events::docker_logs: Internal log [Received out of order log message.] has been suppressed 42 times.\nMon-Vector   | 2023-06-08T08:17:24.363165Z ERROR source{component_kind=\"source\" component_id=container_logs component_type=docker_logs component_name=container_logs}: vector::internal_events::docker_logs: Received out of order log message. error_type=\"condition_failed\" stage=\"receiving\" container_id=\"9dbe1251db2b00be94da34d3e5cadc2242fe59b2f3fb1283f66e072ebe4891a6\" timestamp=\"2023-06-08T08:17:24.153357296Z\" internal_log_rate_limit=true\nMon-Vector   | 2023-06-08T08:17:24.363192Z ERROR source{component_kind=\"source\" component_id=container_logs component_type=docker_logs component_name=container_logs}: vector::internal_events::docker_logs: Internal log [Received out of order log message.] is being suppressed to avoid flooding.\nMon-Vector   | 2023-06-08T08:17:25.463817Z ERROR sink{component_kind=\"sink\" component_id=loki_syslog component_type=loki component_name=loki_syslog}: vector::sinks::util::retries: Internal log [Non-retriable error; dropping the request.] is being suppressed to avoid flooding.\nMon-Vector   | 2023-06-08T08:17:25.463836Z ERROR sink{component_kind=\"sink\" component_id=loki_syslog component_type=loki component_name=loki_syslog}: vector_common::internal_event::service: Internal log [Service call failed. No retries or retries exhausted.] is being suppressed to avoid flooding.\nMon-Vector   | 2023-06-08T08:17:25.463845Z ERROR sink{component_kind=\"sink\" component_id=loki_syslog component_type=loki component_name=loki_syslog}: vector_common::internal_event::component_events_dropped: Internal log [Events dropped] is being suppressed to avoid flooding.\nMon-Vector   | 2023-06-08T08:17:33.602248Z ERROR sink{component_kind=\"sink\" component_id=loki_container component_type=loki component_name=loki_container}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{label.\"net.unraid.docker.managed\" }}\" with label_key \"docker_managed\"\".] has been suppressed 54 times.\nMon-Vector   | 2023-06-08T08:17:33.602264Z ERROR sink{component_kind=\"sink\" component_id=loki_container component_type=loki component_name=loki_container}: vector::internal_events::template: Failed to render template for \"label_value \"{{label.\"com.docker.compose.project\" }}\" with label_key \"compose_proyect\"\". error=Missing fields on event: [\"label.\\\"com.docker.compose.project\\\"\"] error_type=\"template_failed\" stage=\"processing\" internal_log_rate_limit=true\nMon-Vector   | 2023-06-08T08:17:35.599995Z ERROR sink{component_kind=\"sink\" component_id=loki_container component_type=loki component_name=loki_container}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{label.\"net.unraid.docker.managed\" }}\" with label_key \"docker_managed\"\".] is being suppressed to avoid flooding.\nMon-Vector   | 2023-06-08T08:17:36.180933Z ERROR source{component_kind=\"source\" component_id=container_logs component_type=docker_logs component_name=container_logs}: vector::internal_events::docker_logs: Internal log [Received out of order log message.] has been suppressed 42 times.\nMon-Vector   | 2023-06-08T08:17:36.180947Z ERROR source{component_kind=\"source\" component_id=container_logs component_type=docker_logs component_name=container_logs}: vector::internal_events::docker_logs: Received out of order log message. error_type=\"condition_failed\" stage=\"receiving\" container_id=\"9dbe1251db2b00be94da34d3e5cadc2242fe59b2f3fb1283f66e072ebe4891a6\" timestamp=\"2023-06-08T08:17:35.599602039Z\" internal_log_rate_limit=true\nMon-Vector   | 2023-06-08T08:17:36.180985Z ERROR source{component_kind=\"source\" component_id=container_logs component_type=docker_logs component_name=container_logs}: vector::internal_events::docker_logs: Internal log [Received out of order log message.] is being suppressed to avoid flooding.\nMon-Vector   | 2023-06-08T08:17:44.176506Z ERROR sink{component_kind=\"sink\" component_id=loki_syslog component_type=loki component_name=loki_syslog}: vector::sinks::util::retries: Internal log [Non-retriable error; dropping the request.] has been suppressed 1 times.\nMon-Vector   | 2023-06-08T08:17:44.176517Z ERROR sink{component_kind=\"sink\" component_id=loki_syslog component_type=loki component_name=loki_syslog}: vector::sinks::util::retries: Non-retriable error; dropping the request. error=Server responded with an error: 400 Bad Request internal_log_rate_limit=true\nMon-Vector   | 2023-06-08T08:17:44.176527Z ERROR sink{component_kind=\"sink\" component_id=loki_syslog component_type=loki component_name=loki_syslog}: vector_common::internal_event::service: Internal log [Service call failed. No retries or retries exhausted.] has been suppressed 1 times.\nMon-Vector   | 2023-06-08T08:17:44.176528Z ERROR sink{component_kind=\"sink\" component_id=loki_syslog component_type=loki component_name=loki_syslog}: vector_common::internal_event::service: Service call failed. No retries or retries exhausted. error=Some(ServerError { code: 400 }) request_id=111 error_type=\"request_failed\" stage=\"sending\" internal_log_rate_limit=true\nMon-Vector   | 2023-06-08T08:17:44.176539Z ERROR sink{component_kind=\"sink\" component_id=loki_syslog component_type=loki component_name=loki_syslog}: vector_common::internal_event::component_events_dropped: Internal log [Events dropped] has been suppressed 1 times.\nMon-Vector   | 2023-06-08T08:17:44.176541Z ERROR sink{component_kind=\"sink\" component_id=loki_syslog component_type=loki component_name=loki_syslog}: vector_common::internal_event::component_events_dropped: Events dropped intentional=false count=2 reason=\"Service call failed. No retries or retries exhausted.\" internal_log_rate_limit=true\nMon-Vector   | 2023-06-08T08:17:44.353417Z ERROR sink{component_kind=\"sink\" component_id=loki_container component_type=loki component_name=loki_container}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{label.\"net.unraid.docker.managed\" }}\" with label_key \"docker_managed\"\".] has been suppressed 47 times.\nMon-Vector   | 2023-06-08T08:17:44.353432Z ERROR sink{component_kind=\"sink\" component_id=loki_container component_type=loki component_name=loki_container}: vector::internal_events::template: Failed to render template for \"label_value \"{{label.\"com.docker.compose.project\" }}\" with label_key \"compose_proyect\"\". error=Missing fields on event: [\"label.\\\"com.docker.compose.project\\\"\"] error_type=\"template_failed\" stage=\"processing\" internal_log_rate_limit=true\nMon-Vector   | 2023-06-08T08:17:44.356514Z ERROR sink{component_kind=\"sink\" component_id=loki_container component_type=loki component_name=loki_container}: vector::internal_events::template: Internal log [Failed to render template for \"label_value \"{{label.\"net.unraid.docker.managed\" }}\" with label_key \"docker_managed\"\".] is being suppressed to avoid flooding.\nMon-Vector   | 2023-06-08T08:17:47.566309Z ERROR source{component_kind=\"source\" component_id=container_logs component_type=docker_logs component_name=container_logs}: vector::internal_events::docker_logs: Internal log [Received out of order log message.] has been suppressed 20 times.\nMon-Vector   | 2023-06-08T08:17:47.566324Z ERROR source{component_kind=\"source\" component_id=container_logs component_type=docker_logs component_name=container_logs}: vector::internal_events::docker_logs: Received out of order log message. error_type=\"condition_failed\" stage=\"receiving\" container_id=\"9dbe1251db2b00be94da34d3e5cadc2242fe59b2f3fb1283f66e072ebe4891a6\" timestamp=\"2023-06-08T08:17:46.969217977Z\" internal_log_rate_limit=true\nMon-Vector   | 2023-06-08T08:17:47.566350Z ERROR source{component_kind=\"source\" component_id=container_logs component_type=docker_logs component_name=container_logs}: vector::internal_events::docker_logs: Internal log [Received out of order log message.] is being suppressed to avoid flooding.\nMon-Vector   | 2023-06-08T08:17:48.283088Z ERROR sink{component_kind=\"sink\" component_id=loki_syslog component_type=loki component_name=loki_syslog}: vector::sinks::util::retries: Internal log [Non-retriable error; dropping the request.] is being suppressed to avoid flooding.\nMon-Vector   | 2023-06-08T08:17:48.283111Z ERROR sink{component_kind=\"sink\" component_id=loki_syslog component_type=loki component_name=loki_syslog}: vector_common::internal_event::service: Internal log [Service call failed. No retries or retries exhausted.] is being suppressed to avoid flooding.\nMon-Vector   | 2023-06-08T08:17:48.283123Z ERROR sink{component_kind=\"sink\" component_id=loki_syslog component_type=loki component_name=loki_syslog}: vector_common::internal_event::component_events_dropped: Internal log [Events dropped] is being suppressed to avoid flooding.\n\n[sources.docker_logs]\n    type = \"file\"\n    #ignore_older_secs = 600\n    include = [\"/var/log/docker.log\"]\n    read_from = \"beginning\"\n[sinks.loki_docker]\n    type = \"loki\"\n    inputs = [\"docker_logs\"]\n    endpoint = \"http://loki:3100\"\n    encoding.codec = \"json\"\n    buffer.type = \"memory\"\n    buffer.max_events = 50000\n    buffer.when_full = \"block\"\n    batch.max_events = 10000\n    [sinks.loki_docker.labels]\n        hostname =\"Unraid\"\n        service =\"docker\"\n\n\n\n[sources.libvirt]\n    type = \"file\"\n    #ignore_older_secs = 600\n    include = [\"/var/log/libvirt/libvirtd.log\"]\n    read_from = \"beginning\"\n[sinks.loki_libvirt]\n    type = \"loki\"\n    inputs = [\"libvirt\"]\n    endpoint = \"http://loki:3100\"\n    encoding.codec = \"json\"\n    buffer.type = \"memory\"\n    buffer.max_events = 50000\n    buffer.when_full = \"block\"\n    batch.max_events = 10000\n    [sinks.loki_libvirt.labels]\n        hostname =\"Unraid\"\n        service =\"libvirt\"\n\n\n\n[sources.syslog_file]\n    type = \"file\"\n    include = [\"/var/log/syslog\"]\n[transforms.parse_syslog]\n    type = \"remap\"\n    inputs = [\"syslog_file\"]\n    source = '''\n    . = parse_syslog!(string!(.message))\n    '''\n[sinks.loki_syslog]\n    type = \"loki\"\n    inputs = [\"parse_syslog\"]\n    endpoint = \"http://loki:3100\"\n    encoding.codec = \"json\"\n    buffer.type = \"memory\"\n    buffer.max_events = 50000\n    buffer.when_full = \"block\"\n    batch.max_events = 10000\n    [sinks.loki_syslog.labels]\n        appname = \"{{ appname }}\"\n        hostname = \"{{ hostname }}\"\n        service =\"syslog\"\n\n\n\n[sources.container_logs]\n    type = \"docker_logs\"\n    docker_host = \"/var/run/docker.sock\"\n[sinks.loki_container]\n    type = \"loki\"\n    inputs = [\"container_logs\"]\n    endpoint = \"http://loki:3100\"\n    encoding.codec = \"json\"\n    buffer.type = \"memory\"\n    buffer.max_events = 50000\n    buffer.when_full = \"block\"\n    batch.max_events = 10000\n    [sinks.loki_container.labels]\n        container = \"{{ container_name }}\"\n        hostname =\"Unraid\"\n        service = \"containers\"\n        stream = \"{{ stream }}\"\n        docker_managed = \"{{label.\\\"net.unraid.docker.managed\\\" }}\"\n        compose_proyect = \"{{label.\\\"com.docker.compose.project\\\" }}\"",
        "url": "https://github.com/vectordotdev/vector/discussions/17636",
        "createdAt": "2023-06-08T08:19:52Z",
        "updatedAt": "2023-06-08T08:19:53Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "lordraiden"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 2
    },
    {
        "number": 17635,
        "title": "Should the Host Metrics source scrape all network interfaces on the host?",
        "bodyText": "I'm using the host_metrics sink with the collectors: network in a k3s cluster and the only metrics I see are for tunl0, eth0 and lo devices.  The hosts network has several more NICs. Is there something wrong with my setup, or is it monitoring only within the node/pod?\nExample metric output:\n{\n  \"name\": \"network_transmit_errs_total\",\n  \"namespace\": \"host\",\n  \"tags\": {\n    \"collector\": \"network\",\n    \"device\": \"tunl0\",\n    \"host\": \"vector-zx4pc\"\n  },\n  \"timestamp\": \"2023-06-08T07:35:27.245175986Z\",\n  \"kind\": \"absolute\",\n  \"counter\": {\n    \"value\": 0\n  }\n}\n{\n  \"name\": \"network_receive_bytes_total\",\n  \"namespace\": \"host\",\n  \"tags\": {\n    \"collector\": \"network\",\n    \"device\": \"eth0\",\n    \"host\": \"vector-zx4pc\"\n  },\n  \"timestamp\": \"2023-06-08T07:35:27.245175986Z\",\n  \"kind\": \"absolute\",\n  \"counter\": {\n    \"value\": 584086\n  }\n}",
        "url": "https://github.com/vectordotdev/vector/discussions/17635",
        "createdAt": "2023-06-08T07:40:20Z",
        "updatedAt": "2023-06-08T07:40:21Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "hickersonj"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 17620,
        "title": "file source, do not decompress",
        "bodyText": "Hi Vector team,\nWe have an use case where we want to push compressed files directly to S3.\nWe are seeing that Vector is opening those compressed files (as described here). In our case, we don't need to parse the logs inside those files, we just want to store them for eventual offline analysis.\nWould it be possible for Vector not to open those files and just push them to S3 directly?",
        "url": "https://github.com/vectordotdev/vector/discussions/17620",
        "createdAt": "2023-06-07T15:34:49Z",
        "updatedAt": "2023-06-14T09:44:53Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "luisloros"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17612,
        "title": "Node name insted of vector pod name",
        "bodyText": "Good day\ni used  vector to send the kubernets audit-logs to kafka\neverything works fine.\nbut i didnt find where i can change the vector pod name to the node name of kubernetes\nit looks like this\n\nmy vector configs:\ntype: file\n        include:\n          - /var/lib/rancher/rke2/server/logs/audit.log\n        host_key: host\n        read_from: end",
        "url": "https://github.com/vectordotdev/vector/discussions/17612",
        "createdAt": "2023-06-06T14:29:32Z",
        "updatedAt": "2023-06-06T16:10:59Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "therus000"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 17515,
        "title": "The way Vector discover changes in file input type",
        "bodyText": "Hi! I'm trying to replace big filebeat deployment in company, and started testing Vector recently. The question I have - is there any documentation, or maybe someone know about the way how Vector find changes in discovered files? I'm trying to fine-tune instance, but the time delivery is quite a huge - 99 percentile is about 2-6sec. Is there any way to tune the time discovery? Maybe decrease file polling interval? Or maybe frequency of sending events in sink?\nVector version: 0.30.0\nOS details:\n> uname -r\n2.6.32-5-amd64\n\n> cat /etc/debian-release\nsqueeze\nExample of config:\nsources:\n  my_events_src:\n    type: file\n    include:\n      - /var/lib/state/SVC/*.log*\n    exclude:\n      - '*.gz$'\n    file_key: ''\n    host_key: ''\n    glob_minimum_cooldown_ms: 100\n    ignore_not_found: false\n    line_delimiter: \"\\n\"\n    max_line_bytes: 1048576\n    max_read_bytes: 1048576\n    multi_line_timeout: 1000\n    oldest_first: false\n    read_from: beginning\n    fingerprint:\n      ignored_header_bytes: 0\n      lines: 1\n      strategy: checksum\ntransforms:\n  my_events_tfm:\n    type: remap\n    inputs:\n      - my_events_src\n    drop_on_error: true\n    source: |\n      . = parse_json!(.message)\n      .@Event.ReadTime = now()\nsinks:\n  my_events_snk:\n    inputs:\n      - my_events_tfm\n    address: \"logstash_tcp_input:1221\"\n    mode: tcp\n    type: socket\n    encoding:\n      codec: json\n    healthcheck:\n      enabled: true\n    buffer:\n      type: disk\n      when_full: block\n      max_size: 536870912",
        "url": "https://github.com/vectordotdev/vector/discussions/17515",
        "createdAt": "2023-05-26T16:57:31Z",
        "updatedAt": "2023-09-07T02:23:22Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "gl1ridae"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17520,
        "title": "More than one file has the same fingerprint.",
        "bodyText": "Im seeing a ton of these for our logs...and i would like to understand it a bit better.\nMy understanding from reading the docs is that the default fingerprint strategy is checksum...and that this means a portion of the file is  used for that purpose.\nDoes this mean the file will be skipped?\nIs there a way to avoid that not being so, since in fact there is content in these files we check to insure every run was successful?\nfile_server: file_source::file_server: More than one file has the same fingerprint.\nSwitching to watch most recently modified file",
        "url": "https://github.com/vectordotdev/vector/discussions/17520",
        "createdAt": "2023-05-27T01:32:56Z",
        "updatedAt": "2023-06-05T21:25:28Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "dss010101"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17584,
        "title": "Is it possible to obtain kubernetes (node/pod level) metrics and forward them to a sink?",
        "bodyText": "I see that vector can obtain host metrics (CPU/Mem/Disk), but is there anyway to obtain node level metrics and pod level metrics as a source and forward them via a sink? I looked at internal metrics, but that doesn't seem to be what I am looking for. Thanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/17584",
        "createdAt": "2023-06-02T18:37:31Z",
        "updatedAt": "2023-06-02T18:37:31Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "hickersonj"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 17519,
        "title": "is it possible to parse the folder paths and add as labels",
        "bodyText": "i have a folder structure that looks like this\n/app/{batch_id}/{partition_id}/logs\ni'm using the following to read filter for these logs:\ninclude = [\"/app/*/*.log\"]\nso for example:\n/app/1000/0001/logs/run.log\n/app/1001/0001/logs/run.log\n/app/1001/0002/logs/run.log\n\nI can read and parse the logs fine.  But as you can see above the log names are all the same.  What is different is the sub paths that has folders serving as key identifiers.  i'd like to parse those out into labels associated with every row parsed for the respective log.\nAny thoughts on how i can do this?",
        "url": "https://github.com/vectordotdev/vector/discussions/17519",
        "createdAt": "2023-05-26T22:24:39Z",
        "updatedAt": "2023-06-02T10:22:46Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "dss010101"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 17550,
        "title": "Increase expiration period for \"log_to_metric\"/\"prometheus_exporter\"",
        "bodyText": "Hello!\nI am trying to count incoming log messages based on specific fields. For example, if I'll receive the following entry:\n{\"message\": \"blabla\", \"facility\": \"some-service\", \"level\": \"error\"}\nVector will transform it into the following Prometheus counter metric:\nvector_sent_log_entries_count{facility=\"some-service\",level=\"error\"} 1\nVersion\n0.30.0 running as Deployment in k8s\nProblem\nI have no expire_metrics global parameter configured. Despite this, when no new log entries are received for over 5 minutes, metrics expire. Can I somehow increase the expiration period?\n\nRelevant Vector Config\n    [transforms.count_logs]\n      type = \"log_to_metric\"\n      inputs = [ \"filter_logs\" ]\n\n      [[transforms.count_logs.metrics]]\n        type = \"counter\"\n        field = \"facility\"\n        name = \"sent_log_entries_count\"\n        namespace = \"vector\"\n        increment_by_value = false\n\n        tags.kind = \"logger\"\n        tags.env = \"test\"\n        tags.region = \"eu-central-1\"\n        tags.region_domain = \"eu-central-1...\"\n        tags.facility = \"{{ facility }}\"\n        tags.level = \"{{ level }}\"\n\n    [sinks.prom_metrics]\n      type = \"prometheus_exporter\"\n      inputs = [\"count_logs\"]\n      address = \"${POD_IP}:12201\"\n      default_namespace = \"vector\"",
        "url": "https://github.com/vectordotdev/vector/discussions/17550",
        "createdAt": "2023-05-31T13:29:37Z",
        "updatedAt": "2023-06-01T13:52:24Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "arch-xtof"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 17558,
        "title": "Getting Null Values, instead of parsed values",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nUse Case: We are designing a pipeline to ingest a json object that will output/post a multiline string to an api.\nWe are able to create the string and add it successfully to a json object however when we try to replace the json document entirely with the string the variables are returning null values.\nVRL Playground link: Link for Playground code\nVector transformation config:  refer config section.\nconsole output\n# TYPE  gauge\n# HELP   gauge\n{sloId=\"\",status=\"APPROVED\"} \n# EOF\n\nExpected output:\n# TYPE availability_slo_total_count gauge\n# HELP availability_slo_total_count availability slo total count gauge\navailability_slo_total_count{sloId=\"033df0fd9c4040b98d94a5ec325fd3e9\",status=\"APPROVED\"} 76.8\n# EOF\n\n\nConfiguration\ntransforms:\n  parse_dd_api_data:\n    type: remap\n    inputs: \n      - dd_api_data\n    source: |-\n      .metricName, err= to_string(.metricname)\n      if err != null {\n        log(\"Error on metricname: \" + err, level: \"error\")\n      } \n      .sloID, err= to_string(.sloid)\n      if err != null {\n        log(\"Error on serviceid: \" + err, level: \"error\")\n      } \n      .status=\"APPROVED\"\n      .metricV= .series[0].pointlist[0]\n      .metricValue, err=to_string(.metricV[1])\n      if err != null {\n        log(\"Error on pointlist value: \" + err, level: \"error\")\n      } \n      .=\"# TYPE \"+.metricName+\" gauge\n      # HELP \"+.metricName+\" \"+replace(.metricName, \"_\", \" \")+\" gauge\n      \"+.metricName+\"{sloId=\\\"\"+.sloID+\"\\\",status=\\\"\"+.status+\"\\\"} \"+.metricValue+\"\n      # EOF\n     \"\nsinks:\n  console_output:\n    type: console\n    inputs:\n      - parse_dd_api_data\n    encoding:\n      codec: text\n\nVersion\nvector-0.30.0-1.x86_64.rpm\nDebug Output\nNo response\nExample Data\n{\n\t\"status\": \"ok\",\n\t\"res_type\": \"time_series\",\n\t\"resp_version\": 1,\n\t\"query\": \"sum:Pipeline_Event_Count{service.environment:test}.as_count()\",\n\t\"from_date\": 1683064680000,\n\t\"to_date\": 1683064980000,\n\t\"series\": [\n\t\t{\n\t\t\t\"unit\": null,\n\t\t\t\"query_index\": 0,\n\t\t\t\"aggr\": \"sum\",\n\t\t\t\"metric\": \"Pipeline_Event_Count\",\n\t\t\t\"tag_set\": [],\n\t\t\t\"expression\": \"sum:Pipeline_Event_Count{service.environment:test}.as_count()\",\n\t\t\t\"scope\": \"service.environment:test\",\n\t\t\t\"interval\": 2,\n\t\t\t\"length\": 1,\n\t\t\t\"start\": 1683064700000,\n\t\t\t\"end\": 1683064701000,\n\t\t\t\"pointlist\": [\n\t\t\t\t[\n\t\t\t\t\t1683064700000,\n\t\t\t\t\t76.8\n\t\t\t\t]\n\t\t\t],\n\t\t\t\"display_name\": \"Pipeline_Event_Count\",\n\t\t\t\"attributes\": {}\n\t\t}\n\t],\n\t\"values\": [],\n\t\"times\": [],\n\t\"message\": \"\",\n\t\"group_by\": [],\n\t\"sloid\": \"033df0fd9c4040b98d94a5ec325fd3e9\",\n\t\"metricname\": \"availability_slo_total_count\"\n}\n\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/17558",
        "createdAt": "2023-05-31T19:30:49Z",
        "updatedAt": "2023-05-31T20:30:42Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "MV45716"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17530,
        "title": "converting string to timestamp...",
        "bodyText": "Im using the following to parse log times: %{TIMESTAMP_ISO8601:log_time}\nThe result looks like this: \"2023-05-16 17:50:49,565\" \nI have two questions here...i am tryng to convert this value to timestamp to compare it with the ingestion time (.timestamp).  The following code is not working for me...\n    if .log_time != null{\n        .log_time = replace!(.log_time, \",\", \".\")\n        .log_time, err = to_timestamp(.log_time)\n        if err != null {\n            log(\"ERROR converting log_time to time: \" + err, level: \"error\")\n        } else {\n            .timestamp = .log_time\n        }\n    }\n\n\nException:\nfunction call error for \"to_timestamp\" at (655:678): No matching timestamp format found for \"2023-05-16 17:50:49.565\" internal_log_rate_secs=1 vrl_position=716\nHow would i convert this string to timestamp?\nAlso, how can i do a diff on two timestamps? i would like to assign .timestamp to .log_time if the difference is more than say a a few minutes.",
        "url": "https://github.com/vectordotdev/vector/discussions/17530",
        "createdAt": "2023-05-30T03:43:51Z",
        "updatedAt": "2023-05-31T14:09:25Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "dss010101"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 17552,
        "title": "how to unespace unicode using VRL?",
        "bodyText": "A note for the community\nNo response\nUse Cases\nhow to unescape this string of unicode? I cannot find the document\nusing VRL\nthank you !\n{\\x22name\\x22:\\x22aa\\x22,\\x22password\\x22:\\x22bb\\x22}\nAttempted Solutions\nNo response\nProposal\nNo response\nReferences\nNo response\nVersion\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/17552",
        "createdAt": "2023-05-31T07:41:59Z",
        "updatedAt": "2023-05-31T14:48:27Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "humanhuang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 17545,
        "title": "there's no journalctl in alpine/distroless image variant",
        "bodyText": "can we include that in the alpine/ distroless variant",
        "url": "https://github.com/vectordotdev/vector/discussions/17545",
        "createdAt": "2023-05-31T06:45:28Z",
        "updatedAt": "2023-05-31T06:45:28Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tuananh"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 17528,
        "title": "How metrics behave with Vector with multiple pods",
        "bodyText": "Hi All,\nI am using vector helm chart to receive http json events (source), apply logs to metrics transformations and expose the metrics in prometheus endpoint (sink).\nI\u00b4ve been testing it with 1 replica (default) and it seems to work properly, but i am quite confused on how vector balances events if i have replica set to 2 if i need to scale up the service.\n# replicas -- Specify the number of Pods to create. Valid for the \"Aggregator\" and \"Stateless-Aggregator\" roles.\nreplicas: 2\n\nCan someone provide some guidance on this?\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/17528",
        "createdAt": "2023-05-29T16:31:52Z",
        "updatedAt": "2023-05-30T17:14:31Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bmgante"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17539,
        "title": "Need help: usage error in integrating vault with vector",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nFollowing the instruction as stated in these:\nhttps://vector.dev/docs/reference/configuration/global-options/#secret\nhttps://github.com/vectordotdev/vector/blob/aa45cefabc165cce5cc2135ad8b23705b524b769/website/content/en/highlights/2022-07-07-secrets-management.md\nThe intent is to give Vector access: vault > specific namespace > folder > <secrets> \nNot sure where and how the secret.exec or secret.exec.command to be used.\nFor example: the secrets are stored in the VAULT= https://mystg.vault.net, inside namespace dev, folder mysecrets , mykey\nlooks like we are doing an incorrect usage in order to fetch the secrets from Vault. Any guidance and correction are much appreciated on how to set up the configuration file such that Vector can access the Vault.\nConfiguration\nNo response\nVersion\n0.29.1\nDebug Output\nNo response\nExample Data\nNo response\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/17539",
        "createdAt": "2023-05-30T05:07:00Z",
        "updatedAt": "2023-12-13T19:08:55Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "puppet-py"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17397,
        "title": "post parsing already parsed fields...",
        "bodyText": "i've converted my toml to use the parse_grok function.  i have two questions:\n\n\nthe 'msg' field i parse out may contain other patterns i want to parse out, such as 'app_id=app_1234'.  Can i post parse msg and add these fields afterwards, rather than have to keep adding patterns for each?  It seems it would be cleaner to post parses for optional fields that may or may not be present in the parsed 'msg' field?\n\n\nThis all now parses and ends up in loki/grafana as json, but the fields do not end up as labels i can use in queries. i still am limited to 'app' and 'env' only (labels added by the 'sink.to_indexer' section in my toml.  So im still forced to do a text search to find certain things i'm looking for.\n\n\ntoml\n[sources.app_logs]\ntype = \"file\"\n#ignore_older_secs = 600\ninclude = [\"/var/logs/*.log\"]\nread_from = \"beginning\"\n\n[sources.app_logs.multiline]\nstart_pattern = \"^[^\\\\s]\"\nmode = \"continue_through\"\ncondition_pattern = \"^[\\\\s]+from\"\ntimeout_ms = 1000\n\n[transforms.log_parser]\ntype   = \"remap\"\ninputs = [\"app_logs\"]\nsource = '''\n    . |= parse_groks!(\n            .message, \n            patterns:[\n                \"\\\\[%{TIMESTAMP_ISO8601:log_time}\\\\]\\\\[%{INT:process}:%{INT:thread}\\\\]\\\\[%{WORD:severity}\\\\]\\\\[%{WORD:module}.%{WORD:func}\\\\] app_id:%{NOTSPACE:app_id} %{GREEDYDATA:msg}\",\n                \"\\\\[%{TIMESTAMP_ISO8601:log_time}\\\\]\\\\[%{INT:process}:%{INT:thread}\\\\]\\\\[%{WORD:severity}\\\\]\\\\[%{WORD:module}.%{WORD:func}\\\\] %{GREEDYDATA:msg}\",\n                \"\\\\[%{TIMESTAMP_ISO8601:log_time}\\\\]\\\\[%{INT:process}:%{INT:thread}\\\\]%{WORD:severity}:%{WORD:module}:%{GREEDYDATA:msg}\"\n            ]\n    )\n\n'''\n\n[sinks.to_indexer]\ntype = \"loki\"\ninputs = [\"log_parser\"]\nendpoint = \"http://loki:3100\"\nencoding.codec = \"json\"\nlabels = {app=\"app_logs\", env=\"dev\"}\nhealthcheck = false\n\n'''\n\nexample logs with/and without app_id:\n[2023-05-15 11:52:29,401][2555:140431094425152][INFO][default.run] initalizing application\n[2023-05-15 11:52:29,401][2555:140431094425152][INFO][default.run] app_id:my_app_12345 initialized.",
        "url": "https://github.com/vectordotdev/vector/discussions/17397",
        "createdAt": "2023-05-15T17:43:06Z",
        "updatedAt": "2023-05-30T02:22:23Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "dss010101"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 1
    },
    {
        "number": 17527,
        "title": "Over notification and service control",
        "bodyText": "Hi,\nI used the commands below to both generate and retrieve the name of reporting apps [\"ahmadajmi\", \"benefritz\", \"devankoshal\", \"jesseddy\", \"Karimmove\", \"meln1ks\", \"shaneIxD\"], which where notifying the Sematext service for me. Although I like how verbose it is, so much traffic starts to be annoying. How can I reduce the reporting periodicity or have more control over these reporters?\njournalctl --since yesterday -u vector.service > vector_logs.txt\nchmod +x vector_logs.txt\n#!/bin/bash\n\nlogfile=\"$(pwd)/vector_logs.txt\"\n\n# Extract the appname values using grep and cut\nappnames=$(grep -o '\"appname\":\"[^\"]*' \"$logfile\" | cut -d '\"' -f 4)\n\n# Print unique appname values\necho \"Unique appname values:\"\necho \"$appnames\" | sort -u",
        "url": "https://github.com/vectordotdev/vector/discussions/17527",
        "createdAt": "2023-05-29T13:51:37Z",
        "updatedAt": "2023-05-29T18:15:07Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "brunolnetto"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 17463,
        "title": "Internal_metrics add additional context from Kubernetes",
        "bodyText": "Is there any plan to add additional more universal approach in enrich context of internal_metrics ??.\nJust like pid_key or host_key It would be great to add specific service from Kubernetes or any other dimension to all component scopes and not only use static defined tags added to all internal_metrics.",
        "url": "https://github.com/vectordotdev/vector/discussions/17463",
        "createdAt": "2023-05-22T18:24:06Z",
        "updatedAt": "2023-05-29T15:10:13Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "szibis"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 17518,
        "title": "How to reprocess/reparse all logs?",
        "bodyText": "Hi,\nthis is probably due to my lack of the stack...so why im asking here.\nAs im working through to figure out the transforms and how/what to parse into labels - find that i need to reparse old logs that have already been parsed - as there may be additional labels that im now extracting.\nIt's not clear to me how to do this - right now, i basically do docker-compose down followed by a force pruning of the volumes.\nIs there a better/cleaner way of telling vector to reprocess/parse all logs?\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/17518",
        "createdAt": "2023-05-26T22:15:49Z",
        "updatedAt": "2023-05-26T22:15:49Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "dss010101"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 17498,
        "title": "Filter events based on a list of values",
        "bodyText": "Is it possible to filter events using a list of values?\nFor example, I have a list of VIP sources that I would like to send to a specific sink. I was hoping to do something like the enrichment flow where you load the list of ids from a file and during the filter step, filter only the VIP events.",
        "url": "https://github.com/vectordotdev/vector/discussions/17498",
        "createdAt": "2023-05-26T09:39:50Z",
        "updatedAt": "2023-05-26T15:59:23Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "gwosty"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 17492,
        "title": "Default value for tag if field does not exist",
        "bodyText": "Hi All, i have the following transformation sector to apply a filter on idr json input and then create a metric to count the errors.\nThe point is that for some json events, some tags do not exist and vector creates the metric anyway only with the existing tags.\nIs there any approach to set a default value for a tag if it does not exist in the json (\"\", or \"N/A\", etc). I do not want to filter those json events on filter with exists(.field) beucase they can be \"valid\" events.\n    filter_idr_trunk_errors:\n      type: filter\n      inputs:\n        - idr\n      condition: >-\n        .service == \"idr\" && .call.type == \"trunk\" && to_int!(.sip.status.code) >=\n        500\n    idr_metrics_trunk_errors:\n      type: log_to_metric\n      inputs:\n        - filter_idr_trunk_errors\n      metrics:\n        - field: .call.direction\n          name: idr_trunk_errors\n          type: counter\n          tags:\n            direction: \"{{`{{.call.direction}}`}}\"\n            domain: \"{{`{{.call.domain.name}}`}}\"\n            trunk: \"{{`{{.call.trunk.name}}`}}\"\n            cluster: \"{{`{{cluster}}`}}\"\n            sip_status_name: \"{{`{{.sip.status.name}}`}}\"\n            transport: \"{{`{{.sip.transport}}`}}\"\n            codec: \"{{`{{.sdp.codec}}`}}\"",
        "url": "https://github.com/vectordotdev/vector/discussions/17492",
        "createdAt": "2023-05-25T17:46:12Z",
        "updatedAt": "2023-05-26T10:55:34Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "bmgante"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17454,
        "title": "not discard the log when the parsing rules cannot be matched",
        "bodyText": "How does the vector log collection tool not discard the log when the parsing rules cannot be matched, for example, write to another elasticsearch index when the rules do not match\nERROR\n2023-05-22T14:23:39.145659Z ERROR transform{component_kind=\"transform\" component_id=parse_logs component_type=remap component_name=parse_logs}: vector::internal_events::remap: Mapping failed with event. error=\"function call error for \\\"parse_json\\\" at (40:70): unable to parse json: expected value at line 1 column 1\" error_type=\"conversion_failed\" stage=\"processing\" internal_log_rate_limit=true",
        "url": "https://github.com/vectordotdev/vector/discussions/17454",
        "createdAt": "2023-05-22T14:26:32Z",
        "updatedAt": "2023-05-23T08:07:27Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "uglyliu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17433,
        "title": "How to filed merge",
        "bodyText": "My sourcelog is as follows:\n{\n    \"@timestamp\":\"2023-05-18T12:23:41.983Z\",\n    \"@metadata\":{\n        \"beat\":\"filebeat\",\n        \"type\":\"_doc\",\n        \"version\":\"7.10.0\"\n    },\n    \"ecs\":{\n        \"version\":\"1.6.0\"\n    },\n    \"host\":{\n        \"name\":\"VM-20-8-centos\"\n    },\n    \"agent\":{\n        \"type\":\"filebeat\",\n        \"version\":\"7.10.0\",\n        \"hostname\":\"VM-20-8-centos\",\n        \"ephemeral_id\":\"99fb2668-cf36-4889-a533-9ff4ce7acc63\",\n        \"id\":\"e0941af7-9260-46ea-8486-7740c5a2ddba\",\n        \"name\":\"VM-20-8-centos\"\n    },\n    \"log\":{\n        \"offset\":81,\n        \"file\":{\n            \"path\":\"/etc/vector/java.log\"\n        }\n    },\n    \"message\":\"2023-05-18 20:21:13.144 httpLog log log2\",\n    \"tags\":[\n        \"aliyun-vector\"\n    ],\n    \"input\":{\n        \"type\":\"log\"\n    },\n    \"fields\":{\n        \"kafka_topic\":\"filter-java\"\n    }\n}\n\nAfter I use parse_grok to cut the message field, I also want to get the value in the tags field\uff0cMy transform config is not work,the value of mtags is null\n[transforms.parse_remap]\ntype = \"remap\"\ninputs = [\"dummy_logs\"]\ndrop_on_error = false\nsource        = \"\"\"\n           . = parse_json!(.message)\n           if contains(string!(.message), \"httpLog\") {\n               . = parse_grok!(string!(.message),\"%{TIMESTAMP_ISO8601:datestamp} %{GREEDYDATA:extra}\")\n             .@timestamp = parse_timestamp!(string!(.datestamp), format: \"%F %T%.3f\")\n             .timestamp = now()\n             .unix_time,error = to_int(format_timestamp!(.timestamp, format: \"%s\"))\n             .utc8_time = .unix_time + 28800\n             .currentdate = format_timestamp!(to_timestamp!(.utc8_time), format: \"%Y.%m.%d\")\n             .mtags = .tags[0]\n             del(.timestamp)\n           } else {\n                abort\n           }\n\"\"\"",
        "url": "https://github.com/vectordotdev/vector/discussions/17433",
        "createdAt": "2023-05-18T12:41:06Z",
        "updatedAt": "2023-05-19T15:23:31Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "uglyliu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 1
    },
    {
        "number": 17437,
        "title": "unknown variant `grok_parser`",
        "bodyText": "Hi All,\nJust using latest vector release and when insalling vector through ansible role with the default config i get the error:\nMay 18 17:53:06 obs-dev vector[27551]: Failed to load [\"/etc/vector/vector.toml\"]\nMay 18 17:53:06 obs-dev vector[27551]: ------------------------------------------\nMay 18 17:53:06 obs-dev vector[27551]: x unknown variant `grok_parser`, expected one of `aggregate`, `aws_ec2_metadata`, `dedupe`, `filter`, `log_to_metric`, `lua`, `metric_to_log`, `reduce`, `remap`, `route`, `sample`, `tag_cardinality_limit`, `throttle`\nMay 18 17:53:06 obs-dev vector[27551]: in `transforms.grok`\n\nBelow my config:\n[root@obs-dev ~]# cat /etc/vector/vector.toml\n# Set global options\ndata_dir = \"/var/lib/vector\"\n\n\n[sinks.console]\n    encoding.codec = \"json\"\n    inputs = [\"grok\"]\n    type = \"console\"\n\n[sources.journald]\n    current_boot_only = true\n    type = \"journald\"\n\n[transforms.grok]\n    inputs = [\"journald\"]\n    pattern = \"(?<capture>\\\\d+)%{GREEDYDATA}\"\n    type = \"grok_parser\"\n\n\nCan someone help on this? Thanks",
        "url": "https://github.com/vectordotdev/vector/discussions/17437",
        "createdAt": "2023-05-18T18:09:27Z",
        "updatedAt": "2023-05-18T21:10:32Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bmgante"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17374,
        "title": "Preserve timezone for kubernetes_logs (containerd)",
        "bodyText": "Hello, community \u270b I have a question about the container logs timezone.\nContainerd keeps the time zone and adds it to container log timestamps. Example of timestamps in /var/log/pods directory:\n2023-05-11T16:18:40.212624044+03:00 stderr F W0511 13:18:40.212327       1 warnings.go:70] flowcontrol.apiserver.k8s.io/v1beta1 FlowSchema is deprecated in v1.23+, unavailable in v1.26+; use flowcontrol.apiserver.k8s.io/v1beta2 FlowSchema\n\nTimezone is present, but it is dropped on collecting\n\n  \n    \n      vector/src/sources/kubernetes_logs/parser/cri.rs\n    \n    \n         Line 101\n      in\n      74ae15e\n    \n  \n  \n    \n\n        \n          \n           Value::Timestamp(dt.with_timezone(&Utc)), \n        \n    \n  \n\n\nHow can I keep the original timezone?",
        "url": "https://github.com/vectordotdev/vector/discussions/17374",
        "createdAt": "2023-05-12T09:42:22Z",
        "updatedAt": "2023-05-29T08:25:39Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "nabokihms"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 3
    },
    {
        "number": 17381,
        "title": "How to debug - Message production error: MessageSizeTooLarge",
        "bodyText": "When the vector tries to send a message to Kafka we get an error like this:\nSome(KafkaError (Message production error: MessageSizeTooLarge (Broker: Message size too large)))\nI know how to fix this on kafka cluster side, but I want to debug this problem and understand which of the services trying to send so large massage. How I can catch this problem message? Thanks\nPS. Before I tried to use /usr/bin/vector tap, but there are a lot of messages and difficult to identify it.",
        "url": "https://github.com/vectordotdev/vector/discussions/17381",
        "createdAt": "2023-05-14T14:48:46Z",
        "updatedAt": "2023-05-16T07:20:08Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "VitaliiNykyforov"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17391,
        "title": "Consume logs from other containers running in docker compose",
        "bodyText": "I'm interested in adopting vector. We run our workloads in kubernetes and run our local developer environments with docker compose. I can see a fairly easy path to integrating with kubernetes but I'm having trouble figuring out how to configure vector to collect the logs of other containers running in docker compose. Is there a pre-defined source which I can use for this? Is there a sample config I can follow to set this up?\nI've tried using the following config\n[api]\nenabled = true\naddress = \"0.0.0.0:8686\"\n\n[sources.demo_logs]\ntype = \"demo_logs\"\ninterval = 1.0\nformat = \"json\"\n\n[sources.docker_logs]\ntype = \"docker_logs\"\n\n[sinks.console]\ninputs = [\"demo_logs\", \"docker_logs\"]\ntarget = \"stdout\"\ntype = \"console\"\nencoding.codec = \"json\"\n\nbut then when I start my docker compose script I only see the demo logs\n \u276f\u276f\u276f pnpm start | rg vector\n[+] Running 12/0\n \u283f Container e2e-tests-vector-1                 Created 0.0s\n \u283f Container e2e-tests-ipfs-1                   Created 0.0s\n \u283f Container e2e-tests-postgres-1               Created 0.0s\n \u283f Container e2e-tests-stripe-cli-1             Created 0.0s\n \u283f Container e2e-tests-rpc-node-proxy-1         Created 0.0s\n \u283f Container e2e-tests-influxdb-1               Created 0.0s\n \u283f Container e2e-tests-temporalite-1            Created 0.0s\n \u283f Container e2e-tests-hasura-1                 Created 0.0s\n \u283f Container e2e-tests-telegraf-1               Created 0.0s\n \u283f Container e2e-tests-enterprise-hasura-1      Created 0.0s\n \u283f Container e2e-tests-enterprise-graph-node-1  Created 0.0s\n \u283f Container e2e-tests-graph-node-1             Created 0.0s\nAttaching to e2e-tests-enterprise-graph-node-1, e2e-tests-enterprise-hasura-1, e2e-tests-graph-node-1, e2e-tests-hasura-1, e2e-tests-influxdb-1, e2e-tests-ipfs-1, e2e-tests-postgres-1, e2e-tests-rpc-node-proxy-1, e2e-tests-stripe-cli-1, e2e-tests-telegraf-1, e2e-tests-temporalite-1, e2e-tests-vector-1\ne2e-tests-vector-1                 | <jemalloc>: MADV_DONTNEED does not work (memset will be used instead)\ne2e-tests-vector-1                 | <jemalloc>: (This is the expected behaviour if you are running under QEMU)\ne2e-tests-vector-1                 | 2023-05-15T13:45:32.802233Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=trace,rdkafka=info,buffers=info,lapin=info,kube=info\"\ne2e-tests-vector-1                 | 2023-05-15T13:45:32.864250Z  INFO vector::app: Loading configs. paths=[\"/etc/vector/vector.toml\"]\ne2e-tests-vector-1                 | 2023-05-15T13:45:33.083897Z  INFO source{component_kind=\"source\" component_id=docker_logs component_type=docker_logs component_name=docker_logs}: vector::sources::docker_logs: Capturing logs from now on. now=2023-05-15T13:45:33.079407799+00:00\ne2e-tests-vector-1                 | 2023-05-15T13:45:33.093584Z  INFO source{component_kind=\"source\" component_id=docker_logs component_type=docker_logs component_name=docker_logs}: vector::sources::docker_logs: Listening to docker log events.\ne2e-tests-vector-1                 | 2023-05-15T13:45:33.129386Z  INFO vector::topology::running: Running healthchecks.\ne2e-tests-vector-1                 | 2023-05-15T13:45:33.130200Z  INFO vector::topology::builder: Healthcheck passed.\ne2e-tests-vector-1                 | 2023-05-15T13:45:33.210172Z ERROR source{component_kind=\"source\" component_id=docker_logs component_type=docker_logs component_name=docker_logs}: vector::sources::docker_logs: Listing currently running containers failed. error=error trying to connect: No such file or directory (os error 2)\ne2e-tests-vector-1                 | 2023-05-15T13:45:33.226936Z  INFO vector: Vector has started. debug=\"false\" version=\"0.29.1\" arch=\"x86_64\" revision=\"74ae15e 2023-04-20 14:50:42.739094536\"\ne2e-tests-vector-1                 | {\"message\":\"{\\\"host\\\":\\\"47.118.136.150\\\",\\\"user-identifier\\\":\\\"shaneIxD\\\",\\\"datetime\\\":\\\"15/May/2023:13:45:33\\\",\\\"method\\\":\\\"PATCH\\\",\\\"request\\\":\\\"/apps/deploy\\\",\\\"protocol\\\":\\\"HTTP/1.0\\\",\\\"status\\\":\\\"400\\\",\\\"bytes\\\":7403,\\\"referer\\\":\\\"https://up.org/booper/bopper/mooper/mopper\\\"}\",\"source_type\":\"demo_logs\",\"timestamp\":\"2023-05-15T13:45:33.169230466Z\"}\ne2e-tests-vector-1                 | 2023-05-15T13:45:33.583346Z  INFO vector::internal_events::api: API server running. address=0.0.0.0:8686 playground=http://0.0.0.0:8686/playground\ne2e-tests-vector-1                 | {\"message\":\"{\\\"host\\\":\\\"12.14.132.121\\\",\\\"user-identifier\\\":\\\"Karimmove\\\",\\\"datetime\\\":\\\"15/May/2023:13:45:34\\\",\\\"method\\\":\\\"POST\\\",\\\"request\\\":\\\"/do-not-access/needs-work\\\",\\\"protocol\\\":\\\"HTTP/2.0\\\",\\\"status\\\":\\\"500\\\",\\\"bytes\\\":38427,\\\"referer\\\":\\\"https://random.org/do-not-access/needs-work\\\"}\",\"source_type\":\"demo_logs\",\"timestamp\":\"2023-05-15T13:45:34.154350966Z\"}\n\nI have many services in my docker compose file outputting many logs. I guess that in this instance I haven't configured vector to correctly communicate with docker from within docker?",
        "url": "https://github.com/vectordotdev/vector/discussions/17391",
        "createdAt": "2023-05-15T13:47:52Z",
        "updatedAt": "2023-05-15T13:55:14Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "paymog"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17291,
        "title": "How to process this log with remap",
        "bodyText": "My source log is as follows:\n2023-05-03 13:49:54.004  INFO    87 --- [test-http-epoll-32] c.u.g.s.LogUtil                          : httpLog [#traceId=Root=2-645ww1f602-175bwwc7ac5d320f5244a600f3#][#startTime=2023-05-03 13:49:54#][#time=3#][#code=200#][#uid=#][#extraData=#][#domain=www.baidu.com#]\n\nI have tried many times but failed, can someone help me? thank u very much",
        "url": "https://github.com/vectordotdev/vector/discussions/17291",
        "createdAt": "2023-05-03T07:48:28Z",
        "updatedAt": "2023-05-14T03:47:09Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "uglyliu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 17361,
        "title": "how to discard the logs",
        "bodyText": "I have the following log in my sourcelog\n2023-05-10 httplog log log log\n2023-05-10 log log log\n\nHow to discard the logs with httplog and not collect them",
        "url": "https://github.com/vectordotdev/vector/discussions/17361",
        "createdAt": "2023-05-10T11:48:29Z",
        "updatedAt": "2023-05-10T13:34:23Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "uglyliu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17341,
        "title": "Helm - Quoting env vars that have special chars in values",
        "bodyText": "I admit right off the start this is really more generally a Helm, YAML, and k8s ConfigMap question. But I can't tell if this almost might be an issue with how env vars are substituted into the Vector config before processing.\nI have passwords that contain special characters, namely sometimes single quotes and double quotes (despite loud screaming from me to the password generator). I for the life of me cannot get a Helm quoting escape sequence that would properly preserve wrapping the password field with the needed double quotes, single quotes, etc.\nRight now, say I have a Kafka password field, it's written as such:\npassword: ${PASS}\nAt startup, this substitutes in a string with a single quote, which then in Vector causes the config parser to panic, and startup fails.\nI've tried password: \"${FOO}\", which by the time Helm installs the generated ConfigMap, the double quotes have been removed. This is a well-known \"feature\" of Helm's toYaml marshalling. Also tried password: '${FOO}', same results.\nFor completeness, I've also tried the other YAML escaping variations, with no success:\n\npassword: '${FOO}' generates password: ${FOO}\npassword: \"${FOO}\" generates password: ${FOO}\npassword: \"'${FOO}'\" generates password: '''${FOO}''' (literal wrapping single-quotes in the password value. wrong password)\npassword: '\"${FOO}\"' generates password: '\"${FOO}\"' (literal wrapping double-quotes in the password value, wrong password)\npassword: \"\\\"${FOO}\\\"\" generates password: '\"${FOO}\"' (same as above)\n\nshowing the examples of multi-line block quotes (e.g. password: |, password: |-, and password: >) screws up formatting in here. But they also vary from prepending the leading spaces into the password value, causing incorrect password, or having no effect at all.\nAny other ideas or suggestions?",
        "url": "https://github.com/vectordotdev/vector/discussions/17341",
        "createdAt": "2023-05-08T17:11:24Z",
        "updatedAt": "2023-05-09T13:20:07Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "sbalmos"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17312,
        "title": "When running unit tests for dropped events; how can I disable the stderr message \"ERROR transform\"?",
        "bodyText": "I have a unit test that uses a log event that is supposed to:\n\nTest a remap transform that specifies reroute_dropped = true and drop_on_abort = true\nTrigger an abort via parse_linux_authorization!\nThen capture the dropped event by a downstream dropped handler.\n\nThe test works as expected except that it prints out to stderr the details of the dropped event. Is there a way to configure the test so that it's aware that the transform will drop the event and not print to stderr during testing?\nThe stderr that I'm referring to:\n\n2023-05-04T22:50:10.449577Z ERROR transform{component_kind=\"transform\" component_id=syslog_transform component_type=remap component_name=syslog_transform}: vector::internal_events::remap: Mapping failed with event. error=\"function call error for \"parse_linux_authorization\" at (22:58): unable to parse input as valid syslog message\" error_type=\"conversion_failed\" stage=\"processing\" internal_log_rate_limit=true\n2023-05-04T22:50:10.449702Z ERROR transform{component_kind=\"transform\" component_id=syslog_transform component_type=remap component_name=syslog_transform}: vector_common::internal_event::component_events_dropped: Events dropped intentional=false count=1 reason=\"Mapping failed with event.\" internal_log_rate_limit=true\n\nI want to have tests that ensure the messages that should be dropped are correctly dropped. The behavior of parse_key_value wasn't exactly as I had expected but I was unaware until testing the failure states.",
        "url": "https://github.com/vectordotdev/vector/discussions/17312",
        "createdAt": "2023-05-04T23:13:03Z",
        "updatedAt": "2023-05-09T13:07:00Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "patrick-oday-cruise"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17304,
        "title": "Check whether `tag_cardinality_limit` transform has acted on an event (metric) in downstream transformers",
        "bodyText": "Hello!\nImagine I am using tag_cardinality_limit transform to curb tags with drop_tag action. I want to be able to distinguish between metrics that have been affected by that transform, and those who passed through untouched. Use case would be to log them somewhere, or attach a tag to them to mark this and act on it in destination.\nIs there a way to get that info from some metadata using a remap transform or something? In other words, do transforms in general, and tag_cardinality_limit in particular, leave a mark on the event that they modified it?",
        "url": "https://github.com/vectordotdev/vector/discussions/17304",
        "createdAt": "2023-05-04T08:19:14Z",
        "updatedAt": "2023-05-05T13:05:00Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ali-sattari"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 17250,
        "title": "How to add time field",
        "bodyText": "Help me please\nArchitecture\n\nFilebeat-->Kafka-->Vector-->ES\n\nVersion\n\nvector 0.28.1 (x86_64-unknown-linux-gnu ff15924 2023-03-06)\n\nContent\n\nHow to customize a time field currentdate with a value like 2023.04.30 to replace %Y.%m.%d, and then reference it in the sink\n\nConfig\n[sinks.elasticsearch]\ntype = \"elasticsearch\" # required\ninputs = [\"parse_logs\"] # required\napi_version = \"auto\"\ncompression = \"none\" # optional, default\nendpoints = [\"http://127.0.0.1:9200\"] # required\nmode = \"bulk\"\nbulk.action = \"index\"\nbulk.index = \"vector-java-test-%Y.%m.%d\"\n#healthcheck.enabled = true # optional, default",
        "url": "https://github.com/vectordotdev/vector/discussions/17250",
        "createdAt": "2023-04-29T16:01:10Z",
        "updatedAt": "2023-05-05T13:09:04Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "uglyliu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17280,
        "title": "Vector does not appear to work with Python's RotatingFileHandler",
        "bodyText": "Has anybody used RotatingFileHandler with Vector? We're hitting an issue where it appears that Vector is holding references to open file handles (similar issue to #11742) - here is some output from lsof ~\nvector        951                              root 3602r      REG              259,1   38322044     535038 /tmp/ray/session_2023-04-27_13-06-49_673121_114/logs/monitor.log (deleted)\nvector        951                              root 3603r      REG              259,1   40118317     535671 /tmp/ray/session_2023-04-27_13-06-49_673121_114/logs/monitor.log (deleted)\nvector        951                              root 3608r      REG              259,1   44385144     537108 /tmp/ray/session_2023-04-27_13-06-49_673121_114/logs/monitor.log (deleted)\nvector        951                              root 3609r      REG              259,1   49535510     538835 /tmp/ray/session_2023-04-27_13-06-49_673121_114/logs/monitor.log (deleted)",
        "url": "https://github.com/vectordotdev/vector/discussions/17280",
        "createdAt": "2023-05-02T16:41:10Z",
        "updatedAt": "2023-05-03T20:40:53Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "shomilj"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 17284,
        "title": "Help with Google Chronicle Configuration for Non-Default Log Sources",
        "bodyText": "Would anyone happen to have a Google Chronicle configuration that I could use as a reference? I'm uncertain about how to utilize the sink for sources that don't employ a default parser. For instance, I obtain logs from a SaaS product using an HTTP GET request. In situations where there is no default supported parser or log type, how would I push those logs?",
        "url": "https://github.com/vectordotdev/vector/discussions/17284",
        "createdAt": "2023-05-02T23:11:19Z",
        "updatedAt": "2023-05-08T23:52:26Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "TommySarkissian"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17252,
        "title": "Parse log is not correct",
        "bodyText": "Help me please\nArchitecture\n\nFilebeat-->Kafka-->Vector-->ES\n\nVersion\n\nvector 0.28.1 (x86_64-unknown-linux-gnu ff15924 2023-03-06)\n\nOriginal logs collected by kafka\n{\"@timestamp\":\"2023-04-25T07:37:54.450Z\",\"@metadata\":{\"beat\":\"filebeat\",\"type\":\"_doc\",\"version\":\"7.10.0\" },\"ecs\":{\"version\":\"1.6.0\"},\"log\":{\"offset\":92,\"file\":{\"path\":\"/var/log/nginx/java.log\" }},\"message\":\"2023-04-25 ha ha hello\",\"input\":{\"type\":\"log\"},\"fields\":{\"kafka_topic\":\"java-test\"},\"host \":{\"name\":\"VM-20-8-centos\"},\"agent\":{\"id\":\"e0941af7-9260-46ea-8486-7740c5a2ddba\",\"name\":\"VM-20-8- centos\",\"type\":\"filebeat\",\"version\":\"7.10.0\",\"hostname\":\"VM-20-8-centos\",\"ephemeral_id\":\"d7fe6b6e-9fbe-4f10-a8a1-af46f0e1e64c\" }} \nAs far as this kind of log is concerned, I can parse it successfully at https://playground.vrl.dev/, but it will report a parsing failure when it arrives at vector. The parsing rules are as follows:\n. = parse_regex!(.message, r'^(?P<time>\\d+-\\d+-\\d+) (?P<haha>.*)$')\n\n\nMy all config is as follows:\n[sources.dummy_logs]\ntype = \"kafka\" # required\nbootstrap_servers = \"127.0.0.1:9092\" # required\ngroup_id = \"test-vector\" # required\nkey_field = \"vector_key\" # optional, no default\nauto_offset_reset = \"latest\"\ncommit_interval_ms = 2000\ntopics = [\"java-test\"] # required\n[transforms.parse_logs]\ntype = \"remap\"\ninputs = [\"dummy_logs\"]\ndrop_on_error = true\nsource        = '''\n. = parse_regex!(.message, r'^(?P\\d+-\\d+-\\d+) (?P.*)$')\n'''\n[sinks.elasticsearch]\ntype = \"elasticsearch\" # required\ninputs = [\"parse_logs\"] # required\napi_version = \"auto\"\ncompression = \"none\" # optional, default\nendpoints = [\"http://127.0.0.1:9200\"] # required\nmode = \"bulk\"\nbulk.action = \"index\"\nbulk.index = \"vector-java-test-%Y.%m.%d\" # optional, default\n#healthcheck.enabled = true # optional, default",
        "url": "https://github.com/vectordotdev/vector/discussions/17252",
        "createdAt": "2023-04-29T16:17:52Z",
        "updatedAt": "2023-05-02T16:37:12Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "uglyliu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 5
        },
        "upvoteCount": 1
    },
    {
        "number": 17254,
        "title": "bad CPU type in executable: vector error",
        "bodyText": "Hi.\ni installed vector dev binary on mac os (air m2 2022)\nbrew tap vectordotdev/brew && brew install vector\n\nbut i got this error\n~ vector \n\nzsh: bad CPU type in executable: vector",
        "url": "https://github.com/vectordotdev/vector/discussions/17254",
        "createdAt": "2023-04-30T13:38:41Z",
        "updatedAt": "2023-05-01T13:37:59Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17255,
        "title": "how load secrets in vector",
        "bodyText": "Hi.\ni want to load secret from external vault .\ni read about secret in following link : example\nbut i don't understand too much about it.\nis it correct that we use this feature like the below configuration in main yaml file?\n    secret:\n      vault-test: \n        type: \"exec\"\n        command: [\"curl -H 'X-Vault-Token: s.1234'   -H 'X-Vault-Namespace: vault' -X GET http://test.com/v1/demo/data/test\"]\n\n    sources:\n      demo-test:\n        type: demo_logs\n        format: json\n\n    sinks:\n      console_all:\n        type: console\n        inputs:\n          - demo-test\n        target: stdout\n        acknowledgements:\n          enabled: true\n        encoding:\n          codec: json\n\ni would appreciate if anyone help me figure this out.\nis there any example or quick start doc?",
        "url": "https://github.com/vectordotdev/vector/discussions/17255",
        "createdAt": "2023-04-30T13:51:13Z",
        "updatedAt": "2023-05-01T13:37:11Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17151,
        "title": "reduce message field from elasticsearch",
        "bodyText": "Kafka log content is  about as follows:\n{ \"@metadata\": { \"beat\": \"filebeat\", \"type\": \"_doc\", \"version\": \"7.10.0\" }, \"message\": { \"upstream_response_time\": \"0.056\", \"upstream_status\": \"200\" } }\nAnd Finally, there will be more message fields written in elasticsearch\uff0cas follows\n\nThe vector configuration is as follows\n[transforms.parse_logs]\ntype = \"json_parser\"\ninputs = [\"dummy_logs\"]\nsource = '''\n. = parse_json!(.message)\n.message = parse_json!(.message)\n'''\nMy purpose is to finally display on elasticsearch, there is no message field\uff0cas follows",
        "url": "https://github.com/vectordotdev/vector/discussions/17151",
        "createdAt": "2023-04-14T07:33:14Z",
        "updatedAt": "2023-05-01T13:18:02Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "uglyliu"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 17245,
        "title": "Azure Monitor Metrics sink?",
        "bodyText": "Does vector have a sink that could send metrics to Azure Monitor Metrics? I see that there's an Azure Monitor Logs sink, but nothing for metrics. https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/data-platform-metrics",
        "url": "https://github.com/vectordotdev/vector/discussions/17245",
        "createdAt": "2023-04-28T19:12:43Z",
        "updatedAt": "2023-04-28T19:52:09Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ryanhendersonabs"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17177,
        "title": "elasticsearch/opensearch fails to authenticate with 403",
        "bodyText": "im trying to setup a simple stdin -> opensearch pipeline and im having some trouble authenticating to opensearch.\nIm running vector 0.29.0 (x86_64-unknown-linux-gnu 33b3868 2023-04-11 22:19:54.512366263) and my config is\n[sources.in]\ntype = \"stdin\"\n\n[sinks.opensearch]\ninputs = [\"in\"]\ntype = \"elasticsearch\"\nauth.strategy = \"aws\"\nendpoints = [\"https://redacted:443\"]\naws.region = \"eu-west-2\"\nthe verbose output is something like this\n2023-04-19T17:54:07.962263Z DEBUG vector::app: Internal log rate limit configured. internal_log_rate_secs=10\n2023-04-19T17:54:07.962305Z  INFO vector::app: Log level is enabled. level=\"vector=debug,codec=debug,vrl=debug,file_source=debug,tower_limit=trace,rdkafka=debug,buffers=debug,lapin=debug,kube=debug\"\n2023-04-19T17:54:07.963038Z  INFO vector::app: Loading configs. paths=[\"os.toml\"]\n2023-04-19T17:54:07.963400Z DEBUG vector::config::loading: No secret placeholder found, skipping secret resolution.\n2023-04-19T17:54:07.964324Z DEBUG vector::topology::builder: Building new source. component=parse_json\n2023-04-19T17:54:07.964677Z DEBUG vector::topology::builder: Building new sink. component=opensearch\n2023-04-19T17:54:07.964892Z  INFO vector::sources::file_descriptors: Capturing stdin.\n2023-04-19T17:54:08.011376Z DEBUG vector_core::tls::settings: Fetching system root certs.\n2023-04-19T17:54:08.019012Z DEBUG vector_core::tls::settings: Fetching system root certs.\n2023-04-19T17:54:08.028330Z DEBUG http: vector::internal_events::http_client: Sending HTTP request. uri=https://redacted:443/_cluster/state/version method=GET version=HTTP/1.1 headers={\"x-amz-date\": \"20230419T175408Z\", \"authorization\": Sensitive, \"x-amz-security-token\": \"redacted1\", \"user-agent\": \"Vector/0.29.0 (x86_64-unknown-linux-gnu 33b3868 2023-04-11 22:19:54.512366263)\", \"accept-encoding\": \"identity\"} body=[empty]\n2023-04-19T17:54:08.064250Z DEBUG http: vector::internal_events::http_client: HTTP response. status=403 Forbidden version=HTTP/1.1 headers={\"date\": \"Wed, 19 Apr 2023 17:54:08 GMT\", \"content-type\": \"application/json\", \"content-length\": \"1971\", \"connection\": \"keep-alive\", \"x-amzn-requestid\": \"275016dc-e965-42a8-8fdb-a15b59dbfa4a\", \"access-control-allow-origin\": \"*\"} body=[1971 bytes]\n2023-04-19T17:54:08.065084Z DEBUG vector::sinks::elasticsearch::common: Assumed ElasticsearchApi based on config setting suppress_type_name. assumed_version=8 config.suppress_type_name=false\n2023-04-19T17:54:08.065127Z  WARN vector::sinks::elasticsearch::common: Failed to determine Elasticsearch version from `/_cluster/state/version`. Please fix the reported error or set an API version explicitly via `api_version`. assumed_version=8 error=Unexpected response from Elasticsearch endpoint `/_cluster/state/version`. Missing `version`. Consider setting `api_version` option.\n2023-04-19T17:54:08.076573Z DEBUG vector_core::tls::settings: Fetching system root certs.\n2023-04-19T17:54:08.082719Z DEBUG vector_core::tls::settings: Fetching system root certs.\n2023-04-19T17:54:08.089383Z  INFO vector::topology::running: Running healthchecks.\n2023-04-19T17:54:08.089413Z DEBUG vector::topology::running: Connecting changed/added component(s).\n2023-04-19T17:54:08.089431Z DEBUG vector::topology::running: Configuring outputs for source. component=parse_json\n2023-04-19T17:54:08.089447Z DEBUG vector::topology::running: Configuring output for component. component=parse_json output_id=None\n2023-04-19T17:54:08.089504Z DEBUG vector::topology::running: Connecting inputs for sink. component=opensearch\n2023-04-19T17:54:08.089526Z DEBUG vector::topology::running: Adding component input to fanout. component=opensearch fanout_id=parse_json\n2023-04-19T17:54:08.089556Z DEBUG vector::topology::running: Spawning new source. key=parse_json\n2023-04-19T17:54:08.089597Z DEBUG vector::topology::running: Registered new allocation group. component_kind=\"source\" component_type=\"stdin\" component_id=\"parse_json\" group_id=\"2\"\n2023-04-19T17:54:08.089643Z DEBUG vector::topology::running: Registered new allocation group. component_kind=\"sink\" component_type=\"elasticsearch\" component_id=\"opensearch\" group_id=\"3\"\n2023-04-19T17:54:08.089728Z  INFO vector: Vector has started. debug=\"false\" version=\"0.29.0\" arch=\"x86_64\" revision=\"33b3868 2023-04-11 22:19:54.512366263\"\n2023-04-19T17:54:08.089748Z  INFO vector::app: API is disabled, enable by setting `api.enabled` to `true` and use commands like `vector top`.\n2023-04-19T17:54:08.089850Z DEBUG http: vector::internal_events::http_client: Sending HTTP request. uri=https://redacted:443/_cluster/health method=GET version=HTTP/1.1 headers={\"x-amz-date\": \"20230419T175408Z\", \"authorization\": Sensitive, \"x-amz-security-token\": \"redacted2\", \"user-agent\": \"Vector/0.29.0 (x86_64-unknown-linux-gnu 33b3868 2023-04-11 22:19:54.512366263)\", \"accept-encoding\": \"identity\"} body=[empty]\n2023-04-19T17:54:08.089936Z DEBUG source{component_kind=\"source\" component_id=parse_json component_type=stdin component_name=parse_json}: vector::topology::builder: Source starting.\n2023-04-19T17:54:08.089997Z DEBUG sink{component_kind=\"sink\" component_id=opensearch component_type=elasticsearch component_name=opensearch}: vector::topology::builder: Sink starting.\n2023-04-19T17:54:08.089927Z DEBUG source{component_kind=\"source\" component_id=parse_json component_type=stdin component_name=parse_json}: vector::topology::builder: Source pump supervisor starting.\n2023-04-19T17:54:08.090081Z DEBUG source{component_kind=\"source\" component_id=parse_json component_type=stdin component_name=parse_json}: vector::topology::builder: Source pump starting.\n2023-04-19T17:54:08.090506Z DEBUG sink{component_kind=\"sink\" component_id=opensearch component_type=elasticsearch component_name=opensearch}: vector::utilization: utilization=0.04690143198966257\n2023-04-19T17:54:08.106926Z DEBUG http: vector::internal_events::http_client: HTTP response. status=403 Forbidden version=HTTP/1.1 headers={\"date\": \"Wed, 19 Apr 2023 17:54:08 GMT\", \"content-type\": \"application/json\", \"content-length\": \"1964\", \"connection\": \"keep-alive\", \"x-amzn-requestid\": \"32d11167-3559-42c2-acb3-4614a3d6b58c\", \"access-control-allow-origin\": \"*\"} body=[1964 bytes]\n2023-04-19T17:54:08.107001Z ERROR vector::topology::builder: msg=\"Healthcheck failed.\" error=Unexpected status: 403 Forbidden component_kind=\"sink\" component_type=\"elasticsearch\" component_id=opensearch component_name=opensearch\n2023-04-19T17:54:13.091129Z DEBUG sink{component_kind=\"sink\" component_id=opensearch component_type=elasticsearch component_name=opensearch}: vector::utilization: utilization=0.0046901633564615285\n\nIm pretty confident the IAM/opensearch part is setup correctly because if i run from the same ec2 instance a logstash process, it can authenticate and send stuff to my opensearch, for example\ndocker run -it --rm \\\n\t--name logstash \\\n\tpublic.ecr.aws/opensearchproject/logstash-oss-with-opensearch-output-plugin:7.16.2 \\\n\t-e 'input { stdin { } } output { stdout {}\n   opensearch {\n     hosts => [\"https://redacted:443\"]\n     index => \"logstash-%{+YYYY.MM.dd}\"\n     auth_type => {\n     \ttype => \"aws_iam\"\n     \tregion => \"eu-west-2\"\n     }\n     ssl => true\n     ecs_compatibility => disabled\n     ssl_certificate_verification => true\n   }\n }'\n\nAm i missing something obvious in my config ?",
        "url": "https://github.com/vectordotdev/vector/discussions/17177",
        "createdAt": "2023-04-19T18:02:32Z",
        "updatedAt": "2023-04-27T16:10:26Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "mhristof"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 17234,
        "title": "Sample vector.toml file",
        "bodyText": "Hi all,\nI need help with my use case: We have a 3rd-party server/service that uses Vector to enable exporting of logs. Are currently using Datadog already and we would like to connect our datadog instance with their exporter using vector.toml file. Can somebody provide some examples on how we can do this?\nWe're using CubeJS and in their docs they have this: https://cube.dev/docs/cloud/workspace/logs\nI'm not familiar with Vector so I'm not sure what I can put in the vector.toml file they provided to enable it to connect to our Datadog instance. Thanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/17234",
        "createdAt": "2023-04-27T07:44:48Z",
        "updatedAt": "2023-04-27T16:04:28Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "emlim23"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17212,
        "title": "s3 sink to allow append to existing file",
        "bodyText": "Currently S3 sink creates a new file each time it flushes the buffer. It would be nice if it could append data to an existing file.",
        "url": "https://github.com/vectordotdev/vector/discussions/17212",
        "createdAt": "2023-04-25T15:13:12Z",
        "updatedAt": "2023-04-26T22:16:06Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "chaporgin"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17186,
        "title": "ignore_older_secs clarification",
        "bodyText": "Hello.\nAs per vector docs for ignore_older_secs Vector should ignore files with a modification date older than the specified time.\nBut when we restart the vector application we see that vector starts processing the same logs that were already processed before the restart. After some research we found out that during vector restart it using the ignore_older_secs to determine where to resume from.\nThe calculate ignore before and the set state for the checkpointer seems to clearly indicate our findings. The ignore_older_secs is being used by the checkpoints.\nPlease can we get some some clarifications on exactly how to use ignore_older_secs. Our main problem here is that when we restart vector, we want to resume processing from where vector last left off. I am not finding anything specific about the checkpoints in this discussion thread",
        "url": "https://github.com/vectordotdev/vector/discussions/17186",
        "createdAt": "2023-04-20T19:19:54Z",
        "updatedAt": "2023-04-25T15:46:15Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ktamal"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 17200,
        "title": "Elasticsearch custom index pattern with dates",
        "bodyText": "Hi,\nI'm trying to setup vector with Opensearch replacing my current fluentd setup. I have this script to process the logs:\n      timest = \"-%Y.%m.%d\"\n      if logtype == \"my_log\" {\n          timest = \"-%Y.%m\"\n      }\n      if includes([\"content1\", \"content2\"], svc ) {\n        .index, err = \"ae-microservices-\" + .cluster + \"-\" + .environment + \"-\" + logtype + \"-\" + svc + timest\n      } else {\n        .index, err = \"ae-microservices-\" + .cluster + \"-\" + .environment + \"-\" + logtype + timest\n      }\n\n\nWith this sink:\nsinks:\n  elastic:\n    type: elasticsearch\n    inputs:\n      - normalize_logs\n    api_version: auto\n    compression: gzip\n    endpoints:\n      - https://elastic.example.com\n    mode: bulk\n    auth:\n      strategy: basic\n      user: vector\n      password: xxxxxx\n    bulk:\n      index: \"{{ .index }}\"\n\nBut I'm getting the error:\n2023-04-24T08:18:28.222602Z ERROR sink{component_kind=\"sink\" component_id=elastic component_type=elasticsearch component_name=elastic}:request{request_id=1}: vector::sinks::util::retries: Not retriable; dropping the request. reason=\"error type: invalid_index_name_exception, reason: Invalid index name [microservices-pre-dev-my_log-%Y.%m], must be lowercase\" internal_log_rate_limit=true\n\nI have mixed index patterns YYYY.mm or YYYY.mm.dd depending on the content of the log.\nIs there any way to achieve it with vector?",
        "url": "https://github.com/vectordotdev/vector/discussions/17200",
        "createdAt": "2023-04-24T08:24:50Z",
        "updatedAt": "2023-04-25T06:42:50Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "monwolf"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 17201,
        "title": "Loki graduation to stable",
        "bodyText": "Hi Vector Team, with Loki getting some traction among the community, I was wondering how is the graduation to stable for the Loki sink in Vector is going?\nOr in another way of putting the question, what are the issues that need fixing, to graduate Loki sink from beta to stable?",
        "url": "https://github.com/vectordotdev/vector/discussions/17201",
        "createdAt": "2023-04-24T08:55:28Z",
        "updatedAt": "2023-04-26T04:23:42Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "luisloros"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 17192,
        "title": "Refreshing OAuth token for http sink",
        "bodyText": "Hey Folks:\nI am using vector http sink and it has strategy to use OAuth token for authentication.  However I could not find any documentation on how the token can be refreshed when it expires.  I believe some sinks do support that but can http sink be configured to refresh it ?\nIf you have any other advice, please let me know.\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/17192",
        "createdAt": "2023-04-21T12:51:31Z",
        "updatedAt": "2023-04-24T20:02:09Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mans2singh"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17176,
        "title": "Internal log [Received out of order log message.] is being rate limited.",
        "bodyText": "I have configured vector-Loki on the nomad cluster and received an Internal log rate limiting errors from the container. How can I increase the rate limit and solve this issue somehow? Maybe I need to disable rate limiting, Thanks.\nHere is my config:\ndata_dir = \"alloc/data/vector/\"\n[api]\nenabled = true\naddress = \"0.0.0.0:8686\"\nplayground = true\n[sources.logs]\ntype = \"docker_logs\"\n[sinks.out]\ntype = \"console\"\ninputs = [ \"logs\" ]\nencoding.codec = \"json\"\n[sinks.loki]\ntype = \"loki\"\ninputs = [\"logs\"]\nendpoint = \"${var.loki_endpoint}\"\nencoding.codec = \"json\"\nhealthcheck.enabled = true\n# since . is used by Vector to denote a parent-child relationship, and Nomad's Docker labels contain \".\",\n# we need to escape them twice, once for TOML, once for Vector\nlabels.task = \"{{ label.\"com.hashicorp.nomad.task_name\" }}\"\nlabels.group = \"{{ label.\"com.hashicorp.nomad.task_group_name\" }}\"\nlabels.namespace = \"{{ label.\"com.hashicorp.nomad.namespace\" }}\"\nlabels.node = \"{{ label.\"com.hashicorp.nomad.node_name\" }}\"\nlabels.job = \"${var.datacenter}\"\nlabels.stream = \"{{ stream }}\"\nremove_label_fields = true",
        "url": "https://github.com/vectordotdev/vector/discussions/17176",
        "createdAt": "2023-04-19T17:15:06Z",
        "updatedAt": "2023-04-24T16:33:10Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ssantrosyan"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 17205,
        "title": "Kafka / Event Hub Source",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nWhen trying to read from EventHub with Kafka I keep getting errors like the following:\nIs there any suggestions or configuration I should try?\n-- Subject: Unit vector.service has finished start-up\n-- Defined-By: systemd\n-- Support: https://access.redhat.com/support\n--\n-- Unit vector.service has finished starting up.\n--\n-- The start-up result is done.\nApr 19 22:50:01 log-aggregatorvector[16157]: 2023-04-19T22:50:01.604132Z ERROR source{component_kind=\"source\" component_id=azure_activity_event_hub component_type=kafka component_name=azure_activity_event_hub}: vector::internal_events::kafka: Failed to read message. error=Message consumption error: MessageSizeTooLarge (Broker: Message size too large) error_code=\"reading_message\" error_type=\"reader_failed\" stage=\"receiving\" internal_log_rate_limit=true\nApr 19 22:50:01 log-aggregatorvector[16157]: 2023-04-19T22:50:01.717203Z ERROR source{component_kind=\"source\" component_id=azure_activity_event_hub component_type=kafka component_name=azure_activity_event_hub}: vector::internal_events::kafka: Internal log [Failed to read message.] is being rate limited.\nApr 19 22:50:11 log-aggregatorvector[16157]: 2023-04-19T22:50:11.650908Z ERROR source{component_kind=\"source\" component_id=azure_activity_event_hub component_type=kafka component_name=azure_activity_event_hub}: vector::internal_events::kafka: Internal log [Failed to read message.] has been rate limited 29 times.\n\nConfiguration\ntype = \"kafka\"\nbootstrap_servers = \"${AZURE_ACTIVITY_EVENT_HUB_ENDPOINT}:9093\"\ngroup_id = \"vector\"\ntopics = [\"^${AZURE_ACTIVITY_EVENT_HUB_TOPIC_PREFIX}.+\"]\n\n  [sasl]\n  enabled = true\n  mechanism = \"PLAIN\"\n  password = \"${AZURE_ACTIVITY_EVENT_HUB_CONNECTION_STRING}\"\n  username = \"$$ConnectionString\"\n\n  [tls]\n  enabled = true\n\nVersion\n0.28.2\nDebug Output\nNo response\nExample Data\nNo response\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/17205",
        "createdAt": "2023-04-20T00:00:05Z",
        "updatedAt": "2023-04-24T20:28:15Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "zpriddy"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15430,
        "title": "Derive array type parameter from environment variable",
        "bodyText": "Hello,\nSay I have this config:\n[sources.my_source_id]\ntype = \"kafka\"\nbootstrap_servers = \"10.14.22.123:9092,10.14.23.332:9092\"\ngroup_id = \"consumer-group-name\"\nkey_field = \"message_key\"\ntopics = [ \"foo\", \"bar\", \"baz\" ]\nI would like the topic values to come from an environment variable. The most obvious way is to just use a regex:\nKAFKA_TOPICS=\"^(foo|bar|baz)\"\n\nBut I wonder if it's possible to instead set an environment variable like this\nKAFKA_TOPICS=\"foo,bar,baz\"\n\nthen have vector do some sort of split to convert it to an array? Something like:\ntopics = split(\"${KAFKA_TOPICS}\", \",\") \n\nbut obviously, this doesn't work.\nThanks in advance!",
        "url": "https://github.com/vectordotdev/vector/discussions/15430",
        "createdAt": "2022-12-02T03:47:02Z",
        "updatedAt": "2023-04-20T22:13:08Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "pirxthepilot"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 17178,
        "title": "CPU in percentage",
        "bodyText": "Hi,\nI want to report the cpu in percent of each core every 10 secs, however I 'm not seeing a way to get the cpu in perecentage?\nAm I misunderstanding the documents\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/17178",
        "createdAt": "2023-04-19T18:20:37Z",
        "updatedAt": "2023-04-19T21:10:51Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "clumbo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17168,
        "title": "Setting valid_tokens from environment or secrets for the Splunk HEC source",
        "bodyText": "I'm trying to setup a generic Splunk HEC source, where multiple sources will be able to send data. As such, I need to dynamically add and remove entries to the valid_tokens array in the Vector configuration (https://vector.dev/docs/reference/configuration/sources/splunk_hec/#valid_tokens).\nMy initial thoughts were to set that value using environment variables or a secret backend and then send Vector a SIGHUP whenever the tokens change.\nThe documentation doesn't mention sourcing an entire array from an environment variable as something one can do. The same goes for the secret backend, it seem like it just provides strings.\nIs there any way to dynamically source a configuration array?",
        "url": "https://github.com/vectordotdev/vector/discussions/17168",
        "createdAt": "2023-04-18T11:33:32Z",
        "updatedAt": "2023-04-19T16:12:39Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "MadsRC"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 17126,
        "title": "Get http uri from environment variable",
        "bodyText": "Hi expert,\nNow we use http server as our sinks, is it possible to get the buffer size and uri from the environment variable? The uri should be different for our testing  and production!",
        "url": "https://github.com/vectordotdev/vector/discussions/17126",
        "createdAt": "2023-04-12T13:59:44Z",
        "updatedAt": "2023-04-12T22:26:12Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "janiu-001"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16986,
        "title": "Parsing CRI-formatted logs from `file` source?",
        "bodyText": "Is it possible to use the kubernetes_log parser with file, without all the kubernetes runtime stuff? Like as a transform, perhaps?\nMotivation: I'm using the podman driver with nomad which creates a log file in the CRI logging format. (i.e. <timestamp> <stdout|stderr> <P|F> [data]). This is already implemented within https://github.com/vectordotdev/vector/blob/master/src/sources/kubernetes_logs/parser/cri.rs, but is there a way to use this parser with the standard file source?\nWorkarounds considered:\n\n\nUsing the journald log driver for nomad-podman.\n\nThis is doable, but has additional overhead because logs are parsed and copied/written by journald, vector, and nomad.\nNot sure of the best way to annotate these journald logs with nomad metadata. For file logging see: https://github.com/mr-karan/nomad-vector-logger and https://github.com/input-output-hk/nomad-follower\n\n\n\nManual parsing\n\nSeems like it should be possible\n\nthe multiline option + regex is probably insufficient since the two streams and interrupt each other\nseems like you should be able to carry enough state with lua parsing to do the job\n\n\nI'd prefer to avoid re-implementing parsing since it's already handled very well natively by vector",
        "url": "https://github.com/vectordotdev/vector/discussions/16986",
        "createdAt": "2023-03-28T23:04:58Z",
        "updatedAt": "2023-03-29T13:57:22Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "baodrate"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16939,
        "title": "How to config sink to Pulsar with tls key",
        "bodyText": "Hi all,\nI have read the document about the pulsar sink type. But I don't find TLS for config.\nDoes anyone help me?\nThank you so much.\nThanks,\nChienNguyen.",
        "url": "https://github.com/vectordotdev/vector/discussions/16939",
        "createdAt": "2023-03-24T01:40:37Z",
        "updatedAt": "2023-03-29T06:40:40Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ngocchien"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16322,
        "title": "Alerts on vector metrics",
        "bodyText": "Hello, community! I have a question regarding internal vector metrics.\nThere are cases when:\n\nVector stops collecting logs from Kubernetes.\nVector cannot send logs because a sink\n2.1.  is unavailable.\n2.2. blocked by a rate limiter.\n\nThe solution for 2.2 seems pretty straightforward. I came up with the following promql.\nrate(vector_events_discarded_total[2m]) > 0\n\nFor the 2.1, it is possible to alert if there are no out events for the sink.\nrate(vector_events_out_total{component_kind=\"sink\"}[2m]) == 0\n\nYet it needs to be more accurate because the real problem is when the end of a sink responds with errors, but I need help finding a generic approach.\nNo idea how to alert on 1. There are many metrics, and it is not clear to me which one to pick.\nI'd appreciate it if someone could take a look at what I'm doing and give me some hints hot to setup alerting rules correctly for vector.",
        "url": "https://github.com/vectordotdev/vector/discussions/16322",
        "createdAt": "2023-02-07T10:45:37Z",
        "updatedAt": "2023-03-28T21:02:52Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "nabokihms"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 16978,
        "title": "How to parse from stdin, exiting after stream is finished?",
        "bodyText": "Hi!\nI have a few ad-hoc commands where it would be very useful to be able to use vector in a shell pipeline, but I can't seem to find any information on how to make vector exit after being done with the log stream.",
        "url": "https://github.com/vectordotdev/vector/discussions/16978",
        "createdAt": "2023-03-28T10:50:48Z",
        "updatedAt": "2023-03-28T15:57:02Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "strokirk"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16945,
        "title": "Sink one gauge metric to prometheus by remote write, query get multiple",
        "bodyText": "vector version: vector 0.26.0 (x86_64-unknown-linux-gnu c6b5bc2 2022-12-05)\nvector config:\n# Change this to use a non-default directory for Vector data storage:\ndata_dir = \"/var/lib/vector\"\n[sources.vector_logs]\ntype = \"internal_logs\"\n\n[sinks.console]\ntype = \"console\"\ninputs = [\"vector_logs\"]\nencoding.codec = \"text\"\n\n# Random Syslog-formatted logs\n[sources.prometheus_style_metrics]\ntype = \"kafka\"\nbootstrap_servers = \"172.31.16.247:9092,172.31.15.146:9092\"\ngroup_id = \"vector-script-metric-0\"\ntopics = [ \"scriptMetric\" ]\n\n[transforms.test_parse]\ntype = \"remap\"\ninputs = [ \"prometheus_style_metrics\" ]\nsource = '''\n.metric = parse_json!(.message)\n.metric.metric.timestamp = to_timestamp!(parse_json!(.message).metric.timestamp, unit: \"milliseconds\")\n'''\n\n# Parse Syslog logs\n# See the Vector Remap Language reference for more info: https://vrl.dev\n[transforms.make_metric]\ntype = \"lua\"\ninputs = [ \"test_parse\" ]\n# inputs = [ \"prometheus_style_metrics\" ]\nversion = \"2\"\n[transforms.make_metric.hooks]\nprocess = \"\"\"\nfunction (event, emit)\n  emit(event.log.metric)\nend\"\"\"\n\n# Print parsed logs to stdout\n[sinks.print]\ntype = \"console\"\ninputs = [\"make_metric\"]\nencoding.codec = \"json\"\n[sinks.prometheus_write]\ntype = \"prometheus_remote_write\"\ninputs = [ \"make_metric\" ]\nendpoint = \"http://host:9091/insert/0/prometheus/api/v1/write\"\n\nwhen i sink one metric, vector console print\n{\"name\":\"cpu_time_milliseconds\",\"namespace\":\"script_metric\",\"tags\":{\"script_name\":\"monitor/test.php\"},\"timestamp\":\"2023-03-24T08:48:00.383Z\",\"kind\":\"absolute\",\"gauge\":{\"value\":33.0}}\n\nuse grafana query script_metric_cpu_time_milliseconds result\n\nThe result is not one data point, but multiple, the expectation is that only one data point appears for the corresponding event.",
        "url": "https://github.com/vectordotdev/vector/discussions/16945",
        "createdAt": "2023-03-24T08:57:18Z",
        "updatedAt": "2023-03-27T15:32:03Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "xxm404"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16396,
        "title": "Extract http request args (query parameters)",
        "bodyText": "Hi all.\nI assume that the below request  from another vector with an HTTP sink comes to our vector with source http\nhttps://myenv.dev/?app_name=test-app&env=stage\n\nHow can I extract the app_name=test-app&env=stage from the request in the vector HTTP source?\nMy current configuration is this:\nsources:\n  input_logs:\n    type: http\n    address: 0.0.0.0:8080\n    encoding: json\n    path: \"\"\n    strict_path: false\n    path_key: log\n\ntransforms:\n  to_json:\n    type: remap\n    inputs:\n      - input_logs\n    source: |\n       --------------\n\nI also checked the query_parameters but didn't help because could not extract the args dynamically...\nwould you help me figure this out.\nAppreciate it.",
        "url": "https://github.com/vectordotdev/vector/discussions/16396",
        "createdAt": "2023-02-11T10:19:45Z",
        "updatedAt": "2023-03-27T15:13:38Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 16967,
        "title": "Parse multiple lines in a log file",
        "bodyText": "I have a log file which contains information of the following format:\n2023-01-31 19:01:40,019 INFO scm-web-1217235:com.cloudera.server.web.cmf.CmfLdapAuthenticationProvider: LDAP/AD authentication failure for acoe_dxbdr\n2023-01-31 19:01:40,019 ERROR scm-web-1217235:com.cloudera.server.web.cmf.CmfLdapAuthenticationProvider: LDAP/AD authentication failed\norg.springframework.security.authentication.BadCredentialsException: Bad credentials\n\tat org.springframework.security.ldap.authentication.BindAuthenticator.authenticate(BindAuthenticator.java:101)\n\tat org.springframework.security.ldap.authentication.LdapAuthenticationProvider.doAuthentication(LdapAuthenticationProvider.java:187)\n\tat org.springframework.security.ldap.authentication.AbstractLdapAuthenticationProvider.authenticate(AbstractLdapAuthenticationProvider.java:85)\n\tat com.cloudera.server.web.cmf.CmfLdapAuthenticationProvider.authenticate(CmfLdapAuthenticationProvider.java:151)\n\tat com.cloudera.server.web.cmf.CmfLdapAuthenticationProvider.authenticate(CmfLdapAuthenticationProvider.java:39)\n\tat org.springframework.security.authentication.ProviderManager.authenticate(ProviderManager.java:174)\n\tat org.springframework.security.authentication.ProviderManager.authenticate(ProviderManager.java:199)\n\tat org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilterInternal(BasicAuthenticationFilter.java:180)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)\n\tat com.cloudera.api.ApiBasicAuthFilter.doFilter(ApiBasicAuthFilter.java:86)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)\n\tat org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:66)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)\n\tat org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)\n\tat org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105)\n\tat org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)\n\tat org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:214)\n\tat org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177)\n\tat org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347)\n\tat org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1610)\n\tat org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1602)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:540)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1588)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1345)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1557)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\n\tat org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:174)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:502)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:411)\n\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:305)\n\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:159)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\n\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\n\tat com.cloudera.server.common.BoundedQueuedThreadPool$2.run(BoundedQueuedThreadPool.java:94)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\n\tat java.lang.Thread.run(Thread.java:748)\n2023-01-31 19:01:40,019 INFO DataArchiver-88:com.cloudera.cmf.command.datacollection.DataArchiver: Finished data archiver: com.cloudera.cmf.command.datacollection.StacksLogArchiver@ed84375\n\n\nI want to store this multiline stack trace in a single field called stack trace. How do I do it using remap? Would be helpful if you could provide a code for it.",
        "url": "https://github.com/vectordotdev/vector/discussions/16967",
        "createdAt": "2023-03-27T09:17:10Z",
        "updatedAt": "2023-03-27T12:49:34Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "siddharthb78"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16792,
        "title": "Kafka certificates in PEM format",
        "bodyText": "Just wanted to share.\nI've added the feature to specify Kafka certificates in PEM format in line with other providers.\n#15448\nHowever, there is a bug in rdkafka library that only reads a single certificate for CA, not the whole chain.\nconfluentinc/librdkafka@f8830a2\nFor now, we fixed it by building vector with librdkafka >= 2.0\nhttps://github.com/deckhouse/deckhouse/blob/7a4e2dd5af28cd444a94029d1f51392a7d5e3147/modules/460-log-shipper/images/vector/Dockerfile",
        "url": "https://github.com/vectordotdev/vector/discussions/16792",
        "createdAt": "2023-03-14T11:28:07Z",
        "updatedAt": "2023-03-27T06:38:50Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "nabokihms"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 16884,
        "title": "to_timestamp does not honor timezone global setting?",
        "bodyText": "I cannot seem to get to_timestamp to honor global option timezone. Am I holding it wrong, or is this a bug?\nDocumentation says:\n\nThe name of the time zone to apply to timestamp conversions that do not contain an explicit time zone.\n\nSimple reproduction:\ntimezone = \"Europe/Oslo\" # +01:00 at this time\n\n[sources.my_source_id]\ntype = \"stdin\"\n\n[transforms.my_transform_id]\ntype = \"remap\"\ninputs = [ \"my_source_id\" ]\nsource = \".timestamp, err = to_timestamp(.message)\\nlog(err)\"\n\n[sinks.my_sink_id]\ntype = \"console\"\ninputs = [ \"my_transform_id\" ]\nencoding.codec = \"json\"\nRun: docker run -ti --rm -v $(pwd):/etc/vector timberio/vector:0.28.1-debian\nOutput zulu timestamp is OK:\n2023-03-21T13:52:21.187226Z\n2023-03-21T13:52:27.282141Z  INFO transform{component_kind=\"transform\" component_id=my_transform_id component_type=remap component_name=my_transform_id}: vrl_stdlib::log: <null> internal_log_rate_secs=1 vrl_position=41\n{\"host\":\"ec7487bf20ef\",\"message\":\"2023-03-21T13:52:21.187226Z\",\"source_type\":\"stdin\",\"timestamp\":\"2023-03-21T13:52:21.187226Z\"}\n\nSame timestamp with local time and milliseconds fails, reported as feature request 10. january:\n2023-03-21T14:52:21.18722\n2023-03-21T13:52:55.158379Z  INFO transform{component_kind=\"transform\" component_id=my_transform_id component_type=remap component_name=my_transform_id}: vrl_stdlib::log: function call error for \"to_timestamp\" at (18:40): No matching timestamp format found for \"2023-03-21T14:52:21.18722\" internal_log_rate_secs=1 vrl_position=41\n{\"host\":\"ec7487bf20ef\",\"message\":\"2023-03-21T14:52:21.18722\",\"source_type\":\"stdin\",\"timestamp\":\"1970-01-01T00:00:00Z\"}\n\nSame timestamp works with +01:00:\n2023-03-21T14:52:21.18722+01:00\n2023-03-21T13:54:42.329474Z  INFO transform{component_kind=\"transform\" component_id=my_transform_id component_type=remap component_name=my_transform_id}: vrl_stdlib::log: <null> internal_log_rate_secs=1 vrl_position=41\n{\"host\":\"ec7487bf20ef\",\"message\":\"2023-03-21T14:52:21.18722+01:00\",\"source_type\":\"stdin\",\"timestamp\":\"2023-03-21T13:52:21.187220Z\"}\n\nRemoving split seconds makes it parse, but time is wrong. Here I expected 2023-03-21T13:52:21Z, not 2023-03-21T14:52:21Z:\n2023-03-21T14:52:21\n2023-03-21T13:53:13.552961Z  INFO transform{component_kind=\"transform\" component_id=my_transform_id component_type=remap component_name=my_transform_id}: vrl_stdlib::log: <null> internal_log_rate_secs=1 vrl_position=41\n{\"host\":\"ec7487bf20ef\",\"message\":\"2023-03-21T14:52:21\",\"source_type\":\"stdin\",\"timestamp\":\"2023-03-21T14:52:21Z\"}",
        "url": "https://github.com/vectordotdev/vector/discussions/16884",
        "createdAt": "2023-03-21T13:58:26Z",
        "updatedAt": "2023-03-25T00:33:48Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "arve0"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 16889,
        "title": "Error while trying to build vector on Ubuntu instance in Google Cloud",
        "bodyText": "I am following the steps in publish.yml:\n\nI am logged in as root user.\nI have cloned the vector repo at /root\nIssues encountered and fixed:\n\nFirst step works completely fine (sudo -E bash scripts/environment/bootstrap-ubuntu-20.04.sh)\nBut running bash scripts/environment/prepare.sh gives rust not found error\nThis was fixed by adding the /root/.cargo/bin to the PATH before running prepare.sh\nprepare.sh gives error in installing datadog => Fixed by manually installing it\n\nCurrent issue:\n\nRunning make package-x86_64-unknown-linux-gnu-all is giving this error\n\n\nRustc version on system:\n\nRustup default:\n\nI need to build a custom image, any help on this would be appreciated",
        "url": "https://github.com/vectordotdev/vector/discussions/16889",
        "createdAt": "2023-03-21T15:24:01Z",
        "updatedAt": "2023-03-24T13:20:27Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "asingh072318"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16925,
        "title": "Kafka sink and dead letter queue with JSON schema/Avro",
        "bodyText": "Hi! This question is a bit too broad and might not be related to Vector itself. I wanted to ask if having a JSON schema (or Avro) can I build dead letter queue for the pipeline\nproducer -> vector -> kafka\n\nfor the messages that do not comply to the schema.\nI see that Vector supports Avro schema, but what happens if the messages cannot be serialized? There is no option to specify a specific secondary topic for such messages (IIUC). So what could be the options here? Thanks in advance!",
        "url": "https://github.com/vectordotdev/vector/discussions/16925",
        "createdAt": "2023-03-23T09:57:31Z",
        "updatedAt": "2023-03-23T13:57:19Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "chaporgin"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16868,
        "title": "recursive func for parse_json",
        "bodyText": "Hello,\nCase:\njson: logs = {\"time\": \"2023-02-16T15:57:07.523036823Z\", \"auth\": {\"display_name\": \"root\",  \"policies\": [\"root\"], ' \\ '\"policy_results\": { \"allowed\": True, \"granting_policies\": [ {\"name\": \"root\", \"namespace_id\": \"root\", ' \\ '\"type\": \"acl\" } ] }, \"token_type\": \"service\" }, \"request\": { \"operation\": \"help\", \"mount_type\": \"ldap\", ' \\ '\"namespace\": { \"id\": \"root\" }, \"path\": \"auth/ldap/\" }}\nhow to get this result with VRL:\n{\".time\": \"2023-02-16T15:57:07.523036823Z\", \".auth_display_name\": \"root\", \".auth_policies_root\": \"root\", \".auth_policy_results_allowed\": \"True\", \".auth_policy_results_granting_policies_name\": \"root\", \".auth_policy_results_granting_policies_namespace_id\": \"root\", \".auth_policy_results_granting_policies_type\": \"acl\", \".auth_token_type\": \"service\", \".request_operation\": \"help\", \".request_mount_type\": \"ldap\", \".request_namespace_id\": \"root\", \".request_path\": \"auth/ldap/\"} \nwith python:\nfrom typing import Dict, List\nimport json\n\nlogs = {\"time\": \"2023-02-16T15:57:07.523036823Z\", \"auth\": {\"display_name\": \"root\",  \"policies\": [\"root\"], ' \\\n       '\"policy_results\": {\"allowed\": True, \"granting_policies\": [{\"name\": \"root\", \"namespace_id\": \"root\", ' \\\n       '\"type\": \"acl\"}]}, \"token_type\": \"service\"}, \"request\": {\"operation\": \"help\", \"mount_type\": \"ldap\", ' \\\n       '\"namespace\": {\"id\": \"root\"}, \"path\": \"auth/ldap/\"}}\n\n\npath = '.'\nkeys = {}\n\n\ndef get_dict(json, path):\n    if isinstance(json, Dict):\n        for key in json:\n            p_ath = f'{path}_{str(key).strip()}'\n            if isinstance(json.get(key), Dict):\n                get_dict(json.get(key), p_ath)\n            elif isinstance(json.get(key), List):\n                get_list(json.get(key), p_ath)\n            else:\n                keys.update([(p_ath.replace(\"._\", \".\"), str(json.get(key)).strip())])\n        return keys\n\n\ndef get_list(json, path):\n    if isinstance(json, list):\n        for lkey in json:\n            p_ath = f'{path}_{str(lkey).strip()}'\n            if isinstance(lkey, Dict):\n                get_dict(lkey, str(path).strip())\n            elif isinstance(lkey, List):\n                get_list(lkey, p_ath)\n            else:\n                keys.update([(path.replace(\"._\", \".\"), str(lkey).strip())])\n        return keys\n\n\nfoo = (get_dict(logs, path))\n\nprint(json.dumps(foo))\n\nresponce:\n{\".time\": \"2023-02-16T15:57:07.523036823Z\", \".auth_display_name\": \"root\", \".auth_policies_root\": \"root\", \".auth_policy_results_allowed\": \"True\", \".auth_policy_results_granting_policies_name\": \"root\", \".auth_policy_results_granting_policies_namespace_id\": \"root\", \".auth_policy_results_granting_policies_type\": \"acl\", \".auth_token_type\": \"service\", \".request_operation\": \"help\", \".request_mount_type\": \"ldap\", \".request_namespace_id\": \"root\", \".request_path\": \"auth/ldap/\"}",
        "url": "https://github.com/vectordotdev/vector/discussions/16868",
        "createdAt": "2023-03-20T09:40:22Z",
        "updatedAt": "2023-03-23T13:52:58Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Fritzss"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 16887,
        "title": "Kafka sink metrics to track network traffic",
        "bodyText": "Hello!\nWe're using the Kafka sink to ship logs to Kafka and we're trying to keep track of network traffic this generates. Looking at the documentation there are a couple of different options:\n\nkafka_produced_messages_bytes_total\ncomponent_sent_event_bytes_total\n\nThe values for component_sent_event_bytes_total seem off though. Additionally, when comparing it to component_received_event_bytes_total it seems like the delta between them is constantly growing:\n\nWhy is there such a discrepancy between received and sent? I understand adding metadata and everything but that should be fairly constant, no? Looking here https://github.com/vectordotdev/vector/blob/master/src/sinks/kafka/service.rs#L110 but I might be missing something.\nAnother odd thing we're seeing: the kafka_produced_messages_bytes_total metric doesn't have a component_id label even though the documentation says it should. It's not a problem now since we have a singular use-case to send to Kafka from this particular instance of Vector but that might not always be the case at which point we wouldn't be able to tell the difference.",
        "url": "https://github.com/vectordotdev/vector/discussions/16887",
        "createdAt": "2023-03-21T14:57:37Z",
        "updatedAt": "2023-03-22T16:09:45Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "almarcotte"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16827,
        "title": "Acknowledgement stream shutdown signaling and kafka source rebalance handling",
        "bodyText": "Hi team! A few months ago I opened #14761 and started on a PR implementing a fix (see #14846). My focus at work shifted away to other things for a while but I want to come back to this and start a discussion about the approach I started on in that PR to get some feedback. Rather than resurrect that PR and resolve all of the conflicts that there might be, I'd like to split the changes into a few different logical parts, which I'll describe below.\nHigh-level review of the problem\nFirst, to recap the problem:\n\n\nWhen acknowledgements are enabled and vector receives a shutdown signal, some sources may shut down before processing pending acknowledgements. I've observed this in the kafka and file sources, but this is very likely true for any source that uses a FinalizerSet for acknowledgements (so in addition to file and kafka, the splunk, journald, gcp pubsub, aws sqs, and amqp sources probably have this behavior).\n\nThis happens because the FinalizerSet takes a ShutdownSignal and when that signal arrives it stops processing acknowledgements that may be waiting in its new_entries channel\n\n\n\nFor the kafka source, we see this problem at shutdown as well as a very similar problem that can happen during consumer group rebalance events. In the rebalance case, it is the kafka source's main task which is not able to process acknowledgements during a rebalance, because the kafka client and acknowledgement stream are handled in the same task loop. I believe we can fix both of these at the same time but it will take a little refactoring of the kafka source.\n\n\nProposed changes\nThe first 2 items here could be a single PR, and the basic idea behind this was discussed in the previous PR. The kafka source changes are a little more invasive in order to address the rebalance issue too, and merit a separate PR:\n\n\nUpdate FinalizerSet to accept an Option<ShutdownSignal> and only handle the shutdown signal if it has a Some. All sources that use this will be updated to pass in Some, preserving existing behavior. Unfortunately this can't just be removed, because some sources may be unintentionally relying on the acknowledgement stream ending on the shutdown signal for proper shutdown (e.g. the file source hangs at exit if some other stream handling is not also fixed). See discussion about this in the previous PR here: #14846 (comment)\n\n\nUpdate the file source to pass None to its FinalizerSet and fix stream handling\n\n\nUpdate the kafka source to pass None to its FinalizerSet, and also to handle messages (i.e. drive the main kafka client) and process acknowledgements on separate tasks, so that during a rebalance event, acknowledgements can be handled. Separate tasks are required so that while the kafka client task drives the rebalance process, the acknowledgement task can continue draining acknowledgements\n\n\nDuring a rebalance that revokes partitions from a given vector instance, the acknowledgement task needs to coordinate with the kafka client's rebalance handler\n\n\nIf it can be done without too much performance impact, handling separate acknowledgement streams for each partition is beneficial. A group rebalance can revoke a subset of the partitions a client is handling, and processing acks for each partition through dedicated streams makes it easy to know when all of the acknowledgements for the particular subset being revoked have been processed. Tokio's StreamMap or something like it should be a reasonable thing to use for this, I think\n\n\nThe coordination sequence should look something like this:\n\nKafka client: pre_rebalance handler is notified of a set of partitions being removed\nKafka client: remove and close the sending end of the FinalizerSet for each of the revoking partitions; no new entries can be added for these partitions at this point and any pending acks will be drained in the subsequent steps\nKafka client: send a signal to the acknowledgement task that specific partitions are being revoked. This signal includes the partition information and a rendezvous channel (sender) that the ack task can use to signal back to the client task\nKafka client: poll the rendezvous channel (receiver) - when it's signaled (from the next step), commit consumer offsets. When it's closed, return from the pre_rebalance handler, allowing the rebalance process to proceed\nAck task: upon receiving a rebalance signal with revokcations, do the following:\n\nset a timer that will last no longer than the kafka client's session timeout setting (this timer prevents the consumer group coordinator from thinking this consumer is dead, and kicking it out of the group if the draining takes too long. Consider using something like half the session timeout, or expose a new optional rebalance_drain_ms setting for the kafka source)\nMove the ack streams for the revoking partitions into a set of \"draining\" streams (ignore other partitions that are being retained for now)\nPoll the revoking ack streams and the timer, handling acknowledgements from the draining streams as usual\nWhen all of the revoking ack streams are drained, or the timer fires, signal the kafka client task to commit, and close the rendezvous channel\nIf the timer fired before all acks for the revoking partitions are processed, drop those streams; we did our best but the consumer that gets reassigned these partitions will still re-process those events\n\n\n\n\n\n\n\nI don't have expertise in (or access to infrastructure for testing) all of the other affected sources, they would continue to work as they currently do after the change in 1) but could hopefully be updated at some point by other folks with the knowledge and resources to do so\n\n\nLet me know if this sounds reasonable or if there are any big red flags here. I have most of this implemented against an older version of vector if seeing code helps (https://github.com/vectordotdev/vector/compare/master...jches:vector:kafka-rebalance-ack-draining?diff=split) but I wanted to get some feedback/discussion before getting back into that branch and updating it against the latest commits.",
        "url": "https://github.com/vectordotdev/vector/discussions/16827",
        "createdAt": "2023-03-16T23:20:31Z",
        "updatedAt": "2023-03-21T20:06:04Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "jches"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10780,
        "title": "Vector - Syslog Logs",
        "bodyText": "Hello guys.\nI am trying to capture \"severity\" from a Cisco device. Is this possible to do? Is there a way to capture the header or can I use regex to parse \"Severity notice (5)\".\nThanks,\nRyan\n13:43:26.102526 IP (tos 0x0, ttl 254, id 1342, offset 0, flags [none], proto UDP (17), length 141)\n10.100.1.1.62095 > 10.12.2.241.syslog: [udp sum ok] SYSLOG, length: 113\nFacility local7 (23), Severity notice (5)\nMsg: 1384: *Mar  4 00:49:56.339: %SYS-5-PRIV_AUTH_PASS: Privilege level set to 15 by csroot on vty0 (10.12.2.233)\n0x0000:  3c31 3839 3e31 3338 343a 202a 4d61 7220\n0x0010:  2034 2030 303a 3439 3a35 362e 3333 393a\n0x0020:  2025 5359 532d 352d 5052 4956 5f41 5554\n0x0030:  485f 5041 5353 3a20 5072 6976 696c 6567\n0x0040:  6520 6c65 7665 6c20 7365 7420 746f 2031\n0x0050:  3520 6279 2063 7372 6f6f 7420 6f6e 2076\n0x0060:  7479 3020 2831 302e 3132 2e32 2e32 3333\n0x0070:  29\n{\"host\":\"10.100.1.1\",\"message\":\"<189>1384: *Mar  4 00:49:56.339: %SYS-5-PRIV_AUTH_PASS: Privilege level set to 15 by csroot on vty0 (10.12.2.233)\",\"source_ip\":\"10.100.1.1\",\"source_type\":\"syslog\",\"timestamp\":\"2022-01-10T21:43:26.102749856Z\"}",
        "url": "https://github.com/vectordotdev/vector/discussions/10780",
        "createdAt": "2022-01-10T21:44:39Z",
        "updatedAt": "2023-03-19T00:57:05Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ryansliceup"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 9
        },
        "upvoteCount": 1
    },
    {
        "number": 16813,
        "title": "Vector Agent - Kubernetes source fails to follow configs \"ignore_older_secs: 86400\" and \"read_from: end\"",
        "bodyText": "In Vector Agent the kubernetes source is configured to not replay old log events, but old logs are still being processed.\nusing vector 0.27.0 with helm chart 0.16.3 on kubernetes v1.22.13\nHere is the source stanza from the helm chart - is this configured properly? to pick up logs that are a day old or less and follow events added to the end of the file.\n         sources:\n             kubernetes_logs:\n               type: kubernetes_logs\n               ignore_older_secs: 86400\n               read_from: end\n               self_node_name: ${VECTOR_SELF_NODE_NAME}\n\nThis should prevent older logs from being sourced, right?  Currently prior events, in some cases going back many months, are being processed.   Any suggestions are appreciated\nAlso using journald source with \"since_now: true\" and getting expected results - thanks",
        "url": "https://github.com/vectordotdev/vector/discussions/16813",
        "createdAt": "2023-03-16T00:46:24Z",
        "updatedAt": "2023-03-17T23:14:20Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "EdN-terascope"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 16816,
        "title": "Kafka source missing message",
        "bodyText": "Data flow\nscript -> kafka -> vector -> console\n\nvector version\nvector 0.26.0 (x86_64-unknown-linux-gnu c6b5bc2 2022-12-05)\nvector config\n[sources.prometheus_style_metrics]\ntype = \"kafka\"\nbootstrap_servers = \"172.31.16.247:9092,172.31.15.146:9092\"\ngroup_id = \"vector-script-metric-0\"\ntopics = [ \"scriptMetric\" ]\ncommit_interval_ms = 10\n\n[sinks.print]\ntype = \"console\"\ninputs = [\"prometheus_style_metrics\"]\nencoding.codec = \"text\"\n\nwhen i use script pub 10 messages to kafka script log is as follows\n{\"metric\":{\"counter\":{\"value\":0}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n{\"metric\":{\"counter\":{\"value\":1}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n{\"metric\":{\"counter\":{\"value\":2}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n{\"metric\":{\"counter\":{\"value\":3}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n{\"metric\":{\"counter\":{\"value\":4}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n{\"metric\":{\"counter\":{\"value\":5}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n{\"metric\":{\"counter\":{\"value\":6}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n{\"metric\":{\"counter\":{\"value\":7}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n{\"metric\":{\"counter\":{\"value\":8}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n{\"metric\":{\"counter\":{\"value\":9}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n\nvector just print 3 messages\n{\"metric\":{\"counter\":{\"value\":3}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n{\"metric\":{\"counter\":{\"value\":4}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n{\"metric\":{\"counter\":{\"value\":8}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n\nkafka console consumer print 10 messages\n{\"metric\":{\"counter\":{\"value\":3}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n{\"metric\":{\"counter\":{\"value\":4}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n{\"metric\":{\"counter\":{\"value\":1}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n{\"metric\":{\"counter\":{\"value\":5}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n{\"metric\":{\"counter\":{\"value\":9}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n{\"metric\":{\"counter\":{\"value\":2}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n{\"metric\":{\"counter\":{\"value\":6}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n{\"metric\":{\"counter\":{\"value\":7}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n{\"metric\":{\"counter\":{\"value\":8}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n{\"metric\":{\"counter\":{\"value\":0}},\"kind\":\"incremental\",\"namespace\":\"test\",\"name\":\"test_metric\",\"tags\":{\"script_name\":\"test_prometheus.php\"},\"timestamp\":1678943637}\n\ndescribe consumer group\nTOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                 HOST            CLIENT-ID\nscriptMetric    0          18              18              0               vector-7154169f-5e21-4469-87cd-d5ebc78ebaf3 /172.31.140.85  vector\nscriptMetric    1          38              38              0               vector-7154169f-5e21-4469-87cd-d5ebc78ebaf3 /172.31.140.85  vector\nscriptMetric    2          20              20              0               vector-7154169f-5e21-4469-87cd-d5ebc78ebaf3 /172.31.140.85  vector\nscriptMetric    3          27              27              0               vector-9ff2a462-0394-4835-8bb8-dca95ebcfef7 /172.31.140.85  vector\nscriptMetric    4          22              22              0               vector-9ff2a462-0394-4835-8bb8-dca95ebcfef7 /172.31.140.85  vector\nscriptMetric    5          28              28              0               vector-9ff2a462-0394-4835-8bb8-dca95ebcfef7 /172.31.140.85  vector\n\nlooks like missing message\nIs there something wrong with my configuration?",
        "url": "https://github.com/vectordotdev/vector/discussions/16816",
        "createdAt": "2023-03-16T05:17:10Z",
        "updatedAt": "2023-03-17T04:58:49Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "xxm404"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16709,
        "title": "how to use protobuf decode in source",
        "bodyText": "my message in kafka is encode by protobuf ,  How do I decode the message inside?",
        "url": "https://github.com/vectordotdev/vector/discussions/16709",
        "createdAt": "2023-03-07T07:43:12Z",
        "updatedAt": "2023-03-15T13:19:09Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "xizhimen"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16801,
        "title": "log_to_metric transform errors",
        "bodyText": "Hi,\nWe are using a log_to_metric transform on kubernetes logs as shown below\n# Simplified for readability\nkubernetes_logs_parsed:\n  type: remap\n  inputs:\n  - kubernetes_logs_multiline_reduce\n  source: |-\n    # Sclayr Options\n    if .kubernetes.pod.labels.\"scalyr.logging.dev/forwarding\" == \"enabled\" || .kubernetes.namespace.labels.\"scalyr.logging.dev/forwarding\" == \"enabled\" {\n      # attempt to get token for this log event.\n      # prioritise pod labels over namespace labels\n      .scalyr_options.token = string(.kubernetes.pod.labels.\"scalyr.logging.dev/token\") ?? string(.kubernetes.namespace.labels.\"scalyr.logging.dev/token\") ?? \"\"\n      if .scalyr_options.token != \"\" {\n        .scalyr_options.token, err = decode_base64(.scalyr_options.token)\n        if err == null {\n          .scalyr_options.forwarding = true\n        } else {\n          log({\n            \"msg\":       \"Could not decode sclayr token\",\n            \"namespace\": .kubernetes.namespace.name,\n            \"pod\":       .kubernetes.pod.name,\n          }, level: \"warn\", rate_limit_secs: 60)\n        }\n      } else {\n        log({\n          \"msg\":       \"Scalyr API key missing in labels\",\n          \"namespace\": .kubernetes.namespace.name,\n          \"pod\":       .kubernetes.pod.name,\n        }, level: \"warn\", rate_limit_secs: 60)\n      }\n    }\n\nvolume_metrics_scalyr:\n  type: log_to_metric\n  inputs:\n  - kubernetes_logs_parsed\n  metrics:\n   - type: \"counter\"\n      field: scalyr_options.forwarding\n      name: \"route_scalyr\"\n      namespace: \"vector\"\n      tags:\n        namespace: \"{{kubernetes.namespace.name}}\"\n\nThe vector logs are being flooded with the below error:\n2023-03-14T21:30:22.317925Z ERROR transform{component_kind=\"transform\" component_id=volume_metrics_scalyr component_type=log_to_metric component_name=volume_metrics_scalyr}: vector::internal_events::parser: Internal log [Field does not exist.] is being rate limited.\n2023-03-14T21:30:22.317971Z ERROR transform{component_kind=\"transform\" component_id=volume_metrics_scalyr component_type=log_to_metric component_name=volume_metrics_scalyr}: vector_common::internal_event::component_events_dropped: Internal log [Events dropped] is being rate limited.\n\nVector version: 0.27.0\nWe cannot seem to figure out what is causing this. Any guidance would be much appreciated. Thanks",
        "url": "https://github.com/vectordotdev/vector/discussions/16801",
        "createdAt": "2023-03-14T21:34:34Z",
        "updatedAt": "2023-03-16T16:16:01Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "mattzech"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 2
    },
    {
        "number": 16796,
        "title": "Forking policy",
        "bodyText": "Hello vector team!\nWe are trying to fork the repository to our organization and we get the error:\nYou cannot fork this repository to the selected destination due to a policy.\n\nDo you have some specific restrictions related to forking this repository?\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/16796",
        "createdAt": "2023-03-14T16:18:39Z",
        "updatedAt": "2023-03-16T09:22:17Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "slawomirbabicz"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16543,
        "title": "Defining per-metric buckets in Prometheus Exporter/Remote Write sinks",
        "bodyText": "Is there a way to configure a custom buckets value for specific metrics in the Prometheus Exporter and/or Remote write sinks?\nI'm trying to generate two metrics from log events: one with response times and another with response sizes and these two metrics have pretty different behaviours so fitting both of them on the same bucket configuration doesn't yield good results. I guess I could workaround it by having separate pipelines for each metric with a different remote write sink, but it feels like there has to be a better way. Thanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/16543",
        "createdAt": "2023-02-22T10:23:24Z",
        "updatedAt": "2023-03-14T13:37:24Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ivantopo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16682,
        "title": "Vector memory explodes with recursion",
        "bodyText": "I have a Vector source of type Splunk HEC and the workload that ships data to it sends it to the /services/collector/raw endpoint. The data shipped is newline delimited JSON (fi t helps, it is logs from Cloudflare, using their LogPush to Splunk HEC).\nWhen piping that directly to a sink, Vectors memory usage idles at around 30-40MB. For that I'm using this config:\n[sources.http]\ntype = \"splunk_hec\"\naddress = \"0.0.0.0:8080\"\nvalid_tokens = [ \"${HEC_TOKEN}\" ]\n\n[sinks.queue]\ntype = \"kafka\"\ninputs = [ \"filter\" ]\nbootstrap_servers = \"${KAFKA_BROKERS}\"\ntopic = \"${KAFKA_TOPIC}\"\n\n  [sinks.queue.encoding]\n  codec = \"json\"\n  [sinks.queue.tls]\n  enabled = true\nHowever, since one big blog of newline delimited JSON is of limited use, I want to explode the ndjson to individual events. When doing that with this config, Vectors memory usage explodes to 5GB+\n[sources.http]\ntype = \"splunk_hec\"\naddress = \"0.0.0.0:8080\"\nvalid_tokens = [ \"${HEC_TOKEN}\" ]\n\n[transforms.filter]\ntype = \"remap\"\ninputs = [ \"http\" ]\nsource = '''\n  events = []\n  buf = split!(.message, \"\\n\")\n  map_values(buf) -> |value| {\n    if value != \"\" {\n        temp = {}\n        temp.event = parse_json!(value)\n        events = push(events, temp)\n    }\n  }\n  . = events\n'''\n\n[sinks.queue]\ntype = \"kafka\"\ninputs = [ \"filter\" ]\nbootstrap_servers = \"${KAFKA_BROKERS}\"\ntopic = \"${KAFKA_TOPIC}\"\n\n  [sinks.queue.encoding]\n  codec = \"json\"\n  [sinks.queue.tls]\n  enabled = true\nI've tried replicating it on a local instance of Vector, but I can't seem to replicate it there. The environment where it happens is a Kubernetes cluster running the 0.27.1-debian image (which I also tried using locally to replicate the issue).\nDoes anyone have any idea why the above config would cause memory to explode like that?",
        "url": "https://github.com/vectordotdev/vector/discussions/16682",
        "createdAt": "2023-03-03T10:37:12Z",
        "updatedAt": "2023-03-13T15:35:45Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "MadsRC"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 16767,
        "title": "Exit status of vector when sink does not stop gracefully",
        "bodyText": "Hi:\nI am testing vector job in kubernetes using job workflow on aws eks cluster. When the eks cluster is updated and vector is not able to shutdown the sink, it kills the components as shown in the logs below.  However, it looks like the job is marked a completed and pod is not restarted.\nIt looks like the relevant line is \n  \n    \n      vector/src/topology/running.rs\n    \n    \n         Line 146\n      in\n      8377429\n    \n  \n  \n    \n\n        \n          \n           error!( \n        \n    \n  \n\n.\nI was trying to find out what is the exit status if the vector kills a component that cannot be shutdown gracefully.\nHere are logs from vector:\n2023-03-08T20:18:59.354456Z  INFO vector::topology::running: Shutting down... Waiting on running components. remaining_components=\"my_sink\" time_remaining=\"49 seconds left\" 2023-03-08T20:19:04.355028Z  INFO vector::topology::running: Shutting down... Waiting on running components. remaining_components=\"my_sink\" time_remaining=\"44 seconds left\" 2023-03-08T20:19:09.355224Z  INFO vector::topology::running: Shutting down... Waiting on running components. remaining_components=\"my_sink\" time_remaining=\"39 seconds left\" 2023-03-08T20:19:14.354519Z  INFO vector::topology::running: Shutting down... Waiting on running components. remaining_components=\"my_sink\" time_remaining=\"34 seconds left\" 2023-03-08T20:19:19.354853Z  INFO vector::topology::running: Shutting down... Waiting on running components. remaining_components=\"my_sink\" time_remaining=\"29 seconds left\" 2023-03-08T20:19:24.356994Z  INFO vector::topology::running: Shutting down... Waiting on running components. remaining_components=\"my_sink\" time_remaining=\"24 seconds left\" 2023-03-08T20:19:29.355147Z  INFO vector::topology::running: Shutting down... Waiting on running components. remaining_components=\"my_sink\" time_remaining=\"19 seconds left\" 2023-03-08T20:19:34.354367Z  INFO vector::topology::running: Shutting down... Waiting on running components. remaining_components=\"my_sink\" time_remaining=\"14 seconds left\" 2023-03-08T20:19:39.355347Z  INFO vector::topology::running: Shutting down... Waiting on running components. remaining_components=\"my_sink\" time_remaining=\"9 seconds left\" 2023-03-08T20:19:44.355181Z  INFO vector::topology::running: Shutting down... Waiting on running components. remaining_components=\"my_sink\" time_remaining=\"4 seconds left\" 2023-03-08T20:19:49.355508Z ERROR vector::topology::running: Failed to gracefully shut down in time. Killing components. components=\"my_sink\"",
        "url": "https://github.com/vectordotdev/vector/discussions/16767",
        "createdAt": "2023-03-10T21:01:37Z",
        "updatedAt": "2023-03-11T22:30:13Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mans2singh"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16762,
        "title": "How to include timestamp in a log event?",
        "bodyText": "Currently the timestamp is included inside the log structure:\nlog: { attribute1: data1, timestamp: datetime1}\nHow to make vector to send to Clickhouse as:\nlog: { attribute1: data1}, timestamp: datetime1\nFluent bit can append the time stamp next to the log message\nHow to perform the same thing with Vector?\nhttps://clickhouse.com/blog/nginx-logs-to-clickhouse-fluent-bit\nThank you in advance.",
        "url": "https://github.com/vectordotdev/vector/discussions/16762",
        "createdAt": "2023-03-10T17:17:53Z",
        "updatedAt": "2023-03-11T07:04:30Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "patrickdung"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16698,
        "title": "Arch Linux build fails since 0.27.1 with Rust 1.67.1",
        "bodyText": "The build of the Arch Linux vector package fails since 0.27.1. I'm not sure if it's caused by 0.27.1 or some other dependency update.\nThis is the error I'm getting:\nerror: internal compiler error: no errors encountered even though `delay_span_bug` issued\n\nerror: internal compiler error: broken MIR in Item(WithOptConstParam { did: DefId(0:2573 ~ vector_buffers[92e1]::variants::disk_v2::record::_::{impl#0}::resolve), const_param_did: None }) (after phase change to runtime-optimized) at bb4[53]:\n                                Field projection `(*_4).field[3]` specified type `rkyv::boxed::ArchivedBox<[u8]>`, but actual type is `<rkyv::with::With<rkyv::with::With<&[u8], rkyv::with::RefAsBox>, rkyv::with::CopyOptimize> as rkyv::Archive>::Archived`\n  --> lib/vector-buffers/src/variants/disk_v2/record.rs:46:10\n\nThis is the full build log.\nThis is the build script I'm using. The prepare() and build() functions run in order. Basically it's just this:\ncargo fetch --locked\ncargo build \\\n        --frozen \\\n        --release \\\n        --locked \\\n        --target x86_64-unknown-linux-gnu\nThe following (build-) dependencies are provided:\n\ngcc-libs 12.2.1\nzlib 1.2.13\ncargo / rust 1.67.1\nprotobuf 21.12\npython 3.10.9\nperl 5.36.0\ncmake 3.25.2\n\nAny ideas?",
        "url": "https://github.com/vectordotdev/vector/discussions/16698",
        "createdAt": "2023-03-06T16:35:51Z",
        "updatedAt": "2023-03-06T18:43:50Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "hashworks"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16696,
        "title": "vector source There are too many source files",
        "bodyText": "Use sources as file, hundreds or thousands of files, cpu directly 100%\nWhy doesn't the \"ignore_older_secs\" parameter to file feel valid?\nIt seems that time beyond \"ignore_older_secs\" should not be detected. I don't know whether there is a problem with my other parameter configuration.\ntop metrics:\nPID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n22280 root      20   0  819592  95112  11596 S 118.4  0.3   4:13.89 vector -c vector.toml\n22614 root      20   0  158064   6636   4640 S   5.3  0.0   0:02.56 sshd: root@pts/0\nconf sources:\n[sources.victory_phpapp]\ntype = \"file\"\ninclude = [\"/data/logs/app*.log\"]\nhost_key = \"hostname\"\nignore_older_secs = 60\nmax_line_bytes = 5242880\nWhy load so many files already beyond \"ignore_older_secs\"?",
        "url": "https://github.com/vectordotdev/vector/discussions/16696",
        "createdAt": "2023-03-06T09:34:11Z",
        "updatedAt": "2023-03-06T09:34:12Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "mlonV"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 16667,
        "title": "How can I aggregate logs based on fields",
        "bodyText": "Hello,\nI'm trying to write a configuration to aggregate logs received by UDP by devname, srcip, dstip and dstport fields.\nThe logs I'm working on are Fortinet FW logs.\nExample log :\nFeb  1 13:12:34 <Redacted_IP> <180>date=2023-01-23 time=13:57:22 devname=\"SOME_DEVICE_NAME\" devid=\"...\" logid=\"...\" type=\"...\" subtype=\"...\" level=\"...\" vd=\"...\" eventtime=... tz=\"...\" srcip=1.2.3.4 srcport=1234 srcintf=\"...\" srcintfrole=\"...\" dstip=1.2.3.4 dstport=1234 dstintf=\"...\" dstintfrole=\"...\" sessionid=... proto=... action=\"...\" policyid=... policytype=\"...\" poluuid=\"...\" service=\"...\" dstcountry=\"...\" srccountry=\"...\" trandisp=\"...\" duration=... sentbyte=... rcvdbyte=... sentpkt=... rcvdpkt=... appcat=\"...\" sentdelta=... rcvddelta=...\nIf, in a 10 seconds interval, 8 logs have the same devname, srcip, dstip and dstport, this should be outputed :\nFeb  1 13:12:34 <Redacted_IP> <180>date=2023-01-23 time=13:57:22 devname=\"SOME_DEVICE_NAME\" devid=\"...\" logid=\"...\" type=\"...\" subtype=\"...\" level=\"...\" vd=\"...\" eventtime=... tz=\"...\" srcip=1.2.3.4 srcport=1234 srcintf=\"...\" srcintfrole=\"...\" dstip=1.2.3.4 dstport=1234 dstintf=\"...\" dstintfrole=\"...\" sessionid=... proto=... action=\"...\" policyid=... policytype=\"...\" poluuid=\"...\" service=\"...\" dstcountry=\"...\" srccountry=\"...\" trandisp=\"...\" duration=... sentbyte=... rcvdbyte=... sentpkt=... rcvdpkt=... appcat=\"...\" sentdelta=... rcvddelta=... log_count=8\nHow can I do that ? I tried using the reduce transformation, but the problem is that when I receive 3 same logs and 1 unique log, it outputs only one log, based on the first 3.\nThank you,",
        "url": "https://github.com/vectordotdev/vector/discussions/16667",
        "createdAt": "2023-03-02T15:41:24Z",
        "updatedAt": "2023-03-03T08:07:40Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "nicolasmf"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 2
    },
    {
        "number": 16663,
        "title": "aarch64-unknown-linux-gnu build ERROR",
        "bodyText": "rustc --version \nrustc 1.66.0 (69f9c33d7 2022-12-12)\n\nrustup target list\naarch64-unknown-linux-gnu (installed)\n\nrpm -qa|grep aarch\ngcc-aarch64-linux-gnu-4.8.5-16.el7.1.x86_64\ngcc-c++-aarch64-linux-gnu-4.8.5-16.el7.1.x86_64\nbinutils-aarch64-linux-gnu-2.27-9.el7.1.x86_64\n\n\n[root@mydev vector-0.27.0]# aarch64-linux-gnu-g++ -v\nUsing built-in specs.\nCOLLECT_GCC=aarch64-linux-gnu-g++\nCOLLECT_LTO_WRAPPER=/usr/libexec/gcc/aarch64-linux-gnu/4.8.5/lto-wrapper\nTarget: aarch64-linux-gnu\nConfigured with: ../gcc-4.8.5-20150702/configure --bindir=/usr/bin --build=x86_64-redhat-linux-gnu --datadir=/usr/share --disable-decimal-float --disable-dependency-tracking --disable-gold --disable-libgomp --disable-libmudflap --disable-libquadmath --disable-libssp --disable-nls --disable-plugin --disable-shared --disable-silent-rules --disable-sjlj-exceptions --disable-threads --enable-checking= --enable-gnu-unique-object --enable-initfini-array --enable-languages=c,c++ --enable-linker-build-id --enable-nls --enable-obsolete --enable-targets=all --exec-prefix=/usr --host=x86_64-redhat-linux-gnu --includedir=/usr/include --infodir=/usr/share/info --libexecdir=/usr/libexec --localstatedir=/var --mandir=/usr/share/man --prefix=/usr --program-prefix=aarch64-linux-gnu- --sbindir=/usr/sbin --sharedstatedir=/var/lib --sysconfdir=/etc --target=aarch64-linux-gnu --with-bugurl=http://bugzilla.redhat.com/bugzilla/ --with-linker-hash-style=gnu --with-newlib --with-sysroot=/usr/aarch64-linux-gnu/sys-root --with-system-libunwind --with-system-zlib --without-headers --with-isl=/builddir/build/BUILD/gcc-4.8.5-20150702/isl-install --with-cloog=/builddir/build/BUILD/gcc-4.8.5-20150702/cloog-install\nThread model: single\ngcc version 4.8.5 20150623 (Red Hat 4.8.5-16) (GCC) \n[root@mydev vector-0.27.0]# aarch64-linux-gnu-gcc -v\nUsing built-in specs.\nCOLLECT_GCC=aarch64-linux-gnu-gcc\nCOLLECT_LTO_WRAPPER=/usr/libexec/gcc/aarch64-linux-gnu/4.8.5/lto-wrapper\nTarget: aarch64-linux-gnu\nConfigured with: ../gcc-4.8.5-20150702/configure --bindir=/usr/bin --build=x86_64-redhat-linux-gnu --datadir=/usr/share --disable-decimal-float --disable-dependency-tracking --disable-gold --disable-libgomp --disable-libmudflap --disable-libquadmath --disable-libssp --disable-nls --disable-plugin --disable-shared --disable-silent-rules --disable-sjlj-exceptions --disable-threads --enable-checking= --enable-gnu-unique-object --enable-initfini-array --enable-languages=c,c++ --enable-linker-build-id --enable-nls --enable-obsolete --enable-targets=all --exec-prefix=/usr --host=x86_64-redhat-linux-gnu --includedir=/usr/include --infodir=/usr/share/info --libexecdir=/usr/libexec --localstatedir=/var --mandir=/usr/share/man --prefix=/usr --program-prefix=aarch64-linux-gnu- --sbindir=/usr/sbin --sharedstatedir=/var/lib --sysconfdir=/etc --target=aarch64-linux-gnu --with-bugurl=http://bugzilla.redhat.com/bugzilla/ --with-linker-hash-style=gnu --with-newlib --with-sysroot=/usr/aarch64-linux-gnu/sys-root --with-system-libunwind --with-system-zlib --without-headers --with-isl=/builddir/build/BUILD/gcc-4.8.5-20150702/isl-install --with-cloog=/builddir/build/BUILD/gcc-4.8.5-20150702/cloog-install\nThread model: single\ngcc version 4.8.5 20150623 (Red Hat 4.8.5-16) (GCC)\n\n\n### build\n\nJEMALLOC_SYS_WITH_LG_PAGE=16 cargo build --target=aarch64-unknown-linux-gnu --release\n\n\nerror: failed to run custom build command for `ring v0.16.20`\n\nCaused by:\n  process didn't exit successfully: `/root/vector-0.27.0/target/release/build/ring-acc1622deae2f3ba/build-script-build` (exit status: 101)\n  --- stdout\n  OPT_LEVEL = Some(\"3\")\n  TARGET = Some(\"aarch64-unknown-linux-gnu\")\n  HOST = Some(\"x86_64-unknown-linux-gnu\")\n  cargo:rerun-if-env-changed=CC_aarch64-unknown-linux-gnu\n  CC_aarch64-unknown-linux-gnu = None\n  cargo:rerun-if-env-changed=CC_aarch64_unknown_linux_gnu\n  CC_aarch64_unknown_linux_gnu = None\n  cargo:rerun-if-env-changed=TARGET_CC\n  TARGET_CC = None\n  cargo:rerun-if-env-changed=CC\n  CC = None\n  RUSTC_LINKER = Some(\"aarch64-linux-gnu-gcc\")\n  cargo:rerun-if-env-changed=CROSS_COMPILE\n  CROSS_COMPILE = None\n  cargo:rerun-if-env-changed=CFLAGS_aarch64-unknown-linux-gnu\n  CFLAGS_aarch64-unknown-linux-gnu = None\n  cargo:rerun-if-env-changed=CFLAGS_aarch64_unknown_linux_gnu\n  CFLAGS_aarch64_unknown_linux_gnu = None\n  cargo:rerun-if-env-changed=TARGET_CFLAGS\n  TARGET_CFLAGS = None\n  cargo:rerun-if-env-changed=CFLAGS\n  CFLAGS = None\n  cargo:rerun-if-env-changed=CRATE_CC_NO_DEFAULTS\n  CRATE_CC_NO_DEFAULTS = None\n  DEBUG = Some(\"false\")\n  CARGO_CFG_TARGET_FEATURE = Some(\"neon\")\n\n  --- stderr\n  running \"aarch64-linux-gnu-gcc\" \"-O3\" \"-ffunction-sections\" \"-fdata-sections\" \"-fPIC\" \"-I\" \"include\" \"-Wall\" \"-Wextra\" \"-std=c1x\" \"-Wbad-function-cast\" \"-Wnested-externs\" \"-Wstrict-prototypes\" \"-pedantic\" \"-pedantic-errors\" \"-Wall\" \"-Wextra\" \"-Wcast-align\" \"-Wcast-qual\" \"-Wconversion\" \"-Wenum-compare\" \"-Wfloat-equal\" \"-Wformat=2\" \"-Winline\" \"-Winvalid-pch\" \"-Wmissing-field-initializers\" \"-Wmissing-include-dirs\" \"-Wredundant-decls\" \"-Wshadow\" \"-Wsign-compare\" \"-Wsign-conversion\" \"-Wundef\" \"-Wuninitialized\" \"-Wwrite-strings\" \"-fno-strict-aliasing\" \"-fvisibility=hidden\" \"-fstack-protector\" \"-g3\" \"-DNDEBUG\" \"-c\" \"-o/root/vector-0.27.0/target/aarch64-unknown-linux-gnu/release/build/ring-f3e1eb66bc1f3ec4/out/aes_nohw.o\" \"crypto/fipsmodule/aes/aes_nohw.c\"\n  In file included from include/GFp/base.h:66:0,\n                   from include/GFp/aes.h:52,\n                   from crypto/fipsmodule/aes/aes_nohw.c:15:\n  /usr/lib/gcc/aarch64-linux-gnu/4.8.5/include/stdint.h:9:26: fatal error: stdint.h: No such file or directory\n   # include_next <stdint.h>\n                            ^\n  compilation terminated.\n  thread 'main' panicked at 'execution failed', /root/.cargo/registry/src/github.com-1ecc6299db9ec823/ring-0.16.20/build.rs:656:9\n  note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\nwarning: build failed, waiting for other jobs to finish...",
        "url": "https://github.com/vectordotdev/vector/discussions/16663",
        "createdAt": "2023-03-02T09:49:16Z",
        "updatedAt": "2023-03-02T12:17:45Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "FengZh61"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16662,
        "title": "data enhancements like logstash",
        "bodyText": "such as the jdbc connection to an external database in vector tansforms . for example logstash\u2018s elasticsearch filter or jdbc_static filter.  reference url  https://www.elastic.co/guide/en/logstash/current/lookup-enrichment.html",
        "url": "https://github.com/vectordotdev/vector/discussions/16662",
        "createdAt": "2023-03-02T09:02:37Z",
        "updatedAt": "2023-03-15T13:19:52Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "hzhuchao"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16627,
        "title": "How to debug the aws_cloudwatch_logs sink?",
        "bodyText": "Hi,\nConfig:\n[sources.journald]\ntype = \"journald\"\n\n[sinks.cloudwatch_logs]\ntype = \"aws_cloudwatch_logs\"\ninputs = [\"journald\"]\ncreate_missing_group = true\ncreate_missing_stream = true\ngroup_name = \"test\"\nstream_name = \"test\"\nregion = \"us-east-1\"\ncompression = \"gzip\"\nencoding.codec = \"json\"\n\nOutput with debug enabled (VECTOR_LOG=debug)\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal systemd[1]: Starting Vector...\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.427853Z  INFO vector::app: Internal log rate limit configured. internal_log_rate_secs=10\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: \u221a Loaded [\"/etc/vector/vector.toml\"]\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.429599Z DEBUG vector::topology::builder: Building new source. component=journald\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.430028Z DEBUG vector::topology::builder: Building new sink. component=cloudwatch_logs\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.437812Z DEBUG aws_config::fs_util: loaded home directory src=\"HOME\"\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.437872Z DEBUG load_config_file{file=Default(Config)}: aws_config::profile::parser::source: performing home directory substitution home=\"/var/lib/vector\" path=\"~/.aws/config\"\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.437916Z DEBUG load_config_file{file=Default(Config)}: aws_config::profile::parser::source: home directory expanded before=\"~/.aws/config\" after=\"/var/lib/vector/.aws/config\"\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.437952Z DEBUG load_config_file{file=Default(Config)}: aws_config::profile::parser::source: config file not found path=~/.aws/config\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.437971Z DEBUG load_config_file{file=Default(Config)}: aws_config::profile::parser::source: config file loaded path=Some(\"/var/lib/vector/.aws/config\") size=0\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.438024Z DEBUG load_config_file{file=Default(Credentials)}: aws_config::profile::parser::source: performing home directory substitution home=\"/var/lib/vector\" path=\"~/.aws/credentials\"\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.438046Z DEBUG load_config_file{file=Default(Credentials)}: aws_config::profile::parser::source: home directory expanded before=\"~/.aws/credentials\" after=\"/var/lib/vector/.aws/credentials\"\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.438069Z DEBUG load_config_file{file=Default(Credentials)}: aws_config::profile::parser::source: config file not found path=~/.aws/credentials\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.438084Z DEBUG load_config_file{file=Default(Credentials)}: aws_config::profile::parser::source: config file loaded path=Some(\"/var/lib/vector/.aws/credentials\") size=0\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.457393Z DEBUG vector_core::tls::settings: Fetching system root certs.\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.463618Z DEBUG vector_core::tls::settings: Fetching system root certs.\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.480119Z DEBUG vector_core::tls::settings: Fetching system root certs.\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.486291Z DEBUG vector_core::tls::settings: Fetching system root certs.\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: \u221a Component configuration\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.493384Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}: aws_config::meta::credentials::chain: provider in chain did not provide credentials provider=Environment context=the credential provider was not enabled: environment variable not set (CredentialsNotLoaded(CredentialsNotLoaded { source: \"environment variable not set\" }))\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.493453Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}:load_credentials{provider=Profile}: aws_config::fs_util: loaded home directory src=\"HOME\"\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.493494Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}:load_credentials{provider=Profile}:load_config_file{file=Default(Config)}: aws_config::profile::parser::source: performing home directory substitution home=\"/var/lib/vector\" path=\"~/.aws/config\"\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.493523Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}:load_credentials{provider=Profile}:load_config_file{file=Default(Config)}: aws_config::profile::parser::source: home directory expanded before=\"~/.aws/config\" after=\"/var/lib/vector/.aws/config\"\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.493557Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}:load_credentials{provider=Profile}:load_config_file{file=Default(Config)}: aws_config::profile::parser::source: config file not found path=~/.aws/config\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.493583Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}:load_credentials{provider=Profile}:load_config_file{file=Default(Config)}: aws_config::profile::parser::source: config file loaded path=Some(\"/var/lib/vector/.aws/config\") size=0\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.493615Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}:load_credentials{provider=Profile}:load_config_file{file=Default(Credentials)}: aws_config::profile::parser::source: performing home directory substitution home=\"/var/lib/vector\" path=\"~/.aws/credentials\"\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.493645Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}:load_credentials{provider=Profile}:load_config_file{file=Default(Credentials)}: aws_config::profile::parser::source: home directory expanded before=\"~/.aws/credentials\" after=\"/var/lib/vector/.aws/credentials\"\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.493674Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}:load_credentials{provider=Profile}:load_config_file{file=Default(Credentials)}: aws_config::profile::parser::source: config file not found path=~/.aws/credentials\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.493693Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}:load_credentials{provider=Profile}:load_config_file{file=Default(Credentials)}: aws_config::profile::parser::source: config file loaded path=Some(\"/var/lib/vector/.aws/credentials\") size=0\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.493742Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}: aws_config::meta::credentials::chain: provider in chain did not provide credentials provider=Profile context=the credential provider was not enabled: No profiles were defined (CredentialsNotLoaded(CredentialsNotLoaded { source: NoProfilesDefined }))\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.493795Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}: aws_config::meta::credentials::chain: provider in chain did not provide credentials provider=WebIdentityToken context=the credential provider was not enabled: $AWS_WEB_IDENTITY_TOKEN_FILE was not set (CredentialsNotLoaded(CredentialsNotLoaded { source: \"$AWS_WEB_IDENTITY_TOKEN_FILE was not set\" }))\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.493835Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}: aws_config::meta::credentials::chain: provider in chain did not provide credentials provider=EcsContainer context=the credential provider was not enabled: ECS provider not configured (CredentialsNotLoaded(CredentialsNotLoaded { source: \"ECS provider not configured\" }))\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.493859Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}:load_credentials{provider=Ec2InstanceMetadata}: aws_config::imds::credentials: loading credentials from IMDS\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.494141Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}:load_credentials{provider=Ec2InstanceMetadata}:send_operation{operation=\"get\" service=\"imds\"}:async_map_request{name=\"attach_imds_token\"}:send_operation{operation=\"get-token\" service=\"imds\"}:dispatch: hyper::client::connect::http: connecting to 169.254.169.254:80\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.494958Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}:load_credentials{provider=Ec2InstanceMetadata}:send_operation{operation=\"get\" service=\"imds\"}:async_map_request{name=\"attach_imds_token\"}:send_operation{operation=\"get-token\" service=\"imds\"}:dispatch: hyper::client::connect::http: connected to 169.254.169.254:80\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.495272Z DEBUG hyper::proto::h1::io: flushed 242 bytes\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.496325Z DEBUG hyper::proto::h1::io: parsed 6 headers\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.496345Z DEBUG hyper::proto::h1::conn: incoming body is content-length (56 bytes)\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.496388Z DEBUG hyper::proto::h1::conn: incoming body completed\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.496703Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}:load_credentials{provider=Ec2InstanceMetadata}:send_operation{operation=\"get\" service=\"imds\"}:dispatch: hyper::client::connect::http: connecting to 169.254.169.254:80\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.496883Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}:load_credentials{provider=Ec2InstanceMetadata}:send_operation{operation=\"get\" service=\"imds\"}:dispatch: hyper::client::connect::http: connected to 169.254.169.254:80\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.496979Z DEBUG hyper::proto::h1::io: flushed 307 bytes\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.497665Z DEBUG hyper::proto::h1::io: parsed 8 headers\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.497680Z DEBUG hyper::proto::h1::conn: incoming body is content-length (21 bytes)\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.497699Z DEBUG hyper::proto::h1::conn: incoming body completed\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.497854Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}:load_credentials{provider=Ec2InstanceMetadata}: aws_config::imds::credentials: loaded profile profile=export-s3-kinesis-dev\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.497935Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}:load_credentials{provider=Ec2InstanceMetadata}:send_operation{operation=\"get\" service=\"imds\"}:dispatch: hyper::client::connect::http: connecting to 169.254.169.254:80\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.498482Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}:load_credentials{provider=Ec2InstanceMetadata}:send_operation{operation=\"get\" service=\"imds\"}:dispatch: hyper::client::connect::http: connected to 169.254.169.254:80\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.498589Z DEBUG hyper::proto::h1::io: flushed 328 bytes\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.499196Z DEBUG hyper::proto::h1::io: parsed 8 headers\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.499227Z DEBUG hyper::proto::h1::conn: incoming body is content-length (1586 bytes)\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.499243Z DEBUG hyper::proto::h1::conn: incoming body completed\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.499431Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}:lazy_load_credentials:provide_credentials{provider=default_chain}: aws_config::meta::credentials::chain: loaded credentials provider=Ec2InstanceMetadata\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.499463Z  INFO send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:async_map_request{name=\"retrieve_credentials\"}: aws_credential_types::cache::lazy_caching: credentials cache miss occurred; retrieved new AWS credentials (took 6.121907ms)\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.501041Z DEBUG hyper::client::connect::dns: resolving host=\"logs.us-east-1.amazonaws.com\"\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.502033Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:dispatch: hyper::client::connect::http: connecting to 10.24.74.235:443\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.503667Z DEBUG send_operation{operation=\"DescribeLogGroups\" service=\"cloudwatchlogs\"}:dispatch: hyper::client::connect::http: connected to 10.24.74.235:443\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.506991Z DEBUG hyper::proto::h1::io: flushed 2027 bytes\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.510841Z DEBUG hyper::proto::h1::io: parsed 5 headers\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.510871Z DEBUG hyper::proto::h1::conn: incoming body is content-length (229 bytes)\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.510907Z DEBUG hyper::proto::h1::conn: incoming body completed\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: 2023-02-28T12:18:42.511302Z ERROR vector::topology::builder: msg=\"Healthcheck failed.\" error=DescribeLogGroups failed: service error component_kind=\"sink\" component_type=\"aws_cloudwatch_logs\" component_id=cloudwatch_logs component_name=cloudwatch_logs\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal vector[6501]: x Health check for \"cloudwatch_logs\" failed: DescribeLogGroups failed: service error\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal systemd[1]: vector.service: control process exited, code=exited status=78\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal systemd[1]: Failed to start Vector.\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal systemd[1]: Unit vector.service entered failed state.\nFeb 28 12:18:42 ip-10-24-196-120.ec2.internal systemd[1]: vector.service failed.\n\nMy first reaction is that it must be some sort of permission issue, but aws logs describe-log-groups --region us-east-1 works fine on the same machine (no other credentials defined than the IAM role attached to the EC2 instance, and the role has the CloudWatchAgentServerPolicy attached).\nIs there any additional logs I can enable to make the debugging easier? Any idea that you would suggest?\nWorth mentioing: I have a very similar set up working on another machine, using a different AMI. Could it potentially be related to IMDSv2 not being supported, or something similar?\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/16627",
        "createdAt": "2023-02-28T12:31:32Z",
        "updatedAt": "2023-02-28T15:06:43Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "samidalouche"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 2
    },
    {
        "number": 16579,
        "title": "about \"is being rate limited\"",
        "bodyText": "About \"is being rate limited\"\nsource: \"file\"\nsink: loki\nStarting a vector into loki always causes this error. Why?\nFeb 24 20:23:18 test-server vector[25183]:  2023-02-24T12:23:18.555798Z ERROR sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}:request{request_id=25}: vector_common::internal_event::service:  Internal log [Service call failed. No retries or retries exhausted.] is being rate limited.\nFeb 24 20:23:18 test-server vector[25183]:  2023-02-24T12:23:18.555822Z ERROR sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}:request{request_id=25}: vector_common::internal_event::component_events_dropped:  Internal log [Events dropped] is being rate limited.\nFeb 24 20:23:28 test-server vector[25183]:  2023-02-24T12:23:28.070274Z ERROR sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}:request{request_id=32}: vector::sinks::util::retries:  Internal log [Non-retriable error; dropping the request.] has been rate limited 7 times.\nFeb 24 20:23:28 test-server vector[25183]:  2023-02-24T12:23:28.070314Z ERROR sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}:request{request_id=32}: vector::sinks::util::retries: Non-retriable error;  dropping the request. error=Server responded with an error: 400 Bad Request internal_log_rate_limit=true\nFeb 24 20:23:28 test-server vector[25183]:  2023-02-24T12:23:28.070350Z ERROR sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}:request{request_id=32}: vector_common::internal_event::service:  Internal log [Service call failed. No retries or retries exhausted.] has been rate limited 7 times.\nFeb 24 20:23:28 test-server vector[25183]:  2023-02-24T12:23:28.070358Z ERROR sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}:request{request_id=32}: vector_common::internal_event::service:  Service call failed. No retries or retries exhausted. error=Some(ServerError { code:  400 }) request_id=32 error_type=\"request_failed\" stage=\"sending\" internal_log_rate_limit=true\nFeb 24 20:23:28 test-server vector[25183]:  2023-02-24T12:23:28.070391Z ERROR sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}:request{request_id=32}: vector_common::internal_event::component_events_dropped:  Internal log [Events dropped] has been rate limited 7 times.\nFeb 24 20:23:28 test-server vector[25183]:  2023-02-24T12:23:28.070398Z ERROR sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}:request{request_id=32}: vector_common::internal_event::component_events_dropped:  Events dropped intentional=false count=9 reason=\"Service call failed. No retries or retries exhausted.\"  internal_log_rate_limit=true\nFeb 24 20:23:29 test-server vector[25183]:  2023-02-24T12:23:29.086355Z ERROR sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}:request{request_id=33}: vector::sinks::util::retries:  Internal log [Non-retriable error; dropping the request.] is being rate limited.\nFeb 24 20:23:29 test-server vector[25183]:  2023-02-24T12:23:29.086422Z ERROR sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}:request{request_id=33}: vector_common::internal_event::service:  Internal log [Service call failed. No retries or retries exhausted.] is being rate limited.\nFeb 24 20:23:29 test-server vector[25183]:  2023-02-24T12:23:29.086447Z ERROR sink{component_kind=\"sink\" component_id=loki component_type=loki component_name=loki}:request{request_id=33}: vector_common::internal_event::component_events_dropped:  Internal log [Events dropped] is being rate limited.",
        "url": "https://github.com/vectordotdev/vector/discussions/16579",
        "createdAt": "2023-02-24T12:35:26Z",
        "updatedAt": "2023-02-28T12:26:19Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mlonV"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 16588,
        "title": "Configure dynamic url for http sinks",
        "bodyText": "For the http sinks, the url is like below, the url is static, and could we support dynamic url like:\nhttps://10.22.212.22:9000/:clientId/:deviceId, the clientId and deviceId is got from the messages, for example:https://10.22.212.22:9000/aaaa/id_1111111\n\nWe are deciding on the final solution. Very appreciated for your quick response",
        "url": "https://github.com/vectordotdev/vector/discussions/16588",
        "createdAt": "2023-02-24T23:35:00Z",
        "updatedAt": "2023-03-15T13:19:58Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "janiu-001"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 16556,
        "title": "buffer messages when network is bad and sync the messages to remote when network is good",
        "bodyText": "Hi expert,\nI read the vector doc, it is really a good tool to sync data from source to dest, and the source and sinks support more than 40 types.\nWould you help on a question?\n\nour service are deployed on the customer server, it data could be stored in kafka/rabbitmq\nthe destination is a http server\n\nFrom the guide, it is possible to sync the data from kafka/rabbitmq to http server, but in my case, the network is not good always, our customer only could connect to internet sometimes, could it support by vector? could it work with acknowledgements.enabled?\nVery appreciated for you response",
        "url": "https://github.com/vectordotdev/vector/discussions/16556",
        "createdAt": "2023-02-22T23:01:39Z",
        "updatedAt": "2023-03-15T13:20:12Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "janiu-001"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 0
    },
    {
        "number": 16581,
        "title": "Parsing IIS logs (W3C log file format)",
        "bodyText": "Given log file content is:\n#Software: Microsoft Internet Information Services\n#Version: 1.0\n#Date: 2023-02-23 00:00:01\n#Fields: date time\n2023-02-23 00:00:02\n2023-02-23 00:00:03\n#Software: Microsoft Internet Information Services\n#Version: 1.0\n#Date: 2023-02-24 00:00:01\n#Fields: date cs-method\n2023-02-24 GET\n2023-02-24 POST\n\nDo you see an approach to dynamically detect structure and transform it into messages like this:\n{\"date\": \"2023-02-23\", \"time\": \"00:00:02\"}\n{\"date\": \"2023-02-23\", \"time\": \"00:00:03\"}\n{\"date\": \"2023-02-24\", \"cs-method\": \"GET\"}\n{\"date\": \"2023-02-24\", \"cs-method\": \"POST\"}\nThank you in advance!",
        "url": "https://github.com/vectordotdev/vector/discussions/16581",
        "createdAt": "2023-02-24T14:24:39Z",
        "updatedAt": "2023-02-24T15:21:38Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "yavulan"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16560,
        "title": "About kafka sourse with sasl_ss",
        "bodyText": "this is my vector config\uff1a\ntls.enabled = true\nsasl.enabled = true\nsasl.security.protocol = \"SASL_SSL\"\nsasl.mechanism = \"PLAIN\"\nsasl.username = \"\"\nsasl.password = \"\"\nwhen i run\uff1a vector validate -C .\ni got error! this is the error info:\n2023-02-23T08:37:45.402222Z ERROR rdkafka::client: librdkafka: Global error: SSL (Local: SSL error): sasl_ssl://11.11.11.11\uff1a9092/bootstrap: SSL handshake failed: ssl/statem/statem_clnt.c:1919: error:1416F086:SSL routines:tls_process_server_certificate:certificate verify failed: broker certificate could not be verified, verify that ssl.ca.location is correctly configured or root CA certificates are installed (install ca-certificates package) (after 42ms in state SSL_HANDSHAKE)\nThe kafka service provider told me that I need to download the certificate, and then configure the certificate on the client. I downloaded the certificate, but I don't know where to put it\uff01\uff01",
        "url": "https://github.com/vectordotdev/vector/discussions/16560",
        "createdAt": "2023-02-23T08:43:20Z",
        "updatedAt": "2023-02-27T10:47:59Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "yangshike"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16562,
        "title": "Vector internal logs redirected to file?",
        "bodyText": "Is there a way to redirect the vector generated internal logs to a file? I am not sure if I missed this someplace.",
        "url": "https://github.com/vectordotdev/vector/discussions/16562",
        "createdAt": "2023-02-23T12:40:06Z",
        "updatedAt": "2023-02-23T12:42:44Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bmallya"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16498,
        "title": "Archiving a directory for sinking to a server",
        "bodyText": "Can vector agent archive a directory and sink it off to a server using either socket or websocket? Any hints?\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/16498",
        "createdAt": "2023-02-17T19:14:23Z",
        "updatedAt": "2023-02-22T10:55:28Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "petermp79"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16509,
        "title": "I want to recompile the 0.12.2 version of vevtor, how can I see which version of rust it is using?",
        "bodyText": "#16361\nVector-0.12.2-x86_64-unknown-linux-musl can run on Linux 5.7 with kernel version 2.6.18-274.el5.\nBut it runs with a little error report in use host_metrics.\nFeb 20 11:27:16.730 ERROR source{component_kind=\"source\" component_name=host component_type=host_metrics}: vector::sources::host_metrics: Internal log [Failed to load memory info.] is being rate limited.\nFeb 20 11:27:16.731 ERROR source{component_kind=\"source\" component_name=host component_type=host_metrics}: vector::sources::host_metrics: Internal log [Failed to load/parse network data.] is being rate limited.\nI want to recompile it.\nSo which version of rust should I use, or in which file is the information about the rust version recorded?",
        "url": "https://github.com/vectordotdev/vector/discussions/16509",
        "createdAt": "2023-02-20T03:32:36Z",
        "updatedAt": "2023-02-22T05:33:37Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "FengZh61"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16519,
        "title": "exclude pod or namespace or containers from Kubernates logs",
        "bodyText": "Good day!\nI just read the documentation but didn't understand how to do that\nmay be someone can help me .\nI'm using vector to collect Kubernetes logs. My config by default in configmap.yaml\nin source ad kubernetes_logs\nhow can i exclude logs some containers or namespace ro pods?",
        "url": "https://github.com/vectordotdev/vector/discussions/16519",
        "createdAt": "2023-02-20T17:00:52Z",
        "updatedAt": "2023-03-15T13:21:02Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "therus000"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16486,
        "title": "Passing the source input name to a transform",
        "bodyText": "Is it possible to pass the source name in a transform?\nFor example I have multiple sources for my application (nginx, php error logs, application logs), each have their own source definition. In one transform that takes in all of these as inputs, I want to append some Datadog tags to help identify and process them in Datadog Logs.\nFor ddsource I need to pass the source input's name. In other words, the value should be either webapp, access or php.\n[transforms.ddtags]\ntype = \"remap\"\ninputs = [\"webapp\", \"access\", \"php\"]\nsource = '''\n.ddsource = ???\n\nI just don't know if it's possible to pass the current event's source to the .ddsource tag.\nI apologize in advance if this was asked before or it's an easy docs search, I couldn't find anything from my own searching.",
        "url": "https://github.com/vectordotdev/vector/discussions/16486",
        "createdAt": "2023-02-16T23:12:53Z",
        "updatedAt": "2023-02-17T14:38:52Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "victorjkhalaf"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16479,
        "title": "Redis auth and tls",
        "bodyText": "Hello,\nI am having trouble to understand how to configure connection to redis cluster with tls and auth enabled.\nI looked into the documentation, but did not found anything about specifying auth and tls certs.\nThanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/16479",
        "createdAt": "2023-02-16T18:40:32Z",
        "updatedAt": "2023-02-17T11:19:56Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "idenkov"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16449,
        "title": "How to use client-side load balancing with Vector as source",
        "bodyText": "Hi\nIn the documentation below, it says that vector is capable of client-side load balancing.\nhttps://vector.dev/docs/setup/going-to-prod/architecting/#aggregator-role\n\nHowever, it is difficult to find the client-side load balancing settings on https://vector.dev/docs/reference/configuration/sinks/vector/.\nI would like to know which ones are client load balancing in vector.\nPlease let me know.\nRegards,\n\nEDIT:\nChatGPT said the wrong answer",
        "url": "https://github.com/vectordotdev/vector/discussions/16449",
        "createdAt": "2023-02-15T09:19:09Z",
        "updatedAt": "2023-02-15T12:10:08Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "hannuriha"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16064,
        "title": "Repetitious logs from vector source file",
        "bodyText": "Hi all.\nI think i see some duplication with below configuration from my vector source file reading:\nsources:\n  my-file:\n    type: file\n    ignore_older_secs: 600\n    include:\n      - \"/var/log/test.log\"\n    read_from: beginning\n\ntransforms:\n  parse-test:\n    type: remap\n    inputs:\n      - my-file\n    source: |\n       .tag = \"hello\"\n\n\nsinks:\n  log-forwarder-test:\n    type: http\n    inputs:\n      - parse-test\n    uri: https://test.com/\n\n\nMy problem is that the rate of my log entries in my data sources is too high from reading this log file and also I see log duplication.\nI think enabling health checks or acknowledgments in my vector sink may cause this issue but haven't proven it yet. am I wrong?\nAny suggestion/advice would help.\nthanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/16064",
        "createdAt": "2023-01-22T09:49:10Z",
        "updatedAt": "2023-02-14T14:54:30Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16395,
        "title": "multiline support in vrl",
        "bodyText": "Hi all.\nCan we specify a multiline parser in the transform for multi line logs ?\nsomething like https://docs.fluentd.org/parser/multiline\nThanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/16395",
        "createdAt": "2023-02-11T09:32:16Z",
        "updatedAt": "2023-02-13T10:29:48Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 16278,
        "title": "Is support available for GCP authentication via workload identity federation ?",
        "bodyText": "Is vector able to support type: \"external_account\", for GCP authentication?   Workload identity federation allows service account impersonation via an external identity provider.  It allows authentication using short-lived (sts) access tokens rather than long-lived \"service_account\" keys.\nhttps://cloud.google.com/iam/docs/workload-identity-federation\nWIF is basically the gcp equivalent of using aws (sts) 'role_arn' and 'web_identity_token' for authentication.\nThanks in advance.",
        "url": "https://github.com/vectordotdev/vector/discussions/16278",
        "createdAt": "2023-02-03T19:48:41Z",
        "updatedAt": "2023-02-10T21:07:16Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "cahartma"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 16327,
        "title": "How to use fields in clickhouse sincs (table, database ?)",
        "bodyText": "scenario:\nserver A sends data to a vector on server B\nserver B receives and sends to clickhouse\n(server A .... n, there can be many)\n################\nsrc logs_server cat /etc/vector/logs.toml\ndata_dir = \"/var/lib/vector\"\n\n[acknowledgements]\nenabled = true\n\n[sources.logs_svc]\ntype = \"file\"\nfile_key = \"file\"\nglob_minimum_cooldown_ms = 1_000\nhost_key = \"host\"\nignore_older_secs = 600\ninclude = [\n            \"/var/log/svc/*.log\"\n          ]\n\nmax_line_bytes = 1024000\nmax_read_bytes = 1048576\nread_from = \"beginning\"\ndata_dir = \"/var/lib/vector\"\n\n[transforms.json_svc]\ntype = \"remap\"\ninputs = [\"logs_svc\"]\nsource = \"\"\"\n        . = merge(object!(parse_json!(.message, max_depth: 1)), . )\n        .table = del(.message)\n        .table = \"svc_logs\"\n        table = .table\n         \"\"\"\n[sinks.vector_ch]\n  type = \"vector\"\n  inputs = [ \"json_svc\" ]\n  address = \"10.0.0.123:6000\"\n  compression = true\n  buffer.type = \"memory\"\n  buffer.max_events = 5000\n  buffer.when_full = \"block\"\n  batch.max_events = 1000\n  batch.timeout_secs = 60\n  request.concurrency = \"adaptive\"\n\n#############\ndst_server ckickhouse + vector cat /etc/vector/vector.toml\ndata_dir = \"/var/lib/vector\"\n\n\n[acknowledgements]\nenabled = true\n\n\n# Random Syslog-formatted logs\n[sources.vector]\n  type = \"vector\"\n  version = \"2\"\n  address = \"0.0.0.0:6000\"\n\n\n# Print parsed logs to stdout\n#[sinks.print]\n# type = \"console\"\n# inputs = [\"vector\"]\n# encoding.codec = \"json\"\n\n[sinks.clickhouse_vault]\n  type = \"clickhouse\"\n  inputs = [ \"vector\" ]\n  endpoint = \"http://127.0.0.1:8123\"\n  database = \"logs\"\n  table = {{ table }}\n  encoding.timestamp_format = \"unix\"\n  buffer.type = \"memory\"\n  buffer.max_events = 5000\n  buffer.when_full = \"block\"\n  batch.max_events = 1000\n  batch.timeout_secs = 20\n  skip_unknown_fields = true\n  request.concurrency = \"adaptive\"",
        "url": "https://github.com/vectordotdev/vector/discussions/16327",
        "createdAt": "2023-02-07T12:06:18Z",
        "updatedAt": "2023-02-15T16:44:52Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Fritzss"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16361,
        "title": "Questions about OS compatibility",
        "bodyText": "Which version of vector can run on Linux 5.7 with kernel version 2.6.18-274.el5?\nMy server has several ancient but still productive!",
        "url": "https://github.com/vectordotdev/vector/discussions/16361",
        "createdAt": "2023-02-08T09:53:21Z",
        "updatedAt": "2023-02-09T01:35:10Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "FengZh61"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16274,
        "title": "How to use Log to metric\uff1f",
        "bodyText": "This is my configuration\ndata_dir = \"/opt/vector\"\n[sources.mydev]\ntype = \"file\"\nignore_older_secs = 600\ninclude = [ \"/opt/fluent-bit/metric.log\" ]\nread_from = \"end\"\n[transforms.json]\ntype = \"remap\"\ninputs = [ \"mydev\" ]\nsource = \". = parse_json!(string!(.message))\"\n[transforms.gauge]\ntype = \"log_to_metric\"\ninputs = [ \"json\" ]\n[[transforms.gauge.metrics]]\ntype = \"gauge\"\nfield = \"load_avg_1m\"\n[transforms.gauge.metrics.tags]\nhost = \"{{host}}\"\n\n[[transforms.gauge.metrics]]\ntype = \"gauge\"\nfield = \"load_avg_5m\"\n[transforms.gauge.metrics.tags]\nhost = \"{{host}}\"\n\n[[transforms.gauge.metrics]]\ntype = \"gauge\"\nfield = \"load_avg_15m\"\n[transforms.gauge.metrics.tags]\nhost = \"{{host}}\"\n\n[sinks.print]\ntype = \"console\"\ninputs = [\"gauge\"]\nencoding.codec = \"json\"\n[sinks.prom]\ntype = \"prometheus_remote_write\"\ninputs = [ \"gauge\" ]\nendpoint = \"http://192.168.159.133:9090/api/v1/write\"\ndefault_namespace = \"service\"\nrunning\n[root@mydev config]# ../bin/vector -c gau.toml\n2023-02-03T08:42:51.792741Z  INFO vector::app: Internal log rate limit configured. internal_log_rate_secs=10\n2023-02-03T08:42:51.792870Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=trace,rdkafka=info,buffers=info,lapin=info,kube=info\"\n2023-02-03T08:42:51.792929Z  INFO vector::app: Loading configs. paths=[\"gau.toml\"]\n2023-02-03T08:42:51.815819Z  INFO vector::topology::running: Running healthchecks.\n2023-02-03T08:42:51.816065Z  INFO source{component_kind=\"source\" component_id=mydev component_type=file component_name=mydev}: vector::sources::file: Starting file server. include=[\"/opt/fluent-bit/metric.log\"] exclude=[]\n2023-02-03T08:42:51.816261Z  INFO vector: Vector has started. debug=\"false\" version=\"0.27.0\" arch=\"x86_64\" revision=\"5623d1e 2023-01-18\"\n2023-02-03T08:42:51.816704Z  INFO vector::app: API is disabled, enable by setting api.enabled to true and use commands like vector top.\n2023-02-03T08:42:51.816977Z  INFO vector::topology::builder: Healthcheck passed.\n2023-02-03T08:42:51.817897Z  INFO source{component_kind=\"source\" component_id=mydev component_type=file component_name=mydev}:file_server: file_source::checkpointer: Loaded checkpoint data.\n2023-02-03T08:42:51.818381Z ERROR vector::topology::builder: msg=\"Healthcheck failed.\" error=Unexpected status: 405 Method Not Allowed component_kind=\"sink\" component_type=\"prometheus_remote_write\" component_id=prom component_name=prom\n2023-02-03T08:42:51.818522Z  INFO source{component_kind=\"source\" component_id=mydev component_type=file component_name=mydev}:file_server: vector::internal_events::file::source: Resuming to watch file. file=/opt/fluent-bit/metric.log file_position=5183\nInsert Log\necho '{\"load_avg_15m\": 48.7,\"load_avg_1m\": 78.2,\"load_avg_5m\": 56.2,\"host\": \"10.22.11.222\",\"message\": \"CPU activity sample\"}' >>metric.log\nI can't find the data in prometheus\uff01\nwhen endpoint = \"http://192.168.159.133:9090/\"\n2023-02-03T08:55:20.045388Z ERROR vector::topology::builder: msg=\"Healthcheck failed.\" error=Unexpected status: 302 Found component_kind=\"sink\" component_type=\"prometheus_remote_write\" component_id=prom component_name=prom\nAgain, I didn't find the data in prometheus\nIs there a mistake in my configuration?\ntks\uff01\nVersion & Command\nprometheus, version 2.36.2\nvector 0.27.0 (x86_64-unknown-linux-musl 5623d1e 2023-01-18)\n/opt/prometheus/prometheus --config.file=/opt/prometheus/prometheus.yml --web.read-timeout=5m --web.max-connections=10 --storage.tsdb.retention=15d --storage.tsdb.path=/opt/prometheus/data --query.max-concurrency=20 --query.timeout=2m --web.enable-admin-api --web.enable-lifecycle --web.enable-remote-write-receiver",
        "url": "https://github.com/vectordotdev/vector/discussions/16274",
        "createdAt": "2023-02-03T08:57:03Z",
        "updatedAt": "2023-02-07T01:18:44Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "FengZh61"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16207,
        "title": "Event filtering to different sinks",
        "bodyText": "Hello,\nI have two instances of vector running (one forwarding events(say A) to the second instance (say B)using sinks.vector).\nThe instance A sink gets events from multiple sources, the ones that I am using currently are http, syslog and file\nOnce these events are received on instance B, whats the most efficient way to separate these out again and write them into 3 different files? For ex: if source_type == \"syslog\", write the events into syslogevents.txt, if source_type == \"file\", write it into filevents.txt, else write to others.txt.\nOne way that I can think of doing this is to pass the events through 3 different filter transforms, and then these output would go to 3 different sinks with type as file.  Is there any other way of doing it without passing it through 3 filters?\nAnd say if I have to also append something to the event (like enrichment data), would I be able to do it through the filter transform? Or should I pass it again through a remap transform?\nThanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/16207",
        "createdAt": "2023-01-31T08:42:50Z",
        "updatedAt": "2023-02-06T06:21:54Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bmallya"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16275,
        "title": "Using LuaJIT",
        "bodyText": "Hello,\nIt would be nice to have the benefits of using JIT for transforms written in Lua. mlua crate supports LuaJIT as a feature. Have you considered enabling this feature for Vector build? Is there any reason why LuaJIT is not being used in Vector now? At a minimum, it would be nice to be able to customize Lua version when building Vector. Currently it is hardcoded as lua54.\nThanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/16275",
        "createdAt": "2023-02-03T12:01:00Z",
        "updatedAt": "2023-02-05T03:40:59Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "espdev"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 2
    },
    {
        "number": 16257,
        "title": "How to properly glob k8s logs source?",
        "bodyText": "I'm trying to get my kubernetes_logs source to properly exclude log files I'm not interested in. I have a list of keywords that need to be part of the file path, and it should also not watch files that were logrotated or dated.\nvector v0.24.2\nThe filename keywords I'm trying to include are\n\nk8s-secrets-controller\nzk-bridge\nk8s-vault-updater\nsynapse\nenvoy\n\nTo do this I've added two exclude globs to my source\n\"sources\":\n  \"stdout_logs\":\n    \"exclude_paths_glob_patterns\":\n      - \"**/*[!.log]\"\n      - \"/var/log/pods/*/!(k8s-secrets-controller|zk-bridge|k8s-vault-updater|synapse|envoy)/*\",\n    \"type\": \"kubernetes_logs\"\n\n\nHowever, I'm still seeing vector logs showing it's watching files that should be excluded. I'm not sure if I've done this right.",
        "url": "https://github.com/vectordotdev/vector/discussions/16257",
        "createdAt": "2023-02-02T20:24:50Z",
        "updatedAt": "2023-02-03T19:38:00Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Quyzi"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 16095,
        "title": "error i have got while running vector",
        "bodyText": "While running vector in Kubernetes, everything went smoothly, but a few days later, it began to report these errors, and I had no idea what they were. If anyone can help, that would be deeply appreciated.\nsource{component_kind=\"source\" component_id=syslog component_type=syslog component_name=syslog}: vector::source thread 'vector-worker' panicked at 'assertion failed: min <= max', /rustc/69f9c33d71c871fc16ac445211281c6e7a340943/library/core/src/cmp.rs:860:9 note: run with RUST_BACKTRACE=1 environment variable to display a backtrace thread 'vector-worker' panicked at 'called Result::unwrap() on an Err value: PoisonError { .. }'",
        "url": "https://github.com/vectordotdev/vector/discussions/16095",
        "createdAt": "2023-01-24T10:25:51Z",
        "updatedAt": "2023-02-03T08:19:36Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "muneesMohammed"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 2
    },
    {
        "number": 16219,
        "title": "`http_server` accepting OPTIONS method",
        "bodyText": "Hello!\nIs there a reason the http_server source doesn't handle the OPTIONS method when a request is coming in?\nI'm tinkering with vector and am trying to send info in the browser to vector through a library and that request starts with OPTIONS which fails and vector outputs this info:\n2023-01-31T19:07:08.625049Z ERROR vector::internal_events::http: Internal error. error_type=\"connection_failed\" stage=\"receiving\" internal_log_rate_limit=true\n2023-01-31T19:07:08.650134Z ERROR vector::internal_events::http: Internal log [Internal error.] is being rate limited.\n\nI'm able to send POST through Postman with my config but not from the browser because of this.",
        "url": "https://github.com/vectordotdev/vector/discussions/16219",
        "createdAt": "2023-01-31T19:26:19Z",
        "updatedAt": "2023-01-31T20:27:10Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ijsnow"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16173,
        "title": "Public Vector roadmap",
        "bodyText": "Hi!\nDoes Vector have a publicly accessible roadmap? It would help a lot to people to see, in which direction(s) Vector will evolve in the (near) future. Also, it helps with estimation regarding the most important/fixes (without a commitment ofc - because that's an open source :).\nAnother use-case for the public roadmap - helping the new contributors concentrate their work on the most important stuff from Vector's point of view.\nI am almost sure that Vector dev team already has some plans, they are just discussed internally. From time to time these plans are mentioned in multiple issues, but that's just a piece of the whole picture.\nI understand that for some reason it could be unacceptable for your current workflow (because workflows are complex systems itself with multiple traps) - I just want to start a discussion about it and how it could be implemented.\nI think (and hope) @jszwedko knows more about the topic.",
        "url": "https://github.com/vectordotdev/vector/discussions/16173",
        "createdAt": "2023-01-28T03:05:18Z",
        "updatedAt": "2023-01-30T18:06:15Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "zamazan4ik"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 16109,
        "title": "Vector stops processing Exec streaming logs after ~12hrs from python script command",
        "bodyText": "After about 12 hours, vector just stops processing logs coming in from a python script - running \"vector top\" shows other log sources are still processing, but the counters for the exec source logs just stop and don't increase.\nIf I look at the running processes on the box, I can see there is still a vector pid for the /usr/bin/python3 /etc/vector/scripts/log_script.py.\nIf i run the python script manually and output to a text file on the system, I can see new logs coming through and writing to the log file.\nHere is the vector config:\nsources:\ns_log_test:\ntype: exec\nmode: streaming\ncommand: [\"/etc/vector/scripts/log_script.py\"]\ninclude_stderr: false\nstreaming:\nrespawn_on_exit: true\ntransforms:\nt_log_test:\ntype: remap\ninputs: [s_log_test]\nsource: |\ndel(.source_type)\n. = parse_json!(.message)\n.source_type = \"test\"\n.host = \"hostname.x.x.x\"",
        "url": "https://github.com/vectordotdev/vector/discussions/16109",
        "createdAt": "2023-01-25T00:03:12Z",
        "updatedAt": "2023-01-25T18:20:49Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tegantaylor"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16098,
        "title": "VRL return early",
        "bodyText": "Hi! I see that return is a reserved keyword:\n\nerror[E205]: reserved keyword\n\u250c\u2500 :56:3\n\u2502\n56 \u2502   return\n\u2502   ^^^^^^\n\u2502   \u2502\n\u2502   this identifier name is reserved for future use in the language\n\u2502   use a different name instead\n\u2502\n= see language documentation at https://vrl.dev\n= try your code in the VRL REPL, learn more at https://vrl.dev/examples\n\nWhat are people doing instead of returning early? Using several transforms, with some condition?\nI find the nested if err != null { if err != null { .... a bit annoying.",
        "url": "https://github.com/vectordotdev/vector/discussions/16098",
        "createdAt": "2023-01-24T13:19:10Z",
        "updatedAt": "2023-02-03T21:40:54Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "arve0"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16082,
        "title": "Splunk to vector",
        "bodyText": "Colleagues hello everyone.\nI have this question.\ndid someone set up sending with Splunk Universal Forwarders to kafka ? If yes. please tell me",
        "url": "https://github.com/vectordotdev/vector/discussions/16082",
        "createdAt": "2023-01-23T18:12:55Z",
        "updatedAt": "2023-01-24T17:43:45Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "gitingua"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16056,
        "title": "Unable to pull latest alpine docker image",
        "bodyText": "I'm trying to pull the latest 0.27 Alpine docker image, but I'm get an authorization error:\n$ docker pull timberio/vector:0.27.X-alpine\nError response from daemon: Head \"https://registry-1.docker.io/v2/timberio/vector/manifests/0.27.X-alpine\": unauthorized: please use personal access token to login",
        "url": "https://github.com/vectordotdev/vector/discussions/16056",
        "createdAt": "2023-01-20T23:14:31Z",
        "updatedAt": "2023-02-03T21:40:18Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "irvintim"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 16000,
        "title": "parse multiline xmlns in json",
        "bodyText": "Hi. I have trouble with xmlns parse.\nthere is a json event with nested xmlns field. Example:\n{\"data\":{\"job_id\":\"692db8c2-9d54-11eb-a8b3-0242ac130003\",\"mime\":\"application\\/eventlog\",\"historical\":false,\"tag\":\"wineventlog\",\"input_id\":\"00000000-0000-0000-0000-000000000000\",\"tenant_id\":\"00000000-0000-0000-0000-000000000000\",\"body\":{\"Event\":{\"System\":{\"Version\":\"3\",\"Execution\":{\"ThreadID\":\"2680\",\"ProcessID\":\"1364\"},\"EventRecordID\":\"449939\",\"Channel\":\"Microsoft-Windows-Sysmon\\/Operational\",\"Task\":\"10\",\"EventID\":\"10\",\"Keywords\":\"0x8000000000000000\",\"Level\":\"4\",\"Computer\":\"a-rs.alpha.local\",\"TimeCreated\":{\"SystemTime\":\"2022-12-22T20:43:18.684134900Z\"},\"Provider\":{\"Guid\":\"{5770385f-c22a-43e0-bf4c-06f5698ffbd9}\",\"Name\":\"Microsoft-Windows-Sysmon\"},\"Correlation\":null,\"Security\":{\"UserID\":\"S-1-5-18\"},\"Opcode\":\"0\"},\"EventData\":{\"Data\":[{\"text\":\"-\",\"Name\":\"RuleName\"},{\"text\":\"2022-12-22 20:43:18.674\",\"Name\":\"UtcTime\"},{\"text\":\"{e15a5220-c148-63a4-c68a-000000001c00}\",\"Name\":\"SourceProcessGUID\"},{\"text\":\"3788\",\"Name\":\"SourceProcessId\"},{\"text\":\"3732\",\"Name\":\"SourceThreadId\"},{\"text\":\"C:\\\\Windows\\\\System32\\\\Upfc.exe\",\"Name\":\"SourceImage\"},{\"text\":\"{e15a5220-c166-63a4-c88a-000000001c00}\",\"Name\":\"TargetProcessGUID\"},{\"text\":\"356\",\"Name\":\"TargetProcessId\"},{\"text\":\"C:\\\\Windows\\\\System32\\\\sihclient.exe\",\"Name\":\"TargetImage\"},{\"text\":\"0x1fffff\",\"Name\":\"GrantedAccess\"},{\"text\":\"C:\\\\Windows\\\\SYSTEM32\\\\ntdll.dll+a0fb4|C:\\\\Windows\\\\System32\\\\KERNELBASE.dll+47241|C:\\\\Windows\\\\System32\\\\KERNELBASE.dll+46196|C:\\\\Windows\\\\System32\\\\KERNEL32.DLL+1c2e3|C:\\\\Windows\\\\System32\\\\Upfc.exe+3769|C:\\\\Windows\\\\System32\\\\Upfc.exe+da46|C:\\\\Windows\\\\System32\\\\Upfc.exe+dc3f|C:\\\\Windows\\\\System32\\\\Upfc.exe+16d4|C:\\\\Windows\\\\System32\\\\Upfc.exe+1170c|C:\\\\Windows\\\\System32\\\\KERNEL32.DLL+17974|C:\\\\Windows\\\\SYSTEM32\\\\ntdll.dll+6a271\",\"Name\":\"CallTrace\"},{\"text\":\"NT AUTHORITY\\\\SYSTEM\",\"Name\":\"SourceUser\"},{\"text\":\"NT AUTHORITY\\\\SYSTEM\",\"Name\":\"TargetUser\"}]},\"xmlns\":\"http:\\/\\/schemas.microsoft.com\\/win\\/2004\\/08\\/events\\/event\"}},\"corrections\":{\"expected_datetime_formats\":[\"DATETIME_ISO8601\",\"DATETIME_YYYYMMDD_HHMMSS\"]},\"uuid\":\"45d01c5b-8239-01ed-8b0c-42149f0148a7\",\"recv_time\":\"2022-12-22T20:43:21Z\"},\"id\":\"45d01c5b-8239-01ed-8b0c-42149f0148a7\"} \nVrl don't allowed convert to string this field, apply grok for this field and etc...:\n.xmldata = del(.data.body.Event.EventData.Data)\n.xmldata = to_string!(.xmldata)\n.xmldata = replace(.xmldata, \"[\", \"\") #example\nparse_groks!( .xmldata, pattern)\n\nIn output i see:\nERROR transform{component_kind=\"transform\" component_id=transform-w component_type=remap component_name=transform-w}: vector::internal_events::remap: Mapping failed with event. error=\"function call error for \\\"to_string\\\" at (1813:1833): unable to coerce [{ Name: string, text: string }, { Name: string, text: string }, { Name: string, text: string }, { Name: string, text: string }, { Name: string, text: string }, { Name: string, text: string }, { Name: string, text: string }, { Name: string, text: string }, { Name: string, text: string }, { Name: string, text: string }, { Name: string, text: string }, { Name: string, text: string }, { Name: string, text: string }] into string\" error_type=\"conversion_failed\" stage=\"processing\" internal_log_rate_limit=true\nMain questions:\n\nHow can i parse this field?\nif an error occurs in the configuration file when parsing an event, then the event passes without parsing. How can I make the event honor other transformations in program.vrl ?\nThx.",
        "url": "https://github.com/vectordotdev/vector/discussions/16000",
        "createdAt": "2023-01-18T22:55:09Z",
        "updatedAt": "2023-02-03T21:40:26Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "OlesyaShell"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 14837,
        "title": "syslog source not emitting anything?",
        "bodyText": "I'm trying to get a simple setup running, where vector echoes my syslog.\nHere's what I have:\n[sources.generate_syslog]\n  type = \"syslog\" # must be: \"syslog\"\n  mode = \"tcp\" # example, enum\n  address = \"127.0.0.1:514\"\n\n[transforms.remap_syslog]\ninputs = [ \"generate_syslog\"]\ntype = \"remap\"\nsource = '''\n  structured = parse_syslog!(.message)\n  . = merge(., structured)\n'''\n\n[sinks.emit_syslog]\ninputs = [\"remap_syslog\"]\ntype = \"console\"\nencoding.codec = \"json\"\n\nI run vector with:\nsudo /home/vedantroy/.vector/bin/vector\n\nVector outputs the following logs:\n2022-10-13T23:46:23.276701Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=trace,rdkafka=info,buffers=info,kube=info\"\n2022-10-13T23:46:23.276793Z  INFO vector::app: Loading configs. paths=[\"/etc/vector/vector.toml\"]\n2022-10-13T23:46:23.279731Z  INFO vector::topology::running: Running healthchecks.\n2022-10-13T23:46:23.279986Z  INFO vector: Vector has started. debug=\"false\" version=\"0.24.1\" arch=\"x86_64\" build_id=\"8935681 2022-09-12\"\n2022-10-13T23:46:23.280017Z  INFO vector::app: API is disabled, enable by setting `api.enabled` to `true` and use commands like `vector top`.\n2022-10-13T23:46:23.280137Z  INFO vector::topology::builder: Healthcheck: Passed.\n2022-10-13T23:46:23.280271Z  INFO source{component_kind=\"source\" component_id=generate_syslog component_type=syslog component_name=generate_syslog}: vector::sources::syslog: Listening. addr=127.0.0.1:514 type=\"udp\"\n\nhowever, there is nothing after that. I.e, there's no actual syslog messages.\nIn contrast, tail -f /var/log/syslog does seem to output the latest syslog messages.\nOutput of lsb_release -a:\nNo LSB modules are available.\nDistributor ID:\tUbuntu\nDescription:\tUbuntu 20.04.4 LTS\nRelease:\t20.04\nCodename:\tfocal",
        "url": "https://github.com/vectordotdev/vector/discussions/14837",
        "createdAt": "2022-10-14T00:06:29Z",
        "updatedAt": "2023-01-18T17:19:34Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "vedantroy"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 15960,
        "title": "Fix multiline issue with mysql slow query logs",
        "bodyText": "Hi all,\nI want to ask if you could help or provide a working configuration for fixing multi line slow query logs in vector configurations\ne.g\nThe slow query log is located at /var/log/mysql/slow.log\nMy problem is that i see slow query logs multi line in loki/elasticsearch sink.\nthese logs consumed from kafka source vector\ni do not know how can i fix this multi line issue in vector transform .\ni would appreciate if you could help me on this matter\nthanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/15960",
        "createdAt": "2023-01-14T09:11:29Z",
        "updatedAt": "2023-01-22T09:25:58Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15961,
        "title": "How to use metadata field in templates like `path`?",
        "bodyText": "I have noticed that vector have supported setting metadata field like %kafka.partition = del(.partition), but how can I use this metadata field in the template syntax?\nI want to split file by kafka partition like path = \"/output/%Y-%m-%d-%H-{{ %kafka.partition) }}.json.gz\"",
        "url": "https://github.com/vectordotdev/vector/discussions/15961",
        "createdAt": "2023-01-15T04:23:15Z",
        "updatedAt": "2023-02-03T21:40:39Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "0akarma"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 2
    },
    {
        "number": 15959,
        "title": "Add headers to the CSV output",
        "bodyText": "Hi all,\nI have this requirement to get data from kafka and save it to s3 into csv files.\nWhile waiting for #7124 I tried my hand with remap to convert from json to csv lines and this works. However, I'd like to add the headers on the first line of each file and I have no idea how so...help, please!\nThis is where I got to:\nkeys = [\"a\", \"b\"]\nvalues = []\nfor_each(keys) -> |idx, val| {\n  values = push(values, to_string!(get!(., [val])))\n}\n.message = \"\\\"\" + join!(values, separator: \"\\\",\\\"\") + \"\\\"\"\n\nGiven 2 events like\n{ \"a\": \"foo1\", \"b\": \"bar1\" }\n{ \"a\": \"foo2\", \"b\": \"bar2\" }\n\nthis produces the output\n\"foo1\",\"bar1\"\n\"foo2\",\"bar2\"\n\nWhat I'd like to do is:\n\"a\",\"b\"\n\"foo1\",\"bar1\"\n\"foo2\",\"bar2\"\n\nfor each saved file.\nAny ideas?",
        "url": "https://github.com/vectordotdev/vector/discussions/15959",
        "createdAt": "2023-01-14T07:00:21Z",
        "updatedAt": "2023-01-24T17:27:40Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "terebentina"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15951,
        "title": "Decompression at Source/Remap (GZIP)",
        "bodyText": "Hello!\nI know that vector sinks can compress the data before sending it out. Is there any way that sources or remaps can decompress data before they send it to the next stage?\nIn my case, I have GZIP compressed data available via UDP which I would like to process via Vector. Unfortunately, I have no option of removing compression.",
        "url": "https://github.com/vectordotdev/vector/discussions/15951",
        "createdAt": "2023-01-13T10:27:21Z",
        "updatedAt": "2023-01-13T14:23:28Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "arch-xtof"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15495,
        "title": "Performance consulting",
        "bodyText": "We are interested in Vector, considering using Vector in out production environments. We saw the performance tests posted here:\nhttps://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_to_tcp_performance\nThe Vector's File to TCP  disk thrpt is 76.7MiB/s.\nIs the test running in the VM environments?  And what is the number of this VM CPU cores and memory? (I didn't found the info)",
        "url": "https://github.com/vectordotdev/vector/discussions/15495",
        "createdAt": "2022-12-08T03:48:23Z",
        "updatedAt": "2023-01-12T02:45:42Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "aftersnow"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15866,
        "title": "Received a ServerHelloDone handshake message while expecting CertificateRequest",
        "bodyText": "While upgrading vector version from 0.20 to 0.26, I see a warning message being generated in the logs:\n2023-01-09T12:30:35.784453Z  WARN rustls::check: Received a ServerHelloDone handshake message while expecting [CertificateRequest]\n\nHas anyone else faced this issue?",
        "url": "https://github.com/vectordotdev/vector/discussions/15866",
        "createdAt": "2023-01-09T13:28:04Z",
        "updatedAt": "2023-01-11T14:34:05Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "amanmahajan26"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15802,
        "title": "parse_grok failing on proper ISO8601 timestamp",
        "bodyText": "'lo all,\nI'm trying to parse a timestamp in a field at beginning of line, as such:\nSOURCE is a kafka event:\n{\n  \"@timestamp\": \"2023-01-04T14:22:22.22Z\",\n  \"message\": \"2023-01-04 14:03:30.745 blah blah blah blah\",\n  \"logtype\": \"flatlog\"\n }\n\nAnd my transform looks like this:\n  transforms:\n    parser:\n      type: remap\n      inputs: [kafka]\n      source: |-\n        if .logtype == \"flatlog\" {\n          parse_grok!(.message, \"^%{TIMESTAMP_ISO8601:@timestamp} %{GREEDYDATA:logline}\")\n          .message = del(.logline)\n        }\n        .fingerprint = md5(to_string(.) ?? .@timestamp)\n        .timestamp = to_timestamp!(del(.@timestamp))\n\nWhen this is processed by Vector, it gives following error:\n...\n2023-01-04T13:31:40.423554Z DEBUG transform{component_kind=\"transform\" component_id=parser component_type=remap component_name=parser}: vector::utilization: utilization=0.0000435\n2093853987604\n2023-01-04T13:31:40.423655Z ERROR transform{component_kind=\"transform\" component_id=parser component_type=remap component_name=parser}: vector::internal_events::remap: Mapping failed with event. error=\"function call error for \\\"parse_grok\\\" at (29:108): unable to parse input with grok pattern\" error_type=\"conversion_failed\" stage=\"processing\" internal_log_rate_limit=true\n...\n\nWhat am I missing here ?  It seems to be a valid ISO8601 timestamp, as mentioned here\nAlso, is this efficient, or should I try doing it with VRL conditions ?\nI had also tried using the VRL playground , but it didn't recognize the parse_grok function\nerror[E105]: call to undefined function\n  \u250c\u2500 :2:11\n  \u2502\n2 \u2502           parse_grok!(.message, \"^%{TIMESTAMP_ISO8601:@timestamp} %{GREEDYDATA:logline}\")\n  \u2502           ^^^^^^^^^^\n  \u2502           \u2502\n  \u2502           undefined function\n  \u2502           did you mean \"parse_glog\"?\n  \u2502\n  = learn more about error code 105 at https://errors.vrl.dev/105\n  = see language documentation at https://vrl.dev\n  = try your code in the VRL REPL, learn more at https://vrl.dev/examples\n\ngrtz & thnx\nGert",
        "url": "https://github.com/vectordotdev/vector/discussions/15802",
        "createdAt": "2023-01-04T14:03:06Z",
        "updatedAt": "2023-02-03T21:42:32Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Djeezus"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 15884,
        "title": "Improve vector kafka source performance",
        "bodyText": "Hi all,\ni want to ask if  you could help or provide a working configuration for improving Kafka consuming performance  with vector dev Kafka source.\ni want to know how can i  parallel Kafka consuming or etc.\nmy current configuration:\n        type: kafka\n        bootstrap_servers: kafka.default.svc:9092\n        commit_interval_ms: 2000\n        fetch_wait_max_ms: 1000\n        session_timeout_ms: 20000\n        socket_timeout_ms: 60000\n        group_id: test\n        topics:\n          - test_topic\n\n\nbut my problem is my Kafka lag is too high per topic\nfor example the rate of Kafka produce on specific topic is about 1800 rps and the vector consume with 10 replica is 1200 rps (k8s env)\ncould you help me figure this out . i think there should be a configuration to increase Kafka librdkafka parallelism within vector  or something\nusing vector version 0.25.2\nthanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/15884",
        "createdAt": "2023-01-10T08:20:11Z",
        "updatedAt": "2023-01-14T08:56:50Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15778,
        "title": "Elasticsearch sink issue",
        "bodyText": "Hi.\ni set up kafka ------> vector[es sink] ------> es\nafter 11:40  in screenshot  my data insertion in Elasticsearch became zero in some cases and i don't know why.  after i restarting my vector dev on 18:30 pm the issue seems to be fixed.\n\ni am using vector using 0.26.0 with kafka source and elasticsearch sink\nwould you please help me figure this out\ni never have seen this  before  in previous versions",
        "url": "https://github.com/vectordotdev/vector/discussions/15778",
        "createdAt": "2022-12-29T15:47:41Z",
        "updatedAt": "2023-01-10T08:04:50Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15864,
        "title": "Source->File (Globbing)",
        "bodyText": "Hello,\nIs there a way to include multiple formats of file to be included in the search when this \"source\" is used?\nFor ex: In the /tmp directory, I want to monitor files\n\nThat end with .sh.log\nthat end with _error.log",
        "url": "https://github.com/vectordotdev/vector/discussions/15864",
        "createdAt": "2023-01-09T11:05:03Z",
        "updatedAt": "2023-01-10T04:21:40Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bmallya"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15806,
        "title": "Histogram counter drops to 0 and then returns to the last value during vector pod restart",
        "bodyText": "Hi,\nI am using prometheus_remote_write to send metrics.\nDuring vector pods restart I see strange behavior when the histogram_counter metric is reset to 0 and then shows the last known state.\n\nAny idea what can cause such an outcome?\nConfig of remote_write:\n    [sinks.victoria_local]\n      type = \"prometheus_remote_write\"\n      inputs = [\"vector_metrics_source\"]\n      endpoint = \"http://10.187.0.47:8428/api/v1/write\"\n      buffer.type = \"memory\"\n      buffer.max_events = 60000\n      buckets = [ 0.005, 0.1, 0.5, 1, 3, 20 ]\n      healthcheck.enabled = false\n      default_namespace = \"service\" # required",
        "url": "https://github.com/vectordotdev/vector/discussions/15806",
        "createdAt": "2023-01-04T15:27:29Z",
        "updatedAt": "2023-01-09T18:42:56Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ShukhratE"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15854,
        "title": "\"splunk_hec_logs\" Sink - Timestamp with Nano seconds not appearing in Splunk",
        "bodyText": "Hi,\nI seem to be having an issue where if we specify a java running in a container timestamp format like below:\n<timestampPattern>yyyy-MM-dd'T'HH:mm:ss.SSS'Z'</timestampPattern>\nor\n<timestampPattern>yyyy-MM-dd'T'HH:mm:ss.SSS</timestampPattern>\nPrinting to console shows the time as:\n{\"timestamp\":\"2023-01-06T19:37:15.128Z\"}\nHowever when sent to spunk sink (w/ encoding json), the event is shown as\n2023-01-06T13:37:15.000-06:00 for the _time in the UI - seemingly dropping or not having sent the microseconds.\nOddly enough not setting the timestamp field and letting vector figure out the timestamp shows the ms properly, just i'd like to have the app timestamp as it's more accurate (maybe this isn't even a concern?).\nUsing the docker image timberio/vector:0.25.2-debian\nconfig is:\ndata_dir: \"somedir\"\napi:\n  enabled: true\n  address: \"0.0.0.0:8686\"\n  playground: true\nsources:\n  json_logs:\n    type: \"docker_logs\"\n    include_containers:\n      - somejavacontainer\ntransforms:\n  json_logs_remap:\n    type: remap\n    inputs:\n      - json_logs\n    source: |-\n      labels =\n        .docker_image_sha = .label.COMMIT_SHA\n        .alloc_id = .label.\"com.hashicorp.nomad.alloc_id\"\n        .job_name = .label.\"com.hashicorp.nomad.job_name\"\n        .node_id = .label.\"com.hashicorp.nomad.node_id\"\n        .node_name = .label.\"com.hashicorp.nomad.node_name\"\n        .task_group_name = .label.\"com.hashicorp.nomad.task_group_name\"\n        .task_name = .label.\"com.hashicorp.nomad.task_name\"\n        del(.label)\n      . = merge(., parse_json!(.message)) ?? .\nsinks:\n  out:\n    type: console\n    inputs:\n     - json_logs_remap\n    encoding:\n      codec: \"json\"\n  splunk:\n    type: splunk_hec_logs\n    inputs:\n      - json_logs_remap\n    endpoint: ${SPLUNK_HEC_ENDPOINT}\n    host_key: host\n    compression: gzip\n    index: ${SPLUNK_HEC_INDEX}\n    tls:\n      verify_certificate: false\n    encoding:\n      codec: json\n    healthcheck: true\n    default_token: ${SPLUNK_HEC_TOKEN}\nI am unsure on how to continue to troubleshoot this. Is there a way I can see if this is something I'm doing wrong, or something I need to do on my end?\nThank you!",
        "url": "https://github.com/vectordotdev/vector/discussions/15854",
        "createdAt": "2023-01-06T19:43:27Z",
        "updatedAt": "2023-01-07T02:43:20Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "goatmale"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 15850,
        "title": "Issue with datadog source and decoding.codec = \"json\"",
        "bodyText": "Hello,\nI've an issue with datadog source, json decoding codec and apache log which raised an error about json parsing.\nMy vector.toml config to reproduce :\n[sources.dlogs]\ntype = \"datadog_agent\"\naddress = \"0.0.0.0:8089\"\ndecoding.codec = \"json\"\n\n[sources.stdin]\ntype = \"stdin\"\ndecoding.codec = \"json\"\n\n[sinks.stdout]\ntype = \"console\"\ninputs =  [\"dlogs\",\"stdin\"]\ntarget = \"stdout\"\nencoding.codec = \"json\"\n\ndatadog.yml\nlogs_enabled: true\n\napache.d/conf.yaml\ninit_config:\ninstances:\n  - apache_status_url: http://localhost:8090/server-status?auto\nlogs:\n  - type: file\n    path: /var/log/apache2/other_vhosts_access.log\n    source: apache\n    service: apache\n    sourcecategory: http_web_access\n\noutput when decoding.codec = \"json\" is commented\n# vector -c vector.toml\n2023-01-06T05:31:59.208801Z  INFO vector::app: Internal log rate limit configured. internal_log_rate_secs=10\n2023-01-06T05:31:59.208880Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=trace,rdkafka=info,buffers=info,lapin=info,kube=info\"\n2023-01-06T05:31:59.208933Z  INFO vector::app: Loading configs. paths=[\"vector.toml\"]\n2023-01-06T05:31:59.210314Z  INFO vector::sources::datadog_agent: Building HTTP server. address=0.0.0.0:8089\n2023-01-06T05:31:59.210614Z  INFO vector::topology::running: Running healthchecks.\n2023-01-06T05:31:59.210837Z  INFO vector: Vector has started. debug=\"false\" version=\"0.26.0\" arch=\"x86_64\" revision=\"c6b5bc2 2022-12-05\"\n2023-01-06T05:31:59.210936Z  INFO vector::app: API is disabled, enable by setting `api.enabled` to `true` and use commands like `vector top`.\n2023-01-06T05:31:59.211012Z  INFO vector::sources::file_descriptors: Capturing stdin.\n2023-01-06T05:31:59.211208Z  INFO vector::topology::builder: Healthcheck: Passed.\n{\"ddsource\":\"apache\",\"ddtags\":\"filename:other_vhosts_access.log,sourcecategory:http_web_access\",\"hostname\":\"debian\",\"message\":\"127.0.0.1 - - [06/Jan/2023:06:33:30 +0100] \\\"GET /server-status?auto HTTP/1.1\\\" 200 719 \\\"-\\\" \\\"Datadog Agent/7.41.1\\\"\",\"service\":\"apache\",\"source_type\":\"datadog_agent\",\"status\":\"info\",\"timestamp\":\"2023-01-06T05:33:31.074Z\"}\n{\"ddsource\":\"apache\",\"ddtags\":\"filename:other_vhosts_access.log,sourcecategory:http_web_access\",\"hostname\":\"debian\",\"message\":\"127.0.0.1 - - [06/Jan/2023:06:33:43 +0100] \\\"GET / HTTP/1.1\\\" 200 11012 \\\"-\\\" \\\"Wget/1.21\\\"\",\"service\":\"apache\",\"source_type\":\"datadog_agent\",\"status\":\"info\",\"timestamp\":\"2023-01-06T05:33:44.083Z\"}\n{\"ddsource\":\"apache\",\"ddtags\":\"filename:other_vhosts_access.log,sourcecategory:http_web_access\",\"hostname\":\"debian\",\"message\":\"127.0.0.1 - - [06/Jan/2023:06:33:45 +0100] \\\"GET /server-status?auto HTTP/1.1\\\" 200 722 \\\"-\\\" \\\"Datadog Agent/7.41.1\\\"\",\"service\":\"apache\",\"source_type\":\"datadog_agent\",\"status\":\"info\",\"timestamp\":\"2023-01-06T05:33:45.084Z\"}\n\noutput when dedocing.codec = \"json\" is activated\nvector -c vector.toml\n2023-01-06T05:34:21.520749Z  INFO vector::app: Internal log rate limit configured. internal_log_rate_secs=10\n2023-01-06T05:34:21.520821Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=trace,rdkafka=info,buffers=info,lapin=info,kube=info\"\n2023-01-06T05:34:21.520876Z  INFO vector::app: Loading configs. paths=[\"vector.toml\"]\n2023-01-06T05:34:21.522283Z  INFO vector::sources::datadog_agent: Building HTTP server. address=0.0.0.0:8089\n2023-01-06T05:34:21.522539Z  INFO vector::topology::running: Running healthchecks.\n2023-01-06T05:34:21.522660Z  INFO vector: Vector has started. debug=\"false\" version=\"0.26.0\" arch=\"x86_64\" revision=\"c6b5bc2 2022-12-05\"\n2023-01-06T05:34:21.522675Z  INFO vector::app: API is disabled, enable by setting `api.enabled` to `true` and use commands like `vector top`.\n2023-01-06T05:34:21.522960Z  INFO vector::sources::file_descriptors: Capturing stdin.\n2023-01-06T05:34:21.523187Z  INFO vector::topology::builder: Healthcheck: Passed.\n2023-01-06T05:34:23.682172Z ERROR source{component_kind=\"source\" component_id=dlogs component_type=datadog_agent component_name=dlogs}: vector::internal_events::codecs: Failed deserializing frame. error=Error parsing JSON: Error(\"trailing characters\", line: 1, column: 6) error_type=\"parser_failed\" stage=\"processing\" internal_log_rate_limit=true\n2023-01-06T05:34:29.075815Z ERROR source{component_kind=\"source\" component_id=dlogs component_type=datadog_agent component_name=dlogs}: vector::internal_events::codecs: Internal log [Failed deserializing frame.] is being rate limited.\n2023-01-06T05:34:34.071983Z ERROR source{component_kind=\"source\" component_id=dlogs component_type=datadog_agent component_name=dlogs}: vector::internal_events::codecs: Internal log [Failed deserializing frame.] has been rate limited 1 times.\n2023-01-06T05:34:34.072029Z ERROR source{component_kind=\"source\" component_id=dlogs component_type=datadog_agent component_name=dlogs}: vector::internal_events::codecs: Failed deserializing frame. error=Error parsing JSON: Error(\"trailing characters\", line: 1, column: 6) error_type=\"parser_failed\" stage=\"processing\" internal_log_rate_limit=true\n\nIf I put log level to trace, no more information is printed\nvector -c vector.toml -vv\n2023-01-06T05:35:22.742659Z  INFO vector::app: Internal log rate limit configured. internal_log_rate_secs=10\n2023-01-06T05:35:22.742752Z  INFO vector::app: Log level is enabled. level=\"vector=trace,codec=trace,vrl=trace,file_source=trace,tower_limit=trace,rdkafka=trace,buffers=trace,lapin=trace,kube=trace\"\n2023-01-06T05:35:22.742810Z  INFO vector::app: Loading configs. paths=[\"vector.toml\"]\n2023-01-06T05:35:22.743333Z DEBUG vector::config::loading: No secret placeholder found, skipping secret resolution.\n2023-01-06T05:35:22.743954Z DEBUG vector::topology::builder: Building new source. component=dlogs\n2023-01-06T05:35:22.744279Z  INFO vector::sources::datadog_agent: Building HTTP server. address=0.0.0.0:8089\n2023-01-06T05:35:22.744304Z DEBUG vector::topology::builder: Building new source. component=stdin\n2023-01-06T05:35:22.744491Z DEBUG vector::topology::builder: Building new sink. component=stdout\n2023-01-06T05:35:22.744570Z  INFO vector::topology::running: Running healthchecks.\n2023-01-06T05:35:22.744579Z DEBUG vector::topology::running: Connecting changed/added component(s).\n2023-01-06T05:35:22.744588Z DEBUG vector::topology::running: Configuring outputs for source. component=dlogs\n2023-01-06T05:35:22.744885Z DEBUG vector::topology::running: Configuring output for component. component=dlogs output_id=None\n2023-01-06T05:35:22.744902Z DEBUG vector::topology::running: Configuring outputs for source. component=stdin\n2023-01-06T05:35:22.744911Z DEBUG vector::topology::running: Configuring output for component. component=stdin output_id=None\n2023-01-06T05:35:22.744921Z DEBUG vector::topology::running: Connecting inputs for sink. component=stdout\n2023-01-06T05:35:22.744931Z DEBUG vector::topology::running: Adding component input to fanout. component=stdout fanout_id=dlogs\n2023-01-06T05:35:22.744973Z  INFO vector::topology::builder: Healthcheck: Passed.\n2023-01-06T05:35:22.745035Z DEBUG vector::topology::running: Adding component input to fanout. component=stdout fanout_id=stdin\n2023-01-06T05:35:22.745062Z DEBUG vector::topology::running: Spawning new source. key=dlogs\n2023-01-06T05:35:22.745085Z DEBUG vector::topology::running: Spawning new source. key=stdin\n2023-01-06T05:35:22.745101Z TRACE vector::topology::running: Spawning new sink. key=stdout\n2023-01-06T05:35:22.745116Z DEBUG source{component_kind=\"source\" component_id=dlogs component_type=datadog_agent component_name=dlogs}: vector::topology::builder: Source pump supervisor starting.\n2023-01-06T05:35:22.745136Z DEBUG source{component_kind=\"source\" component_id=dlogs component_type=datadog_agent component_name=dlogs}: vector::topology::builder: Source pump starting.\n2023-01-06T05:35:22.745139Z DEBUG source{component_kind=\"source\" component_id=dlogs component_type=datadog_agent component_name=dlogs}: vector::topology::builder: Source starting.\n2023-01-06T05:35:22.745154Z DEBUG source{component_kind=\"source\" component_id=stdin component_type=stdin component_name=stdin}: vector::topology::builder: Source pump supervisor starting.\n2023-01-06T05:35:22.745177Z DEBUG source{component_kind=\"source\" component_id=stdin component_type=stdin component_name=stdin}: vector::topology::builder: Source pump starting.\n2023-01-06T05:35:22.745184Z DEBUG source{component_kind=\"source\" component_id=stdin component_type=stdin component_name=stdin}: vector::topology::builder: Source starting.\n2023-01-06T05:35:22.745206Z DEBUG sink{component_kind=\"sink\" component_id=stdout component_type=console component_name=stdout}: vector::topology::builder: Sink starting.\n2023-01-06T05:35:22.745362Z  INFO vector::sources::file_descriptors: Capturing stdin.\n2023-01-06T05:35:22.745519Z  INFO vector: Vector has started. debug=\"false\" version=\"0.26.0\" arch=\"x86_64\" revision=\"c6b5bc2 2022-12-05\"\n2023-01-06T05:35:22.745539Z  INFO vector::app: API is disabled, enable by setting `api.enabled` to `true` and use commands like `vector top`.\n2023-01-06T05:35:22.746429Z DEBUG sink{component_kind=\"sink\" component_id=stdout component_type=console component_name=stdout}: vector::utilization: utilization=0.01327304755343417\n2023-01-06T05:35:53.747075Z TRACE vector: Beep.\n2023-01-06T05:35:54.746660Z TRACE vector: Beep.\n2023-01-06T05:35:55.746409Z TRACE vector: Beep.\n2023-01-06T05:35:55.904753Z TRACE source{component_kind=\"source\" component_id=dlogs component_type=datadog_agent component_name=dlogs}: vector::internal_events::http: Bytes received. byte_size=592 http_path=/api/v2/logs protocol=http\n2023-01-06T05:35:55.904900Z ERROR source{component_kind=\"source\" component_id=dlogs component_type=datadog_agent component_name=dlogs}: vector::internal_events::codecs: Internal log [Failed deserializing frame.] has been rate limited 4 times.\n2023-01-06T05:35:55.904923Z ERROR source{component_kind=\"source\" component_id=dlogs component_type=datadog_agent component_name=dlogs}: vector::internal_events::codecs: Failed deserializing frame. error=Error parsing JSON: Error(\"trailing characters\", line: 1, column: 6) error_type=\"parser_failed\" stage=\"processing\" internal_log_rate_limit=true\n2023-01-06T05:35:55.904970Z ERROR source{component_kind=\"source\" component_id=dlogs component_type=datadog_agent component_name=dlogs}: vector::internal_events::codecs: Internal log [Failed deserializing frame.] is being rate limited.\n2023-01-06T05:35:55.904993Z TRACE source{component_kind=\"source\" component_id=dlogs component_type=datadog_agent component_name=dlogs}: vector_common::internal_event::events_received: Events received. count=0 byte_size=2\n2023-01-06T05:35:55.905033Z TRACE source{component_kind=\"source\" component_id=dlogs component_type=datadog_agent component_name=dlogs}: vector_common::internal_event::events_sent: Events sent. count=0 byte_size=0 output=_default\n2023-01-06T05:35:56.746581Z TRACE vector: Beep.\n2023-01-06T05:35:57.747185Z TRACE vector: Beep.\n2023-01-06T05:35:57.747257Z DEBUG sink{component_kind=\"sink\" component_id=stdout component_type=console component_name=stdout}: vector::utilization: utilization=0.00000004693665614420876\n2023-01-06T05:35:58.746515Z TRACE vector: Beep.\n\nAny idea ?",
        "url": "https://github.com/vectordotdev/vector/discussions/15850",
        "createdAt": "2023-01-06T05:37:44Z",
        "updatedAt": "2023-01-06T16:25:41Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "fredericpellin"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 15760,
        "title": "Elasticsearch sink data_stream @timestamp field overwritten or not recognized ?",
        "bodyText": "'lo all,\nmy vector config somehow doesn't transfer the \"@timestamp\" field from the source into the actual \"@timestamp\" field in opensearch document. Vector seems to insert/replace(?) the original Kafka message @timestamp with a timestamp that reflects the time of consumption.\nI'm using following config to stream incoming Kafka messages towards an Opensearch 2.2.x., I originally didn't have a transform-section, but with and without result stays the same.\nVECTOR configuration:\n  sources:\n    kafka:\n...\n  transforms:\n    parse_timestamp:\n      type: remap\n      inputs: [kafka]\n      source: |-\n        .@timestamp = to_timestamp!(.@timestamp)\n\n  sinks:\n    opensearch:\n      type: elasticsearch\n      inputs: [parse_timestamp]\n      endpoints:\n        - http://opensearch-test-cluster-coordinator.opensearch.svc:9200\n      data_stream:\n        auto_routing: false\n        type: vectorstream\n        dataset: test\n      mode: data_stream\n      auth:\n        strategy: basic\n        user: xxxxxx\n        password: xxxxxx\n\nINPUT message example:\n  \"userid\": \"EQ7T90RXXAFLZC1Q\",\n  \"name\": \"Kaila Schenk\",\n  \"@timestamp\": \"2022-12-22T05:56:43.735Z\",\n  \"address\": {\n    \"street\": \"4447 Home Avenue\",\n    \"town\": \"Dalbeattie\",\n    \"postode\": \"GU82 8IB\"\n  },\n...\n}\n\nOUTPUT opensearch through Vector:\n{\n  \"_index\": \".ds-vectorstream-test-default-000004\",\n  \"_id\": \"wIQ1WIUBqxDTR_vq1cg6\",\n  \"_version\": 1,\n  \"_score\": null,\n  \"_source\": {\n    \"@timestamp\": \"2022-12-28T10:10:00.136Z\",\n    \"address\": {\n      \"postode\": \"GU82 8IB\",\n      \"street\": \"4447 Home Avenue\",\n      \"town\": \"Dalbeattie\"\n    },\n    \"data_stream\": {\n      \"dataset\": \"test\",\n      \"namespace\": \"default\",\n      \"type\": \"vectorstream\"\n    },\n    \"description\": \"wonderful kent stream comprehensive troops mw resumes let congressional fin clearly chen secrets appearing ep gage know meetings substances brush chances episodes\",\n    \"email\": \"kristina_saxon@yahoo.com\",\n    \"headers\": {},\n    \"logtype\": \"json\",\n    \"message_key\": null,\n    \"name\": \"Kaila Schenk\",\n    \"offset\": 7700542,\n    \"partition\": 1,\n    \"pets\": [\n      \"Lola\",\n      \"Riley\"\n    ],\n    \"salary\": 16384,\n    \"score\": 6.5,\n    \"source_type\": \"kafka\",\n    \"telephone\": \"+505-5900-068-364\",\n    \"topic\": \"test-benchmark\",\n    \"url\": \"http://www.shadow.com\",\n    \"userid\": \"EQ7T90RXXAFLZC1Q\",\n    \"verified\": true\n  },\n  \"fields\": {\n    \"@timestamp\": [\n      \"2022-12-28T10:10:00.136Z\"\n    ]\n  },\n  \"sort\": [\n    1672222200136\n  ]\n}\n\nNotice the @timestamp is not the one from the Kafka source message.\nAm I missing something in my configuration ?\ngrtz,\nGert",
        "url": "https://github.com/vectordotdev/vector/discussions/15760",
        "createdAt": "2022-12-28T10:15:00Z",
        "updatedAt": "2023-01-02T09:37:12Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Djeezus"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15776,
        "title": "CSV parse: coerce String back to Array",
        "bodyText": "Hello everyone!\nI have a csv file where some fields can be either empty or array. For example:\n2022-12-23 13:01:16;111;['Main','Rule','Velocity'];['routerprotocol','rule','rule'];[2,1,0];[0,0,0];'','VelocityCheck',''];0;;3;-1.00;174;-1;tr-4;13;2;\n\nI'm using function parse_csv that results in array with strings inside, but array fields from csv are also doublequoted as strings.\nCode:\narr = parse_csv!(.message, delimiter: \";\")\n        .=compact(\n            {\n                \"host\" : \"Test\",\n                \"app\" : \"Test\",\n                \"dateTime\" : {{ arr[0]}},\n                \"Id\" : {{ arr[1]}},\n                \"OneFieldWithArr\" : {{ arr[2]}},\n                \"AnotherFieldWithArr\" : {{ arr[3]}},\n....\n\nResult\n{\"app\":\"Protei_SS7FW\",\"dateTime\":\"2022-12-23 13:01:16\",\"host\":\"SS7FW\",\"OneFieldWithArr\":\"['Main','Rule','Velocity']\", \"AnotherFieldWithArr\":\"['routerprotocol','rule','rule']\" ......}\n\nHow can I correctly coerce value back to Array from String?\nThanks for the help in advance!",
        "url": "https://github.com/vectordotdev/vector/discussions/15776",
        "createdAt": "2022-12-29T13:30:28Z",
        "updatedAt": "2022-12-30T08:26:10Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ADovgalyuk"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15726,
        "title": "http sink bad request  (400) error",
        "bodyText": "Hi.\ni set up vector[http]------>fluentd[http] and getting\ncan't convert String into an exact number\n\nerror .\nusing vector 0.26.0 version with json array request.\nis there any way to fix the vector http sink request?",
        "url": "https://github.com/vectordotdev/vector/discussions/15726",
        "createdAt": "2022-12-25T11:39:09Z",
        "updatedAt": "2022-12-29T15:07:52Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15616,
        "title": "Updating hiem with older transitive dependency on nix",
        "bodyText": "I'm new to rust, so maybe I'm mistaken, and if so, sorry for wasting your time.\nVector depends on nix version 0.26.1, and a Vector-supported version of heim depends on ^0.23.0.  When vector is built will the heim module have its own version of nix, or will it use the nix from the enclosing project?\nSpecifically, I'm hoping to get nix updated to remediate nix-rust/nix#1762 which is in nix versions above 0.25.0.",
        "url": "https://github.com/vectordotdev/vector/discussions/15616",
        "createdAt": "2022-12-15T19:09:44Z",
        "updatedAt": "2022-12-29T08:37:17Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "froismo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 15357,
        "title": "Adding support for systemd-journal-gatewayd",
        "bodyText": "Hello all!\nI've been wandering whether you thought about adding systemd-journal-gatewayd as a source in vector? If not, can you achieve something similar using an http_client? Note that my interest has risen from a quite niche situation where i would require to have constant open connections and http_client cannot (at least as far as i've looked into the code) help me.\nThanks in advance!",
        "url": "https://github.com/vectordotdev/vector/discussions/15357",
        "createdAt": "2022-11-26T15:32:02Z",
        "updatedAt": "2022-12-29T00:06:24Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "nikola-milosa"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 15764,
        "title": "Unable to use otel source with ClickHouse Sink",
        "bodyText": "I have the following configuration\ncustomConfig:\n  data_dir: /vector-data-dir\n  api:\n    enabled: true\n    address: 127.0.0.1:8686\n    playground: false\n  sources:\n    otel:\n      type: opentelemetry\n      acknowledgements:\n        enabled: false\n      grpc:\n        address: \"0.0.0.0:4317\"\n      http:\n        address: \"0.0.0.0:4318\"\n  sinks:\n    clickhouse:\n      type: clickhouse\n      inputs:\n        - otel\n      database: default\n      endpoint: \"https://somewhere:8443\"\n      table: fluent_vector_logs\n      compression: gzip\n      auth:\n        password: blach\n        strategy: basic\n        user: default\n      batch:\n        timeout_secs: 5\n        max_events: 1000\n        max_bytes: 1048576\n      skip_unknown_fields: true\n\nusing the helm chart for vector.\nThis results in the following error:\n2022-12-28T15:25:14.828011Z  INFO vector::app: Internal log rate limit configured. internal_log_rate_secs=10\n2022-12-28T15:25:14.828242Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=trace,rdkafka=info,buffers=info,lapin=info,kube=info\"\n2022-12-28T15:25:14.828319Z  INFO vector::app: Loading configs. paths=[\"/etc/vector\"]\n2022-12-28T15:25:14.829877Z ERROR vector::cli: Configuration error. error=Input \"otel\" for sink \"clickhouse\" doesn't match any components.\n\nWorks with other sources. It seems almost like the otel source is recognized but not configured.",
        "url": "https://github.com/vectordotdev/vector/discussions/15764",
        "createdAt": "2022-12-28T15:34:22Z",
        "updatedAt": "2022-12-28T17:09:23Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "gingerwizard"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15218,
        "title": "tag_cardinality_limit per metric",
        "bodyText": "Hi,\nWe are starting to use tag_cardinality_limit transform to enforce some cardinality controls on our tags.\nVector is setup to receive metrics from dozens of applications. Vector seems to be printing the cardinality metric like this:\n{\n  \"name\": \"value_limit_reached_total\",\n  \"namespace\": \"vector\",\n  \"tags\": {\n    \"component_id\": \"drop_high_cardinality_tags\",\n    \"component_kind\": \"transform\",\n    \"component_name\": \"drop_high_cardinality_tags\",\n    \"component_type\": \"tag_cardinality_limit\"\n  },\n  \"timestamp\": \"2022-11-15T14:32:51.657818181Z\",\n  \"kind\": \"absolute\",\n  \"counter\": {\n    \"value\": 1\n  }\n},\n{\n  \"name\": \"tag_value_limit_exceeded_total\",\n  \"namespace\": \"vector\",\n  \"tags\": {\n    \"component_id\": \"drop_high_cardinality_tags\",\n    \"component_kind\": \"transform\",\n    \"component_name\": \"drop_high_cardinality_tags\",\n    \"component_type\": \"tag_cardinality_limit\"\n  },\n  \"timestamp\": \"2022-11-15T14:32:51.657818181Z\",\n  \"kind\": \"absolute\",\n  \"counter\": {\n    \"value\": 1080\n  }\n}\n\nLooking at the internal logs, I am getting entries like this:\n{\n  \"timestamp\": \"2022-11-15T14:31:46.054273Z\",\n  \"level\": \"DEBUG\",\n  \"message\": \"Rejecting tag after hitting configured value_limit.\",\n  \"tag_key\": \"name\",\n  \"tag_value\": \"7bfadae4ec1fc4eb7247fedd385641314940085384283b7b45df5d4f6e8a51a1\",\n  \"internal_log_rate_secs\": 10,\n  \"target\": \"vector::internal_events::tag_cardinality_limit\",\n  \"span\": {\n    \"component_id\": \"drop_high_cardinality_tags\",\n    \"component_kind\": \"transform\",\n    \"component_name\": \"drop_high_cardinality_tags\",\n    \"component_type\": \"tag_cardinality_limit\",\n    \"name\": \"transform\"\n  },\n  \"spans\": [\n    {\n      \"component_id\": \"drop_high_cardinality_tags\",\n      \"component_kind\": \"transform\",\n      \"component_name\": \"drop_high_cardinality_tags\",\n      \"component_type\": \"tag_cardinality_limit\",\n      \"name\": \"transform\"\n    }\n  ]\n}\nAs you can see, the above log entry just says tag_key: name which is the most common tag name. How will I know which metric its coming from?\nThis implies that Vector is just counting cardinality for a label regardless of which metric its coming from. Is this supposed to be for tag cardinality per metric or is it independent of the metric?\nIf its the later then it doesn't really seem to be useful. I would appreciate you help in understanding this transform and its correct usage.\n\nWhat I am really looking for is how will I determine which metric is emitting a high cardinality tag and then talk to the relevant team to fix it.\n\nKind regards,\nNas",
        "url": "https://github.com/vectordotdev/vector/discussions/15218",
        "createdAt": "2022-11-15T14:41:36Z",
        "updatedAt": "2022-12-27T17:01:31Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "NasAmin"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15629,
        "title": "How to send a POST http with request body",
        "bodyText": "With the Sources type \"http_client\", how to send a POST http with request body. I know how to send a \"GET\" request, but I do not know which filed to construct POST request body.  I am trying to send a POST request with the request body to obtain the returned json results.",
        "url": "https://github.com/vectordotdev/vector/discussions/15629",
        "createdAt": "2022-12-18T05:47:19Z",
        "updatedAt": "2022-12-26T08:45:14Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "lvbaiquan12"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 15725,
        "title": "kubernetes_log error for self signed certificate",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nI deployed a vector daemonset in my kubernetes cluster. but when i see the log output, i get error bellow:\n2022-12-02T16:11:08.077716Z  WARN vector::kubernetes::reflector: Watcher Stream received an error. Retrying. error=InitialListFailed(HyperError(hyper::Error(Connect, Ssl(Error { code: ErrorCode(1), cause: Some(Ssl(ErrorStack([Error { code: 337047686, library: \"SSL routines\", function: \"tls_process_server_certificate\", reason: \"certificate verify failed\", file: \"ssl/statem/statem_clnt.c\", line: 1919 }]))) }, X509VerifyResult { code: 18, error: \"self signed certificate\" }))))\n2022-12-02T16:11:08.077763Z DEBUG HTTP{http.method=GET http.url=https://kubernetes.default.svc/api/v1/namespaces?&labelSelector=vector.dev%2Fexclude%21%3Dtrue otel.name=\"list\" otel.kind=\"client\"}: kube_client::client::builder: requesting\n2022-12-02T16:11:08.084769Z ERROR HTTP{http.method=GET http.url=https://kubernetes.default.svc/api/v1/pods?&fieldSelector=spec.nodeName%3D172.16.0.54&labelSelector=vector.dev%2Fexclude%21%3Dtrue otel.name=\"list\" otel.kind=\"client\" otel.status_code=\"ERROR\"}: kube_client::client::builder: failed with error error trying to connect: error:1416F086:SSL routines:tls_process_server_certificate:certificate verify failed:ssl/statem/statem_clnt.c:1919: (self signed certificate)\nmy k8s cluster is deployed by self signed certificate.\nhow can i config vector to ignore he ssl verify?\nConfiguration\nmy configmap content is :\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vector\n  labels:\n    app.kubernetes.io/name: vector\n    app.kubernetes.io/instance: vector\n    app.kubernetes.io/component: Agent\n    app.kubernetes.io/version: \"0.24.0-alpine\"\ndata:\n  agent.yaml: |\n    data_dir: /vector-data-dir\n    api:\n      enabled: false\n      address: 127.0.0.1:8686\n      playground: false\n    sources:\n      kubernetes_logs:\n        type: kubernetes_logs\n    sinks:\n      stdout:\n        type: console\n        inputs: [kubernetes_logs]\n        encoding:\n          codec: json\n\nVersion\nvector 0.24.0 (x86_64-unknown-linux-musl 43267b9 2022-08-30)\nDebug Output\nNo response\nExample Data\nNo response\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/15725",
        "createdAt": "2022-12-02T16:36:04Z",
        "updatedAt": "2022-12-23T23:02:38Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "yuanqixun"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15698,
        "title": "Assume role with aws s3 source",
        "bodyText": "I'm wondering if I'm missing something with the auth.assume role option for an S3 source?\nThis is our vector.toml file:\n[sources.mlp-data-lake]\n    type = \"aws_s3\"\n    region = \"$REGION\"\n    auth.assume_role = \"$IAM_ROLE\" \n    sqs.queue_url = \"$SQS_QUEUE\"\n\n[sinks.opensearch]\n    type = \"elasticsearch\" # required\n    inputs = [\"mlp-data-lake\"] # required\n    auth.strategy = \"aws\"\n    aws.region = \"$REGION\"\n    endpoints = [\"$OS_DOMAIN\"]\n\n\nThis works locally without assume_role (we use SSO locally), but in our cluster we're baffled. Assume role simply doesn't work. We worked around this by creating an IAM User and just adding the AKID/SKID to the env variables on the pod, but we couldn't figure out why the assume_role option wasn't working. We're able to assume the role from the CLI when we exec into the pod, we also tried creating a k8s service account that assumes the role. The role does have the correct permissions in its access policy.\nBut it kept giving us this error:\n\nThe credential provider was not enabled: no providers in chain provided credentials\n\nuntil we created the service account, then the error changed to:\n\nAn error occurred while loading credentials: Error { code: \"AccessDenied\", message: \"Not authorized to perform sts:AssumeRoleWithWebIdentity\"\n\nBut we triple checked this iam role access policy it definitely has this permission.\nI was wondering where assume_role falls into this hierarchy in the docs:\n\nVector checks for AWS credentials in the following order:\n\nThe access_key_id and secret_access_key options.\nThe AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables.\nThe AWS credentials file (usually located at ~/.aws/credentials).\nThe IAM instance profile (only works if running on an EC2 instance with an instance profile/role). Requires IMDSv2 to be enabled. For EKS, you may need to increase the metadata token response hop limit to 2.\n\n\nLike am I missing something here, do we still need one of these when assuming a role?\nHas anyone else run into this issue with auth.assume_role and found a solution?",
        "url": "https://github.com/vectordotdev/vector/discussions/15698",
        "createdAt": "2022-12-22T17:16:06Z",
        "updatedAt": "2022-12-22T17:20:45Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ambroserb3"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15601,
        "title": "Parsing CSV with multi line (postgres csvlog)",
        "bodyText": "Hi,\nI have postgres log using csvlog such as:\n2022-12-14 09:48:53.026 CET,\"myuser\",\"postgres\",270752,\"[local]\",63998df5.421a0,2,\"authentication\",2022-12-14 09:48:53 CET,10/2532,0,LOG,00000,\"connection authorized: user=myuser database=postgres application_name=psql\",,,,,,,,\"PerformAuthentication, postinit.c:292\",\"\",\"client backend\"\n2022-12-15 04:41:25.924 CET,\"writeuser\",\"mydb\",128237,\"10.4.3.9:55578\",639a918c.1f4ed,3,\"idle\",2022-12-15 04:16:28 CET,,0,LOG,00000,\"disconnection: session time: 0:24:57.588 user=writeuser database=mydb host=10.4.3.9 port=55578\",,,,,,,,\"log_disconnections, postgres.c:4767\",\"master_conn\",\"client backend\"\n2022-12-15 04:41:20.727 CET,\"writeuser\",\"mydb\",243567,\"10.2.3.2:42096\",639a938d.3b76f,16,\"INSERT\",2022-12-15 04:25:01 CET,94/42049981,1759425267,ERROR,23505,\"duplicate key value violates unique constraint \"\"col_unique\"\"\",\"Key (col1)=(\\x9busbdhd) already exists.\",,,,,\"WITH par AS (INSERT INTO table (col1\n\t\t\tVALUES ($1)\n\t\t\tON CONFLICT (col2 DO UPDATE SET\n\t\t\t\tcol2 = EXCLUDED.col2\n\t\t\tRETURNING col2\n\t\t)\n\n\n\tinsert into tab2\n\t(col1,col2,col3,col4,col5,col6,col7,alias1,alias2,alias3,alias4)\n\tselect 1 as col1, $2,$3,$4,$5,$6,\n\n\t\t\t(SELECT col FROM par),\n\n\n\t        (select new.col4 from (values\n\n\t\t          ($7)\n\t\t         ,\n\t\t          ($8)\n\t\t         ,\n\t\t          ($9)\n\t\t         ,\n\t\t          ($10)\n\t\t         ,\n\t\t          ($11)\n\n\t\t\t   ) as new(col4) left join tab2 a on new.col4 = a.col3 where a.col4 isnull limit 1) as col5,\n\n\t        \t(select new.col2 from (values\n\n\t\t          ($12)\n\t\t         ,\n\t\t          ($13)\n\t\t         ,\n\t\t          ($14)\n\t\t         ,\n\t\t          ($15)\n\t\t         ,\n\t\t          ($16)\n\n\t\t\t   ) as new(col3) left join tab1 a on new.col2 = a.col3 where a.col4 isnull limit 1)\n\n\t          as alias2,\n\n\n\t        NULL\n\n\t        as alias3,\n\n\n\t        NULL\n\n\t         as alias4\n\t\t\t\",,\"_bt_check_unique, nbtinsert.c:656\",\"master_conn\",\"client backend\"\n2022-12-15 04:41:26.050 CET,\"writeuser\",\"mydb\",130056,\"10.40.31.116:50652\",639a92bd.1fc08,392,\"SELECT\",2022-12-15 04:21:33 CET,679/227106160,0,LOG,00000,\"duration: 3442.218 ms  execute <unnamed>: SELECT col1 FROM\n      newtable\n     WHERE col1=$1 AND col2=$2\n        group by col2\",\"parameters: $1 = '4', $2 = 'GB'\",,,,,,,\"exec_execute_message, postgres.c:2265\",\"master_conne\",\"client backend\"\n2022-12-09 14:15:13.093 CET,\"userZ\",\"dbA\",231892,\"10.1.3.15:55124\",639326d5.389d4,11,\"SELECT\",2022-12-09 13:15:17 CET,7/0,0,LOG,X0002,\"duration: 1661.790 ms\",,,,,,,,\"exec_simple_query, postgres.c:1324\",\"psql\",\"client backend\"\n\nI tried parsing this with parse_csv, but multi line doesn't work with this configuration, I believe it is stated parse_csv only supports one liner:\n[sources.postgres_csv]\ntype = \"file\"\ninclude = [ \"postgresql*.csv\" ]\nread_from = \"beginning\"\n\n[transforms.pg_parsed]\ntype = \"remap\"\ninputs = [ \"postgres_csv\" ]\nsource = \"\"\"\n  line = parse_csv!(.message)\n  .timestamp = line[0]\n  .username = line[1]\n  .dbname = line[2]\n  .pid = line[3]\n  .client_addr = line[4]\n  .session_id = line[5]\n  .session_line_num = line[6]\n  .command_tag = line[7]\n  .session_start_time = line[8]\n  .virtual_transaction_id = line[9]\n  .transaction_id = line[10]\n  .severity = line[11]\n  .sqlcode = line[12]\n  .message = line[13]\n  .detail = line[14]\n  .hint = line[15]\n  .internal_query = line[16]\n  .internal_query_pos = line[17]\n  .context = line[18]\n  .query = line[19]\n  .query_pos = line[20]\n  .location = line[21]\n  .application_name = line[22]\n\"\"\"\n\nI also tried following this page https://vector.dev/guides/advanced/parsing-csv-logs-with-lua/ and https://vector.dev/guides/advanced/merge-multiline-logs-with-lua/ with no luck. I keep getting this error, my suspicion is with column naming(?):\nERROR transform{component_kind=\"transform\" component_id=pg_lua component_type=lua component_name=pg_lua}: \nvector::internal_events::lua: Error in building lua script. error=RuntimeErrorHooksProcess { source: RuntimeError(\"[string \\\"src/transforms/lua/v2/mod.rs:196:9\\\"]:7: \ntable index is nil\\nstack traceback:\\n\\t[C]: \nin metamethod 'newindex'\\n\\t[string \\\"src/transforms/lua/v2/mod.rs:196:9\\\"]:7: in function <[string \\\"src/transforms/lua/v2/mod.rs:196:9\\\"]:1>\") }\nerror_type=\"script_failed\" error_code=\"runtime_error_hook_process\" stage=\"processing\" internal_log_rate_limit=true\n\nIf anyone can help or has an example I can follow, I would really appreciate it.\nThank you.\nPS: I am not a programmer",
        "url": "https://github.com/vectordotdev/vector/discussions/15601",
        "createdAt": "2022-12-15T11:20:42Z",
        "updatedAt": "2022-12-16T08:45:32Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "masaiful"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 15600,
        "title": "Modify counter behaviour",
        "bodyText": "Is there any way to create a counter with \"log_to_metric\" that gets the sum of a value instead of getting the number of occurrences of a log? I have a log with turnAroundTime that I want to have in counter format",
        "url": "https://github.com/vectordotdev/vector/discussions/15600",
        "createdAt": "2022-12-15T07:59:03Z",
        "updatedAt": "2023-02-03T21:42:54Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "garceger"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15275,
        "title": "dynamic topic kafka with condition",
        "bodyText": "I have a question that:\nI'm source from http and sink to kafka\nI want when I send log to http have path is: / => log is sent to kafka with topic is named:\nEx:\n/amg.a => topicA\n/amg.b => topicB\n/amg.c => topicC\nHow to do this ?",
        "url": "https://github.com/vectordotdev/vector/discussions/15275",
        "createdAt": "2022-11-17T02:50:13Z",
        "updatedAt": "2022-12-14T15:17:24Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "taquanghung17051999"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15291,
        "title": "Telemetry metrics",
        "bodyText": "Hi there, I am new to vector and still struggling with syntaxes etc. I am trying to figure out how does one go about using the telemetry metrics (component_received_event_bytes_total, component_received_events_count etc...).\nCould someone please post a simple example?\nThanks in advance!",
        "url": "https://github.com/vectordotdev/vector/discussions/15291",
        "createdAt": "2022-11-17T21:48:24Z",
        "updatedAt": "2022-12-14T07:31:07Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "erpiterora"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 1
    },
    {
        "number": 15540,
        "title": "Is there any way to catch the Vector instance Crash case",
        "bodyText": "Hello Team,\nWe have a case where want to introduce the Service level redundancy in our system. We wanted to switch to the redundant server's vector network as soon as there is a crash in the primary server's vector network. In this case we wanted to understand if there is a way to figure out to catch the crash case of vector instance, which would help us to switch from primary to secondary.",
        "url": "https://github.com/vectordotdev/vector/discussions/15540",
        "createdAt": "2022-12-12T06:52:50Z",
        "updatedAt": "2023-02-03T21:42:41Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "hemanthofficial3009"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15530,
        "title": "File source latency",
        "bodyText": "I've been exploring using Vector to ship logs from a PHP(-FPM)/nginx application to Loki.\nI started out using a syslog source, but after discovering the PHP syslog support to be somewhat broken (at least with 7.4.33) I switched to a file source reading ephemeral files inside the container. In switching, I noticed that the file source (feeding straight into a console sink, same for the syslog source) had much higher latency from clicking a link in the web app to the corresponding logs appearing in console.\nWhen using the syslog source, there wasn't really any discernible latency, the same as when letting the web server output to stdout directly. When using a file source instead, the latency is much more variable but can get as high as ~10 seconds.\nFor reference, I tried tail -qc +0 --follow=name /var/log/{nginx,fpm}/*.log, matching the files Vector is monitoring. This also resulted in increased latency, but it seemed to stay roughly under 1 second.\nIs there a way to tune the file source towards lower latency? I wouldn't actually mind that much, but I was hoping to avoid parsing the different date formats from the various kinds of logs the app produces (plus PHP will often log warnings with no timestamp at all), but the latency also affects the default timestamp Vector generates for the file source, and I would prefer to have timestamps more accurate than ~10s ^^",
        "url": "https://github.com/vectordotdev/vector/discussions/15530",
        "createdAt": "2022-12-10T11:30:19Z",
        "updatedAt": "2023-02-03T21:42:48Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ahti"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15520,
        "title": "Custom parser question",
        "bodyText": "I have a log message like this:\n2022/10/19 00:11:04.475886 27619 INFO    thread BasicThreadFactory.cpp:282->execute [obj.34] \nsource = \"\"\"\n. |= parse_regex!(.message, r'^(?P\\d+/\\d+/\\d+ \\d+:\\d+:\\d+\\.\\d+) (?P\\d+) (?P\\w+) (?P\\w+)    (?P<file_name>\\w+).(?P<file_extension>\\w+):(?P\\d+)->(?P<function_name>\\w+) \\[?P<thread_name>\\]? (?P.*)$')\n.tid = to_int!(.tid)\n.source_type = \"dmnMain\"\n\"\"\"\nIt gives me this error.\n2022-12-09T19:47:37.931377Z ERROR transform{component_kind=\"transform\" component_id=parse_dmnMain component_type=remap component_name=parse_dmnMain}: vector::internal_events::remap: Mapping failed with event. error=\"function call error for \"parse_regex\" at (5:247): could not find any pattern matches\" error_type=\"conversion_failed\" stage=\"processing\" internal_log_rate_limit=true\n2022-12-09T19:47:37.931468Z ERROR transform{component_kind=\"transform\" component_id=parse_dmnMain component_type=remap component_name=parse_dmnMain}: vector::internal_events::remap: Internal log [Mapping failed with event.] is being rate limited.is being rate limited.\nI could not decode the error message to pinpoint where the error is:\nI looked at Rust regex document, but cant see where the parser's error maybe. Can anyone help?\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/15520",
        "createdAt": "2022-12-09T19:49:35Z",
        "updatedAt": "2022-12-09T21:04:29Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "petermp79"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15494,
        "title": "Multiple data sources and destinations",
        "bodyText": "I wonder whether an instance of vector route data from multiple data sources, e.g. log, app log, metric, ..., to different indices on the same ES cluster? If not, what is the best practice for this use case?\nPlease advise, appreciated.",
        "url": "https://github.com/vectordotdev/vector/discussions/15494",
        "createdAt": "2022-12-07T21:41:29Z",
        "updatedAt": "2022-12-08T19:17:44Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "petermp79"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 15452,
        "title": "Kafka sink batch messages",
        "bodyText": "Hello\nI'm using Kubernetes logs as a source and Kafka sink as destination to send logs to the Azure Event Hub. After reviewing number of messages in the Event Hub and in log search engine it looks like 1 Event == 1 Message in the Event hub. Recently we had a problem with reading lot off messages produced by one AKS cluster which is based in the different Azure Region (EH also in different region). We are reading messages using Vectors. No problems in the same region, 1 Vector is able to work on similar load.\nI would like to batch messages before sent to the Event Hub to have more logs in one message.\nI tried every option from batch. Still it seams like single messages per log.\nVector 0.24.1\nSink config:\nsinks:\n  event_hub:\n    type: kafka\n    inputs: [\"kubernetes_logs\"]\n    bootstrap_servers: \"${NAMESPACE}.servicebus.windows.net:9093\"\n    topic: \"${EVENT_HUB}\"\n    compression: \"none\"\n    encoding:\n      codec: \"json\"\n    librdkafka_options:\n      \"security.protocol\": sasl_ssl\n      \"sasl.mechanism\": \"PLAIN\"\n      \"sasl.username\": \"$$ConnectionString\"\n      \"sasl.password\": \"${PASSWORD}\"\n      # https://learn.microsoft.com/en-us/azure/event-hubs/apache-kafka-configurations#librdkafka-configuration-properties\n      \"socket.keepalive.enable\": \"true\"\n      \"metadata.max.age.ms\": \"180000\"\n      \"retries\": \"2\"\n      \"request.timeout.ms\": \"60000\"\n      \"partitioner\": \"consistent_random\"",
        "url": "https://github.com/vectordotdev/vector/discussions/15452",
        "createdAt": "2022-12-05T08:35:02Z",
        "updatedAt": "2022-12-08T13:31:57Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "luk-ada"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 15485,
        "title": "Omitting Kubernetes fields in source",
        "bodyText": "Hey all!\nWhen upgrading from 0.24 to 0.26, we seemed to have lost the option to omit specific fields in the kubernetes_logs source.\nIn 0.24 it was possible to omit like so:\nsources:\n  kubernetes_logs:\n    type: kubernetes_logs\n    namespace_annotation_fields:\n      namespace_labels: \"\"\n\nIn 0.26 it gives the next error\n\"Configuration error.\",\"error\":\"Invalid field path \\\"\\\" for key `sources.kubernetes_logs`\"\n\nI would want to avoid implementing a remap for this, because it will just raise the resource cost on the Vector agent in Kubernetes.\nAny advice here?\nMany thanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/15485",
        "createdAt": "2022-12-07T11:04:12Z",
        "updatedAt": "2022-12-08T08:08:17Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ottramst"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 14284,
        "title": "howto use geoip via vrl",
        "bodyText": "I have this bit of vrl\n.geoip = get_enrichment_table_record!(\n  table: \"geoip\",\n  condition: {\"ip\": .srcip}\n)\n\nI also set these globals\nenrichment_tables.geoip.path = \"/var/lib/GeoLite2-City.mmdb\"\nenrichment_tables.geoip.type = \"geoip\"\n\nI get no output, even though I have events with srcip fields.\nAm I missing something? What does condition mean... are there some examples you can share?\n\nvector 0.24.0 (x86_64-unknown-linux-gnu 43267b9 2022-08-30)",
        "url": "https://github.com/vectordotdev/vector/discussions/14284",
        "createdAt": "2022-09-06T01:55:07Z",
        "updatedAt": "2022-12-06T17:57:09Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tshepang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 15456,
        "title": "how to let through only the given fields",
        "bodyText": "Say you have an event with unknown number of fields, and are only interested in some of them that you know of. How do you discard the rest?",
        "url": "https://github.com/vectordotdev/vector/discussions/15456",
        "createdAt": "2022-12-05T18:56:24Z",
        "updatedAt": "2022-12-07T01:34:25Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tshepang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15468,
        "title": "parse_groks not working with parse_grok pattern",
        "bodyText": "Hello\nI need to use parase_groks to have ability for multi patterns check. I have pattern which is working fine on parse_grok but not working with parase_groks.\nWorking:\ntransforms:\n  grok:\n    inputs:\n      - \"file\"\n    type: remap\n    source: |-\n      . |= parse_grok!(\n        .message,s'%{TIMESTAMP_ISO8601:log.timestamp}%{SPACE}+%{LOGLEVEL:log.level}%{SPACE}+\\[(?<microservice>[a-zA-Z0-9\\-]*)\\,[a-zA-Z0-9]*\\,[a-zA-Z0-9]*\\]%{SPACE}+\\[(?<trace.id>[a-zA-Z0-9\\-]*)\\]'\n      )\n\nNot matching:\ntransforms:\n  groks:\n    inputs:\n      - \"file\"\n    type: remap\n    source: |-\n      . |= parse_groks!(\n        .message,\n        patterns: [\n          s'%{TIMESTAMP_ISO8601:log.timestamp}%{SPACE}+%{LOGLEVEL:log.level}%{SPACE}+\\[(?<microservice>[a-zA-Z0-9\\-]*)\\,[a-zA-Z0-9]*\\,[a-zA-Z0-9]*\\]%{SPACE}+\\[(?<trace.id>[a-zA-Z0-9\\-]*)\\]',\n        ]\n      )\n\n2022-12-06T11:40:57.518369Z ERROR transform{component_kind=\"transform\" component_id=groks component_type=remap component_name=groks}: vector::internal_events::remap: Mapping failed with event. error=\"function call error for \\\"parse_groks\\\" at (222:450): unable to parse grok: value does not match any rule\" error_type=\"conversion_failed\" stage=\"processing\" internal_log_rate_limit=true\nTested on version 0.24.1 and 0.25.2.\nCan you please tell me what I'm doing wrong? I don't want to start the issue because I assume that is a configuration problem not Vector itself.",
        "url": "https://github.com/vectordotdev/vector/discussions/15468",
        "createdAt": "2022-12-06T11:47:27Z",
        "updatedAt": "2022-12-08T12:42:54Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "luk-ada"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15467,
        "title": "Large amount files will lead to slow throughput rate to kafka",
        "bodyText": "#15276 (comment)\nA note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nBackground\nHello, we are migrating services logs using vector (1 host) -- kafka (3 hosts) -- logstash (3 hosts) -- opensearch (1 kibana host) pipeline. Our services will output almost 25 gzip files per hour in certain directory, each size is about 900M, and we have a script to remove 1 day ago deprecated logs from this directory to reduce storage pressure.\nWhen we start vector at Nov 16, 2022 @ 9:00:00, the throughput was steady until 12:00:00, during this 3 hour, we could get about 450M indice hits per hour in kibana. However, the hits rate rapidly down after 12 PM, we just got 100M at 3 PM, 80M at 6 PM, 70M at 9 PM, 60 M at Nov 17, 2022 0 AM, 50M at 3 AM and 40M at 6 AM.\n\nOur thinking\nEvery hour our services put 25 files, for one whole day, we will have 24*25=600 files. Even we use a script to keep just one day logs, the file total size is very heavy here - 600 * 900M = 540G.\nQuestion\n\nIs any source constraint for vector? file number? file total size?\nShould we change our script to just keep 12 hrs our 6 hrs logs to reduce the vector pressure? What if this condition do not help?\n\nBasic Infomation\nSources format: gzip files\nFiles directory: /local/vectorLog\nFiles format: /local/vectorLog/requests.log.yyyy-MM-dd-HH.gz\nFiles size: ~900M\nConfiguration\n[sources.gz_logs]\ntype = \"file\"\nmax_line_bytes = 2024000\nignore_older_secs = 600\ninclude = [ \"/local/vectorLog/requests*.gz\" ]\nmultiline.mode = \"halt_with\"\nmultiline.condition_pattern = \"---------------------------------------------*\"\nmultiline.timeout_ms = 5000\nmultiline.start_pattern = '^\\{'\n# Parse gz_logs\n[transforms.parse_logs]\ntype = \"remap\"\ninputs = [\"gz_logs\"]\nsource = \"\"\"\nlines,err = split(.message, \"\\n\")\n.message = lines[0] + lines[1]\n.message = to_string(.message)\n.message = parse_json!(.message)\n\"\"\"\n# Print parsed logs to stdout\n[sinks.print]\ntype = \"kafka\"\ninputs = [ \"parse_logs\" ]\nbootstrap_servers = \"AAA:9092,BBB:9092,CCC:9092\"\ntopic = \"TOPIC\"\ncompression = \"none\"\n [sinks.print.encoding]\n  codec = \"json\"\n\nVersion\nvector 0.23.3 (x86_64-unknown-linux-gnu af8c9e1 2022-08-10)\nDebug Output\n### 96 log files: continue source transfrom sink\n2022-11-16T09:19:38.546454Z DEBUG source{component_kind=\"source\" component_id=gz_logs component_type=file component_name=gz_logs}:file_server: file_source::file_server: event_throughput=230.000/sec bytes_throughput=1.283m/sec ratios={\"discovery\": 0.9952969, \"other\": 0.0002865836, \"reading\": 0.0043958127, \"sending\": 1.8007844e-5, \"sleeping\": 2.8511367e-6}\n2022-11-16T09:19:38.549660Z DEBUG transform{component_kind=\"transform\" component_id=parse_logs component_type=remap component_name=parse_logs}: vector::utilization: utilization=0.028800348759819846\n2022-11-16T09:19:39.471776Z DEBUG vector::internal_events::file::source: Files checkpointed. count=96 duration_ms=2\n2022-11-16T09:19:39.797520Z DEBUG source{component_kind=\"source\" component_id=gz_logs component_type=file component_name=gz_logs}:file_server: file_source::file_server: event_throughput=288.000/sec bytes_throughput=1.504m/sec ratios={\"discovery\": 0.9959729, \"other\": 0.000243444, \"reading\": 0.0037625295, \"sending\": 1.9564282e-5, \"sleeping\": 2.7171284e-6}\n2022-11-16T09:19:40.487837Z DEBUG vector::internal_events::file::source: Files checkpointed. count=96 duration_ms=4\n2022-11-16T09:19:40.645823Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.013442464730048877\n\n### 287 log files: continue source transform sink\n2022-11-16T17:45:30.182521Z DEBUG source{component_kind=\"source\" component_id=gz_logs component_type=file component_name=gz_logs}:file_server: file_source::file_server: event_throughput=450.000/sec bytes_throughput=2.360m/sec ratios={\"discovery\": 0.9924744, \"other\": 0.00024555708, \"reading\": 0.007268966, \"sending\": 9.853384e-6, \"sleeping\": 1.3856921e-6}\n2022-11-16T17:45:30.191391Z DEBUG transform{component_kind=\"transform\" component_id=parse_logs component_type=remap component_name=parse_logs}: vector::utilization: utilization=0.0444073165530334\n2022-11-16T17:45:30.472922Z DEBUG vector::internal_events::file::source: Files checkpointed. count=287 duration_ms=1\n2022-11-16T17:45:30.647038Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.023487958650561292\n2022-11-16T17:45:31.479022Z DEBUG vector::internal_events::file::source: Files checkpointed. count=287 duration_ms=0\n\n### 477 log files: looks like not working\n2022-11-17T02:30:46.375905Z DEBUG source{component_kind=\"source\" component_id=gz_logs component_type=file component_name=gz_logs}:file_server: file_source::file_server: event_throughput=9.267k/sec bytes_throughput=48.290m/sec ratios={\"discovery\": 0.831037, \"other\": 0.0038224799, \"reading\": 0.16500683, \"sending\": 0.00011769667, \"sleeping\": 1.6122965e-5}\n2022-11-17T02:30:47.015750Z DEBUG vector::internal_events::file::source: Files checkpointed. count=477 duration_ms=12\n2022-11-17T02:30:47.405331Z DEBUG source{component_kind=\"source\" component_id=gz_logs component_type=file component_name=gz_logs}:file_server: file_source::file_server: event_throughput=8.586k/sec bytes_throughput=44.756m/sec ratios={\"discovery\": 0.8268399, \"other\": 0.0035622944, \"reading\": 0.16947487, \"sending\": 0.00010818543, \"sleeping\": 1.4872786e-5}\n2022-11-17T02:30:48.027768Z DEBUG vector::internal_events::file::source: Files checkpointed. count=477 duration_ms=2\n2022-11-17T02:30:48.418591Z DEBUG source{component_kind=\"source\" component_id=gz_logs component_type=file component_name=gz_logs}:file_server: file_source::file_server: event_throughput=8.692k/sec bytes_throughput=45.377m/sec ratios={\"discovery\": 0.8270361, \"other\": 0.0035936986, \"reading\": 0.16924557, \"sending\": 0.0001098454, \"sleeping\": 1.4949319e-5}\n\nExample Data\nNo response\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/15467",
        "createdAt": "2022-12-06T09:58:13Z",
        "updatedAt": "2022-12-06T09:58:35Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "WilliamEricCheung"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15389,
        "title": "Can I avoid double json_parse() while filtering and remapping my events?",
        "bodyText": "I have to parse events coming from a file which are stored there as JSON-strings.\nFirst I need to filter and event by some criterion, say value in a field. I do parse_json!(.message) in filter block.\nThen I need to transform a message. I do parse_json!(.message) again in remap block.\nIs there a way to pass an already parsed structure from filter to remap to work with a structure in remap instead of parsing the source string again? I suppose this is CPU consuming after all.",
        "url": "https://github.com/vectordotdev/vector/discussions/15389",
        "createdAt": "2022-11-29T18:14:06Z",
        "updatedAt": "2022-11-29T21:24:17Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "remort"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15339,
        "title": "How to process files with only one line and no line breaks?",
        "bodyText": "I have vector monitoring a directory that an app writes event logs to. Each file contains exactly one event in JSON format, which contains exactly 1 line, but does not have a newline at the end. (When you wc -l this file it will say it has 0 lines)\nI found that vector does not process the line if it does not have that newline. When the file gets created, vector does see it and emits \"Found new file to watch\" but it does not go further. It will parse the line if I echo >> logfile.json (essentially adding that newline).\nI also tried simulating this logging process with python and was able to replicate the issue. If I add a newline on file write, vector does not have a problem parsing the line.\nIs there a way to solve this? Thanks in advance!",
        "url": "https://github.com/vectordotdev/vector/discussions/15339",
        "createdAt": "2022-11-24T06:13:28Z",
        "updatedAt": "2022-11-29T07:46:18Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "pirxthepilot"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14997,
        "title": "How to enrich data with external HTTP-request?",
        "bodyText": "Hello! I'd like to enrich data with response from external URL (just like http-module in logstash) usinf VRL. So, I'm making HTTP-request with some existing fields and taking some fields from response.\nI didn't find any function for this in docs. Is it possible?",
        "url": "https://github.com/vectordotdev/vector/discussions/14997",
        "createdAt": "2022-10-28T16:14:00Z",
        "updatedAt": "2022-11-28T22:21:41Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "D13410N3"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 15217,
        "title": "running multiple vector configs at same time in parallel",
        "bodyText": "Hi. is there any way to run multiple vector configs like vector --config ./\nmy problem is if one of these vector configs have an syntax error , none of them works or something .\nis there any way to run all of them except one who has a syntax error.",
        "url": "https://github.com/vectordotdev/vector/discussions/15217",
        "createdAt": "2022-11-15T14:29:28Z",
        "updatedAt": "2023-02-03T21:43:25Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15183,
        "title": "\"querying a field of a non-object type is unsupported\"",
        "bodyText": "I expected this to work, but it didn't:\n.object.key = \"some value\"\n. |= parse_key_value!(del(.message))\n.object.key = \"some other value\"\n\nError message:\nerror[E642]: parent path segment rejects this mutation\n  \u250c\u2500 :3:11\n  \u2502\n3 \u2502   .object.key = \"some other value\"\n  \u2502   ------- ^^^ querying a field of a non-object type is unsupported\n  \u2502   \u2502\n  \u2502   this path resolves to a value of type string, boolean, undefined or array\n\nIs this a bug?\nThis is using a1b2590 (current latest)",
        "url": "https://github.com/vectordotdev/vector/discussions/15183",
        "createdAt": "2022-11-11T01:04:41Z",
        "updatedAt": "2022-11-29T00:56:04Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tshepang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15114,
        "title": "Prometheus remote write unable to read",
        "bodyText": "A note for the community\n\n\nPlease vote on this issue by adding a \ud83d\udc4d reaction to the original issue to help the community and maintainers prioritize this request\nIf you are interested in working on this issue or have submitted a pull request, please leave a comment\n\n\nProblem\nHello, I am trying to send data to vector via prometheus remote write so that I can further send it to datadog. I have deployed a vector helm chart in the same namespace as prometheus is running. I see the vector service and the headless service deployed in the namespace and I am trying to make prometheus send data to vector. I see the following log messages in prometheus\nts=2022-11-01T00:33:27.942Z caller=dedupe.go:112 component=remote level=warn remote_name=fa13dd url=http://vector.istio-system.svc.cluster.local:9090/ msg=\"Failed to send batch, retrying\" err=\"Post \\\"http://vector.istio-system.svc.cluster.local:9090/\\\": read tcp 192.168.135.32:46236->10.100.5.151:9090: read: connection reset by peer\" ts=2022-11-01T00:33:31.632Z caller=dedupe.go:112 component=remote level=warn remote_name=fa13dd url=http://vector.istio-system.svc.cluster.local:9090/ msg=\"Failed to send batch, retrying\" err=\"Post \\\"http://vector.istio-system.svc.cluster.local:9090/\\\": read tcp 192.168.135.32:50902->10.100.5.151:9090: read: connection reset by peer\" ts=2022-11-01T00:33:32.949Z caller=dedupe.go:112 component=remote level=warn remote_name=fa13dd url=http://vector.istio-system.svc.cluster.local:9090/ msg=\"Failed to send batch, retrying\" err=\"Post \\\"http://vector.istio-system.svc.cluster.local:9090/\\\": read tcp 192.168.135.32:50904->10.100.5.151:9090: read: connection reset by peer\"\nI can see that the vector instance starts with the following message so the remote write source is running. I am running vector as an aggregator using the default helm install command helm install vector vector/vector --namespace istio-system. I see that the documentation states that we need to run vector in daemon mode for prometheus remote write but is there a way that this use case is supported ?\nkubectl logs vector-0 -n istio-system 2022-10-31T23:25:37.008286Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=trace,rdkafka=info,buffers=info,kube=info\" 2022-10-31T23:25:37.008359Z  INFO vector::app: Loading configs. paths=[\"/etc/vector\"] 2022-10-31T23:25:37.009904Z  INFO vector::topology::running: Running healthchecks. 2022-10-31T23:25:37.010087Z  INFO vector: Vector has started. debug=\"false\" version=\"0.24.1\" arch=\"x86_64\" build_id=\"8935681 2022-09-12\" 2022-10-31T23:25:37.010114Z  INFO vector::internal_events::api: API server running. address=127.0.0.1:8686 playground=off 2022-10-31T23:25:37.010229Z  INFO vector::topology::builder: Healthcheck: Passed. 2022-10-31T23:25:37.012883Z  INFO source{component_kind=\"source\" component_id=prom_remote_write component_type=prometheus_remote_write component_name=prom_remote_write}: vector::sources::util::http::prelude: Building HTTP server. address=0.0.0.0:9090\nConfiguration\nFollowing is the config file I added for remote write in prometheus config map\n\n    remote_write:\n      - url: http://vector.istio-system.svc.cluster.local:9090\n\nFollowing is the config for my vector instance for the remote write source\n\n{\n  \"sources\": {\n    \"my_source_id\": {\n      \"type\": \"prometheus_remote_write\",\n      \"acknowledgements\": null,\n      \"address\": \"0.0.0.0:9090\"\n    }\n  }\n}\n\nVersion\nvector 0.24.1 (x86_64-unknown-linux-gnu 8935681 2022-09-12)\nDebug Output\nNo response\nExample Data\nNo response\nAdditional Context\nNo response\nReferences\nNo response",
        "url": "https://github.com/vectordotdev/vector/discussions/15114",
        "createdAt": "2022-11-04T18:54:37Z",
        "updatedAt": "2022-11-28T20:43:11Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "prateek1192"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15190,
        "title": "how to recover data rejected by Elasticsearch",
        "bodyText": "An example error would be...\n{\"type\":\"illegal_argument_exception\",\"reason\":\"Limit of total fields [1000]...\", ...\nIs there an easy way to view such rejected data, similar to how one can do a reroute_dropped of remap transform?",
        "url": "https://github.com/vectordotdev/vector/discussions/15190",
        "createdAt": "2022-11-11T18:12:27Z",
        "updatedAt": "2022-11-29T00:58:57Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tshepang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 15330,
        "title": "How to filter events from a Kafka topic based on messages from another topic?",
        "bodyText": "I have two Kafka topics:\n\nevents: With some application events (continuously)\n\n{ \"id\": 1, \"client\": \"client_1\", \"status\": 200 }\n{ \"id\": 2, \"client\": \"client_2\", \"status\": 200 }\n{ \"id\": 3, \"client\": \"client_1\", \"status\": 302 }\n{ \"id\": 4, \"client\": \"client_3\", \"status\": 400 }\n{ \"id\": 5, \"client\": \"client_4\", \"status\": 200 }\n{ \"id\": 6, \"client\": \"client_5\", \"status\": 201 }\n\n\nconfig: With a list of clients that should be filtered (periodically updated)\n\n[{ \"client\": \"client_1\" }, { \"client\": \"client_2\" }]\n[{ \"client\": \"client_1\" }, { \"client\": \"client_2\" }]\n\nIs there a way to filter messages from events topic based on the latest message of config topic? That way I've expected the following output:\n{ \"id\": 1, \"client\": \"client_1\", \"status\": 200 }\n{ \"id\": 2, \"client\": \"client_2\", \"status\": 200 }\n{ \"id\": 3, \"client\": \"client_1\", \"status\": 302 }\n\nAnother option would be to load the latest message from config topic to something like an enrichment table in-memory, so we could do the filter via VRL, but I didn't find a way to do this.\nI've put Lua transform as a second option because performance is very important for this application. We're doing a benchmark with some streaming platforms and Vector is a candidate.\nI could make it work with the following transform:\n  filter_events:\n    type: lua\n    inputs:\n      - parse_events\n      - parse_config\n    version: '2'\n    source: require('filter-events')\n    hooks:\n      process: filter_events\n      init: init\nWhere filter-events.lua has the following content:\nfunction init()\n  clients = {}\nend\n\nfunction filter_events(event, emit)\n  -- handle config event and store clients to be filtered in-memory\n  if event.log.topic == \"config\" then\n    clients = {}\n    for _, config in pairs(event.log.message) do\n      clients[config.client] = true\n    end\n    return\n  end\n\n  -- handle log event and filter based on configured clientes\n  if clients[event.log.client] then\n    emit(event)\n  end\nend\nThat way messages from both topics will be handled on the same transform, so we've to check each message and also iterate over the list of clients to update the memory table. It seems that's not the best way to do this.\nThanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/15330",
        "createdAt": "2022-11-23T10:26:05Z",
        "updatedAt": "2022-11-26T09:24:36Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "brunomrpx"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15283,
        "title": "How to solve the memory leak issue when using HTTP Server?",
        "bodyText": "HTTP Server source with larger throughput will cause memory leak (in my case the QPS is about 3k~5k)\nThe memory cost increases to 10 GB (then crashes) in one hour and never release.\nI think it's caused by HTTP Server source since the problem is still there after I removed the VRL and replaced the ClickHouse sink with File sink.\nOS: Ubuntu 20.04.2\nVersion: 0.25.1\nconfig\n[api]\nenabled = true\naddress = \"0.0.0.0:8686\"\n\n[sources.vdc]\ntype = \"http_server\"\naddress = \"0.0.0.0:80\"\nencoding = \"binary\"\nmethod = \"GET\"\nstrict_path = false\npath = \"/vdc/v1.0/\"\n\n[transforms.vdc_transform]\ntype = \"remap\"\ninputs = [\"vdc\"]\ndrop_on_error = true\nsource = \"\"\"\ndel(.message)\ndata = split!(del(.path), \"/\")\n.so = to_int!(data[3])\n.stb = data[4]\n.service = to_int!(data[5])\n.datetime = to_int(to_int!(data[6]) / 1000)\n.timestamp = to_unix_timestamp!(.timestamp)\n.server_datetime = del(.timestamp)\n\"\"\"\n\n[sinks.vdc_ch]\ntype = \"clickhouse\"\ninputs = [\"vdc_transform\"]\ndatabase  = \"vdc\"\nendpoint = \"http://xxxxxx:8123\"\ntable = \"vdc_log_entry\"\ncompression = \"gzip\"\nskip_unknown_fields = true\n\n  [sinks.vdc_ch.auth]\n  user = \"xxxxxx\"\n  password = \"xxxxx\"\n  strategy = \"basic\"\n\n  [sinks.vdc_ch.batch]\n  timeout_secs = 1\n\n  [sinks.vdc_ch.buffer]\n  max_size = 268435488\n  type = \"disk\"",
        "url": "https://github.com/vectordotdev/vector/discussions/15283",
        "createdAt": "2022-11-17T13:43:49Z",
        "updatedAt": "2022-11-17T13:43:50Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "catdingding"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 0
        },
        "upvoteCount": 1
    },
    {
        "number": 15215,
        "title": "how to use filter type support wildcard\uff1f",
        "bodyText": "[transforms.service_logs]\ntype = \"filter\"\ninputs = [ \"parse_logs_download\" ]\ncondition = '''\n.service = \"*cross\"\n'''\nI want to filter out that the service field contains 'cross' string\uff0cbut it doesn't work, how to do it?",
        "url": "https://github.com/vectordotdev/vector/discussions/15215",
        "createdAt": "2022-11-15T08:31:16Z",
        "updatedAt": "2022-11-16T07:44:12Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "trustnote-wang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15170,
        "title": "Azure blob storage sink error - no key/value found in connection string",
        "bodyText": "Hi,\nI am trying to setup vector to send the log to Azure Storage bucket but getting the following error.\n2022-11-10T08:27:25.389600Z ERROR vector::topology: Configuration error. error=Sink \"my_sink_id\": no key/value found in connection string: **REDACTED** \nMy configuration for the sink is shown below (copied from the documentation).\n`[sinks.my_sink_id]\ntype = \"azure_blob\"\ninputs = [ \"parse_logs\"]\nconnection_string = \"DefaultEndpointsProtocol=https;AccountName=<account_name>;AccountKey=<account_key>;EndpointSuffix=core.windows.net\"\ncontainer_name = \"device_logs\"\nblob_prefix = \"raspberry01blob/%F/\"\nblob_append_uuid = true\ncompression = \"gzip\"\nblob_time_format = \"%s\"\n[sinks.my_sink_id.encoding]\ncodec = \"json\"\n`",
        "url": "https://github.com/vectordotdev/vector/discussions/15170",
        "createdAt": "2022-11-10T08:32:45Z",
        "updatedAt": "2023-02-03T21:44:23Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ubergeekNZ"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15220,
        "title": "Routing events using route transform",
        "bodyText": "Hi,\nAccording to the documentation: https://vector.dev/docs/reference/configuration/transforms/route/#route,  If an event doesn\u2019t match any route, it will be sent to the <transform_name>._unmatched output.. Can we override this behaviour somehow?\nAs in if the event does not match the previous routes, route it to this route. Just like the logical if {} elseif {} else {}, where else block catches all the rest of the events. The reason I am asking is because it gets very complicated to write the condition to match all the events which didn't match the previous routes.\nWhat is a recommended way to handle this?",
        "url": "https://github.com/vectordotdev/vector/discussions/15220",
        "createdAt": "2022-11-15T16:11:47Z",
        "updatedAt": "2022-11-28T22:17:40Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "amanmahajan26"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15186,
        "title": "how to put dnstap DNS query and response log  into one log event",
        "bodyText": "Currently a pair of DNS query and response is in two event, I want to merge the pair of log into one event. I'm new to vector, any help is appreciate.",
        "url": "https://github.com/vectordotdev/vector/discussions/15186",
        "createdAt": "2022-11-11T09:05:18Z",
        "updatedAt": "2022-11-14T01:50:46Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "slgray"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15185,
        "title": "for vector source, dose it support using AWS EC2 system log ?",
        "bodyText": "for AWS EC2 , https://docs.aws.amazon.com/managedservices/latest/userguide/access-to-logs-ec2.html\n/var/log/audit/audit.log\n/var/log/cron\n/var/log/amazon/ssm/amazon-ssm-agent.log\n/var/log/secure\n/var/log/aws/ams\n/var/log/maillog\n/var/log/yum.log\n/var/log/messages\n/var/log/cloud-init-output.log\n/var/log/cloud-init.log (Amazon Linux 1 / Amazon Linux 2 only)\n\nI dont see any existing sources proper ? https://vector.dev/docs/reference/configuration/sources/\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/15185",
        "createdAt": "2022-11-11T07:44:58Z",
        "updatedAt": "2022-11-11T23:16:08Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "foreversunyao"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15126,
        "title": "http sink support body",
        "bodyText": "Hey!\nI would like to know, does http sink support not only headers, but body as well? It's crucial for my task to send webhooks containing body.",
        "url": "https://github.com/vectordotdev/vector/discussions/15126",
        "createdAt": "2022-11-07T11:57:05Z",
        "updatedAt": "2022-11-10T20:22:04Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Radcriminal"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 15156,
        "title": "`abort` in remap causes ERROR log spam in 0.25?",
        "bodyText": "We've just upgrade to 0.25.1 and now suddenly seeing a bunch of ERROR logs on \"Events dropped\" (being rate limited)\n{\"timestamp\":\"2022-11-09T14:48:14.613851Z\",\"level\":\"DEBUG\",\"message\":\"Event mapping aborted.\",\"internal_log_rate_limit\":true,\"target\":\"vector::internal_events::remap\",\"span\":{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"},\"spans\":[{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"}]}\n{\"timestamp\":\"2022-11-09T14:48:14.613865Z\",\"level\":\"ERROR\",\"message\":\"Internal log [Events dropped] has been rate limited 49 times.\",\"target\":\"vector_common::internal_event::component_events_dropped\",\"span\":{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"},\"spans\":[{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"}]}\n{\"timestamp\":\"2022-11-09T14:48:14.613895Z\",\"level\":\"ERROR\",\"message\":\"Events dropped\",\"intentional\":false,\"count\":1,\"reason\":\"Event mapping aborted.\",\"internal_log_rate_limit\":true,\"target\":\"vector_common::internal_event::component_events_dropped\",\"span\":{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"},\"spans\":[{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"}]}\n\n{\"timestamp\":\"2022-11-09T15:16:13.763091Z\",\"level\":\"ERROR\",\"message\":\"Internal log [Events dropped] is being rate limited.\",\"target\":\"vector_common::internal_event::component_events_dropped\",\"span\":{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"},\"spans\":[{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"}]}\n{\"timestamp\":\"2022-11-09T15:16:21.992999Z\",\"level\":\"ERROR\",\"message\":\"Internal log [Events dropped] has been rate limited 4 times.\",\"target\":\"vector_common::internal_event::component_events_dropped\",\"span\":{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"},\"spans\":[{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"}]}\n{\"timestamp\":\"2022-11-09T15:16:21.993032Z\",\"level\":\"ERROR\",\"message\":\"Events dropped\",\"intentional\":false,\"count\":1,\"reason\":\"Event mapping aborted.\",\"internal_log_rate_limit\":true,\"target\":\"vector_common::internal_event::component_events_dropped\",\"span\":{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"},\"spans\":[{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"}]}\n{\"timestamp\":\"2022-11-09T15:16:24.050915Z\",\"level\":\"ERROR\",\"message\":\"Internal log [Events dropped] is being rate limited.\",\"target\":\"vector_common::internal_event::component_events_dropped\",\"span\":{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"},\"spans\":[{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"}]}\n{\"timestamp\":\"2022-11-09T15:16:33.318782Z\",\"level\":\"ERROR\",\"message\":\"Internal log [Events dropped] has been rate limited 5 times.\",\"target\":\"vector_common::internal_event::component_events_dropped\",\"span\":{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"},\"spans\":[{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"}]}\n{\"timestamp\":\"2022-11-09T15:16:33.318826Z\",\"level\":\"ERROR\",\"message\":\"Events dropped\",\"intentional\":false,\"count\":1,\"reason\":\"Event mapping aborted.\",\"internal_log_rate_limit\":true,\"target\":\"vector_common::internal_event::component_events_dropped\",\"span\":{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"},\"spans\":[{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"}]}\n{\"timestamp\":\"2022-11-09T15:16:35.377651Z\",\"level\":\"ERROR\",\"message\":\"Internal log [Events dropped] is being rate limited.\",\"target\":\"vector_common::internal_event::component_events_dropped\",\"span\":{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"},\"spans\":[{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"}]}\n{\"timestamp\":\"2022-11-09T15:16:43.606665Z\",\"level\":\"ERROR\",\"message\":\"Internal log [Events dropped] has been rate limited 4 times.\",\"target\":\"vector_common::internal_event::component_events_dropped\",\"span\":{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"},\"spans\":[{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"}]}\n{\"timestamp\":\"2022-11-09T15:16:43.606704Z\",\"level\":\"ERROR\",\"message\":\"Events dropped\",\"intentional\":false,\"count\":1,\"reason\":\"Event mapping aborted.\",\"internal_log_rate_limit\":true,\"target\":\"vector_common::internal_event::component_events_dropped\",\"span\":{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"},\"spans\":[{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"}]}\n{\"timestamp\":\"2022-11-09T15:16:45.665458Z\",\"level\":\"ERROR\",\"message\":\"Internal log [Events dropped] is being rate limited.\",\"target\":\"vector_common::internal_event::component_events_dropped\",\"span\":{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"},\"spans\":[{\"component_id\":\"coralogix_prep\",\"component_kind\":\"transform\",\"component_name\":\"coralogix_prep\",\"component_type\":\"remap\",\"name\":\"transform\"}]}\n\nWe currently use abort in remap to filter out pods which we do not want to send logs externally (with drop_on_abort = true on the remap transform):\n# Check to see if we've been explicitly instructed not to forward to Coralogix.\nforward = to_bool!(.kubernetes.pod_annotations.\"xyz/forward\") || true\nif !forward { abort }\n\n# Ensure we have the proper Coralogix labels, abort if either are missing.\napplication = .kubernetes.pod_annotations.\"xyz/application\"\nsubsystem = .kubernetes.pod_annotations.\"xyz/subsystem\"\nif !is_string(application) { abort }\nif !is_string(subsystem) { abort }\n\nWith some search it seems this is an intended event:\n\n  \n    \n      vector/src/internal_events/remap.rs\n    \n    \n        Lines 49 to 63\n      in\n      9348d36\n    \n  \n  \n    \n\n        \n          \n           impl InternalEvent for RemapMappingAbort { \n        \n\n        \n          \n               fn emit(self) { \n        \n\n        \n          \n                   debug!( \n        \n\n        \n          \n                       message = \"Event mapping aborted.\", \n        \n\n        \n          \n                       internal_log_rate_limit = true \n        \n\n        \n          \n                   ); \n        \n\n        \n          \n            \n        \n\n        \n          \n                   if self.event_dropped { \n        \n\n        \n          \n                       emit!(ComponentEventsDropped::<UNINTENTIONAL> { \n        \n\n        \n          \n                           count: 1, \n        \n\n        \n          \n                           reason: \"Event mapping aborted.\", \n        \n\n        \n          \n                       }); \n        \n\n        \n          \n                   } \n        \n\n        \n          \n               } \n        \n\n        \n          \n           } \n        \n    \n  \n\n\nthus the ERROR logs is also intended? is abort not the correct way to archive filtering? Or, if this is intended use of abort then it shouldn't be creating ERROR log spam by default?",
        "url": "https://github.com/vectordotdev/vector/discussions/15156",
        "createdAt": "2022-11-09T15:05:25Z",
        "updatedAt": "2022-11-09T17:06:42Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "aquarhead"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14982,
        "title": "UDP data source & sink",
        "bodyText": "Greetings!\nIs there source & sink to support UDP data stream and forward to other UDP consumers?\nWith that, to retain the source IP as spoofing the data - this is similar to samplicate tool.\nThanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/14982",
        "createdAt": "2022-10-27T13:36:34Z",
        "updatedAt": "2022-11-07T10:20:00Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "amarnathpv"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 15073,
        "title": "Elasticsearch Sink with data_stream not working",
        "bodyText": "Hello,\nI try to setup an Elastic sink with a data stream, but Elastic is not accepting my data. I alway receive following error:\nError-Log:\n2022-11-02T14:30:54.037527Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=trace,rdkafka=info,buffers=info,kube=info\"\n2022-11-02T14:30:54.037634Z  INFO vector::app: Loading configs. paths=[\"/etc/vector\"]\n2022-11-02T14:30:54.528763Z  INFO vector::topology::running: Running healthchecks.\n2022-11-02T14:30:54.529440Z  INFO vector: Vector has started. debug=\"false\" version=\"0.24.2\" arch=\"x86_64\" build_id=\"5e47b89 2022-10-13\"\n2022-11-02T14:30:54.529840Z  INFO source{component_kind=\"source\" component_id=fluent component_type=fluent component_name=fluent}: vector::sources::util::tcp: Listening. addr=0.0.0.0:24224\n2022-11-02T14:30:54.530139Z  INFO source{component_kind=\"source\" component_id=vector component_type=vector component_name=vector}: vector::sources::util::grpc: Building gRPC server. address=0.0.0.0:6000\n2022-11-02T14:30:54.530280Z  INFO vector::internal_events::api: API server running. address=127.0.0.1:8686 playground=off\n2022-11-02T14:30:54.530453Z  INFO source{component_kind=\"source\" component_id=syslog component_type=syslog component_name=syslog}: vector::sources::util::tcp: Listening. addr=0.0.0.0:9000\n2022-11-02T14:30:54.552088Z  INFO vector::topology::builder: Healthcheck: Passed.\n2022-11-02T14:30:54.574614Z  INFO vector::topology::builder: Healthcheck: Passed.\n2022-11-02T14:31:37.450137Z ERROR sink{component_kind=\"sink\" component_id=elk-shared-dev component_type=elasticsearch component_name=elk-shared-dev}:request{request_id=1}: vector::internal_events::elasticsearch: Response failed. error_code=http_response_400 error_type=\"request_failed\" stage=\"sending\" response=Response { status: 400, version: HTTP/1.1, headers: {\"x-elastic-product\": \"Elasticsearch\", \"content-type\": \"application/json;charset=utf-8\", \"content-length\": \"261\"}, body: b\"{\"error\":{\"root_cause\":[{\"type\":\"illegal_argument_exception\",\"reason\":\"Action/metadata line [1] contains an unknown parameter [_type]\"}],\"type\":\"illegal_argument_exception\",\"reason\":\"Action/metadata line [1] contains an unknown parameter [_type]\"},\"status\":400}\" }\n2022-11-02T14:31:37.450239Z ERROR sink{component_kind=\"sink\" component_id=elk-shared-dev component_type=elasticsearch component_name=elk-shared-dev}:request{request_id=1}: vector::sinks::util::retries: Not retriable; dropping the request. reason=\"client-side error, 400 Bad Request: {\"error\":{\"root_cause\":[{\"type\":\"illegal_argument_exception\",\"reason\":\"Action/metadata line [1] contains an unknown parameter [_type]\"}],\"type\":\"illegal_argument_exception\",\"reason\":\"Action/metadata line [1] contains an unknown parameter [_type]\"},\"status\":400}\"\n......\nVector is running with role \"Aggregator\" and installed via helm.\nmy config for the sink:\nelk-shared-dev:\ntype: elasticsearch\ninputs:\n- sbox\nauth:\nuser: ${ELASTICSEARCH_USERNAME}\npassword: ${ELASTICSEARCH_PASSWORD}\nstrategy: basic\nbulk:\naction: create\nindex: vector-test\nendpoint: https://elasticsearch:9200\nmode: data_stream\ncompression: none\nhealthcheck:\nenabled: true\nI already tried several configs, but still not working.\nAny hint?\nMany Thanks",
        "url": "https://github.com/vectordotdev/vector/discussions/15073",
        "createdAt": "2022-11-02T14:51:40Z",
        "updatedAt": "2022-11-03T15:00:38Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "minimax75"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14916,
        "title": "Multiline condition escaping in YAML",
        "bodyText": "If I am writing a multiline filter condition in TOML, I can split the line for readability by appending an escaping backslash.\n[transforms.trial]\ntype = \"filter\"\ninputs = [\"somewhere\"]\ncondition = \"\"\"\n.message.field1 == \"foo\" \\\n  && (includes([\n      \"bar\",\n      \"baz\",\n    ], .message.field2) \\\n    || .message.elsewhere_id == 3 \\\n  )\n\nThis validates and works fine. However, if I try to use that same multiline condition block in YAML format (namely so the Vector config can be placed in a customConfig block of a Vector Helm chart values file...), the condition fails to validate, with an unexpected syntax token \"Escape\"\ntransforms:\n  trial:\n    type: filter\n    inputs:\n    - somewhere\n    condition: |\n      .message.field1 == \"foo\" \\\n        && (includes([\n            \"bar\",\n            \"baz\",\n          ], .message.field2) \\\n          || .message.elsewhere_id == 3 \\\n      )\n\nI've tried various YAML multiline block formats (|, >, single-quoting strings with newlines, etc). If I remove the escape, then the && operator is unexpected by the VRL parser. Moving the && up to the previous line not just makes for bad readability IMO, but then kicks the unexpected token down to the newline, opening parenthesis, or elsewhere.\nIdeas?",
        "url": "https://github.com/vectordotdev/vector/discussions/14916",
        "createdAt": "2022-10-21T20:54:16Z",
        "updatedAt": "2023-02-03T21:44:35Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "sbalmos"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 14640,
        "title": "Got too many parse json error when we facing to huge volume logs",
        "bodyText": "Hi Team,\nThe log format:\n{..................\n}\nEOF\n---------------\n{................\n}\nEOF\n---------------\n\nVector version 0.23.3\nOur config:\ndata_dir = \"/local/vectorLog\"\n    [sources.gz_logs]\n    type = \"file\"\n    max_line_bytes = 1024000\n    ignore_older_secs = 600\n    include = [ \"/.../year/month/**/request*.gz\" ]\n    multiline.mode = \"halt_with\"\n    multiline.condition_pattern = \"-----------------------------------------------------------------------\"\n    multiline.timeout_ms = 1000\n    multiline.start_pattern = '^\\{'\n    # Parse gz_logs\n    [transforms.parse_logs]\n    type = \"remap\"\n    inputs = [\"gz_logs\"]\n    source = \"\"\"\n    lines,err = split(.message, \"\\n\")\n    .message = lines[0] + lines[1]\n    .message = to_string(.message)\n    .message = parse_json!(.message)\n    \"\"\"\n    [sinks.print]\n    type = \"kafka\"\n    inputs = [ \"parse_logs\" ]\n    bootstrap_servers = \"ip\"\n    topic = \"topic1\"\n    compression = \"none\"\n     [sinks.print.encoding]\n      codec = \"json\"\n\nGot too many error messages when Vector facing huge volume of logs. The error like the following messages.\n2022-09-28T18:26:50.846129Z ERROR transform{component_kind=\"transform\" component_id=parse_logs component_type=remap component_name=parse_logs}: vector::internal_events::remap: Internal log [Mapping failed with event.] has been rate limited 3959 times.\n2022-09-28T18:26:50.846175Z ERROR transform{component_kind=\"transform\" component_id=parse_logs component_type=remap component_name=parse_logs}: vector::internal_events::remap: Mapping failed with event. error=\"function call error for \\\"parse_json\\\" at (122:143): unable to parse json: EOF while parsing an object at line 1 column 23465\" error_type=\"conversion_failed\" stage=\"processing\" internal_log_rate_secs=10\n2022-09-28T18:26:50.846977Z ERROR transform{component_kind=\"transform\" component_id=parse_logs component_type=remap component_name=parse_logs}: vector::internal_events::remap: Internal log [Mapping failed with event.] is being rate limited.\n\nI don't know why it happened. But I found that there are lots of logs missing the last two characters \"}\" due to the errors, so we can not parse it via Logstash due to invalid json format.\nDo I need to adjust something? like max_line_bytes or multiline.timeout_ms to fix it?\nThank you so much.",
        "url": "https://github.com/vectordotdev/vector/discussions/14640",
        "createdAt": "2022-09-29T15:20:55Z",
        "updatedAt": "2022-10-27T01:47:45Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "syzcch"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14390,
        "title": "Can not send data to pulsar through Pulsar Rest API",
        "bodyText": "Hi all, I have a problem about send the messages to Pulsar.\nMy systems require TLS, so I can not use vector sink type pulsar (because the type pulsar does not support TLS ), then I used sink type HTTP to send data to Pulsar through Pulsar Rest API.\nWith Pulsar Rest API, the CURL command sends messages to pulsar success.\n\nBut when I use vector then It happen an error.\nThis is my vector configuration :\n[sources.dummy_logs]\ntype = \"generator\" # required\nlines = [\"{\\\"messages\\\":[{\\\"key\\\":\\\"my-key\\\",\\\"payload\\\":\\\"data at here\\\",\\\"eventTime\\\":1603045262772,\\\"sequenceId\\\":2}]}\"]\nsequence = false # optional, default, relevant when [`format`](#format)\nbatch_interval = 5\nformat = \"shuffle\"\n\n#sink http\n[sinks.http]\ntype = \"http\" # required\ninputs = [ \"dummy_logs\" ]\nrequest.headers.Content-Type = \"application/json\"\nrequest.headers.Accept = \"*/*\"\nrequest.headers.User-Agent = \"PostmanRuntime/7.29.2\"\nrequest.headers.Cache-Control = \"no-cache\"\nrequest.headers.Accept-Encoding = \"gzip, deflate, br\"\nencoding.codec = \"text\"\nuri = \"http://192.168.197.135:8080/topics/persistent/common/stream/topic-test-data\"\n\n\n#[sinks.console]\n#type = \"console\"\n#inputs = [\"dummy_logs\"]\n#encoding.codec = \"text\"\n\nThis is the error message.\n2022-09-13T10:35:06.398321Z ERROR sink{component_kind=\"sink\" component_id=http component_type=http component_name=http}:request{request_id=2}: vector::sinks::util::retries: Not retriable; dropping the request. reason=\"response status: 400 Bad Request\"\n2022-09-13T10:35:06.398458Z ERROR sink{component_kind=\"sink\" component_id=http component_type=http component_name=http}:request{request_id=2}: vector::sinks::util::sink: Response failed. response=Response { status: 400, version: HTTP/1.1, headers: {\"broker-address\": \"localhost\", \"cache-control\": \"must-revalidate,no-cache,no-store\", \"content-type\": \"text/html;charset=iso-8859-1\", \"content-length\": \"550\", \"server\": \"Jetty(9.4.44.v20210927)\"}, body: b\"<html>\\n<head>\\n<meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html;charset=ISO-8859-1\\\"/>\\n<title>Error 400 Bad Request</title>\\n</head>\\n<body><h2>HTTP ERROR 400 Bad Request</h2>\\n<table>\\n<tr><th>URI:</th><td>/topics/persistent/common/stream/topic-test-data</td></tr>\\n<tr><th>STATUS:</th><td>400</td></tr>\\n<tr><th>MESSAGE:</th><td>Bad Request</td></tr>\\n<tr><th>SERVLET:</th><td>org.glassfish.jersey.servlet.ServletContainer-1227862a</td></tr>\\n</table>\\n<hr/><a href=\\\"https://eclipse.org/jetty\\\">Powered by Jetty:// 9.4.44.v20210927</a><hr/>\\n\\n</body>\\n</html>\\n\" }\n\n\nI have build a Rest API, then I received data from the vector. (I change only URI in configuration-vector.toml to my REST API URL)\nThis is raw data from my Rest API :\nrequest.data ====>  b'{\"messages\":[{\"key\":\"my-key\",\"payload\":\"data at here\",\"eventTime\":1603045262772,\"sequenceId\":2}]}\\n'\n\nrequest.headers ====>  Content-Type: application/json\nAccept: */*\nAccept-Encoding: gzip, deflate, br\nCache-Control: no-cache\nUser-Agent: PostmanRuntime/7.29.2\nHost: 192.168.197.135:5000\nContent-Length: 98\n\nrequest.method ====>  POST\nrequest.json ====>  {'messages': [{'key': 'my-key', 'payload': 'data at here', 'eventTime': 1603045262772, 'sequenceId': 2}]}\n\nI don't understand this problem.\nPlease help me.\nThanks all.\nChien Nguyen.",
        "url": "https://github.com/vectordotdev/vector/discussions/14390",
        "createdAt": "2022-09-13T10:52:22Z",
        "updatedAt": "2022-10-26T19:17:36Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ngocchien"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14907,
        "title": "Exemplo syntax AWS S3 sinks",
        "bodyText": "I saw all doc about sinks to AWS S3, but I don`t understud how do put \"AWS_ACCESS_KEY_ID\" and \"AWS_SECRET_ACCESS_KEY\" on sinks in vector.toml. You can give me a exemplo about syntax to use it.\nThank you.",
        "url": "https://github.com/vectordotdev/vector/discussions/14907",
        "createdAt": "2022-10-20T20:17:00Z",
        "updatedAt": "2022-10-26T13:08:13Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "Douglaso"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14926,
        "title": "why pulsar sink so much slower than sink kafka\uff1f",
        "bodyText": "Hi,my vector configuration is as follows:\n[sources.syslog]\ntype = \"syslog\"\naddress = \"0.0.0.0:524\"\nmode = \"tcp\"\npath = \"/path/to/socket\"\n\n# Parse Syslog logs\n[transforms.parse_syslogs]\ntype = \"remap\"\ninputs = [\"syslog\"]\nsource = '''\n. |= parse_syslog!(.message)\n.timestamp = to_timestamp!(to_unix_timestamp!(.timestamp)+3600*8)\n'''\n# Print parsed logs to stdout\n[sinks.printsys]\ntype = \"pulsar\"\ninputs = [\"parse_syslogs\"]\nendpoint = \"pulsar://10.65.172.58:6650\"\ntopic = \"public/default/vector\"\nencoding.codec = \"json\"\nbuffer.type = \"memory\"\nbuffer.when_full = \"block\"\nbuffer.max_events = 5000000\n\nfor example\uff1a\nA(syslog) -> B\uff08vector\uff09 ->C\uff08pulsar\uff09\nby vector top can get\uff1a\nA to B \uff1a60k/s \uff08source/transform\uff09\nB to C \uff1a2k/s (pulsar sink)\nvector server is 4C 8G centos8\uff0c\nand if i chance to kafka sink can get\uff1a\nA to B \uff1a60k/s \uff08source/transform\uff09\nB to C \uff1a60k/s (kafka sink)\nfor example\uff1a\n[sources.syslog]\ntype = \"syslog\"\naddress = \"0.0.0.0:524\"\nmode = \"tcp\"\npath = \"/path/to/socket\"\n\n# Parse Syslog logs\n[transforms.parse_syslogs]\ntype = \"remap\"\ninputs = [\"syslog\"]\nsource = '''\n. |= parse_syslog!(.message)\n.timestamp = to_timestamp!(to_unix_timestamp!(.timestamp)+3600*8)\n'''\n# Print parsed logs to stdout\n[sinks.printsys]\ntype = \"pulsar\"\ninputs = [\"parse_syslogs\"]\nbootstrap_servers = \"10.65.172.58:9092/kafka\"\ntopic = \"mykafka\"\nkey_field = \"user_id\"\ncompression = \"none\"\nencoding.codec = \"json\"\nbuffer.type = \"memory\"\nbuffer.when_full = \"block\"\nbuffer.max_events = 5000000\n\nps\uff1abuffer set 5000000 is more than client syslog events\uff0cso is not exist block\nI want to know if there is a configuration problem for pulsar sink\uff1f\nI don't think 2k/s is normal for pulsar sink\uff0cand why doesn't pulsar have batch configuration like kafka?\nThank you very much\uff01Hope to receive your help soon\uff01",
        "url": "https://github.com/vectordotdev/vector/discussions/14926",
        "createdAt": "2022-10-24T11:36:55Z",
        "updatedAt": "2022-10-25T13:52:36Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "littlejoyo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 14881,
        "title": "VRL emitting multiple events loses event metadata?",
        "bodyText": "When a VRL transform emits multiple events, it does not copy over, or otherwise include at all, any event metadata. Subsequent transforms and sinks lose access to the source_type, timestamp, and other Vector-generated fields, along with metadata that is generated from the source. Is there a reason for this?\nCurrently now, I have to do something like the following to basically \"reconstruct\" the metadata in a VRL transform before emitting the multiple events. For reference, this is when reading Azure Event Hub events using a Kafka source. The events are batched by Event Hub into a records array.\nmetadata = .\ndel(metadata.message)\nrecords = parse_json!(.message).records\nrecords = map_values(array!(records)) -> |record| {\n  entry = {}\n  entry |= metadata\n  entry.message = record\n  entry\n}\n. = records\n\nAll so I can maintain the original event structure when emitting the multiple values.",
        "url": "https://github.com/vectordotdev/vector/discussions/14881",
        "createdAt": "2022-10-18T20:32:24Z",
        "updatedAt": "2022-10-21T20:43:18Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "sbalmos"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14824,
        "title": "Testing vector config by reading vector tap from stdin",
        "bodyText": "I thought it would be handy to tap my log stream and pipe it straight into a local instance of vector configured to read from stdin and send to console.  The problem is the stdin source creates a whole new event for each event coming from the tap.  I was hoping to find a raw option on the stdin source that just read in existing events over stdin without creating a new event.\nIs there another source that does this?  Or perhaps I'm not understanding the descriptions of the decoding.codec options, maybe one of those does what I am asking.  Is this possible in some other way I've not considered?  Basically I want to do:\nvector tap foo | vector -C config/ -w\n\nSo I can quickly iterate on my transforms.\nsources:\n  in:\n    type: stdin\n\n# transforms:\n# ...\n\nsinks:\n  out:\n    inputs:\n      - in\n    type: console\n    encoding:\n      codec: json",
        "url": "https://github.com/vectordotdev/vector/discussions/14824",
        "createdAt": "2022-10-12T20:47:20Z",
        "updatedAt": "2022-10-14T22:05:58Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "godber"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14733,
        "title": "failed to parse field [kubernetes.labels.app] of type [text] in document with id",
        "bodyText": "i got this error when try to send logs Kubernetes throw the vector to elasticsearch.\ni already make and index template in elastic.\nCan't get text on a START_OBJECT at 1:363\\\"}}}},{\\\"index\\\":{\\\"_index\\\":\\\"vector-2022-10-05\\\",\\\"_id\\\":\\\"-CKeqIMBTd8AVIkBXvKQ\\\",\\\"status\\\":400,\\\"error\\\":{\\\"type\\\":\\\"mapper_parsing_exception\\\",\\\"reason\\\":\\\"failed to parse field [kubernetes.labels.app] of type [text] in document with id '-CKeqIMBTd8AVIkBXvKQ'. Preview of field's value: '{kubernetes={io/component=mongod}}'\\\",\\\"caused_by\\\":{\\\"type\\\":\\\"illegal_state_exception\\\",\\\"reason\\\":\\\"Can't get text on a START_OBJECT at 1:363\\\"}}}},{\\\"index\\\"",
        "url": "https://github.com/vectordotdev/vector/discussions/14733",
        "createdAt": "2022-10-05T14:52:34Z",
        "updatedAt": "2022-10-14T14:15:03Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "therus000"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14794,
        "title": "Convert timezoned timestamp to UTC",
        "bodyText": "Our application log contains timestamps and timezone data, so according to vectors doco it will not convert it to UTC. We want to convert this local timestamp to UTC time.\nOur elasticsearch instance is expecting UTC time so automatically adds the necessary 10/11 hours onto the timestamp, and the machine is set with the local timezone due to application restrictions.  We are trying to convince the developers to log the times in UTC time but this seems to be an uphill battle.\nHere is an example of a log entry with a timezone:\n[2022-10-07T13:54:27,699+1100] WARN  [class] - <log message>\nI would like this timezone to appear in the vector output as UTC time so that in Elasticsearch, the messages line up with the correct timestamps.\nI have tried to use the parse_timestamp and to_timestamp functions with no success, and have also found the Timestamp format option in the global 'enrichment_tables.file.schema' property, but due to the lack of documentation around this, have not been able to get it working - I don't even know what it would look like in the /etc/vector/vector.toml file. I understand the documentation is automatically generated, but the 'examples' just show the possible values, not how the properties are used, so they don't really help.\nWhen attempting to use parse_timestamp, I did the following:\n.timestamp = parse_timestamp!(.timestamp, format: \"%Y-%m-%dT%H:%M:%S,%f%z\")\nThis did not work, even though the format string worked fine in a python datestamp.strptime() function - and even if it did parse the date properly it it would only be the first step anyway as I still need to convert it.\nWe also tried updating the app logs to remove the timezone offset all together, so now it looks like this:\n[2022-10-07T13:54:27,699] WARN  [class] - <log message>\nAccording to the doco, this should mean that it detects the timestamp as local time, and behind the scenes converts it to UTC, then passes it to Elasticsearch as UTC, which should work for us.  This however did not work, and we continue to see times in Elasticsearch that are 10 hours out of sync.\nCan someone please help me work out what is going wrong here? Thanks",
        "url": "https://github.com/vectordotdev/vector/discussions/14794",
        "createdAt": "2022-10-11T04:59:11Z",
        "updatedAt": "2022-10-14T03:16:18Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "jonathanderham-streamotion"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14683,
        "title": "How to debug AWS errors?",
        "bodyText": "I'm using the S3 source with SQS and we've noticed we're getting quite a few network errors fetching the files from S3, mostly \"TransientError: connection close before message completed\". Rate is about 10 per hour at peak\nIs there a way to get more verbose debugging of the AWS API calls to try and work out what the issue is? I have a vauge recollection on another thread where there was an environment variable we could set to just get verbose debugging on AWS calls but I can't recall it.",
        "url": "https://github.com/vectordotdev/vector/discussions/14683",
        "createdAt": "2022-10-03T10:10:01Z",
        "updatedAt": "2022-10-11T14:00:54Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "NeilJed"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 14592,
        "title": "Why do I need to wait half an hour for INFO: found new file?",
        "bodyText": "you can see I started at 2022-09-27T06:58:56,the first INFO about \"Found new file to watch\" is at 2022-09-27T07:21:23.\n2022-09-27T06:58:56.292414Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.011948470014872512\n2022-09-27T06:59:01.292041Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.0011950196332200598\n2022-09-27T06:59:06.292266Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.00011966161615416054\n2022-09-27T06:59:11.292703Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.000012125088649249184\n2022-09-27T06:59:16.292812Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.0000013469674660780495\n2022-09-27T06:59:21.292293Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.0000002783504350525164\n2022-09-27T06:59:26.292669Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.00000018658386359462327\n2022-09-27T06:59:31.292388Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.00000020280995875050707\n.......\n2022-09-27T07:21:21.292625Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.00000004221577949292724\n2022-09-27T07:21:23.965736Z DEBUG source{component_kind=\"source\" component_id=gz_logs component_type=file component_name=gz_logs}:file_server: file_source::file_watcher: Not reading gzipped file older than `ignore_older`. path=\"/onlinelogs/..../requests.log.2022-09-25-23.msps-t1-na-02-1e-m54-df006168.us-east-1.gz\"\n2022-09-27T07:21:23.965819Z  INFO source{component_kind=\"source\" component_id=gz_logs component_type=file component_name=gz_logs}:file_server: vector::internal_events::file::source: Found new file to watch. file=/onlinelogs/..../requests.log.2022-09-25-23.msps-t1-na-02-1e-m54-df006168.us-east-1.gz\n2022-09-27T07:21:23.970066Z DEBUG source{component_kind=\"source\" component_id=gz_logs component_type=file component_name=gz_logs}:file_server: file_source::file_watcher: Not reading gzipped file older than `ignore_older`. path=\"/onlinelogs/..../requests.log.2022-09-25-23.msps-t1-na-02-1e-m54-d484253c.us-east-1.gz\"\n........\n\nwhen include =  [ \"/onlinelogs/.../2022/09/25/00/request*.gz\u201d ] ,it can INFO:found new file immediately.There are 24 matched gzip files at /onlinelogs/.../2022/09/25/00/.\nwhen include =  [ \"/onlinelogs/.../2022/09/25/**/request*.gz\u201d ] ,I need to wait about half an hour for INFO: found new file.There are 24*24 matched gzip files at /onlinelogs/.../2022/09/25/**/.\nwhen include =  [ \"/onlinelogs/.../2022/09/**/request*.gz\u201d ],there is no INFO:found new file though I have waited for 1 hour.There are 24*24*30 matched gzip files at /onlinelogs/.../2022/09/**/.\neach matched gzip file is 3GB.",
        "url": "https://github.com/vectordotdev/vector/discussions/14592",
        "createdAt": "2022-09-27T08:54:00Z",
        "updatedAt": "2022-10-11T00:11:53Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "summerance"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14773,
        "title": "Splunk HEC Sink support for channel parameter",
        "bodyText": "Hi:\nSplunk HEC collector supports a channel parameter.  In Vector's Splunk Sink docs, I could not find this parameter.\nPlease let me know if this will be supported in the future and whether you have any recommendation on how to pass it with the current implementation.\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/14773",
        "createdAt": "2022-10-07T17:28:35Z",
        "updatedAt": "2022-10-08T18:54:41Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mans2singh"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14777,
        "title": "k8s remap",
        "bodyText": "I want to add tag field, but meet some errors.\nvector config\ntransforms:\n  k8s_logs_remap:\n    type: remap\n    inputs: [kubernetes_logs]\n    source: |-\n      .k8s.container_image = .kubernetes.container_image\n      .k8s.container_name = .kubernetes.container_name\n      .k8s.pod_labels = .kubernetes.pod_labels\n      .k8s.pod_name = .kubernetes.pod_name\n      .k8s.pod_namespace = .kubernetes.pod_namespace\n      del(.file)\n      del(.kubernetes)\n      del(.timestamp_end)\n      app_name =\n        get(.k8s.pod_labels, path: [\"app\"]) ??\n        get!(.k8s.pod_labels, path: [\"app.kubernetes.io/name\"])\n      .tag = join!([.k8s.pod_namespace, app_name, .k8s.container_name], separator: \"_\")\nvector error log\n2022-10-08T08:09:58.381975Z ERROR transform{component_kind=\"transform\" component_id=k8s_logs_remap component_type=remap component_name=k8s_logs_remap}: vector::internal_events::remap: Mapping failed with event. error=\"function call error for \\\"join\\\" at (390:464): all array items must be strings\" error_type=\"conversion_failed\" stage=\"processing\" internal_log_rate_secs=10\noriginal k8s log\n{\"file\":\"/var/log/pods/kube-system_csi-nas-node-2x89w_d65c1800-07b2-456d-84a5-12f1e17a4fc2/csi-nas-driver/0.log\",\"kubernetes\":{\"container_id\":\"containerd://c2328b4339a4ed405eb7c5af56658fb1c84c10285bfda9c0ce313f3d36c31d8d\",\"container_image\":\"cr-cn-beijing.volces.com/vke/nasplugin:v2.2\",\"container_name\":\"csi-nas-driver\",\"node_labels\":{\"beta.kubernetes.io/arch\":\"amd64\",\"beta.kubernetes.io/instance-type\":\"ecs.g2a.xlarge\",\"beta.kubernetes.io/os\":\"linux\",\"cluster.vke.volcengine.com/cluster-name\":\"ccckoecnqtofha2idl2r0\",\"cluster.vke.volcengine.com/machine-name\":\"ncclf926rsfemnke36680\",\"cluster.vke.volcengine.com/machinepool-name\":\"pcckoecnqtofn9k4frlkg\",\"cluster.vke.volcengine.com/machinepool-type\":\"machine-pool\",\"cluster.vke.volcengine.com/node-name\":\"node-rk8g4g\",\"failure-domain.beta.kubernetes.io/region\":\"cn-beijing\",\"failure-domain.beta.kubernetes.io/zone\":\"cn-beijing-a\",\"kubernetes.io/arch\":\"amd64\",\"kubernetes.io/hostname\":\"10.0.1.159\",\"kubernetes.io/os\":\"linux\",\"node.kubernetes.io/instance-type\":\"ecs.g2a.xlarge\",\"topology.kubernetes.io/region\":\"cn-beijing\",\"topology.kubernetes.io/zone\":\"cn-beijing-a\"},\"pod_ip\":\"10.0.1.159\",\"pod_ips\":[\"10.0.1.159\"],\"pod_labels\":{\"app.kubernetes.io/instance\":\"csi-nas\",\"app.kubernetes.io/name\":\"csi-nas-node\",\"controller-revision-hash\":\"6fd79968d7\",\"pod-template-generation\":\"1\"},\"pod_name\":\"csi-nas-node-2x89w\",\"pod_namespace\":\"kube-system\",\"pod_node_name\":\"10.0.1.159\",\"pod_owner\":\"DaemonSet/csi-nas-node\",\"pod_uid\":\"d65c1800-07b2-456d-84a5-12f1e17a4fc2\"},\"message\":\"time=\\\"2022-10-08T08:09:56Z\\\" level=info msg=\\\"return {\\\\\\\"ready\\\\\\\":{\\\\\\\"value\\\\\\\":true}}\\\" event=grpc_response method=/csi.v1.Identity/Probe\",\"source_type\":\"kubernetes_logs\",\"stream\":\"stderr\",\"timestamp\":\"2022-10-08T08:09:56.679886349Z\",\"timestamp_end\":\"2022-10-08T08:09:56.679886349Z\"}",
        "url": "https://github.com/vectordotdev/vector/discussions/14777",
        "createdAt": "2022-10-08T09:04:00Z",
        "updatedAt": "2022-10-08T10:00:03Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "panpan-wu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14730,
        "title": "Expected behaviour? internal_metrics values monotonically increasing even after taking monotonic difference",
        "bodyText": "monotonic_diff(sum:vector.component_sent_event_bytes_total{component_kind:source,component_id:internal_metrics} by {component_id}.as_count())\n\nCurrently using this datadog query but observing the values monotonically increasing even when using the monotonic_diff function (which graphs the delta of the metric like\u00a0diff()\u00a0but only if the delta is positive). We are not sure if this is expected behavior or otherwise as we are only noticing this for internal_metrics source but not kubernetes_logs source?",
        "url": "https://github.com/vectordotdev/vector/discussions/14730",
        "createdAt": "2022-10-05T10:06:44Z",
        "updatedAt": "2022-10-07T06:59:18Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "leongshengmin"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14662,
        "title": "upload crt in vector using helm",
        "bodyText": "Hi, I've a question.\nHow I can upload a CRT file in vector deployed in helm?",
        "url": "https://github.com/vectordotdev/vector/discussions/14662",
        "createdAt": "2022-09-30T13:36:03Z",
        "updatedAt": "2022-10-06T12:56:28Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "francescoblefari"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14700,
        "title": "Error (invalid escape character \\u) while trying to create a message in vrl console",
        "bodyText": "Hey Folks:\nI am trying to go through the examples in vector docs and saw an example with embedded unicode char.  When I try to enter it in vrl console, it gives me an error as shown below.  If I create a message with simple text it succeeds.\nCan you please let me know what could be the issue ?\nThanks\n\n$ message_test = {\n\"message\": \"\\u003c102\\u003e1 2020-12-22T15:22:31.111Z vector-user.biz su 2666 ID389 - Something went wrong\"\n}\n\nerror[E202]: syntax error\n\u250c\u2500 :1:1\n\u2502\n1 \u2502 \u256d message_test = {\n2 \u2502 \u2502   \"message\": \"\\u003c102\\u003e1 2020-12-22T15:22:31.111Z vector-user.biz su 2666 ID389 - Something went wrong\"\n3 \u2502 \u2502 }\n\u2502 \u2570\u2500^ unexpected error: invalid escape character: \\u\n\u2502\n= see language documentation at https://vrl.dev\n= try your code in the VRL REPL, learn more at https://vrl.dev/examples",
        "url": "https://github.com/vectordotdev/vector/discussions/14700",
        "createdAt": "2022-10-03T21:26:48Z",
        "updatedAt": "2022-10-05T20:22:59Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mans2singh"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14660,
        "title": "Http methods support for http_scrape",
        "bodyText": "Hi Folks:\nI am trying to use http_scrape source which is currently in master branch.\nI want to use http POST so that the http server can return a customized response (metrics/events) based on the body.\nHowever, I did not find any configuration for http methods for http_scrape (like in the http component). It looks like http_scrape only supports GET.\nPlease let me know if the http_scrape supports other http methods and where can I find some documentation if I have missed it.\nThanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/14660",
        "createdAt": "2022-09-30T13:27:48Z",
        "updatedAt": "2022-09-30T14:25:26Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mans2singh"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14583,
        "title": "http_scrape query",
        "bodyText": "Hi:\nI am trying to use http_scape and wanted to find out if it supports template for query parameters.    My use case is that I need to pass start and end time parameters to the http request.\nI have checked the documentation for http_scrape but did not see this option.\nIf I have missed the docs, please let me know if it is possible to template the params.\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/14583",
        "createdAt": "2022-09-26T18:14:29Z",
        "updatedAt": "2022-09-30T13:30:17Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mans2singh"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14632,
        "title": "Does Transforms support to get Data from APIs and do some business logic in vector?",
        "bodyText": "There is a requirement in which source is SQS, sink is redis, but Transforms needs to get some data from APIs and merge these data from APIs and source through some business logics.\nIn vector, could Transforms support the requirement? Do you have the plan to support it?",
        "url": "https://github.com/vectordotdev/vector/discussions/14632",
        "createdAt": "2022-09-29T10:11:49Z",
        "updatedAt": "2022-09-30T07:25:21Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "qwqmobile"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 14539,
        "title": "why can't find new file",
        "bodyText": "when my include is\ninclude = \"/onlinelogs/Prod/2022/09/19/00/requests*.gz\"\n\"INFO:Found new file to watch\"appears .But wen\ninclude = \"/onlinelogs/Prod/2022/09/19/**/requests*.gz\"\nThere is no INFO about found new file.\nThere are 600 logs matched the 2022/09/19/**/requests*.gz. Each gz file with a size of 1GB. Is it because there are too many gz files under this folder, so new files cannot be found?\nmy config is\ndata_dir = \"/local/vectorLog\"\n[sources.gz_logs]\ntype = \"file\"\nmax_line_bytes = 102400000\nignore_older_secs = 60\ninclude =[  \"/onlinelogs/Prod/2022/09/19/**/requests*.gz\"]\n......\n\nvery urgent!Looking forward for reply!",
        "url": "https://github.com/vectordotdev/vector/discussions/14539",
        "createdAt": "2022-09-23T15:38:07Z",
        "updatedAt": "2022-09-28T13:24:23Z",
        "isAnswered": true,
        "locked": true,
        "author": {
            "login": "summerance"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 14591,
        "title": "ERROR vector_common::shutdown",
        "bodyText": "Today when I tried it for the first time, everything was normal, and I ran part of the logs then killed the vector process but the second time I tried the same configuration:\n2022-09-26T10:56:27.700249Z  INFO vector: Vector has started. debug=\"false\" version=\"0.23.3\" arch=\"x86_64\" build_id=\"af8c9e1 2022-08-10\"\n2022-09-26T10:56:27.700300Z  INFO vector::app: API is disabled, enable by setting `api.enabled` to `true` and use commands like `vector top`.\n2022-09-26T10:56:27.701532Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.03563312858561618\n2022-09-26T10:56:27.706640Z  INFO vector::topology::builder: Healthcheck: Passed.\n2022-09-26T10:56:27.707175Z  INFO source{component_kind=\"source\" component_id=gz_logs component_type=file component_name=gz_logs}:file_server: file_source::checkpointer: Loaded checkpoint data.\n2022-09-26T10:56:32.701949Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.0035634098708080533\n2022-09-26T10:56:37.701961Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.0003564376470193585\n2022-09-26T10:56:42.702257Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.0000357492388535485\n2022-09-26T10:56:47.701564Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.0000037288444315635125\n2022-09-26T10:56:52.701853Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.0000005094966011423447\n2022-09-26T10:56:57.702025Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.0000001776653743357492\n2022-09-26T10:57:02.701997Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.0000001282872748954471\n2022-09-26T10:57:07.701478Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.00000018403047716643056\n2022-09-26T10:57:12.701871Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.00000029557445404319154\n2022-09-26T10:57:17.702006Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.00000035552820618820824\n2022-09-26T10:57:22.701870Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.0000002760394609204501\n2022-09-26T10:57:27.701512Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.0000002135572931125849\n2022-09-26T10:57:32.701541Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.00000021377695416511442\n2022-09-26T10:57:37.701436Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.00000018086280703363063\n2022-09-26T10:57:42.701365Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.0000004275835011360716\n\n\nAfter waiting for a while, there was no INFO about \" file_server: vector::internal_events::file::source: Found new file to watch.\"So I Ctrl + c to stop the vector\nIt showed these errors additionally:\n2022-09-26T11:02:37.459970Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.00000016625778030534048\n2022-09-26T11:02:37.808384Z  INFO vector::topology::running: Shutting down... Waiting on running components. remaining_components=\"parse_logs, gz_logs, print\" time_remaining=\"4 seconds left\"\n2022-09-26T11:02:42.460248Z DEBUG sink{component_kind=\"sink\" component_id=print component_type=kafka component_name=print}: vector::utilization: utilization=0.00000011021953427571457\n2022-09-26T11:02:42.807773Z ERROR vector_common::shutdown: Source 'gz_logs' failed to shutdown before deadline. Forcing shutdown.\n2022-09-26T11:02:42.807847Z ERROR vector::topology::running: Failed to gracefully shut down in time. Killing components. components=\"parse_logs, gz_logs, print\"\n2022-09-26T11:02:42.807898Z DEBUG source{component_kind=\"source\" component_id=gz_logs component_type=file component_name=gz_logs}: vector::topology::builder: Finished.\n\nmy vector version is 0.23.3.",
        "url": "https://github.com/vectordotdev/vector/discussions/14591",
        "createdAt": "2022-09-27T08:32:29Z",
        "updatedAt": "2022-09-27T12:34:49Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "summerance"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14533,
        "title": "Failed writing checkpoints. error=Permission denied",
        "bodyText": "When I execute vector with sudo, two processes appear,\nroot      1570  0.0  0.0 238136  7344 pts/0    S+   10:48   0:00 sudo ./vector --config /root/.vector/config/vector.toml\nroot      1571  0.5  0.0 1629564 34744 pts/0   Sl+  10:48   0:01 ./vector --config /root/.vector/config/vector.toml\n\nso vector cannot find new files. But when I don't use sudo it gives an error.\nFailed writing checkpoints. error=Permission denied (os error 13) error_code=\"writing_checkpoints\" error_type=\"writer_failed\" stage=\"receiving\"\n\nmy config is\ndata_dir = \"/local/vectorLog\"\n[sources.gz_logs]\ntype = \"file\"\nmax_line_bytes = 1024000\nignore_older_secs = 60\ninclude =[ \"/logs/2022/09/21/**/requests*.gz\"]\n......",
        "url": "https://github.com/vectordotdev/vector/discussions/14533",
        "createdAt": "2022-09-23T13:44:22Z",
        "updatedAt": "2022-09-26T16:13:30Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "summerance"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14047,
        "title": "Handling s3:TestEvent in aws_s3 source",
        "bodyText": "When using the S3 source, AWS sends a s3:TestEvent as the first message into the SQS queue (ref official docs, blog post).\nThe message looks like:\n{\n    \"Messages\": [\n        {\n            \"MessageId\": \"c8f6eb9e-4c90-4154-9502-5a4d9dc5c78e\",\n            \"ReceiptHandle\": \"XXX...XXX==\",\n            \"MD5OfBody\": \"b33da647a485a3745e1111277ec590d2\",\n            \"Body\": \"{\\\"Service\\\":\\\"Amazon S3\\\",\\\"Event\\\":\\\"s3:TestEvent\\\",\\\"Time\\\":\\\"2022-08-11T14:22:40.084Z\\\",\\\"Bucket\\\":\\\"vector-log-pipeline--xxx\\\",\\\"RequestId\\\":\\\"1YXFCPKNH9RYDP5P\\\",\\\"HostId\\\":\\\"Jq470yic8yEPATd6Hw3BmamYkq7l/vb9QKrIZWp46LnrGQmvDsGnitgmylQs/jfW7QbE2RkML1w=\\\"}\"\n        }\n    ]\n}\nAnd vector doesn't like it, with our logs filling up with:\n{\n  \"timestamp\": \"2022-08-15T09:18:16.410889Z\",\n  \"level\": \"ERROR\",\n  \"message\": \"Failed to process SQS message.\",\n  \"message_id\": \"c8f6eb9e-4c90-4154-9502-5a4d9dc5c78e\",\n  \"error\": \"Could not parse SQS message with id c8f6eb9e-4c90-4154-9502-5a4d9dc5c78e as S3 notification: missing field `Records` at line 1 column 247\",\n  \"error_code\": \"failed_processing_sqs_message\",\n  \"error_type\": \"parser_failed\",\n  \"stage\": \"processing\",\n  \"target\": \"vector::internal_events::aws_sqs::s3\",\n  \"span\": {\n    \"component_id\": \"s3\",\n    \"component_kind\": \"source\",\n    \"component_name\": \"s3\",\n    \"component_type\": \"aws_s3\",\n    \"name\": \"source\"\n  },\n  \"spans\": [\n    {\n      \"component_id\": \"s3\",\n      \"component_kind\": \"source\",\n      \"component_name\": \"s3\",\n      \"component_type\": \"aws_s3\",\n      \"name\": \"source\"\n    }\n  ]\n}\nWhat options do we have for making vector handle it quietly/properly?",
        "url": "https://github.com/vectordotdev/vector/discussions/14047",
        "createdAt": "2022-08-22T09:26:08Z",
        "updatedAt": "2022-09-26T14:00:10Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "sean-snyk"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 2
    },
    {
        "number": 14559,
        "title": "ERROR vector::internal_events::file::source: Failed writing checkpoints.",
        "bodyText": "After vector is running,these ERROR appears.\n2022-09-24T13:58:35.012141Z  INFO source{component_kind=\"source\" component_id=gz_logs component_type=file component_name=gz_logs}:file_server: vector::internal_events::file::source: Found new file to watch. file=/log/requests.log.2022-09-22-00.msps-t1-na-01-1a-m54-e7bc3645.us-east-1.amazon.com.gz\n2022-09-24T13:58:35.479899Z  INFO source{component_kind=\"source\" component_id=gz_logs component_type=file component_name=gz_logs}:file_server: vector::internal_events::file::source: Found new file to watch. file=/log/requests.log.2022-09-22-00.msps-t1-na-01-1a-m54-7a1c76c2.us-east-1.amazon.com.gz\n2022-09-24T13:58:35.942251Z  INFO source{component_kind=\"source\" component_id=gz_logs component_type=file component_name=gz_logs}:file_server: vector::internal_events::file::source: Found new file to watch. file=/log/requests.log.2022-09-22-00.msps-t1-na-01-1a-m54-520ebde2.us-east-1.amazon.com.gz\n2022-09-24T13:58:36.399006Z  INFO source{component_kind=\"source\" component_id=gz_logs component_type=file component_name=gz_logs}:file_server: vector::internal_events::file::source: Found new file to watch. file=/log/requests.log.2022-09-22-00.msps-t1-na-01-1a-m54-3f80ff4f.us-east-1.amazon.com.gz\n2022-09-24T13:58:37.511052Z ERROR vector::internal_events::file::source: Failed writing checkpoints. error=Permission denied (os error 13) error_code=\"writing_checkpoints\" error_type=\"writer_failed\" stage=\"receiving\"\n2022-09-24T13:58:38.512938Z ERROR vector::internal_events::file::source: Failed writing checkpoints. error=Permission denied (os error 13) error_code=\"writing_checkpoints\" error_type=\"writer_failed\" stage=\"receiving\"\n2022-09-24T13:58:39.513531Z ERROR vector::internal_events::file::source: Failed writing checkpoints. error=Permission denied (os error 13) error_code=\"writing_checkpoints\" error_type=\"writer_failed\" stage=\"receiving\"\n2022-09-24T13:58:40.515167Z ERROR vector::internal_events::file::source: Failed writing checkpoints. error=Permission denied (os error 13) error_code=\"writing_checkpoints\" error_type=\"writer_failed\" stage=\"receiving\"",
        "url": "https://github.com/vectordotdev/vector/discussions/14559",
        "createdAt": "2022-09-24T14:10:09Z",
        "updatedAt": "2022-09-26T13:22:44Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "summerance"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14555,
        "title": "Any suggestion for making dev env faster",
        "bodyText": "vector is such a huge project that everytime I change some lines of code, I have to wait for rust-analyzer running cargo check for over 2 minutes. If I run some tests, it'll trigger linking, and it's even cost me over 5~8 minutes to wait for the results.\nThat's really frustrating for me to contribute to this project. Any idea to make compiling or developing a little bit happier? I don't know how your dev experience look like",
        "url": "https://github.com/vectordotdev/vector/discussions/14555",
        "createdAt": "2022-09-24T10:16:16Z",
        "updatedAt": "2022-09-26T02:00:41Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "caibirdme"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14554,
        "title": "What's the proxy configuration",
        "bodyText": "I found proxy config in sources like nginx prometheus ... and sinks like clickhouse. But I don't really understand what does it mean?\nWhen should we use the proxy?",
        "url": "https://github.com/vectordotdev/vector/discussions/14554",
        "createdAt": "2022-09-24T07:10:34Z",
        "updatedAt": "2023-02-03T21:45:19Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "caibirdme"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14460,
        "title": "Events miss/ Data loss",
        "bodyText": "Hello Team,\nSo we were using mix panel to capture our mobile events and get insights of user behaviour from it. We came with a case that we need this data in our system as our analyst team needs to do some insights of that data and make querying and availability easy. For this case we came across vector.dev and found it suitable for our scenario.\nSo our vector works in a way that our mobile sdk send events to vector and then vector sinks it to mixpanel as well as to s3 buckets. Vector is running in a Kubernetes environment on AWS eks using c6i.2xlarge(as recommended on documentation) node with ALB attached on it to handle and load balance traffic.\nWe did a small POC and things looked fine. All the data was captured and processed as expected.\nWhen taken into production with 100% load the issue we started to face is events getting missed. We noticed a significant drop in our events being captured. The mobile side is verified that all the events are being published but not sure what can be the case on vector side.\nLooking forward for suggestions and discussion to figure out the root cause. One hypothesis is that ALB is causing a bottleneck when events are fired to vector on high throughput.",
        "url": "https://github.com/vectordotdev/vector/discussions/14460",
        "createdAt": "2022-09-18T14:13:37Z",
        "updatedAt": "2022-09-23T21:38:12Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "yahya-zuberi"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14304,
        "title": "Setting Vector up in production env, but got some wired error msgs, please help us. Urgent!",
        "bodyText": "Hi Team,\nWe are trying to use Vector to fetch gzip log files in production environment and we are facing tight deadline but we got error messages like this:\n2022-09-07T07:48:07.860813Z ERROR transform{component_kind=\"transform\" component_id=parse_logs component_type=remap component_name=parse_logs}: vector::internal_eve\nnts::remap: Internal log [Mapping failed with event.] has been rate limited 3711 times.\n2022-09-07T07:48:07.860936Z ERROR transform{component_kind=\"transform\" component_id=parse_logs component_type=remap component_name=parse_logs}: vector::internal_eve\nnts::remap: Mapping failed with event. error=\"function call error for \\\"parse_json\\\" at (106:127): unable to parse json: invalid number at line 1 column 2\" error_ty\npe=\"conversion_failed\" stage=\"processing\" internal_log_rate_secs=10\n2022-09-07T07:48:07.861021Z ERROR transform{component_kind=\"transform\" component_id=parse_logs component_type=remap component_name=parse_logs}: vector::internal_eve\nnts::remap: Internal log [Mapping failed with event.] is being rate limited.\n2022-09-07T07:48:31.829761Z ERROR transform{component_kind=\"transform\" component_id=parse_logs component_type=remap component_name=parse_logs}: vector::internal_eve\nnts::remap: Internal log [Mapping failed with event.] has been rate limited 4323 times.\n2022-09-07T07:48:31.829801Z ERROR transform{component_kind=\"transform\" component_id=parse_logs component_type=remap component_name=parse_logs}: vector::internal_eve\nnts::remap: Mapping failed with event. error=\"function call error for \\\"parse_json\\\" at (106:127): unable to parse json: invalid number at line 1 column 2\" error_ty\npe=\"conversion_failed\" stage=\"processing\" internal_log_rate_secs=10\n2022-09-07T07:48:31.829894Z ERROR transform{component_kind=\"transform\" component_id=parse_logs component_type=remap component_name=parse_logs}: vector::internal_eve\nnts::remap: Internal log [Mapping failed with event.] is being rate limited.\n\nAnd out config is like that\n[sources.gz_logs]\n    type = \"file\"\n    max_line_bytes = 1024000\n    include = [ \"/.../**/*.gz\" ]\n    exclude = [ \"/../**/*.gz\" , \"/.../**/*.gz\" ......]\n    multiline.mode = \"halt_with\"\n    multiline.condition_pattern = \"-----------------------------------------------------------------------\"\n    multiline.timeout_ms = 1000\n    multiline.start_pattern = '^\\{'\n    # Parse gz_logs\n[transforms.parse_logs]\n    type = \"remap\"\n    inputs = [\"gz_logs\"]\n    source = \"\"\"\n    lines,err = split(.message, \"\\n\")\n    .message = lines[0] + lines[1]\n    .message = to_string(.message)\n    .message = parse_json!(.message)\n    \"\"\"\n    # send parsed logs to Kafka\n   ......\n\nCould you please help us to figure the errors out? Thank you so much.",
        "url": "https://github.com/vectordotdev/vector/discussions/14304",
        "createdAt": "2022-09-07T08:14:58Z",
        "updatedAt": "2022-09-23T09:07:23Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "syzcch"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 14474,
        "title": "update config file",
        "bodyText": "Hi, I'm using helm. Can I update the config file? If yes, What's the method? Thank you.",
        "url": "https://github.com/vectordotdev/vector/discussions/14474",
        "createdAt": "2022-09-19T22:08:44Z",
        "updatedAt": "2022-09-20T18:26:48Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "francescoblefari"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14472,
        "title": "Http Sources ingesting data after 3 invocations each time",
        "bodyText": "Hello,\nso I am using vector and ingesting events in it using http source. The issue I am facing is that after every 3 consecutive invocations vector issue 1 event in. Here is my sources code:\nsources:\n    src:\n      type: http\n      acknowledgements:\n        enabled: false\n      address: \"[::]:9000\"\n      encoding: text\n      path: \"\"\n      strict_path: false\n      headers:\n        - User-Agent\n        - x-forwarded-for\n        - HTTP_X_REAL_IP \n\nAny idea why is this happening?",
        "url": "https://github.com/vectordotdev/vector/discussions/14472",
        "createdAt": "2022-09-19T19:51:08Z",
        "updatedAt": "2022-09-19T21:14:18Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "yahya-zuberi"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14430,
        "title": "How to sink original data through sink type http",
        "bodyText": "My configuration for vector :\n[sources.dummy_logs]\ntype = \"generator\" # required\nlines = [\"{\\\"messages\\\":[{\\\"key\\\":\\\"my-key\\\",\\\"payload\\\":\\\"data at here\\\",\\\"eventTime\\\":1603045262772,\\\"sequenceId\\\":2}]}\"]\nsequence = false # optional, default, relevant when [`format`](#format)\nbatch_interval = 3\nformat = \"shuffle\"\n\n[transforms.remap_data]\ntype = \"remap\"\ninputs = [\"dummy_logs\"]\nsource = \"\"\"\n. = parse_json!(.message) \\n\"\"\"\n\n# Print parsed logs to stdout\n[sinks.http]\ntype = \"http\" \ninputs = [\"remap_data\"]\nencoding.codec = \"json\"\nuri = \"http://192.168.197.135:8080\"\n\n\n[sinks.console]\ntype = \"console\"\ninputs = [\"remap_data\"]\nencoding.codec = \"json\"\n#encoding.only_fields = [\"timestamp\", \"thread_id\", \"query_id\", \"severity\", \"message\" ]\n\nOriginal data and print out console :\n{\n    \"messages\": [\n        {\n            \"eventTime\": 1603045262772,\n            \"key\": \"my-key\",\n            \"payload\": \"data at here\",\n            \"sequenceId\": 2\n        }\n    ]\n}\n\nBut my data received on my API :\n[{\"messages\":[{\"eventTime\":1603045262772,\"key\":\"my-key\",\"payload\":\"data at here\",\"sequenceId\":2}]}]\n\nThrough encoding: JSON (sink type http), Vector changes object JSON to array JSON.\nHow to keep original data to send to my API with HTTP Content-Type: application/json\nThanks all.\nChien Nguyen.",
        "url": "https://github.com/vectordotdev/vector/discussions/14430",
        "createdAt": "2022-09-15T09:46:22Z",
        "updatedAt": "2022-09-19T09:05:41Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "ngocchien"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13828,
        "title": "Buffering to disk based on health status",
        "bodyText": "Hello Team, We have a case where we cannot continuously write to disk for buffering the logs(being streamed out from http/tcp/udp) because of certain hardware limitations. To address this case we will need to persist logs in the disk space of the host system only when streaming is not possible i.e., when the health status of the down stream vector instance is bad.  Can you suggest any solution that would come handy to address this case. Thanks",
        "url": "https://github.com/vectordotdev/vector/discussions/13828",
        "createdAt": "2022-08-03T16:51:50Z",
        "updatedAt": "2022-09-18T00:51:58Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "hemanthofficial3009"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 14433,
        "title": "Vector TLS setup",
        "bodyText": "Hi guys,\nI have few questions on TLS setup for vector sink (on vector agent) to vector source (on a vector aggregator) communication.\nI'm on Kubernetes. Here are my versions:\nvector       \tdefault    \t4       \t2022-09-15 16:46:06.698968764 +0200 CEST\tdeployed\tvector-0.16.0      \t0.24.0-distroless-libc\nvector-server\tdefault    \t3       \t2022-09-15 16:33:51.280147386 +0200 CEST\tdeployed\tvector-0.16.0      \t0.24.0-distroless-libc\n\nI did the following config for testing. (generating two self signed)\nOn my vector agent:\n    to_vector:\n      type: vector\n      inputs:\n        - prom\n      address: vector-server.default.svc.cluster.local:6000\n      version: \"2\"\n      healthcheck: false\n      tls:\n        verify_hostname: false\n        verify_certificate: true\n        crt_file: /etc/tls-vector/client.crt\n        key_file: /etc/tls-vector/client.pem\n        key_pass: test\n\nOn my vector aggregator:\n  sources:\n    vector_in:\n      address: 0.0.0.0:6000\n      type: vector\n      version: \"2\"\n      tls:\n        enabled: true\n        verify_certificate: true\n        crt_file: /etc/tls-vector/server.crt\n        key_file: /etc/tls-vector/server.pem\n        key_pass: test\n        ca_file: /etc/tls-vector/client.crt\n\nBut I got the following errors on agent:\n022-09-15T14:54:46.275749Z DEBUG hyper::client::connect::dns: resolving host=\"vector-server.default.svc.cluster.local\"\n2022-09-15T14:54:46.282132Z DEBUG sink{component_kind=\"sink\" component_id=to_vector component_type=vector component_name=to_vector}:request{request_id=1}: hyper::client::connect::http: connecting to 10.43.81.73:6000\n2022-09-15T14:54:46.282848Z DEBUG sink{component_kind=\"sink\" component_id=to_vector component_type=vector component_name=to_vector}:request{request_id=1}: hyper::client::connect::http: connected to 10.43.81.73:6000\n2022-09-15T14:54:46.282966Z DEBUG sink{component_kind=\"sink\" component_id=to_vector component_type=vector component_name=to_vector}:request{request_id=1}: h2::client: binding client connection\n2022-09-15T14:54:46.283150Z DEBUG sink{component_kind=\"sink\" component_id=to_vector component_type=vector component_name=to_vector}:request{request_id=1}: h2::client: client connection bound\n2022-09-15T14:54:46.283228Z DEBUG sink{component_kind=\"sink\" component_id=to_vector component_type=vector component_name=to_vector}:request{request_id=1}: h2::codec::framed_write: send frame=Settings { flags: (0x0), enable_push: 0, initial_window_size: 2097152, max_frame_size: 16384 }\n2022-09-15T14:54:46.283366Z DEBUG sink{component_kind=\"sink\" component_id=to_vector component_type=vector component_name=to_vector}:request{request_id=1}:Connection{peer=Client}: h2::codec::framed_write: send frame=WindowUpdate { stream_id: StreamId(0), size_increment: 5177345 }\n2022-09-15T14:54:46.283583Z DEBUG sink{component_kind=\"sink\" component_id=to_vector component_type=vector component_name=to_vector}:request{request_id=1}: hyper::client::pool: pooling idle connection for (\"http\", vector-server.default.svc.cluster.local:6000)\n2022-09-15T14:54:46.284773Z DEBUG sink{component_kind=\"sink\" component_id=to_vector component_type=vector component_name=to_vector}:request{request_id=1}:Connection{peer=Client}: h2::codec::framed_write: send frame=Headers { stream_id: StreamId(1), flags: (0x4: END_HEADERS) }\n2022-09-15T14:54:46.284878Z DEBUG sink{component_kind=\"sink\" component_id=to_vector component_type=vector component_name=to_vector}:request{request_id=1}:Connection{peer=Client}: h2::codec::framed_write: send frame=Data { stream_id: StreamId(1) }\n2022-09-15T14:54:46.285013Z DEBUG hyper::proto::h2::client: connection error: connection reset\n2022-09-15T14:54:46.285056Z DEBUG hyper::proto::h2::client: client response error: broken pipe\n2022-09-15T14:54:46.285125Z DEBUG hyper::client::client: client connection error: connection error: broken pipe\n2022-09-15T14:54:46.285273Z  WARN sink{component_kind=\"sink\" component_id=to_vector component_type=vector component_name=to_vector}:request{request_id=1}: vector::sinks::util::retries: Retrying after error. error=Request failed: status: Unknown, message: \"connection error: broken pipe\", details: [], metadata: MetadataMap { headers: {} }\n\n\nOn aggregator:\n2022-09-15T14:53:54.233395Z DEBUG hyper::server::server::new_svc: connection error: connection error: TLS handshake failed: error:1408F10B:SSL routines:ssl3_get_record:wrong version number:ssl/record/ssl3_record.c:332:\n2022-09-15T14:53:56.239503Z DEBUG h2::codec::framed_write: send frame=Settings { flags: (0x0), initial_window_size: 1048576, max_frame_size: 16384, max_header_list_size: 16777216 }\n\nAnother issue, even if I configure verify_certificate: false on the agent, it isn\u00b4t working. I don\u00b4t understand how it is supposed to be setup.\nThanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/14433",
        "createdAt": "2022-09-15T14:57:15Z",
        "updatedAt": "2022-09-16T19:04:53Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "kevinremoue"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14429,
        "title": "dynamic topic kafka",
        "bodyText": "I have a question that:\nI'm source from http and sink to kafka\nI want when I send log to http have path is: / => log is sent to kafka with topic \nEx:\nsource /aaa => topic aaa\nsource /bbb => topic bbb\nsource /ccc => topic ccc\nHow to do this ?",
        "url": "https://github.com/vectordotdev/vector/discussions/14429",
        "createdAt": "2022-09-15T04:16:30Z",
        "updatedAt": "2022-09-16T06:40:07Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "taquanghung1705199"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 14419,
        "title": "Failing to set up AWS auth for a sink?",
        "bodyText": "Hi, first off: I'm a total vector noob so bear with me. My googling has yielded no usable answers to this problem, and it might be something completely obvious but I thought I'd ask here.\nI'm trying to set up a simple forwarding of our Monolog logfile to cloudwatch, but can't get the AWS auth to work. Here's vectors log message:\nERROR vector::cli: Configuration error. error=data did not match any variant of untagged enum AwsAuthentication for key `sinks.cloudwatch`\n\nAnd here's the config:\n{\n    \"sources\": {\n        \"REDACTED_logs\": {\n            \"type\": \"file\",\n            \"include\": [\n                \"/logs/REDACTED.log\"\n            ],\n            \"read_from\": \"beginning\"\n        }\n    },\n    \"sinks\": {\n        \"cloudwatch\": {\n            \"type\": \"aws_cloudwatch_logs\",\n            \"inputs\": [\n                \"REDACTED_logs\"\n            ],\n            \"create_missing_group\": true,\n            \"create_missing_stream\": true,\n            \"group_name\": \"REDACTED-dev\",\n            \"compression\": \"none\",\n            \"region\": \"eu-north-1\",\n            \"stream_name\": \"{{ host }}\",\n            \"auth\": {\n                \"access_key_id\": \"REDACTED\",\n                \"assume_role\": \"arn:aws:iam::REDACTED:user/REDACTED\",\n                \"secret_access_key\": \"REDACTED\"\n            },\n            \"encoding\": {\n                \"codec\": \"text\",\n                \"timestamp_format\": \"rfc3339\"\n            }\n        }\n    }\n}",
        "url": "https://github.com/vectordotdev/vector/discussions/14419",
        "createdAt": "2022-09-14T14:49:26Z",
        "updatedAt": "2022-09-14T15:12:19Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "nahkampf"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 14334,
        "title": "How to convert multiple CSVs, with different number of columns, in one shot, into JSON?",
        "bodyText": "Hi!\nI have three different CSV files. Below the CSVs (with only one row for simplicity sake):\nCSV Type 1:\nTimestamp,a_country,b_country,call_setup_time,test_type\n06/09/2022 10:33,UAE,UAE,7.8470,voice_call\n\nCSV Type 2:\nTimestamp,a_country,b_country,call_setup_time,quality,test_type\n05/09/2022 12:33,Portugal,UK,4.8470,3.9362,voice quality\n\nCSV Type 3:\nTimestamp,a_country,latency,throughput,test type\n05/09/2022 12:33,Portugal,40.8470,20.1,data\n\nNow I'm in doubt on how to do this.\nI guess I would need to collect first, the columns of the CSV being processed (missing code on the first if statement) and based on those columns, fill the associated values.\nSample code below:\n[sources.my_csv_files_id]\ntype = \"file\"\ninclude = [ \"/home/rcmv2/ftp/*.csv\" ]\nstart_at_beginning = true\n\n[transforms.remap]\ninputs = [\"my_csv_files_id\"]\ntype = \"remap\"\nsource = \"\"\"\n    row = parse_csv!(.message)\n    if row[0] == \"Timestamp\" {\n        # code to collect the headers of the CSV type being processed\n    }else{\n\t# code to, based on the headers collected before, get the associated data.\n    }\n    del(.source_type)\n    del(.timestamp)\n    del(.file)\n    del(.host)\n    del(.message)\n   }\n    \"\"\"\n[sinks.console]\ninputs = [\"remap\"]\ntype = \"console\"\nencoding.codec = \"json\"\n\nDesired Output:\n{\"Timestamp\":\"06/09/2022 10:33\",\"a_country\":\"UAE\";\"b_country\":\"UAE\";\"call_setup_time\":\"7.8470\",\"test_type\":\"voice_call\"}\n\n{\"Timestamp\":\"05/09/2022 12:33\",\"a_country\":\"Portugal\",\"b_country\":\"UK\",\"call_setup_time\":\"4.8470\",\"quality\":\"3.9362\",\"test_type\":\"voice quality\"}\n\n{\"Timestamp\":\"05/09/2022 12:33\",\"a_country\":\"Portugal\",\"latency\":\"40.8470\",\"throughput\":\"20.1\",\"test type\":\"data\"}\n\nAny ideas about the code inside those two if statements?\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/14334",
        "createdAt": "2022-09-08T11:11:29Z",
        "updatedAt": "2022-09-12T15:32:56Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "rcmv"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14144,
        "title": "no metric on File source on max_byte_line exceeded",
        "bodyText": "hello there!\nas in the title: seems like File source is not emitting any useful metric signifying this event? or perhaps i am missing something?\nperhaps rejection should count into component_discarded_events_total?\nFor following source configuration:\n  test_source:\n    type: file\n    include:\n      - logs/*.txt\n    read_from: beginning\n    data_dir: vector_work_dir\n    max_line_bytes: 2\nand following log:\na\nb\nc\nabc\n\ni am getting:\nlog_collector_component_received_events_total (...) 5\nlog_collector_component_sent_events_total (...) 5\nlog_collector_events_in_total (...) 5\nlog_collector_events_out_total (...) 5\n\nthank you,\nFilip",
        "url": "https://github.com/vectordotdev/vector/discussions/14144",
        "createdAt": "2022-08-29T14:38:31Z",
        "updatedAt": "2022-09-08T17:38:37Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "filipmnowak"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 2
    },
    {
        "number": 14313,
        "title": "How to delete parts of a parse_csv output?",
        "bodyText": "Hi!\nI have a CSV like the one below which I want to transform into json.\nTimestamp,country,duration,quality\n06/09/2022 10:33,UAE,7.8470,3.5362\n06/09/2022 10:30,Saudi_Arabia,10.0380,3.4051\n06/09/2022 10:27,Switzerland,7.8720,2.4598\n06/09/2022 10:24,Israel,9.9360,2.8292\n\nI've the following piece of code to convert the CSV file into json using remap/parse_csv:\n[sources.my_csv_files_id]\ntype = \"file\"\ninclude = [ \"/home/rcmv2/ftp/*.csv\" ]\nstart_at_beginning = true\n\n[transforms.remap]\ninputs = [\"my_csv_files_id\"]\ntype = \"remap\"\nsource = \"\"\"\n     row = parse_csv!(.message)\n    .Timestamp = row[0]\n    .country = row[1]\n   .duration = row[2]\n   .quality = row[3]\n\"\"\"\n\n[sinks.console]\ninputs = [\"remap\"]\ntype = \"console\"\nencoding.codec = \"json\"\n\nThe output is something like this:\n{\n  \"Timestamp\": \"Timestamp\",\n  \"country\": \"country\",\n  \"duration\": \"duration\",\n  \"file\": \"/home/rcmv2/ftp/sample.csv\",\n  \"host\": \"ip-172-31-28-49.ec2.internal\",\n  \"message\": \"Timestamp,country,duration,quality\\r\",\n  \"quality\": \"quality\",\n  \"source_type\": \"file\",\n  \"timestamp\": \"2022-09-07T13:40:16.916340007Z\"\n}\n{\n  \"Timestamp\": \"06/09/2022 10:33\",\n  \"country\": \"UAE\",\n  \"duration\": \"7.8470\",\n  \"file\": \"/home/rcmv2/ftp/sample.csv\",\n  \"host\": \"ip-172-31-28-49.ec2.internal\",\n  \"message\": \"06/09/2022 10:33,UAE,7.8470,3.5362\\r\",\n  \"quality\": \"3.5362\",\n  \"source_type\": \"file\",\n  \"timestamp\": \"2022-09-07T13:40:16.916355689Z\"\n}\n...\n\nI would like to have instead something like this:\n{\n \"Timestamp\": \"06/09/2022 10:33\",\n  \"country\": \"UAE\",\n  \"duration\": \"7.8470\",\n  \"quality\": \"3.5362\",\n}\n{\n  \"Timestamp\": \"06/09/2022 10:30\",\n  \"country\": \"Saudi_Arabia\",\n  \"duration\": \"10.0380\",\n  \"quality\": \"3.4051\",\n}\n...\n\nI've tried some changes on the source like the one below but got no joy.\nsource = \"\"\" row = parse_csv!(.message) .Timestamp = row[0] .country = row[1] .duration = parse_duration!(row[2], unit:\"s\")  .quality = to_float!(row[3]) del(.source_type) del(.timestamp) del(.file) del(.host) del(.message) \"\"\"\nOutput:\n{\n\"file\":\"/home/rcmv2/ftp/sample.csv\",\n\"host\":\"ip-172-31-28-49.ec2.internal\",\n\"message\":\"06/09/2022 10:33,UAE,7.8470,3.5362\\r\"\n,\"source_type\":\"file\",\n\"timestamp\":\"2022-09-07T14:38:23.509016393Z\"\n}\n\n{\n\"file\":\"/home/rcmv2/ftp/sample.csv\",\n\"host\":\"ip-172-31-28-49.ec2.internal\",\n\"message\":\"06/09/2022 10:30,Saudi_Arabia,10.0380,3.4051\\r\",\n\"source_type\":\"file\",\n\"timestamp\":\"2022-09-07T14:38:23.509022698Z\"\n}\n...\n\nCan someone help me with this?\nBR",
        "url": "https://github.com/vectordotdev/vector/discussions/14313",
        "createdAt": "2022-09-07T16:38:34Z",
        "updatedAt": "2022-09-08T09:10:40Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "rcmv"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14137,
        "title": "syslog TLS error in vector",
        "bodyText": "Hi . i get Connection error. error=TLS handshake failed: error:14094412:SSL routines:ssl3_read_bytes:sslv3 alert bad certificate:ssl/record/rec_layer_s3.c:1544:SSL alert number 42 error_code=\"connection_failed\" error_type=\"writer_failed\" stage=\"sending\" internal_log_rate_secs=10 in vector by adding self signed certificate with openssl .\ni test and verify the openssl command and outputs , but everything seems to be ok.\nusing vector version 0.23.3",
        "url": "https://github.com/vectordotdev/vector/discussions/14137",
        "createdAt": "2022-08-28T16:34:07Z",
        "updatedAt": "2022-09-07T19:16:46Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 14289,
        "title": "how to best aggregate data across components",
        "bodyText": "vector validator doesnt recognize variables across components resulting in \"error[E652]: only objects can be merged\" despite in theory, this  would work. Eg when renaming / initialising .aggregator in test_remap, tapping input of test_remap shows .aggregator as initialized with {}. How should this best be dealt with ?\n[transforms.test_init]\ntype = \"remap\"\ninputs = [\"test_file_in\",\"test_socket_in\"]\nsource = \"\"\"\n.aggregator = {}\n\"\"\"\n[transforms.test_remap]\ntype = \"remap\"\ninputs = [\"test_init\"]\nsource = \"\"\"\n.aggregator |= {\"x\":1}\n\"\"\"",
        "url": "https://github.com/vectordotdev/vector/discussions/14289",
        "createdAt": "2022-09-06T12:19:03Z",
        "updatedAt": "2022-09-19T15:35:43Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ccmsi"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 14287,
        "title": "How to convert a CSV file into a JSON, line by line, using LUA?",
        "bodyText": "Hi!\nI would like to convert a CSV file into JSON line by line. I think I can use the remap function with parse_json for this effect but I'm very noob with this and sure how to make it. My attempts failed.\nCan someone help me with this simple example of a CSV below?\nTimestamp;country;duration;quality\n06/09/2022 10:33;UAE;7.8470;3.5362\n06/09/2022 10:30;Saudi_Arabia;10.0380;3.4051\n06/09/2022 10:27;Switzerland;7.8720;2.4598\n06/09/2022 10:24;Israel;9.9360;2.8292\nEdit:\nAfter some search in the documentation I tried to implement this using Lua with the code below:\n`[sources.my_csv_files_id]\ntype = \"file\"\ninclude = [ \"/home/rcmv2/ftp/\" ]\nstart_at_beginning = true\n\n[transforms.lua]\ninputs = [\"my_csv_files_id\"]\ntype = \"lua\"\nversion = \"2\"\nsource = \"\"\"\n    csv = require(\"csv\") \n    column_names = {  \n        \"Timestamp\",\n\t\"country\",\n\t\"duration\",\n\t\"quality\",\n    }\n\"\"\"\nhooks.process = \"\"\"\n  function (event, emit)\n    fields = csv.openstring(event.log.message):lines()() \n    event.log.message = nil \n\n    for column, value in ipairs(fields) do \n      column_name = column_names[column] \n      event.log[column_name] = value \n    end\n\n    emit(event) \n  end\n\"\"\"\n[sinks.console]\ninputs = [\"my_csv_files_id\"]\ntype = \"console\"\nencoding.codec = \"json\"`\n\nI'm not get any output though...\nAny help?\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/14287",
        "createdAt": "2022-09-06T11:37:16Z",
        "updatedAt": "2022-09-07T15:36:49Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "rcmv"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 0
    },
    {
        "number": 13001,
        "title": "Disk buffer related queries - Version 0.22.0",
        "bodyText": "hello,\nWe are using the vector instance as both agent and collector on different platforms. version v 0.22.0 (the new disk buffer implementation)\nWe would like to understand the behavior of the buffer.max_size. Please correct me if my understanding is wrong.\nNote: The size of each event used in the test is around 616 bytes. (refer to line 1819 of the internal metrics log file attached below)\nHere is config file that i have used for the test:\n\n\nWhat is the significance of buffer.max_size? We expected that at any given point of time the maximum number of bytes that could be held in the buffer = buffer.max_size.\nwhat would happen to the incoming events when the buffer.max_size is met? According to the configuration I have set the buffer.when_full to \"block\", is there any way to test if this is happening?\nWhen the vector agent is down what would happen to the events that are already present in the buffer, would the time out secs be still active to send out the events(present in the buffer) to the downstream vector instance or would it remain in the buffer until the vector agent comes back live?\nWhen would the files \"buffer.lock\", \"buffer.db\" , \"buffer-data-0.dat\" get deleted from the disk? Is there a ceiling point for the size of  .dat file where it would not allow the further growth of the file. We have certain limitations related to the availability of disk space on few systems where we would like to run the vector instances.\nIs there any relation between buffer_byte_size and buffer_max_byte_size, (refer to line 1554 and 1559 in the internal metrics log file)\n\nWe have enabled internal metrics and filtered out the metrics related to buffer to observe the flow of bytes/events in and out of the vector agent instance.\ninternal_metrics-2022-06-06.log",
        "url": "https://github.com/vectordotdev/vector/discussions/13001",
        "createdAt": "2022-06-06T19:31:24Z",
        "updatedAt": "2022-09-07T12:55:42Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "hemanthofficial3009"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 1
    },
    {
        "number": 14158,
        "title": "After removal of checkpoint files multiline logs ingested line by line",
        "bodyText": "Hi :)\nI'm testing configuration for Vector and need to be able to re-ingest the same data in staging system to make sure that it is ingested properly (I did tests on samples, now need real data).\nThe logs are multiline and have timestamp in the beginning of each message. I use source like this :\n  api_logs:\n    type: file\n    glob_minimum_cooldown_ms: 2000\n    include:\n      - /opt/app/data/logs/*.log\n    multiline.start_pattern: '^[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}'\n    multiline.condition_pattern: '^[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}'\n    multiline.mode: \"halt_before\"\n    multiline.timeout_ms: 1000\n\nAnd that data is send to Vector's console.  Logs are not that big.\nAnd I've noticed a strange behavior: if I remove a checkpoint file related to api_logs and restart Vector then vector disregards the multiline configuration and emerges several one-line events instead of a multiline event.\nMore over, some times I have to restart Vector service several times  I see data from some of the files in the console.\nIs this an expected behavior or is it a bug?\nThanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/14158",
        "createdAt": "2022-08-29T22:30:09Z",
        "updatedAt": "2022-09-01T22:21:16Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "podshivalovdv"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14241,
        "title": "vector panic when use disk buffer",
        "bodyText": "I  install Vector using Helm,and use agent role to collect my logs and send to s3\nthe heml value hostpath is\n  hostPath:\n    # persistence.hostPath.path -- Override path used for hostPath persistence\n    ## Valid for Agent role, persistence always used for Agent role\n    path: \"/vector\"\n\nthe config is\ns3_test_logs:\n      type: aws_s3\n      inputs:\n        - enrich_test_log_message\n      region: us-west-2\n      bucket: test-log-us-west-2\n      key_prefix: |-\n        test/base/\n      compression: gzip\n      batch:\n        max_bytes: 104857600 # 100MB\n        timeout_secs: 180 # 3 minutes\n      buffer:\n        type: disk\n         max_size: 1073741824 # 1GB\n      encoding:\n        codec: text\n      auth:\n        assume_role: \"role arn\"\n      tags:\n        k8s: ${K8S_CLUSTER_NAME}\n\nI run it \uff0cbut get error and crashloop\n2022-09-01T12:15:23.835518Z DEBUG sink{component_kind=\"sink\" component_id=s3_test_logs component_type=aws_s3 component_name=s3_test_logs}: vector_buffers::variants::disk_v2::reader: Rolling to next data file.\nthread 'vector-worker' panicked at 'reader encountered unrecoverable error: PartialWrite', /project/lib/vector-buffers/src/topology/channel/receiver.rs:64:18\nstack backtrace:\n   0: rust_begin_unwind\n             at ./rustc/db9d1b20bba1968c1ec1fc49616d4742c1725b4b/library/std/src/panicking.rs:498:5\n   1: core::panicking::panic_fmt\n             at ./rustc/db9d1b20bba1968c1ec1fc49616d4742c1725b4b/library/core/src/panicking.rs:107:14\n   2: core::result::unwrap_failed\n             at ./rustc/db9d1b20bba1968c1ec1fc49616d4742c1725b4b/library/core/src/result.rs:1613:5\n   3: <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll\n   4: <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll\n   5: <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll\n   6: <vector_buffers::topology::channel::receiver::BufferReceiverStream<T> as futures_core::stream::Stream>::poll_next\n   7: <vector::utilization::Utilization<S> as futures_core::stream::Stream>::poll_next\n   8: <futures_util::stream::stream::filter::Filter<St,Fut,F> as futures_core::stream::Stream>::poll_next\n   9: <futures_util::stream::stream::map::Map<St,F> as futures_core::stream::Stream>::poll_next\n  10: <stream_cancel::combinator::TakeUntilIf<S,F> as futures_core::stream::Stream>::poll_next\n  11: <futures_util::stream::stream::flatten::Flatten<St,<St as futures_core::stream::Stream>::Item> as futures_core::stream::Stream>::poll_next\n  12: <futures_util::stream::stream::FlatMap<St,U,F> as futures_core::stream::Stream>::poll_next\n  13: <vector_core::stream::partitioned_batcher::PartitionedBatcher<St,Prt,KT> as futures_core::stream::Stream>::poll_next\n  14: <futures_util::stream::stream::filter_map::FilterMap<St,Fut,F> as futures_core::stream::Stream>::poll_next\n  15: <vector_core::stream::concurrent_map::ConcurrentMap<St,T> as futures_core::stream::Stream>::poll_next\n  16: <futures_util::stream::stream::filter_map::FilterMap<St,Fut,F> as futures_core::stream::Stream>::poll_next\n  17: <futures_util::stream::stream::ready_chunks::ReadyChunks<St> as futures_core::stream::Stream>::poll_next\n  18: <tokio::future::poll_fn::PollFn<F> as core::future::future::Future>::poll\n  19: <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll\n  20: <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll\n  21: <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll\n  22: <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll\n  23: <tracing::instrument::Instrumented<T> as core::future::future::Future>::poll\n  24: tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut\n  25: tokio::runtime::task::harness::Harness<T,S>::poll\n  26: std::thread::local::LocalKey<T>::with\n  27: tokio::runtime::thread_pool::worker::Context::run_task\n  28: tokio::runtime::thread_pool::worker::Context::run\n  29: tokio::macros::scoped_tls::ScopedKey<T>::set\n  30: tokio::runtime::thread_pool::worker::run\n  31: tokio::loom::std::unsafe_cell::UnsafeCell<T>::with_mut\n  32: tokio::runtime::task::harness::Harness<T,S>::poll\n  33: tokio::runtime::blocking::pool::Inner::run\nnote: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.\n2022-09-01T12:15:24.228149Z ERROR sink{component_kind=\"sink\" component_id=s3_test_logs component_type=aws_s3 component_name=s3_test_logs}: vector::topology: An error occurred that Vector couldn't handle.\n2022-09-01T12:15:24.228235Z  INFO vector: Vector has stopped.\n\nhow can I solve this problem",
        "url": "https://github.com/vectordotdev/vector/discussions/14241",
        "createdAt": "2022-09-01T13:34:01Z",
        "updatedAt": "2022-09-01T14:56:30Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "z2665"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14191,
        "title": "How to add a new field which starts with hash symbol (#) in remap transform?",
        "bodyText": "Hi,\nI'm trying to send logs from Vector to Humio and Humio has special meaning for fields which start with the hash symbol #, for example #environment etc.\nI'm using remap transform and it looks like that if I use something like this\n  docker_generic_logs_transform:\n    type: remap\n    inputs:\n      - docker_db_logs_label\n    source: |-\n      \"\"\n      log(\"start\")\n      .#type= \"docker_generic\"\n      .#environment = \"staging\"\n      log(\"end\")\n      \"\"\n\nThen it looks like that Vector omits the #type and #environment fields. I don't see them in the vector tap output and in vector console where I send the events.  But I do see the \"start\" and \"end\" log records in the vector's console, so the transform works.\nIs it possible to add fields like #xxxx in the remap transform?\nP.S. I guess it is possible via add_fields transform, but as I understood it is deprecated.\nThanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/14191",
        "createdAt": "2022-08-31T08:46:37Z",
        "updatedAt": "2022-08-31T13:27:34Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "podshivalovdv"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14138,
        "title": "parsing csv data and mapping values -> how ?",
        "bodyText": "Hi there,\nI have this syslog message, the message content is in CSV format.\nNov 30 16:44:36 PA-220 1,2018/11/30 16:44:36,012801096514,THREAT,url,2049,2018/11/30 16:44:36,192.168.15.224,152.195.55.192,192.168.1.63,152.195.55.192,new_outbound_from_trust,,,ssl,vsys1,trust,untrust,ethernet1/2,ethernet1/1,send_to_mac,2018/11/30 16:44:36,28191,1,52984,443,37679,443,0x403000,tcp,block-url,\"consent.cmp.oath.com/\",(9999),business-and-economy,informational,client-to-server,7726,0x2000000000000000,192.168.0.0-192.168.255.255,United States,0,,0,,,0,,,,,,,,0,0,0,0,0,,PA-220,,,,,0,,0,,N/A,unknown,AppThreat-0-0,0x0,0,4294967295,\n\nHave these CSV mappings / array for it.\n      mappings:\n        client.ip: 7\n        source.ip: 7\n        source.address: 7\n        server.ip: 8\n        destination.ip: 8\n        destination.address: 8\n        source.nat.ip: 9\n        client.nat.ip: 9\n        destination.nat.ip: 10\n        server.nat.ip: 10\n        panw.panos.ruleset: 11\n        client.user.name: 12\n        source.user.name: 12\n        server.user.name: 13\n        destination.user.name: 13\n        network.application: 14\n        panw.panos.virtual_sys: 15\n        panw.panos.source.zone: 16\n        observer.ingress.zone: 16\n        panw.panos.destination.zone: 17\n        observer.egress.zone: 17\n        panw.panos.source.interface: 18\n        observer.ingress.interface.name: 18\n        panw.panos.destination.interface: 19\n        observer.egress.interface.name: 19\n        panw.panos.flow_id: 22\n        client.port: 24\n        source.port: 24\n        destination.port: 25\n        server.port: 25\n        source.nat.port: 26\n        client.nat.port: 26\n        destination.nat.port: 27\n        server.nat.port: 27\n        _temp_.labels: 28\n        network.transport: 29\n        panw.panos.action: 30\n        panw.panos.threat.resource: 31\n        url.original: 31\n        panw.panos.threat.name: 32\n        panw.panos.url.category: 33\n        log.level: 34\n        _temp_.direction: 35\n        _temp_.srcloc: 38\n        _temp_.dstloc: 39\n        panw.panos.network.pcap_id: 42\n        panw.panos.file.hash: 43\n        user_agent.original: 46\n        file.type: 47\n        network.forwarded_ip: 48\n        http.request.referer: 49\n        source.user.email: 50\n        panw.panos.subject: 51\n        destination.user.email: 52\n        observer.hostname: 59\n\n\nExample  client.ip and source.ip get the value from the position 7 from the CSV message.\nWhat is the best way to do this with Vector?\nPlease give an example.\nThank you very much in advance.",
        "url": "https://github.com/vectordotdev/vector/discussions/14138",
        "createdAt": "2022-08-28T19:53:18Z",
        "updatedAt": "2022-08-29T18:59:08Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "StefanSa"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 14029,
        "title": "exclude_paths_glob_patterns does not work",
        "bodyText": "I am using vector to push k8s logs to logtail. Some  of my pods are very verbose and I want to exclude them by using exclude_paths_glob_patterns. However, I can still see logs of excluded pods being pushed to logtail. I also tried annotating pods with vector.dev/exclude: \"true\". That didn't work either. What am I missing here?\ncustomConfig:\n    data_dir: /vector-data-dir\n    api:\n      enabled: true\n      address: 127.0.0.1:8686\n      playground: false\n    sources:\n      kubernetes_logs:\n        type: kubernetes_logs\n        exclude_paths_glob_patterns: \n          - \"**/dockermailserver/**\"\n          - \"**/prep-config/**\"\n          - \"**/timescaledb/**\"\n          - \"**/prometheus-adapter/**\"\n          - \"**/external-dns/**\"\n      host_metrics:\n        filesystem:\n          devices:\n            excludes: [binfmt_misc]\n          filesystems:\n            excludes: [binfmt_misc]\n          mountPoints:\n            excludes: [\"*/proc/sys/fs/binfmt_misc\"]\n        type: host_metrics\n      internal_metrics:\n        type: internal_metrics\n    sinks:\n      logtail:\n        type: http\n        inputs: [kubernetes_logs]\n        uri: https://in.logtail.com/\n        encoding:\n          codec: json\n        auth:\n          strategy: bearer\n          token: xxxxxxxxxx\n      prom_exporter:\n        type: prometheus_exporter\n        inputs: [host_metrics, internal_metrics]\n        address: 0.0.0.0:9090",
        "url": "https://github.com/vectordotdev/vector/discussions/14029",
        "createdAt": "2022-08-18T21:26:54Z",
        "updatedAt": "2022-08-29T13:38:58Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mindrunner"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 14079,
        "title": "difference between component_{id,name}",
        "bodyText": "I was thinking these are interchangeable names, but I see this line on log:\n\nERROR transform{component_kind=\"transform\" component_id=node-activity-to-metric component_type=log_to_metric component_name=node-activity-to-metric}...",
        "url": "https://github.com/vectordotdev/vector/discussions/14079",
        "createdAt": "2022-08-23T11:01:07Z",
        "updatedAt": "2022-08-23T18:44:37Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tshepang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14046,
        "title": "Parsing cloudfront access logs",
        "bodyText": "Hi,\nwe are collecting CloudFront access logs via S3 bucket notifications to SQS, the collection part seems to be working fine, we want to do some extended parsing on the logs and transform/condition specific fields, this is easy with json based logs as Vector has json filters build in but it seems rather complicated with cloudfront access logs as they are in tab-separated values.\nIs there any pre build remap function to help parse default cloudfront logs into fields ? we are using standard aws cloudfront logs.\nThanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/14046",
        "createdAt": "2022-08-22T07:54:34Z",
        "updatedAt": "2022-08-29T13:37:20Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "dannygueta"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14039,
        "title": "HTTP Enrichment",
        "bodyText": "Hello,\nI'm trying to understand if Vector will support my use case.\nI have a source of log events that all contain a UUID. I'm looking to merge these with another JSON object that can only be accessed by a HTTP API, for example, at  http://site.com//data.\nI understand this should be possible with the Lua transform, however I wanted to check if anyone had an idea of how this could be done without Lua to aid the performance of running this over every event.\nOne idea I've had is using the new CSV Enrichment. A script next to vector could refresh the CSV script by calling http://site.com/all/data, and converting this into a CSV every minute, however I'm unsure if the enrichment is reloaded from disk after vector has first started up?",
        "url": "https://github.com/vectordotdev/vector/discussions/14039",
        "createdAt": "2022-08-20T12:36:29Z",
        "updatedAt": "2022-08-29T13:37:10Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "owen5mith"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14038,
        "title": "How To Expire Keys in Redis .",
        "bodyText": "Hi.\nHow To Expire Keys in Redis (setting TTL)  ? while my vector pipeline creates key and send my events to it.",
        "url": "https://github.com/vectordotdev/vector/discussions/14038",
        "createdAt": "2022-08-20T12:07:47Z",
        "updatedAt": "2022-08-29T13:36:59Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14014,
        "title": "How to rename a source  without re-ingestion of data?",
        "bodyText": "Hi guys,\nI have a need to rename a file source  in Vector, because we need to split one source into 2.\nBut if I do so  - vector will create new checkpoints and will read log files from the beginning.\nWhat is the best way to do that split without re-ingestion of all the data?\nI see 1 option - stop vector, duplicate checkpoint folders using names of new sources and start vector again. Is that the right way?\nThanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/14014",
        "createdAt": "2022-08-18T09:31:48Z",
        "updatedAt": "2022-08-18T14:35:29Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "podshivalovdv"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13991,
        "title": "Using log_to_metric + aggregate using event's timestamp",
        "bodyText": "My use-case is to create metrics from logs and aggregate them. Vector's log_to_metric + aggregate transforms doesn't seem to use event's timestamp.\nConfig:\n[log_schema]\ntimestamp_key = \"@timestamp\"\n\n[sources.file_input]\n  type = \"file\" # required\n  include = [\"/logs/*.json\"] # required\n  read_from = \"beginning\"\n\n[transforms.parse_log_json]\n  type = \"remap\"\n  inputs = [\"file_input\"]\n  source = '''\n. = parse_json!(string!(.message))\n'''\n\n#####################\n[transforms.log_to_met]\ntype = \"log_to_metric\" # required\ninputs = [ \"parse_log_json\" ] # required\n\n  [[transforms.log_to_met.metrics]]\n  type = \"counter\"\n  field = \"statuscode\"\n  name = \"response_total\"\n\n    [transforms.log_to_met.metrics.tags]\n    statuscode = \"{{statuscode}}\"\n\n[transforms.msg_agg]\ntype = \"aggregate\"\ninputs = [ \"log_to_met\" ]\ninterval_ms = 5_000\n\n[sinks.console]\ntype = \"console\"\ninputs = [ \"msg_agg\" ]\ntarget = \"stdout\"\n\n  [sinks.console.encoding]\n  codec = \"json\"\nInput file:\n{\"level\":\"info\",\"@timestamp\":\"2022-07-29T11:39:29.230Z\",\"caller\":\"esque/main.go:66\",\"msg\":\"Hello World\",\"reason\":\"Greeting from unidentifed species\",\"statuscode\":200,\"method\":\"PUT\",\"messageid\":\"j1ae77e5-915l-13e9-bc42-556nf7864d64\"}\n{\"level\":\"error\",\"@timestamp\":\"2022-07-29T11:39:29.483Z\",\"caller\":\"esque/main.go:68\",\"msg\":\"End of World\",\"reason\":\"Invasion of unidentifed species\",\"statuscode\":500,\"method\":\"PUT\",\"messageid\":\"j1ae77e5-915l-13e9-bc42-556nf7864d64\",\"stacktrace\":\"main.main\\n\\t/Users/p.kolhe/Documents/projects/esque/main.go:68\\nruntime.main\\n\\t/usr/local/Cellar/go/1.18.4/libexec/src/runtime/proc.go:250\"}\n{\"level\":\"info\",\"@timestamp\":\"2022-07-30T03:48:20.325Z\",\"caller\":\"esque/main.go:66\",\"msg\":\"Hello World\",\"reason\":\"Greeting from unidentifed species\",\"statuscode\":200,\"method\":\"PUT\",\"messageid\":\"j1ae77e5-915l-13e9-bc42-556nf7864d64\"}\n{\"level\":\"error\",\"@timestamp\":\"2022-07-30T03:48:20.576Z\",\"caller\":\"esque/main.go:68\",\"msg\":\"End of World\",\"reason\":\"Invasion of unidentifed species\",\"statuscode\":500,\"method\":\"PUT\",\"messageid\":\"j1ae77e5-915l-13e9-bc42-556nf7864d64\",\"stacktrace\":\"main.main\\n\\t/Users/p.kolhe/Documents/projects/esque/main.go:68\\nruntime.main\\n\\t/usr/local/Cellar/go/1.18.4/libexec/src/runtime/proc.go:250\"}\n{\"level\":\"info\",\"@timestamp\":\"2022-07-31T05:16:33.945Z\",\"caller\":\"esque/main.go:66\",\"msg\":\"Hello World\",\"reason\":\"Greeting from unidentifed species\",\"statuscode\":200,\"method\":\"PUT\",\"messageid\":\"j1ae77e5-915l-13e9-bc42-556nf7864d64\"}\n{\"level\":\"error\",\"@timestamp\":\"2022-07-31T05:16:34.196Z\",\"caller\":\"esque/main.go:68\",\"msg\":\"End of World\",\"reason\":\"Invasion of unidentifed species\",\"statuscode\":500,\"method\":\"PUT\",\"messageid\":\"j1ae77e5-915l-13e9-bc42-556nf7864d64\",\"stacktrace\":\"main.main\\n\\t/Users/p.kolhe/Documents/projects/esque/main.go:68\\nruntime.main\\n\\t/usr/local/Cellar/go/1.18.4/libexec/src/runtime/proc.go:250\"}\n{\"level\":\"info\",\"@timestamp\":\"2022-07-31T06:19:10.245Z\",\"caller\":\"esque/main.go:66\",\"msg\":\"Hello World\",\"reason\":\"Greeting from unidentifed species\",\"statuscode\":200,\"method\":\"PUT\",\"messageid\":\"j1ae77e5-915l-13e9-bc42-556nf7864d64\"}\n{\"level\":\"error\",\"@timestamp\":\"2022-07-31T06:19:10.497Z\",\"caller\":\"esque/main.go:68\",\"msg\":\"End of World\",\"reason\":\"Invasion of unidentifed species\",\"statuscode\":500,\"method\":\"PUT\",\"messageid\":\"j1ae77e5-915l-13e9-bc42-556nf7864d64\",\"stacktrace\":\"main.main\\n\\t/Users/p.kolhe/Documents/projects/esque/main.go:68\\nruntime.main\\n\\t/usr/local/Cellar/go/1.18.4/libexec/src/runtime/proc.go:250\"}\n{\"level\":\"error\",\"@timestamp\":\"2022-08-05T09:09:23.877Z\",\"caller\":\"esque/main.go:68\",\"msg\":\"End of World\",\"reason\":\"Invasion of unidentifed species\",\"statuscode\":500,\"method\":\"PUT\",\"messageid\":\"j1ae77e5-915l-13e9-bc42-556nf7864d64\",\"stacktrace\":\"main.main\\n\\t/Users/p.kolhe/Documents/projects/esque/main.go:68\\nruntime.main\\n\\t/usr/local/Cellar/go/1.18.4/libexec/src/runtime/proc.go:250\"}\n{\"level\":\"info\",\"@timestamp\":\"2022-08-05T09:09:24.129Z\",\"caller\":\"esque/main.go:66\",\"msg\":\"Hello World\",\"reason\":\"Greeting from unidentifed species\",\"statuscode\":200,\"method\":\"PUT\",\"messageid\":\"j1ae77e5-915l-13e9-bc42-556nf7864d64\"}\n{\"level\":\"info\",\"@timestamp\":\"2022-08-08T13:15:09.057Z\",\"caller\":\"esque/main.go:73\",\"msg\":\"Hello World\",\"reason\":\"Greeting from unidentifed species\",\"statuscode\":200,\"method\":\"PUT\",\"messageid\":\"j1ae77e5-915l-13e9-bc42-556nf7864d64\"}\n{\"level\":\"error\",\"@timestamp\":\"2022-08-08T13:15:09.309Z\",\"caller\":\"esque/main.go:75\",\"msg\":\"End of World\",\"reason\":\"Invasion of unidentifed species\",\"statuscode\":500,\"method\":\"PUT\",\"messageid\":\"j1ae77e5-915l-13e9-bc42-556nf7864d64\",\"stacktrace\":\"main.main\\n\\t/Users/p.kolhe/Documents/projects/esque/main.go:75\\nruntime.main\\n\\t/usr/local/Cellar/go/1.18.5/libexec/src/runtime/proc.go:250\"}\n{\"level\":\"info\",\"@timestamp\":\"2022-08-10T07:18:33.044Z\",\"caller\":\"esque/main.go:74\",\"msg\":\"Hello World\",\"reason\":\"Greeting from unidentifed species\",\"statuscode\":200,\"method\":\"PUT\",\"messageid\":\"j1ae77e5-915l-13e9-bc42-556nf7864d64\"}\n{\"level\":\"error\",\"@timestamp\":\"2022-08-10T07:18:33.296Z\",\"caller\":\"esque/main.go:76\",\"msg\":\"End of World\",\"reason\":\"Invasion of unidentifed species\",\"statuscode\":500,\"method\":\"PUT\",\"messageid\":\"j1ae77e5-915l-13e9-bc42-556nf7864d64\",\"stacktrace\":\"main.main\\n\\t/Users/p.kolhe/Documents/projects/esque/main.go:76\\nruntime.main\\n\\t/usr/local/Cellar/go/1.18.5/libexec/src/runtime/proc.go:250\"}\n\nOutput:\n{\"@timestamp\":\"2022-08-16T13:25:03.393968870Z\",\"counter\":{\"value\":7.0},\"kind\":\"incremental\",\"name\":\"response_total\",\"tags\":{\"statuscode\":\"200\"}}\n{\"@timestamp\":\"2022-08-16T13:25:03.393989063Z\",\"counter\":{\"value\":7.0},\"kind\":\"incremental\",\"name\":\"response_total\",\"tags\":{\"statuscode\":\"500\"}}\n\nExpected output:\n{\"@timestamp\":\"2022-07-29T11:39:29.230Z\",\"counter\":{\"value\":1.0},\"kind\":\"incremental\",\"name\":\"response_total\",\"tags\":{\"statuscode\":\"200\"}}\n{\"@timestamp\":\"2022-07-29T11:39:29.483Z\",\"counter\":{\"value\":1.0},\"kind\":\"incremental\",\"name\":\"response_total\",\"tags\":{\"statuscode\":\"500\"}}\n{\"@timestamp\":\"2022-07-30T03:48:20.325Z\",\"counter\":{\"value\":1.0},\"kind\":\"incremental\",\"name\":\"response_total\",\"tags\":{\"statuscode\":\"200\"}}\n{\"@timestamp\":\"2022-07-30T03:48:20.576Z\",\"counter\":{\"value\":1.0},\"kind\":\"incremental\",\"name\":\"response_total\",\"tags\":{\"statuscode\":\"500\"}}\n{\"@timestamp\":\"2022-07-31T05:16:33.945Z\",\"counter\":{\"value\":1.0},\"kind\":\"incremental\",\"name\":\"response_total\",\"tags\":{\"statuscode\":\"200\"}}\n{\"@timestamp\":\"2022-07-31T05:16:34.196Z\",\"counter\":{\"value\":1.0},\"kind\":\"incremental\",\"name\":\"response_total\",\"tags\":{\"statuscode\":\"500\"}}\n{\"@timestamp\":\"2022-07-31T06:19:10.245Z\",\"counter\":{\"value\":1.0},\"kind\":\"incremental\",\"name\":\"response_total\",\"tags\":{\"statuscode\":\"200\"}}\n{\"@timestamp\":\"2022-07-31T06:19:10.497Z\",\"counter\":{\"value\":1.0},\"kind\":\"incremental\",\"name\":\"response_total\",\"tags\":{\"statuscode\":\"500\"}}\n{\"@timestamp\":\"2022-08-05T09:09:23.877Z\",\"counter\":{\"value\":1.0},\"kind\":\"incremental\",\"name\":\"response_total\",\"tags\":{\"statuscode\":\"500\"}}\n{\"@timestamp\":\"2022-08-05T09:09:24.129Z\",\"counter\":{\"value\":1.0},\"kind\":\"incremental\",\"name\":\"response_total\",\"tags\":{\"statuscode\":\"200\"}}\n{\"@timestamp\":\"2022-08-08T13:15:09.057Z\",\"counter\":{\"value\":1.0},\"kind\":\"incremental\",\"name\":\"response_total\",\"tags\":{\"statuscode\":\"200\"}}\n{\"@timestamp\":\"2022-08-08T13:15:09.309Z\",\"counter\":{\"value\":1.0},\"kind\":\"incremental\",\"name\":\"response_total\",\"tags\":{\"statuscode\":\"500\"}}\n{\"@timestamp\":\"2022-08-10T07:18:33.044Z\",\"counter\":{\"value\":1.0},\"kind\":\"incremental\",\"name\":\"response_total\",\"tags\":{\"statuscode\":\"200\"}}\n{\"@timestamp\":\"2022-08-10T07:18:33.296Z\",\"counter\":{\"value\":1.0},\"kind\":\"incremental\",\"name\":\"response_total\",\"tags\":{\"statuscode\":\"500\"}}",
        "url": "https://github.com/vectordotdev/vector/discussions/13991",
        "createdAt": "2022-08-16T14:58:17Z",
        "updatedAt": "2022-08-18T12:07:56Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "fpytloun"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 14005,
        "title": "issue with config file trying to send from vector agent in windows to vector in linux",
        "bodyText": "Wanted to see what I was doing wrong on this config. I'm trying to move the raw evtx files from the windows host to the linux host (I tried doing this locally using the \"file\" type and it worked as expected). Now i'm trying to move it over via vector and I'm getting this error below. Any help is appreciated. I double checked the firewall rules and port 6000 and port 9000 are allowed.\nsource{component_kind=\"source\" component_id=vector_windows_application_evtx component_type=vector component_name=vector_windows_application_evtx}: vector::sources::vector::v2: Source future failed. error=TCP bind failed: Cannot assign requested address (os error 99)\nWindows:\n[sources.vector_windows_application_evtx]\ntype = \"file\"\ninclude = [\"C:\\Windows\\System32\\winevt\\Logs\\Application.evtx\"]\nread_from = \"beginning\"\n[sinks.inputs_application]\ntype = \"vector\"\ninputs = [\"vector_windows_application_evtx\"]\naddress = \"X.X.X.X:6000\"\nversion=\"2\"\n#[sinks.inputs_application.encoding]\n#codec =  \"text\" #changing this to json makes each field value its own json object\nlinux:\n[sources.vector_windows_application_evtx]\ntype = \"vector\"\naddress = \"X.X.X.X:9000\"\nversion =\"2\"\n[sinks.vector_windows_application]\ntype = \"file\"\ninputs = [\"vector_windows_application_evtx\"]\npath = \"/tmp/vector_windows-%Y-%m-%d.log\"\n[sinks.vector_windows_application.encoding]\ncodec = \"text\"",
        "url": "https://github.com/vectordotdev/vector/discussions/14005",
        "createdAt": "2022-08-17T14:46:17Z",
        "updatedAt": "2022-08-17T18:56:34Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "umpa385"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13875,
        "title": "Document upgrade and downgrade processes",
        "bodyText": "Hi.\nAfter reading the documentation it is not clear for me, how upgrade and downgrade processes should be performed over the solution. Could you please describe somewhere in the documentation:\n\nHow to upgrade Vector to the newer version with no downtime\nHow to downgrade Vector to the older version (hopefully with no downtime)\nMaybe some compatibility policies and notes between versions\n\nThanks in advance!",
        "url": "https://github.com/vectordotdev/vector/discussions/13875",
        "createdAt": "2022-08-07T08:26:35Z",
        "updatedAt": "2022-08-17T12:06:02Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "zamazan4ik"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13639,
        "title": "Vector cannot read the host address of Kafka",
        "bodyText": "I read the local log file and send it to Kafka\nI read the log from Kafka and send it to Loki\n\nAfter deploying using docker compose, I found that in the first step, it can normally read the address of Kafka, but in the second step, it cannot read the address of Kafka, and always get 127.0.0.1\n  vector_agent:\n    image: timberio/vector:latest-alpine\n    container_name: vector_agent\n    ports:\n      - 8383:8383\n    volumes:\n    - ./vector/vector_agent.toml:/etc/vector/vector.toml:ro\n    environment:\n    - TZ=Asia/Shanghai\n    restart: always\n    privileged: false\n\n  vector_aggregate:\n    image: timberio/vector:latest-alpine\n    container_name: vector_aggregate\n    ports:\n      - 8385:8383\n    volumes:\n    - ./vector/vector_aggregate.toml:/etc/vector/vector.toml:ro\n    environment:\n    - TZ=Asia/Shanghai\n    restart: always\n    privileged: false\n\n# cat vector/vector_agent.toml \n[sources.in]\ntype         = \"file\"\ninclude      = [\"/var/log/*.log\"]\nignore_older = 86400   \n\n[sinks.out]\ntype = \"kafka\"\ninputs = [ \"in\" ]\nbootstrap_servers = \"kafka:9092\"\nkey_field = \"user_id\"\ntopic = \"test\"\ncompression = \"none\"\n\n  [sinks.out.encoding]\n  codec = \"json\"\n\n# cat vector/vector_aggregate.toml \n[sources.in]\ntype = \"kafka\"\nbootstrap_servers = \"kafka:9092\"\nkey_field = \"user_id\"\ngroup_id = \"test\"\ntopics = [ \"test\" ]\n\n[sinks.out]\ntype = \"loki\"\ninputs = [ \"in\" ]\nendpoint = \"http://loki:3100/loki/api/v1/push\"\ncompression = \"none\"\n\n  [sinks.out.labels]\n  forwarder = \"vector\"\n  event = \"{{ event_field }}\"\n  key = \"value\"\n  \"\\\"{{ event_field }}\\\"\" = \"{{ another_event_field }}\"\n  \"pod_labels_*\" = \"{{ kubernetes.pod_labels }}\"\n\n  [sinks.out.encoding]\n  codec = \"json\"\n\nagent logs:\n2022-07-20T15:02:03.767949Z ERROR rdkafka::client: librdkafka: Global error: BrokerTransportFailure (Local: Broker transport failure): kafka:9092/bootstrap: Connect to ipv4#172.20.0.9:9092 failed: Connection refused (after 1ms in state CONNECT, 1 identical error(s) suppressed)    \n2022-07-20T15:02:05.777883Z ERROR vector::topology::builder: msg=\"Healthcheck: Failed Reason.\" error=Meta data fetch error: BrokerTransportFailure (Local: Broker transport failure) component_kind=\"sink\" component_type=\"kafka\" component_id=out component_name=out\n2022-07-20T15:04:43.407641Z  INFO vector: Vector has stopped.\n2022-07-20T15:04:43.411215Z  INFO vector::topology::running: Shutting down... Waiting on running components. remaining_components=\"out\" time_remaining=\"59 seconds left\"\n2022-07-20T15:04:43.824526Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=trace,rdkafka=info,buffers=info,kube=info\"\n2022-07-20T15:04:43.824608Z  INFO vector::app: Loading configs. paths=[\"/etc/vector/vector.toml\"]\n2022-07-20T15:04:43.828500Z  INFO vector::topology::running: Running healthchecks.\n2022-07-20T15:04:43.828662Z  INFO vector: Vector has started. debug=\"false\" version=\"0.23.0\" arch=\"x86_64\" build_id=\"38c2435 2022-07-11\"\n2022-07-20T15:04:43.828707Z  INFO vector::app: API is disabled, enable by setting `api.enabled` to `true` and use commands like `vector top`.\n2022-07-20T15:04:43.829589Z  INFO source{component_kind=\"source\" component_id=in component_type=file component_name=in}:file_server: file_source::checkpointer: Loaded checkpoint data.\n2022-07-20T15:04:43.843942Z  INFO vector::topology::builder: Healthcheck: Passed.\n\naggregate logs:\n2022-07-20T15:04:39.331620Z  INFO vector::topology::running: Shutting down... Waiting on running components. remaining_components=\"out, in\" time_remaining=\"59 seconds left\"\n2022-07-20T15:04:43.365802Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=trace,rdkafka=info,buffers=info,kube=info\"\n2022-07-20T15:04:43.365874Z  INFO vector::app: Loading configs. paths=[\"/etc/vector/vector.toml\"]\n2022-07-20T15:04:43.402433Z  INFO vector::topology::running: Running healthchecks.\n2022-07-20T15:04:43.402570Z  INFO vector: Vector has started. debug=\"false\" version=\"0.23.0\" arch=\"x86_64\" build_id=\"38c2435 2022-07-11\"\n2022-07-20T15:04:43.402586Z  INFO vector::app: API is disabled, enable by setting `api.enabled` to `true` and use commands like `vector top`.\n2022-07-20T15:04:43.408324Z ERROR source{component_kind=\"source\" component_id=in component_type=kafka component_name=in}: rdkafka::client: librdkafka: Global error: BrokerTransportFailure (Local: Broker transport failure): GroupCoordinator: 127.0.0.1:9092: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)    \n2022-07-20T15:04:43.461853Z  INFO vector::topology::builder: Healthcheck: Passed.\n2022-07-20T15:04:43.552739Z ERROR source{component_kind=\"source\" component_id=in component_type=kafka component_name=in}: rdkafka::client: librdkafka: Global error: BrokerTransportFailure (Local: Broker transport failure): GroupCoordinator: 127.0.0.1:9092: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT, 1 identical error(s) suppressed)    \n2022-07-20T15:05:19.014991Z ERROR source{component_kind=\"source\" component_id=in component_type=kafka component_name=in}: rdkafka::client: librdkafka: Global error: BrokerTransportFailure (Local: Broker transport failure): GroupCoordinator: 127.0.0.1:9092: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT, 8 identical error(s) suppressed)    \n2022-07-20T15:05:58.607699Z ERROR source{component_kind=\"source\" component_id=in component_type=kafka component_name=in}: rdkafka::client: librdkafka: Global error: BrokerTransportFailure (Local: Broker transport failure): GroupCoordinator: 127.0.0.1:9092: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT, 4 identical error(s) suppressed)    \n2022-07-20T15:06:38.605734Z ERROR source{component_kind=\"source\" component_id=in component_type=kafka component_name=in}: rdkafka::client: librdkafka: Global error: BrokerTransportFailure (Local: Broker transport failure): GroupCoordinator: 127.0.0.1:9092: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT, 4 identical error(s) suppressed)",
        "url": "https://github.com/vectordotdev/vector/discussions/13639",
        "createdAt": "2022-07-20T15:07:20Z",
        "updatedAt": "2022-08-17T01:07:57Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ktpktr0"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 6067,
        "title": "Logfiles when running as a Windows service",
        "bodyText": "vector 0.11.1 (v0.11.1 x86_64-pc-windows-msvc 2020-12-17)\nHi!\nIs there a way to get the output of vector when running as a Windows service?\nOtherwise it is a little bit like wandering in the dark.\nCheers\nS\u00f6nke",
        "url": "https://github.com/vectordotdev/vector/discussions/6067",
        "createdAt": "2021-01-15T10:32:23Z",
        "updatedAt": "2022-08-16T11:18:49Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "xgcssch"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 2
    },
    {
        "number": 13834,
        "title": "Split logs by docker container label",
        "bodyText": "There are several running docker containers with labels.\nAll logs records go through common transforms and then I need to split stream based on label.\nBut I can't find right condition for swimlanes transform\nvector.toml\n[sources.in]\n  type = \"docker\"\n  include_labels = [\"com.atuko.log_shipping=junk\", \"com.atuko.log_shipping=elastic\"]\n\n[transforms.json]\n  type = \"json_parser\"\n  inputs = [\"in\"]\n  drop_invalid = true\n\n[transforms.remove_tokens]\n  type = \"lua\"\n  inputs = [\"json\"]\n  version = \"2\"\n\n  hooks.process = \"\"\"\n  function (event, emit)\n    if event.log.request ~= nil then\n      r = event.log.request:gsub('X%-System%-Token:.*', '')\n      r = r:gsub('Authorization:.*', '')\n      event.log.request = r\n    end\n\n    emit(event)\n  end\n  \"\"\"\n\n[transforms.split]\n  type = \"swimlanes\"\n  inputs = [\"remove_tokens\"]\n\n  # Lanes\n  [transforms.split.lanes.files]\n    type = \"check_fields\"\n    \"com.atuko.log_shipping.eq\" = \"junk\"\n\n  [transforms.split.lanes.elastic]\n    type = \"check_fields\"\n    \"com.atuko.log_shipping.eq\" = \"elastic\"\n\n[sinks.http]\n  type = \"http\"\n  inputs = [\"split.files\"]\n  uri = \"10.1.1.1:80\"\n  encoding.codec = \"ndjson\"\n\n[sinks.elastic]\n  type = \"elasticsearch\"\n  inputs = [\"split.elastic\"]\n  host = \"10.1.1.1:9000\"\n  index = \"vector-%F\"\n\nvector.log\nJul 23 07:52:51.315  INFO vector: Vector is starting. version=\"0.10.0\" git_version=\"v0.9.0-266-g5e5d806\" released=\"Thu, 25 Jun 2020 14:43:00 +0000\" arch=\"x86_64\"\nJul 23 07:52:51.346 TRACE source{name=in type=docker}: vector::sources::docker: Found already running container id=2e6e4e80c1566e3c526bab2f2dabb60d2e539b4a165ea94e4ce4c030b0760057 names=[\"/target-actions\"]\nJul 23 07:52:51.348  INFO vector::sources::docker: Started listening logs on docker container id=2e6e4e80c1566e3c526bab2f2dabb60d2e539b4a165ea94e4ce4c030b0760057\nJul 23 07:53:08.330 TRACE vector::sources::docker: Received one event. event=Log(LogEvent { fields: {\"container_created_at\": Timestamp(2020-07-23T07:49:25.591113769Z), \"container_id\": Bytes(b\"2e6e4e80c1566e3c526bab2f2dabb60d2e539b4a165ea94e4ce4c030b0760057\"), \"container_name\": Bytes(b\"target-actions\"), \"image\": Bytes(b\"target-actions:debug\"), \"label\": Map({\"com\": Map({\"atuko\": Map({\"log_shipping\": Bytes(b\"junk\")})}), \"logs_enabled\": Bytes(b\"true\")}), \"message\": Bytes(b\"{\\\"level\\\":\\\"info\\\",\\\"host\\\":\\\"d4\\\",\\\"service\\\":\\\"target-actions\\\",\\\"event_type\\\":\\\"debug\\\",\\\"response\\\":\\\"HTTP/1.1 204 No Content\\\",\\\"response_code\\\":204,\\\"time\\\":\\\"2020-07-23T10:53:08+03:00\\\",\\\"message\\\":\\\"ok\\\"}\"), \"source_type\": Bytes(b\"docker\"), \"stream\": Bytes(b\"stderr\"), \"timestamp\": Timestamp(2020-07-23T07:53:08.325606575Z)} })",
        "url": "https://github.com/vectordotdev/vector/discussions/13834",
        "createdAt": "2020-07-23T08:20:01Z",
        "updatedAt": "2022-08-16T11:13:33Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "andrew4fr"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 7159,
        "title": "What is the best way to filter logs with lowest cpu consumption?",
        "bodyText": "Hello,\nCan someone help here?\nI have a question about \"filter\" transform.\nI have a config like this:\n[sources.in]\n  type = \"file\"\n  include = [\"/opt/app/*/logs/*/*.log\"]\n  fingerprint.strategy = \"device_and_inode\"\n\n  [sources.in.multiline]\n     start_pattern = '^[0-9]{4}-[0-9]{2}-[0-9]{2}'\n     mode = \"halt_before\"\n     condition_pattern = '^[0-9]{4}-[0-9]{2}-[0-9]{2}'\n     timeout_ms = 999999999\n\n\n[transforms.in_filter]\n  type = \"filter\"\n  inputs = [\"in\"]\n  condition.type = \"check_fields\"\n  condition.\"message.not_contains\" = \"NOT_FOUND - no exchange\"\n  condition.\"message.not_regex\" = \" (INFO|DEBUG|TRACE) \"\n\nthe problem is that sometimes in the app I have a vast amount of messages coming to files, about 1-2 thousands per one second.\nThis activity lasts for about one hour.\nAnd vector start consuming 3+ Gb of ram and making performance impact on the running node.\nIs there is a better way to reduce ram and cpu consumption while have all messages with \"INFO\" filtered?",
        "url": "https://github.com/vectordotdev/vector/discussions/7159",
        "createdAt": "2021-04-18T14:18:14Z",
        "updatedAt": "2022-08-16T11:11:37Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "alpiua"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 5
        },
        "upvoteCount": 0
    },
    {
        "number": 7778,
        "title": "Exploring vector to expose metrics related logs to prometheus",
        "bodyText": "Current Vector Version\nv0.14.0\n\nUse-cases\nI have some logs from Cloudwtach ECS services, some others from web servers and even K8s pods. i want to use Vector to explore metrics related logs from maybe the webserver to Prometheus.\nAttempted Solutions\nI have gone through all the documentation but it seems nothing like that exists but I am of a surety that it could be possible with Vector\nProposal\nI am proposing that some sort of solution is rolled out here and probably merged with the repository then finally added to the documentation. But if there is already an existing solution, I'd like to know about it - it'd really be helpful in my project.\nI hope someone could help me beat this issue. thanks\n@blt @jamtur01 @hdhoang @neoeinstein @thoughtpolice and any others that could help",
        "url": "https://github.com/vectordotdev/vector/discussions/7778",
        "createdAt": "2021-06-05T14:17:19Z",
        "updatedAt": "2022-08-16T11:08:40Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "samuelarogbonlo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 8700,
        "title": "Is there a performance issue with the http module ?",
        "bodyText": "When I use http as input source and distribute it through Nginx in front, I find that there are a lot of error logs in Nginx. I don't know if this is limited by the performance of the http module. But I didn't find any relevant information about the configuration in the configuration documentation.\nThe flow is as follows.\nNginx upstream => vector http => transforms => kafka\nNginx error message.\nconnect() failed (111: Connection refused) while connecting to upstream\n\nAddendum: I have a very high volume of Nginx requests, and not all of them are rejected, but most of them are.",
        "url": "https://github.com/vectordotdev/vector/discussions/8700",
        "createdAt": "2021-08-13T01:42:06Z",
        "updatedAt": "2022-08-16T11:05:03Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "PanHywel"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 2
    },
    {
        "number": 10133,
        "title": "Identify when Vector has processed all the logs",
        "bodyText": "Hi,\nWe are running Vector as a sidecar container inside K8s pods. Vector is ingesting log files from some locations and outputting it to some sinks. Is there a way to identify how much of the logs Vector has processed? Or has Vector processed all the logs and passed them to the sink?",
        "url": "https://github.com/vectordotdev/vector/discussions/10133",
        "createdAt": "2021-11-22T05:41:05Z",
        "updatedAt": "2022-08-16T11:02:52Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "anupamdialpad"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11745,
        "title": "[parse_url] unable to parse url: relative URL without a base\"",
        "bodyText": "Hi there,\nI get this error message with such URLs test.com:\nMapping failed with event. error=\"function call error for \\\"parse_url\\\" at (78:130): unable to parse url: relative URL without a base\" internal_log_rate_secs=30\nWARN transform{component_kind=\"transform\" component_id=ecs_url_mapping_atp component_type=remap component_name=ecs_url_mapping_atp}: vector::internal_events::remap: Internal log [Mapping failed with event.] is being rate limited.\n\nHow do i handle something like this ?\nI would like to be able to parse such URLs.",
        "url": "https://github.com/vectordotdev/vector/discussions/11745",
        "createdAt": "2022-03-09T14:10:13Z",
        "updatedAt": "2022-08-16T10:56:59Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "StefanSa"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12942,
        "title": "Field parsing chain",
        "bodyText": "Assume I collect k8s logs and internal message format differs as I have a lot of different apps. I need to get log level from the internal message into separate field. For now I have:\n          msg_parsed =\n                       parse_json(.message) ??\n                       parse_klog(.message) ??\n                       # parse_logfmt(.message) ??\n                       parse_regex(.message, r'^\\[(?P<level>[A-Z]+)?\\]\\s') ??\n                       parse_regex(.message, r'^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z\\s\\[(?P<level>[A-Z]+)\\]\\s') ??\n                       parse_regex(.message, r'^[A-Z],\\s\\[.+\\]\\s+(?P<level>[A-Z]+) -- :') ??\n                       parse_regex(.message, r'^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z\\s\\d{3}\\s(?P<level>[a-z]+)\\s') ??\n                       parse_regex(.message, r'^\\d{4}/\\d{2}/\\d{2}\\s\\d{2}:\\d{2}:\\d{2}\\s\\[(?P<level>[a-z]+)?\\]\\s') ??\n                       {}\n          if msg_parsed == {} {\n            maybe_nginx = parse_nginx_log(.message, \"combined\") ?? {}\n            if maybe_nginx != {} {\n              msg_parsed = { \"level\": \"info\" }\n            }\n            maybe_nginx_error = parse_nginx_log(.message, \"error\") ?? {}\n            if maybe_nginx_error != {} {\n              msg_parsed = { \"level\": \"error\" }\n            }\n          }\n          .level = downcase(msg_parsed.level || \"unknown\")\n          if .level == \"warn\" {\n            .level = \"warning\"\n         }\n\nIssues:\n\nparse_logfmt is infallible - I can't use it because the rest of the chain not works.\nFor now everywhere the field called level and only that fact allows me to .level = downcase(msg_parsed.level || \"unknown\") at the end\n\nDoes VRL has ||= ruby-like operator? It would be very useful in such cases?\nDo you guys know how to improve the current config to make it less ugly?\nI'm absolutely new to VRL and examples are very scarce :)",
        "url": "https://github.com/vectordotdev/vector/discussions/12942",
        "createdAt": "2022-06-02T11:14:01Z",
        "updatedAt": "2022-08-16T10:42:56Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Antiarchitect"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 13055,
        "title": "Windowing streams on Vector?",
        "bodyText": "I have worked with Kafka Streams in the past where I have been able to define a rolling time window in which to do transformations/filters/aggregations. Is something similar possible on Vector?\nI would like to define a rolling time window, say 1 minute and perform some deduplications.",
        "url": "https://github.com/vectordotdev/vector/discussions/13055",
        "createdAt": "2022-06-09T12:56:49Z",
        "updatedAt": "2022-08-16T10:38:14Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "igr001-galactica"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12868,
        "title": "Failed to read S3 .gz files using aws_s3",
        "bodyText": "Hi Team,\ni am trying to read .gz file contents from s3 using Sqs with source component https://vector.dev/docs/reference/configuration/sources/aws_s3. But getting below error ,\nERROR source{component_kind=\"source\" component_id=s3-source component_type=aws_s3 component_name=s3-source}: vector::internal_events::aws_s3::source: Failed to process SQS message. message_id=51c80ed6-05e1-4a3a-93e4-0ff207e0ef32 error=Could not parse SQS message with id 51c80ed6-05e1-4a3a-93e4-0ff207e0ef32 as S3 notification: missing field Records at line 1 column 30821 error_type=\"processing_failed\" stage=\"processing\"\nVersion:\nvector 0.20.1 (x86_64-pc-windows-msvc 38b806e 2022-04-07)\nvector.yaml:\nsources:\n  s3-source:\n    type: aws_s3\n    region: ****\n    auth:\n      access_key_id: ****\n      secret_access_key: *****\n    strategy: sqs\n    sqs:\n      queue_url: https://****/1234/ap-4849384938498\n\nsinks:\n  stdout:\n    type: console\n    inputs:\n      - s3-source\n    target: stdout\n    encoding:\n      codec: ndjson\n\nBut With the help of type: aws_sqs , i could collect the below message as json format (default its xml).\n{\n  \"bucket\": \"****\",\n  \"cid\": \"******\",\n  \"fileCount\": 2,\n  \"files\": [\n    {\n      \"checksum\": \"000000000001111000000000000000000\",\n      \"path\": \"data/location123/1.gz\",\n      \"size\": 9404149\n    },\n    {\n      \"checksum\": \"000000000002222000000000000000000\",\n      \"path\": \"data/location123/2.gz\",\n      \"size\": 9356626\n    }\n   ],\n  \"pathPrefix\": \"data/location123\",\n  \"timestamp\": 1653550032975,\n  \"totalSize\": 2894819445\n}\n\nDebug verbose logs as follows,\nDEBUG source{component_kind=\"source\" component_id=s3-source component_type=aws_s3 component_name=s3-source}:http: vector::internal_events::http_client: HTTP response. status=200 OK version=HTTP/1.1 headers={\"x-amzn-requestid\": \"58501423-9128-560e-b799-4f2aa4b79a65\", \"date\": \"Thu, 26 May 2022 07:59:42 GMT\", \"content-type\": \"text/xml\", \"content-length\": \"240\"} body=[240 bytes]\nDEBUG source{component_kind=\"source\" component_id=s3-source component_type=aws_s3 component_name=s3-source}:http: vector::internal_events::http_client: Sending HTTP request. uri=https://url/ method=POST version=HTTP/1.1 headers={\"authorization\": Sensitive, \"content-length\": \"222\", \"content-type\": \"application/x-www-form-urlencoded\", \"host\": \"url\", \"x-amz-content-sha256\": \"blababsfsfbasbsbsbsbsbsabsabaababb\", \"x-amz-date\": \"20220526T075942Z\", \"user-agent\": \"Vector/0.20.1 (x86_64-pc-windows-msvc 38b806e 2022-04-07)\", \"accept-encoding\": \"identity\"} body=[unknown]\n\nDid i miss any config in type: aws_s3 file. Any idea?",
        "url": "https://github.com/vectordotdev/vector/discussions/12868",
        "createdAt": "2022-05-26T07:44:15Z",
        "updatedAt": "2022-08-16T10:37:10Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "satscreate"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13168,
        "title": "need to send data to clickhouse strongly every 10 seconds",
        "bodyText": "The clickhouse sink is configured and works well, though it sends every new chunk of data to clickhouse more than 10 seconds. It looks like it varies from 10 to 11 secs. I was unable to find how to configure this time for the certain sink in documentation. We rely on strong periodicity of incoming data in DB. Is there any way to configure Vector like this?",
        "url": "https://github.com/vectordotdev/vector/discussions/13168",
        "createdAt": "2022-06-15T16:54:05Z",
        "updatedAt": "2022-08-16T10:35:48Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "remort"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13250,
        "title": "High utilization for lua transform",
        "bodyText": "Hi,\nI have a lua transform in my vector pipeline. When the traffic goes up, the utilization metric for this transform goes up to 100% (value of 1) and flattens. I want to understand a couple of things related to this event:\n\nWhat impact does it have on the pipeline? Will it result in dropping of some events or just slow the processing?\nHow can I improve the performance for this particular transform?\n\nDetails about the transform:\nIn the init hook it reads a yaml configuration and creates a map.\nIn the process hook, it just uses the map to output the value for a given key.\nVector version: 0.20.1\nInfra:\nIt is running in K8s on dedicated nodes (2 * c5.2xlarge) with 8vCPU each.",
        "url": "https://github.com/vectordotdev/vector/discussions/13250",
        "createdAt": "2022-06-21T10:04:06Z",
        "updatedAt": "2022-08-17T10:53:40Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "amanmahajan26"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 13286,
        "title": "Vector doesnt support high load of traffic (kubernetess_logs)",
        "bodyText": "Hi,\nwe need to support load of 6000 RPS for our load,\neach request is with size of 600 byte.\nwe are using vector 0.21\nmachine type https://docs.microsoft.com/en-us/azure/virtual-machines/fsv2-series (we tried Standard_F4s_v2 & Standard_F8s_v2 ) follows the vector sizing guide for CPU intensive machine type.\nwe are using 3 vector instance on 3 machine (overall 24CPU) ,each vector instance should support 2000RPS.\nThe sync is k8s logs to a file( read specific pod logs and send it to a file)\nThe problem:\nCurrently vector is losing data, for more than 60% of the request we dont see on the file,\nthe vector pods is not overloaded (we verify in Prometheus) .\nwhen reducing the load to 300RPS per instance all the data is written the the file ( no data loss),\nwhat could be the issue? please advice",
        "url": "https://github.com/vectordotdev/vector/discussions/13286",
        "createdAt": "2022-06-22T16:34:38Z",
        "updatedAt": "2022-08-16T10:20:45Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ALNBAND"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 12914,
        "title": "Bad rate on log to metric counter",
        "bodyText": "If I increase the replica number of vector instances on the Kubernetes cluster and I put a log to metric transformation on the pipeline that counts a specific field, the rate of that count will be Invalid...\nit gives us zero rates but we are using demo logs with a time intervals of 1 second...",
        "url": "https://github.com/vectordotdev/vector/discussions/12914",
        "createdAt": "2022-05-31T17:07:06Z",
        "updatedAt": "2022-08-16T10:07:19Z",
        "isAnswered": true,
        "locked": true,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 13976,
        "title": "mongodb sink",
        "bodyText": "Hi, is there a mongodb sink for vector?  I'd like to use vector to send events to a mongodb collection.  If not, can I extend vector by writing my own sink as a plugin?",
        "url": "https://github.com/vectordotdev/vector/discussions/13976",
        "createdAt": "2022-08-15T17:46:32Z",
        "updatedAt": "2022-08-15T19:26:03Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "rkedward"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 13856,
        "title": "RPM installation issue",
        "bodyText": "Hi, there\nThe error occurred when I tried to install the newest rpm vector-0.23.0-1.x86_64.rpm.\nThe linux operator system is CentOS-7-x86_64-Minimal-2009.iso\n\nThen I upgrade the libc version to 2.18, but still get the same error\n\nAnyone help plz.",
        "url": "https://github.com/vectordotdev/vector/discussions/13856",
        "createdAt": "2022-08-05T01:41:03Z",
        "updatedAt": "2022-08-12T17:34:32Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "zzcpower"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13957,
        "title": "Update Docs for heroku_logs/logplex",
        "bodyText": "I am ingesting logs from Heroku and struggled configuring it.\nThe vector part is pretty good documented. But i missed the Heroku part. I took me a day to figure out how the drain should look like which i had to add with the Heroku-cli.\nHere is what worked for me:\nheroku drains:add \"https://vector.mydomain.ch/events?application=myCustomAppTag\" -a myApplication\n\nBut overall, still very happy with vector. Great work!\nPS: Since the docs are generated, i was not able to open a PR.",
        "url": "https://github.com/vectordotdev/vector/discussions/13957",
        "createdAt": "2022-08-12T10:01:08Z",
        "updatedAt": "2022-08-12T13:47:07Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "mms-gianni"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13922,
        "title": "Xml_parsing multiline",
        "bodyText": "{                           \"xml_array\": [\n                                       \"\",\n\n                                       \"<DoNot xmlns=\\http://www.AAMA-google.org/dummy/adms-311/css/aaa\\><RefID>123</RefID><device><UNKnown/></device><doNot>false</doNot></DoNot>--UniqueDataType:applicationLength:123\",\n\n                                       \"<DoNot xmlns=\\http://www.AAMA-google.org/dummy/adms-311/css/aaa\\><RefID>456</RefID><device><UNKnown/></device><doNot>false</doNot></DoNot>--UniqueDataType:applicationLength:123\",\n\n                                       \"<DoNot xmlns=\\http://www.AAMA-google.org/dummy/adms-311/css/aaa\\><RefID>678</RefID><device><UNKnown/></device><doNot>false</doNot></DoNot>--UniqueDataType:applicationLength:123\",\n\n                                       \"<DoNot xmlns=\\http://www.AAMA-google.org/dummy/adms-311/css/aaa\\><RefID>345</RefID><device><UNKnown/></device><doNot>false</doNot></DoNot>\"\n\n                         ],\n\n}\n[transforms.xtrans_events]\ntype = \"remap\"\n\ninputs = [\"data\"]\n\nsource ='''\n\n.array= split(string!(.message),\"|\")\n\n\n\nif contains!(.array[-1],\"xml version\"){\n\n        .xml_data= replace(replace!(.array[-1],\"\\r\\n\",\"\"),\"\\n\",\"\")\n\n        . |= flatten!(parse_regex_all!(.xml_data, r'(?P<xml><.?xml version.*>)')[0])        \n\n        .xml_array= split(string!(.xml),\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\")\n\n        for_each(.xml_array) -> |index, value| {\n\n           \n\n            if match(value, r'<.*>'){\n\n                . |= flatten!(parse_regex_all!(value, r'(?P<xml_1><.*>)')[0])\n\n                . |= flatten!(parse_xml!(string!(.xml_1), parse_number: true,include_attr :true))\n\n            }\n\n        }\n\n\n\n   \n\n}\n\nexpected output\n{\"DoNot.RefID_1\":123,\n\"DoNot.RefID_2\":456,\n\"DoNot.RefID_3\":678,\n\"DoNot.RefID_4\":345} @tobz\ndemo.txt",
        "url": "https://github.com/vectordotdev/vector/discussions/13922",
        "createdAt": "2022-08-10T17:15:17Z",
        "updatedAt": "2022-08-10T18:59:20Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "BlackSpy1231"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13876,
        "title": "Use regular expressions to determine whether the content contains",
        "bodyText": "I want to use if / else if / else regular expression to determine whether it contains content to add fields. I don't know how to write it. After testing, it can't be realized. Is there a code example? Thank you\nold code\nif [request_uri] =~ \"png\" {\n}\nHow to write code with remap",
        "url": "https://github.com/vectordotdev/vector/discussions/13876",
        "createdAt": "2022-08-07T13:27:16Z",
        "updatedAt": "2022-08-08T15:12:05Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tiantian34"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 12131,
        "title": "Using parse_json with unicode in message",
        "bodyText": "Is there a way how to handle unicode when parsing JSON with parse_json function like this:\nmsg_parsed, err = parse_json(string!(.message))\n\nIf JSON contains unicode in field value, eg.\n\\\"req_body\\\":\\\"\ufffd\ufffd\\\\u0000\\\\u0005\\\"\n\nparsing will fail with:\nfunction call error for \\\"parse_json\\\" at (20:49): unable to parse json: invalid unicode code point at line 1 column 8587\n\nMessage is read from kafka.\nAny idea how to properly escape or url-encode message before parsing or some other way?",
        "url": "https://github.com/vectordotdev/vector/discussions/12131",
        "createdAt": "2022-04-08T06:29:54Z",
        "updatedAt": "2022-08-05T17:50:19Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "fpytloun"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 13839,
        "title": "How can I exclude ipv6 from filebeat field host.ip?",
        "bodyText": "I use source type logstach to collect filebeat inputs, the log is json format and has the field host.ip, such as { \"host\": { \"ip\": [ \"10.4.139.11\", \"fe80::5054:ff:fe4f:893e\" ] } }.\nNow I want to exclude the ipv6 address such as fe80::5054:ff:fe4f:893e, how can I reach this? (I found remap don't have the for operation?) Or maybe I have to use lua?",
        "url": "https://github.com/vectordotdev/vector/discussions/13839",
        "createdAt": "2022-08-04T02:13:43Z",
        "updatedAt": "2022-08-05T01:20:21Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "jmjoy"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13832,
        "title": "filtering empty fields",
        "bodyText": "Hi,\nwe are using log 2 metric and we are trying to replace a specific field in a json if its null/empty, here is an example log line:\n\n{\"hostname\":\"xxxxxxx\",\"method\":\"POST\",\"path\":\"YYYYYYY\",\"status\":\"200\",\"request_time\":\"0.000\",\"partner\":\"111\",\"cache_status\":\"HIT\",\"nginx_host\":\"knginx-785bf9c75f-5mmbq\",\"error_output\":\"\"}\n\nwhat we'd like to do is if error_output is empty we want to replace it with 'OK' if it contains an error we want to keep the error, we've tried multiple options, none seem to work properly, this is what we have been trying:\n[transforms.filter_notempty]\ntype = \"filter\"\ninputs = [\"nginx_source_metrics\"]\ncondition = \".error_output != \"\"\"\n[transforms.filter_empty]\ntype = \"filter\"\ninputs = [\"nginx_source_metrics\"]\ncondition = \".error_output == \"\"\"\n[transforms.empty_remap]\ntype = \"remap\" # required\ninputs = [\"filter_empty\"] # required\nsource = '''\nstructured, err = parse_json(.message)\nif err != null {\nlog(\"Unable to parse_json RAW:\" + string!(.message), level: \"error\")\n} else {\n., err = merge(., structured)\n.error_output = \"Ok\"\n}\n'''\n[sinks.logs_to_aggregator]\ntype = \"vector\" # required\ninputs = [\"empty_remap\",\"filter_notempty\"] # required\naddress = \"vector-aggregator:9001\" # required\nthe above doesn't seem to work, we are still getting an empty value in error_output field instead of 'Ok'm any lead would help, thanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/13832",
        "createdAt": "2022-08-03T18:14:29Z",
        "updatedAt": "2022-08-04T08:15:14Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "dannygueta"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 13795,
        "title": "Vector Source kubernetes_log Using Pod Annotations",
        "bodyText": "Vector Agent version: v0.23.0\nInstalled by: Helm\nVector Configuration\ndata_dir: /vector-data-dir\nsources:\n  kubernetes_logs:\n    extra_field_selector: metadata.annotations.logging/enabled=true\n    type: kubernetes_logs\nsinks:\n  kafka:\n    bootstrap_servers: server:port\n    encoding:\n      codec: json\n    inputs:\n      - kubernetes_logs\n    topic: topic_logs\n    type: kafka\n\nOutput Log\n2022-08-02T05:25:00.234216Z WARN vector::kubernetes::reflector: Watcher Stream received an error. Retrying. error=InitialListFailed(Api(ErrorResponse { status: \"Failure\", message: \"field label not supported: metadata.annotations.logging/enabled\", reason: \"BadRequest\", code: 400 }))\n\nPod Manifest\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nginx\n  replicas: 2\n  template:\n    metadata:\n      annotations:\n        logging/enabled: \"true\"\n      labels:\n        app.kubernetes.io/name: nginx\n        logging/enabled: \"true\"\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n\nI have tried filter using extra_label_selector:logging/enabled=true and it works fine, but how it work using annotations.\nI've been looking for documentation, issues, discussions, but can't find how to do it.\nThank you!",
        "url": "https://github.com/vectordotdev/vector/discussions/13795",
        "createdAt": "2022-08-02T05:30:22Z",
        "updatedAt": "2022-08-03T16:48:10Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "cahyahikmawan"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13797,
        "title": "is there a guide on how Reduce works",
        "bodyText": "I think I don't understand Reduce, because given the following, I expected to have an event whenever .count reached 10 events (assuming sum merge strategy would happen magically on that field):\ninputs = [\"activity\"]\ntype = \"reduce\"\nends_when = \"to_float!(.count) == 10\"\ngroup_by = [\n  \"name\",\n  \"node\",\n]\nThis gets fed events of this shape\n{\n  \"count\": 1.0,\n  \"name\": \"some-name\",\n  \"node\": \"some-node\",\n  \"timestamp\": \"2022-08-02T13:40:22.815Z\"\n}\nWhat I get instead is just an assortment of events with varying .count values.",
        "url": "https://github.com/vectordotdev/vector/discussions/13797",
        "createdAt": "2022-08-02T13:42:09Z",
        "updatedAt": "2022-08-03T16:48:05Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tshepang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 8628,
        "title": "High cardinality metrics in k8s environments",
        "bodyText": "Hello to everyone!\nSome vector metrics have potentially unbound cardinality, like following:\n\nvector_files_added_total\nvector_files_unwatched_total\nvector_files_resumed_total\nvector_processed_bytes_total\n\nBecause each metrics contains label \"file\" (autogenerated by log driver) or \"pod_name\" (autogenertated by k8s).\nI tried to mitigate these issues with configuration transforms:\nsources:\n  kubernetes_logs:\n    type: kubernetes_logs\n  internal_metrics:\n    type: internal_metrics\ntransforms:\n  prometheus_sink_transform_cleanup:\n    inputs:\n    - internal_metrics\n    source: |\n      del(.tags.file)\n      del(.tags.pod_name)\n    type: remap\n  prometheus_sink_transform_aggregate:\n    inputs:\n    - prometheus_sink_transform_cleanup\n    type: aggregate\nsinks:\n  prometheus_sink:\n    address: 0.0.0.0:9090\n      inputs:\n      - prometheus_sink_transform_aggregate\n      type: prometheus_exporter\nHere first remap transform is used to delete high cardinality tags and then aggregate transform is used to aggregate multiple metric events values to a single metric event.\nBut it seems, that after the first transform, the metric values began to be calculated incorrectly - displayed only the first value, not the sum of all metrics events. For example:\n(before transform)\n# HELP vector_files_resumed_total files_resumed_total\n# TYPE vector_files_resumed_total counter\nvector_files_resumed_total{component_kind=\"source\",component_name=\"kubernetes_logs\",component_type=\"kubernetes_logs\",file=\"/var/log/pods/dev-mlc_cookinginfo-create-queue-cws5p_ea730cd7-731d-4865-b871-c65977c80659/create-queue/0.log\"} 1 1628443140171\nvector_files_resumed_total{component_kind=\"source\",component_name=\"kubernetes_logs\",component_type=\"kubernetes_logs\",file=\"/var/log/pods/dev-cc_datacatalog-migrator-zgw6j_3d14844c-68da-4d78-9401-418a0d448b9d/migrator/0.log\"} 1 1628443140171\nvector_files_resumed_total{component_kind=\"source\",component_name=\"kubernetes_logs\",component_type=\"kubernetes_logs\",file=\"/var/log/pods/dev-ninja_ratings-migrator-qgdxc_df6be0da-9211-4490-bda0-2ddfbbe9fadf/migrator/0.log\"} 1 1628443140171\nvector_files_resumed_total{component_kind=\"source\",component_name=\"kubernetes_logs\",component_type=\"kubernetes_logs\",file=\"/var/log/pods/dev-mct_shopwindow-migrator-mk2tm_bac2c59e-36e5-4c0c-94fa-8f4e8ab5e3a7/migrator/0.log\"} 1 1628443140171\nvector_files_resumed_total{component_kind=\"source\",component_name=\"kubernetes_logs\",component_type=\"kubernetes_logs\",file=\"/var/log/pods/dev-ninja_ratings-56847b6d64-lggl6_5d3d5cc8-666e-4deb-b5e4-0832112bcd75/backend/0.log\"} 1 1628443140171\n...\n\n(after transform):\n# HELP vector_files_resumed_total files_resumed_total\n# TYPE vector_files_resumed_total counter\nvector_files_resumed_total{component_kind=\"source\",component_name=\"kubernetes_logs\",component_type=\"kubernetes_logs\"} 1 1628443267424\n\nI also read about tag cardinality limit. But it seems, this transform is more like a safeguard for high cardinality metrics.\nIs there any way to drop high cardinality tags and aggregate metric values?",
        "url": "https://github.com/vectordotdev/vector/discussions/8628",
        "createdAt": "2021-08-08T17:40:00Z",
        "updatedAt": "2022-08-01T15:24:28Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ArtemTrofimushkin"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 2
    },
    {
        "number": 13773,
        "title": "Redis Authentication required error",
        "bodyText": "Hi.\nHow can we add the Redis password on the sink block in the vector pipeline?\nI can't find any docs on this case",
        "url": "https://github.com/vectordotdev/vector/discussions/13773",
        "createdAt": "2022-07-31T11:07:34Z",
        "updatedAt": "2022-08-01T16:05:35Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13565,
        "title": "Question on Vector PubSub Source throughput capability",
        "bodyText": "Hi @bruceg @jszwedko\nI have setup a Vector Deployment in GKE (Agg Mode) which consumes message from a Google PubSub Subscription.\nThe pipeline looks like this : PubSub Src -> Log Processing Transform Layer -> Loki\nWhen I look at the utilisation metric for the transform stage (first stage after the source) I see it remains very low (~0.02). On the Google Subscription stats I see that the number of unacknowledged message ~ (200K - 300K). I want to understand if there is a bottleneck somewhere in the pubsub->vector_source part and how to identify / fix it.\nThis is different from #12990 though where the number of unack'ed mesages kept on growing. In this case, message are being processed correctly by Vector. I am trying to explain the constant ~200K-300K unacked  messages.\nDeployment Details :\n100 Vector Pods part of a GKE Deployment\nEach pod requests: 3-3.5 vCPU and 6-7GB of memory\nNode type: c2-standard-8 Google Instances\nConfig:\n[sources.fst_cloud_log_source]\n  type = \"gcp_pubsub\"\n  ack_deadline_secs = 60\n  retry_delay_secs = 1 \n  project = \"fstelephony\"\n  subscription = \"log-ingest-vector\"                                                                                                           \n  acknowledgements.enabled = true",
        "url": "https://github.com/vectordotdev/vector/discussions/13565",
        "createdAt": "2022-07-15T09:28:20Z",
        "updatedAt": "2022-07-29T22:33:26Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "atibdialpad"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 13747,
        "title": "Docker CPU and Memory metrics via prometheus_exporter",
        "bodyText": "Hi,\nI'm looking for a way to monitor cpu and memory usage by vector which run in a docker container. The metrics for in -and output is already very useful, but I can't find a solution yet for simple cpu and memory, I tried the host metrics but then it will show only a total of my server.\nI was hoping to select via a dropdown in Grafana, so if anyone has suggestions, hints that would be great!\nNext week we like to test the performance of vector encrypting SSIDs from modem data till we reach around 500.000 messages per minute, so we like to monitor the pipeline if it keeps up in speed without discarding / dropping messages...",
        "url": "https://github.com/vectordotdev/vector/discussions/13747",
        "createdAt": "2022-07-28T07:14:31Z",
        "updatedAt": "2022-08-16T09:50:28Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "lzwaan"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13726,
        "title": "Config Validation for Helm Charts",
        "bodyText": "Too many times, I've ended up with vector pods in CrashLoopBackoff when I make a mistake in my helm values file.  I'm trying to use vector validate to validate my configuration locally before deploying the helm chart, but validation fails when it tries to test the Splunk sink.\n~ Unknown env var in config. name = \"SPLUNK_HEC_TOKEN\"\n~ Unknown env var in config. name = \"SPLUNK_HEC_ENDPOINT\"\n~ Unknown env var in config. name = \"SPLUNK_HEC_INDEX\"\n~ Unknown env var in config. name = \"DATADOG_API_KEY\"\n\n\u221a Component configuration\n2022-07-26T21:32:09.502593Z ERROR vector::topology::builder: msg=\"Healthcheck: Failed Reason.\" error=Unexpected status: 403 Forbidden component_kind=\"sink\" component_type=\"datadog_metrics\" component_id=vector_metrics_out component_name=vector_metrics_out\nx Health check for \"vector_metrics_out\" failed\n2022-07-26T21:32:09.502905Z ERROR vector::internal_events::http_client: HTTP error. error=client requires absolute-form URIs error_type=\"request_failed\" stage=\"processing\"\n2022-07-26T21:32:09.502957Z ERROR vector::topology::builder: msg=\"Healthcheck: Failed Reason.\" error=Failed to make HTTP(S) request: client requires absolute-form URIs component_kind=\"sink\" component_type=\"splunk_hec_logs\" component_id=splunk_logs_out component_name=splunk_logs_out\nx Health check for \"splunk_logs_out\" failed\n\nI would have assumed that this scenario is what the --no-environment flag was for, but when I pass that, it doesn't seem to validate the configuration at all.  Validation seems to always pass without catching fallible functions that need error handling or even blatant syntax errors.  Am I missing something?",
        "url": "https://github.com/vectordotdev/vector/discussions/13726",
        "createdAt": "2022-07-26T21:52:12Z",
        "updatedAt": "2022-07-28T15:33:10Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "jblang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13654,
        "title": "Multiline source won't include java stacktrace in message",
        "bodyText": "We need some help using the multiline feature.\nWe're using a log like the following:\n[2022-07-18T11:46:25,410+1000] DEBUG [java.class] - first line\n[2022-07-18T11:46:25,418+1000] ERROR [java.class] - Begin event threw exception java.lang.Exception: class name\n        at stacktrace.info(javaClass.java:77)\n        at stacktrace.info(javaClass.java:276)\n        at stacktrace.info(javaClass.java:1350)\n        stacktrace.info(javaClass.java:377)\n        at net.sf.saxon.event.ProxyReceiver.startElement(ProxyReceiver.java:140)\n        ...\nError on line 304 column 11 of filename.xsl:\n  SXCH0003  org.xml.sax.SAXParseException; systemId:\n  file:/filepath/filename.xsl;\n[2022-07-18T11:46:25,418+1000] ERROR [java.class] - Content parsing failed - see exception\n[2022-07-18T11:46:27,240+1000] DEBUG [java.class] - Logging Provider: org.jboss.logging.Slf4jLoggerProvider\n\nIn our /etc/vector/vector.toml, we have the following for our transforms which successfully groups the timestamp, the log severity, the class name and the message for when the application is working properly:\n[transforms.parse_logs]\ntype = \"remap\"\ninputs = [\"logs\"]\nsource = '''\n. = parse_regex!(.message,r'\\[(?P<timestamp>\\d+-\\d+-\\d+T\\d+:\\d+:\\d+),\\d+\\+\\d+\\].(?P<severity>\\w+)\\s+\\[(?P<class>(?:.*))\\]...(?P<message>(?:.*))')\n.timestamp = .timestamp + \"Z\"\n.application_id = \"applications\"\n.log_type = \"schedule-loader\"\n.platform = \"bookable-promos\"\n'''\n\nHowever this does not handle the stacktrace, so in the 'sources' section,  we added a 'multiline' configuration as follows:\n[sources.logs]\ntype = \"file\"\ninclude = [\"/tmp/schedule-loader-2022-07-18.log\"]\nread_from = \"beginning\"\nignore_checkpoints = true\n\n  [sources.logs.multiline]\n  mode = \"halt_before\"\n  start_pattern = \"^\\\\[\"\n  condition_pattern = \"^\\\\[\"\n  timeout_ms = 1000\n\nWe've been reading the documentation here:\nhttps://vector.dev/docs/reference/configuration/sources/file/#multiline-messages\nAccording to that documentation, using the config we have, the 'halt_before' setting should parse the line beginning with a stacktrace (ie, starting with a '[' character), and continue aggregating the lines until it reaches the condition line, the next line beginning with a '[' character.\nHowever, no aggregation occurs, and only the lines with the timestamps appear in the output:\n{\"application_id\":\"applications\",\"class\":\"java.class\",\"log_type\":\"schedule-loader\",\"message\":\"first line\",\"platform\":\"bookable-promos\",\"severity\":\"DEBUG\",\"timestamp\":\"2022-07-18T11:46:25Z\"}\n{\"application_id\":\"applications\",\"class\":\"java.class\",\"log_type\":\"schedule-loader\",\"message\":\"Begin event threw exception java.lang.Exception: class name\",\"platform\":\"bookable-promos\",\"severity\":\"ERROR\",\"timestamp\":\"2022-07-18T11:46:25Z\"}\n{\"application_id\":\"applications\",\"class\":\"java.class\",\"log_type\":\"schedule-loader\",\"message\":\"Content parsing failed - see exception\",\"platform\":\"bookable-promos\",\"severity\":\"ERROR\",\"timestamp\":\"2022-07-18T11:46:25Z\"}\n{\"application_id\":\"applications\",\"class\":\"java.class\",\"log_type\":\"schedule-loader\",\"message\":\"Logging Provider: org.jboss.logging.Slf4jLoggerProvider\",\"platform\":\"bookable-promos\",\"severity\":\"DEBUG\",\"timestamp\":\"2022-07-18T11:46:27Z\"}\n\nWhat are we missing here?  Is there some setting that we have misconfigured? or does the multiline not work as documented?\nWe have also tried using the 'continue_through' setting and changing the 'condition_pattern' to be `^[\\s]+' to at least catch the indented lines, but that did not work either. Given the stacktraces also have lines beginning with 'Caused by' it made more sense to use the 'halt_before' setting.\nAny help would be much appreciated.\nWe are using vector versions 0.23.0 and 0.22.3",
        "url": "https://github.com/vectordotdev/vector/discussions/13654",
        "createdAt": "2022-07-21T05:49:14Z",
        "updatedAt": "2022-07-27T01:40:15Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "jonathanderham-streamotion"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 6
        },
        "upvoteCount": 1
    },
    {
        "number": 13682,
        "title": "I use vector+kafka+vector+loki to write the service log to cloki, but there are some problems",
        "bodyText": "Use vector1 to collect file logs to Kafka; Use Vector2 to send logs from Kafka to cloki. But I have some questions:\n\n\nWhen I have multiple Vector2, can I set the number of partitions consumed by each vector, such as the consumer of logstash_ Same configuration as threads\n\n\nI don't know which step is better to use transform to parse logs on vector1 or Vector2\n\n\nI see that transform can use VRL and grok at the same time. Are there any performance differences between them\n\n\nIt is recommended that VRL provide more log examples of programming languages or services when parsing logs, such as Java, golang, nginx, redis, /var/log, and so on",
        "url": "https://github.com/vectordotdev/vector/discussions/13682",
        "createdAt": "2022-07-22T09:39:00Z",
        "updatedAt": "2022-08-16T09:51:55Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ktpktr0"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 13519,
        "title": "How to filter out blank lines and specific entries from log4net files",
        "bodyText": "My Windows application is using the log4net library to generate log files. The logs contain blank lines and entries with the word \"Setting:\" along with the DEBUG entries. Here's a sample of the logs:\n2022-07-12 12:12:20,615 [5928] DEBUG VehicleScore Calculate Driving Range - Start.\n2022-07-12 12:12:20,615 [5928] DEBUG VehicleScore Updated By: 0\nSetting: FlagToReset\n\n2022-07-12 12:12:20,616 [5928] DEBUG VehicleScore Calculate Calculate Driving Range  - Start.\n\nCan I use the Filter transform to exclude blank lines and entries containing the word \"Setting:\" ?",
        "url": "https://github.com/vectordotdev/vector/discussions/13519",
        "createdAt": "2022-07-12T15:42:59Z",
        "updatedAt": "2022-08-16T10:03:59Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "marketier"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13585,
        "title": "is geoip enrichment table missed in v0.23 announcement",
        "bodyText": "This refers to this change #13338",
        "url": "https://github.com/vectordotdev/vector/discussions/13585",
        "createdAt": "2022-07-16T20:48:08Z",
        "updatedAt": "2022-07-18T19:01:42Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tshepang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13593,
        "title": "Using docker container envrioment variables as loki log labels",
        "bodyText": "Looking to lift environment variables out of docker and use to define loki log labels. Wondering if anyone knows if this is possible.\nHere is the config toml\n          [api]\n            enabled = true\n            address = \"0.0.0.0:8686\"\n            playground = true\n          [sources.logs]\n            type = \"docker_logs\"\n          [sinks.out]\n            type = \"console\"\n            inputs = [ \"logs\" ]\n            encoding.codec = \"json\"\n          [sinks.loki]\n            type = \"loki\"\n            inputs = [\"logs\"]\n            endpoint = \"url\"\n            encoding.codec = \"json\"\n            healthcheck.enabled = true\n            [sinks.loki.labels]\n              testing = '{{ env.\\\\[1\\\\] }}'`\n\nI've tried with this and escaping out the characters.\nAnyone any ideas?\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/13593",
        "createdAt": "2022-07-18T13:36:35Z",
        "updatedAt": "2022-07-18T14:26:33Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "dpewsey"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13487,
        "title": "source_type not documented for file source",
        "bodyText": "Am seeing \"source_type\":\"file\" in the output of file source, but I see it's not documented here https://vector.dev/docs/reference/configuration/sources/file/#line-fields. I wonder if it is intentional, because it's not documented for the few other sources I checked.",
        "url": "https://github.com/vectordotdev/vector/discussions/13487",
        "createdAt": "2022-07-09T09:15:19Z",
        "updatedAt": "2022-08-16T10:04:22Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tshepang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13548,
        "title": "acknowledgements",
        "bodyText": "Hi.\nwould you please help me figure this out?\nI am trying to minimize the event loss in my vector pipeline with disk buffers and acknowledgments\nKafka(source)--->loki(sink)\nbut my question is that if I just add enabled acknowledgments on Loki sink, this means that if Loki doesn't answer the requests, the vector sink buffer doesn't block and my vector consume events again after Loki becomes up again. it correct?\nor should I add disk buffer anyway in the case of restarting the vector or the time that vector experiencing OOMKIll for example?",
        "url": "https://github.com/vectordotdev/vector/discussions/13548",
        "createdAt": "2022-07-14T07:27:52Z",
        "updatedAt": "2022-07-14T16:38:49Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13283,
        "title": "Enrich file source with docker metadata",
        "bodyText": "Hi\nDocker is using json-file log driver and are writing logs to the file system. Was using Filebeat earlier and there it was possible to enrich these log files with Docker metadata like container id and container name. Is there a way of doing the same in Vector using source: file?\nOr is there maybe a different way of getting docker logs with the possibility to see which container they belong to?\nOrjanp",
        "url": "https://github.com/vectordotdev/vector/discussions/13283",
        "createdAt": "2022-06-22T12:09:47Z",
        "updatedAt": "2022-07-14T12:34:50Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "OrjanPettersen"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 13520,
        "title": "Use Windows System environment variable in host_key",
        "bodyText": "I have several System environment variables defined on a Windows host. Can an environment variable be defined for host_key?\nMy source is:\n[sources.winapplogs]\ntype = \"file\"\nignore_older_secs = 600\nglob_minimum_cooldown_ms = 1_800\ninclude = [ \"L:/Logs/**/Logfile.log\" ]\nread_from = \"beginning\"\nmax_line_bytes = 102_400\nmax_read_bytes = 2_048\nfile_key = \"file\"\nhost_key = \"host\"\n\nIs something like the following possible?\nhost_key = \"HOST_ALIAS_ENV_VAR\"",
        "url": "https://github.com/vectordotdev/vector/discussions/13520",
        "createdAt": "2022-07-12T15:53:57Z",
        "updatedAt": "2022-07-13T18:19:42Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "marketier"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 13511,
        "title": "How to parse labels of the format `kubernetes.io/name: my-app`",
        "bodyText": "I am pushing logs from a kubernetes source to a loki sink. Everything works great and the parsing is quite fast. Loving it so far.\nThis configuration works except for the app_k8s_name key. This is because of the \".\" in the key for the label being parsed.\napp_k8s_name: '{{`{{kubernetes.pod_labels.app.kubernetes.io/name}}`}}'\nThe relevant config in context is here:\ncustomConfig:\n  sinks:\n    loki:\n      type: loki\n      inputs:\n        - kubernetes\n      endpoint: http://loki:3100\n      compression: none\n      encoding:\n        codec: json\n      labels:\n        forwarder: vector\n        job: kubernetes\n        namespace: \"{{`{{kubernetes.pod_namespace}}`}}\"\n        pod_name: \"{{`{{kubernetes.pod_name}}`}}\"\n        pod_owner: \"{{`{{kubernetes.pod_owner}}`}}\"\n        container_name: \"{{`{{kubernetes.container_name}}`}}\"\n        container_image: \"{{`{{kubernetes.container_image}}`}}\"\n        app: \"{{`{{kubernetes.pod_labels.app}}`}}\"\n        app_k8s_name: '{{`{{kubernetes.pod_labels.app.kubernetes.io/name}}`}}'\n      out_of_order_action: \"accept\"\n      remove_timestamp: true\n      acknowledgements:\n        enabled: false\n      tenant_id: \"kubernetes\"\n  sources:\n    kubernetes:\n      type: kubernetes_logs\n      auto_partial_merge: true\n      self_node_name: ${VECTOR_SELF_NODE_NAME}\n      max_read_bytes: 2048\n      max_line_bytes: 32768\n      fingerprint_lines: 1\n      glob_minimum_cooldown_ms: 60000\n      delay_deletion_ms: 60000\n      timezone: local\n  data_dir: /vector-data-dir\nAny pointers on how this can be resoled?",
        "url": "https://github.com/vectordotdev/vector/discussions/13511",
        "createdAt": "2022-07-12T02:54:42Z",
        "updatedAt": "2022-07-12T16:07:38Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "suryaoruganti"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13484,
        "title": "Can vector ignore configuration errors on launch?",
        "bodyText": "Can I configure vector to ignore errors in some configuration files on launch?\nI launch vector with /usr/bin/vector --config-dir /etc/vector --watch-config as a kube sidecar.\nUse case here is I have one main configuration that provides common sinks and transforms that changes infrequently at /etc/vector/main.yaml. I also have many /etc/vector/*.yaml configuration files that do change frequently, and rely on the main configuration.\nA bad configuration file would cause the container to fail and the pods would crash loop. I'd like to launch vector to only require main.yaml to be valid, and ignore config files with errors. Can this be done?",
        "url": "https://github.com/vectordotdev/vector/discussions/13484",
        "createdAt": "2022-07-08T22:54:15Z",
        "updatedAt": "2022-07-14T21:04:20Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Quyzi"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13419,
        "title": "parse_key_value: howto discard stand-alone keys",
        "bodyText": "$ parse_key_value!(\"one=1 two=2 three\", accept_standalone_key: false)\nfunction call error for \"parse_key_value\" at (0:67): could not parse whole line successfully\n\nWould be nice to have an option to simply discard three and keep the rest. Is there a workaround?",
        "url": "https://github.com/vectordotdev/vector/discussions/13419",
        "createdAt": "2022-07-02T09:04:32Z",
        "updatedAt": "2022-07-05T20:42:50Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tshepang"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13433,
        "title": "getting message about failed acknowledgement when such is not enabled",
        "bodyText": "I have this log:\n\n2022-07-05T05:44:16.908503Z ERROR source{component_kind=\"source\" component_id=logstash-in component_type=logstash component_name=logstash-in}:connection{peer_addr=:35754}: vector::internal_events::tcp: Error writing acknowledgement, dropping connection. error=Broken pipe (os error 32) error_code=\"ack_failed\" error_type=\"writer_failed\" stage=\"sending\" internal_log_rate_secs=10\n\nLooking at docs, one has to explicitly enable acknowledgements, so makes me wonder if this is a false message.",
        "url": "https://github.com/vectordotdev/vector/discussions/13433",
        "createdAt": "2022-07-05T05:48:54Z",
        "updatedAt": "2022-07-05T16:20:41Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tshepang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13349,
        "title": "removing multiple fields",
        "bodyText": "Hi!\nI want to remove few fields by some mask, but can't understand how to achieve this\nFor example, i have multiple fields like:\nkubernetes.pod_annotations.checksum/result-backend-secret\nkubernetes.pod_annotations.checksum/webserver-secret-key\n\nso first i was tried to use del function with mask\ndel(.\"kubernetes.pod_annotations.checksum.*\")\nbut no luck, seems that it's impossible to use some regex pattern for that, neither to use variable as argument for del function, so my next step is to remove fields with remove function in for_each loop like this\nfor_each(.) -> |key, val| {\n            if match(key, r'^kubernetes\\.pod_annotations\\.checksum*') {\n              ., err = remove(value: ., path: [key])\n              if err != null {\n                log(err)\n              }\n            }\n\nBut still got messages with this fields\nThis looks like common usecase, but i wasn't able to find some leads in documentation, can anyone help me with this?",
        "url": "https://github.com/vectordotdev/vector/discussions/13349",
        "createdAt": "2022-06-27T16:12:45Z",
        "updatedAt": "2022-07-02T09:58:49Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Wilderone"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13410,
        "title": "How do I use the configuration file in yaml format when I use docker",
        "bodyText": "How do I use the configuration file in yaml format when I use docker\nThe example only show me use toml format",
        "url": "https://github.com/vectordotdev/vector/discussions/13410",
        "createdAt": "2022-07-01T10:29:57Z",
        "updatedAt": "2022-07-01T12:51:42Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Fury76"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13361,
        "title": "how to remove the special character  '\\\\\\n' with vector's vrl",
        "bodyText": "filebeat (multiline) ---> vector\nvrl:\nsource = '''\n. |= parse_regex!(.message, r'^(?P\\d+-\\d+-\\d+ \\d+:\\d+:\\d+.\\d++\\d+): (?P.+)$')\n.timestamp = parse_timestamp(.timestamp, \"%Y-%m-%d %H:%M:%S.%ms+%z\") ?? now()\nmessage_parts = split(.message, \", \", limit: 20)\nstructured = parse_key_value(message_parts[1], key_value_delimiter: \":\", field_delimiter: \",\") ?? {}\n.message = replace(.message, r'\\\\n', \" \")\n. = merge(., structured)\n'''\ni get  result blow\n\"message\": \"2022-06-28 08:45:45.241+0000: starting up libvirt version: 4.5.0, package: 36.el7_9.5 (CentOS BuildSystem http://bugs.centos.org, 2021-04-28-13:32:22, x86-01.bsys.centos.org), qemu version: 1.5.3 (qemu-kvm-1.5.3-175.el7_9.6), kernel: 3.10.0-1160.66.1.el7.x86_64, hostname: localhost.localdomain\\nLC_ALL=C \\\\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin \\\\nQEMU_AUDIO_DRV=none \\\\n/usr/libexec/qemu-kvm \\\\n-name centos7 \\\\n-S \\\\n-machine pc-i440fx-rhel7.0.0,accel=kvm,usb=off,dump-guest-core=off \\\\n-cpu Westmere-IBRS,+spec-ctrl,+ssbd \\\\n-m 2048 \\\\n-realtime mlock=off \\\\n-smp 1,sockets=1,cores=1,threads=1 \\\\n-uuid 4e165ef3-af3b-44fa-b5b5-61f3e3a4313b \\\\n-display none \\\\n-no-user-config \\\\n-nodefaults \\\\n-chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/domain-15-centos7/monitor.sock,server,nowait \\\\n-mon chardev=charmonitor,id=monitor,mode=control \\\\n-rtc base=utc,driftfix=slew \\\\n-global kvm-pit.lost_tick_policy=delay \\\\n-no-hpet \\\\n-no-shutdown \\\\n-global PIIX4_PM.disable_s3=1 \\\\n-global PIIX4_PM.disable_s4=1 \\\\n-boot strict=on \\\\n-device ich9-usb-ehci1,id=usb,bus=pci.0,addr=0x4.0x7 \\\\n-device ich9-usb-uhci1,masterbus=usb.0,firstport=0,bus=pci.0,multifunction=on,addr=0x4 \\\\n-device ich9-usb-uhci2,masterbus=usb.0,firstport=2,bus=pci.0,addr=0x4.0x1 \\\\n-device ich9-usb-uhci3,masterbus=usb.0,firstport=4,bus=pci.0,addr=0x4.0x2 \\\\n-device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x5 \\\\n-drive file=/data/testvm/centos7/centos7.qcow2,format=qcow2,if=none,id=drive-virtio-disk0 \\\\n-device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x6,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 \\\\n-drive file=/data/testvm/centos7/centos7-1.qcow2,format=qcow2,if=none,id=drive-virtio-disk1 \\\\n-device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x7,drive=drive-virtio-disk1,id=virtio-disk1 \\\\n-drive if=none,id=drive-ide0-0-0,readonly=on \\\\n-device ide-cd,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 \\\\n-netdev tap,fd=27,id=hostnet0,vhost=on,vhostfd=29 \\\\n-device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:a5:a2:54,bus=pci.0,addr=0x3 \\\\n-chardev pty,id=charserial0 \\\\n-device isa-serial,chardev=charserial0,id=serial0 \\\\n-chardev socket,id=charchannel0,path=/var/lib/libvirt/qemu/channel/target/domain-15-centos7/org.qemu.guest_agent.0,server,nowait \\\\n-device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charchannel0,id=channel0,name=org.qemu.guest_agent.0 \\\\n-device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x8 \\\\n-object rng-random,id=objrng0,filename=/dev/urandom \\\\n-device virtio-rng-pci,rng=objrng0,id=rng0,bus=pci.0,addr=0x9 \\\\n-msg timestamp=on\\nchar device redirected to /dev/pts/2 (label charserial0)\",",
        "url": "https://github.com/vectordotdev/vector/discussions/13361",
        "createdAt": "2022-06-28T09:33:38Z",
        "updatedAt": "2022-08-16T10:17:32Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "omyhub"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13398,
        "title": "upcoming website?",
        "bodyText": "This is mentioned here #13391 (comment)\nIs this tracked somewhere in public? I look forward to it because the current one is a pain to navigate... it's slow, and has not-great search experience.",
        "url": "https://github.com/vectordotdev/vector/discussions/13398",
        "createdAt": "2022-06-30T20:46:44Z",
        "updatedAt": "2022-06-30T21:17:37Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tshepang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13392,
        "title": "Development environment vector (build speed)",
        "bodyText": "On my machine (32GB memory, i7-10510U CPU 1.80GHz) the build speed of vector is slow (1-2min).\nI'm rather new to Rust so I rebuild my changes fairly often to understand if my changes would work. But if every build takes 1-2 minutes this is really painful.\nDo others have the same problem? Is there a trick to speed up the build times? How does your development flow look like?",
        "url": "https://github.com/vectordotdev/vector/discussions/13392",
        "createdAt": "2022-06-30T06:21:02Z",
        "updatedAt": "2022-07-01T06:39:08Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "dvob"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13388,
        "title": "Confusion regarding DataDog Agent vs Vector",
        "bodyText": "I started with the following problem at hand: DataDog doesn't provide an OpenTelemetry Collector exporter for logs:\n\nopen-telemetry/opentelemetry-collector-contrib#2651\n\nSince I want to keep vendor code away from my apps, I started looking into potential solutions.\nThe first thing that came to my mind was routing otel-collector logs to a DataDog Agent that would then push the logs into DD itself. However, after a quick search, it appears that the DD Agent doesn't support logs at all...\nThen I came back to the original issue and found that one poster was using a tool called \"vector\" in combination with the otel-collector for processing logs into DataDog to overcome the limitation of lack of an exporter in otel-collector.\nI proceeded to search for vector and then found this..... which is..... \"by DataDog\"?! I'm very confused now. Can someone clarify please? Are all of these true?\n\nDD has not implemented an otel-collector-compatible log exporter\nDD has an agent called \"DataDog Agent\", that also doesn't support processing logs in open telemetry protocol format\nDD has.... this project, which is another agent, similar to the purpose of both the DataDog Agent as well as the otel-collector, that does support consuming and pushing logs in otel format\n\nWhy does the DataDog Agent exist at all at this point, if this is supposedly a full-blown solution that does everything the DD Agent does and is done by the same company?",
        "url": "https://github.com/vectordotdev/vector/discussions/13388",
        "createdAt": "2022-06-29T21:03:03Z",
        "updatedAt": "2022-06-30T15:17:24Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "julealgon"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13335,
        "title": "howto build executable so that it has vrl subcommand",
        "bodyText": "cargo build does not work on latest (754d52b)\n\u276f cargo run --release -- vrl\n    Finished release [optimized] target(s) in 0.30s\n     Running `target/release/vector vrl`\nerror: Found argument 'vrl' which wasn't expected, or isn't valid in this context",
        "url": "https://github.com/vectordotdev/vector/discussions/13335",
        "createdAt": "2022-06-25T04:23:18Z",
        "updatedAt": "2022-06-27T18:28:11Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tshepang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 13342,
        "title": "How to call python script from exec component.",
        "bodyText": "Hi, I used to call python script using below lines with custom vector linux image. This is working under docker perspective.\ncommand: [\"bash\", \"-c\", \"python script.py\"]\nNow how can i run this in windows command line using vector installed,\nCMD versions:\nC:\\Users\\test>vector --version\nvector 0.20.1 (x86_64-pc-windows-msvc 38b806e 2022-04-07)\n\nC:\\Users\\test>python -V\nPython 3.9.1\n\nC:\\Users\\test>python script.py\nRunning....\n\nC:\\Users\\test>vector -c test.yaml\n/bin/bash: line 1: python: command not found\" internal_log_rate_secs=1 vrl_position=76\n\nAny way i get it running in local windows machine?\nEdit - FYI,\nIt does work with below config: Is this the only way?\ncommand: [\"C:/Python39/python.exe\",\"C:/Users/test/script.py\"]",
        "url": "https://github.com/vectordotdev/vector/discussions/13342",
        "createdAt": "2022-06-27T05:38:40Z",
        "updatedAt": "2022-07-22T07:32:44Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "satscreate"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13282,
        "title": "Docker + Boto3 + Python + Vector Seems not finding boto3",
        "bodyText": "I've been trying to add some python libs to vector image as i wanted to run python scripts via vector file.\nBut getting below error,\n\"ImportError: No module named boto3\"\nDockerFile:\nFROM timberio/vector:0.22.2-debian\nRUN apt update && apt install --assume-yes \\\n    python \\\n    python3-pip\nVOLUME [/var/lib/vector/]\nRUN pip install boto3\nRUN vector --version # buildkit\nENTRYPOINT [\"/usr/bin/vector\"]\n\nBuilt command:\ndocker build -t vector/custom .\nCompose.yaml\nservices:\n  vector:\n    image: vector/custom\n    command: -c /vector.yaml\n.....\n\nvector.yaml calls a python script:\nsources:\n  exec:\n    type: exec\n    command: [\"bash\", \"-c\", \"python test.py\"]\n.....\n\ntest.py:\nimport json\nimport os\nimport io\nimport time\nimport gzip\nimport boto3\nboto3.resource(....)\n....\n\nWhen i deploy above stack i am always getting below error, any idea?\n{\"command\":[\"bash\",\"-c\",\"python /test.py\"],\"host\":\"54353535\",\"message\":\"    import boto3\",\"pid\":33,\"source_type\":\"exec\",\"stream\":\"stderr\"}\n{\"command\":[\"bash\",\"-c\",\"python /test.py\"],\"host\":\"54353535\",\"message\":\"ImportError: No module named boto3\",\"pid\":33,\"source_type\":\"exec\",\"stream\":\"stderr\"}",
        "url": "https://github.com/vectordotdev/vector/discussions/13282",
        "createdAt": "2022-06-22T10:03:24Z",
        "updatedAt": "2022-06-27T05:00:12Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "satscreate"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13316,
        "title": "Anyone know why this metric is considered invalid?",
        "bodyText": "Got this happening all of a sudden...\nWARN sink{component_kind=\"sink\" component_id=to_sfx_agent component_type=statsd component_name=to_sfx_agent}: vector::internal_events::statsd_sink: Invalid metric received; dropping event. value=AggregatedHistogram { buckets: [Bucket { upper_limit: 0.015625, count: 3 }, Bucket { upper_limit: 0.03125, count: 1 }, Bucket { upper_limit: 0.0625, count: 0 }, Bucket { upper_limit: 0.125, count: 0 }, Bucket { upper_limit: 0.25, count: 1 }, Bucket { upper_limit: 0.5, count: 0 }, Bucket { upper_limit: 1.0, count: 0 }, Bucket { upper_limit: 2.0, count: 0 }, Bucket { upper_limit: 4.0, count: 0 }, Bucket { upper_limit: 8.0, count: 0 }, Bucket { upper_limit: 16.0, count: 0 }, Bucket { upper_limit: 32.0, count: 0 }, Bucket { upper_limit: 64.0, count: 0 }, Bucket { upper_limit: 128.0, count: 0 }, Bucket { upper_limit: 256.0, count: 0 }, Bucket { upper_limit: 512.0, count: 0 }, Bucket { upper_limit: 1024.0, count: 0 }, Bucket { upper_limit: 2048.0, count: 0 }, Bucket { upper_limit: 4096.0, count: 0 }, Bucket { upper_limit: inf, count: 0 }], count: 5, sum: 0.215534437 } kind=Absolute internal_log_rate_secs=30\n\nIt's just submitting the internal vector metrics to a statsd endpoint.\nOther stats are being submitted just fine.\nI am not sure why this would be considered invalid. Any ideas?",
        "url": "https://github.com/vectordotdev/vector/discussions/13316",
        "createdAt": "2022-06-24T06:59:58Z",
        "updatedAt": "2022-06-26T23:25:11Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "cetanu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 13310,
        "title": "Clarification on group_by setting in reduce transform",
        "bodyText": "I wanted to ask about this line from the docs:\n\nEach group is combined independently, allowing you to keep independent events separate.\n\nThis is a bit confusing--does \"each group\" consist of all the fields in a single group_by, or is each field listed in the group_by section an independent group? And if the latter interpretation is correct, how would I accomplish grouping by a combination of fields? Could I combine the desired fields into an array and group by that, or would I need to concatenate them into a string?",
        "url": "https://github.com/vectordotdev/vector/discussions/13310",
        "createdAt": "2022-06-23T19:20:06Z",
        "updatedAt": "2022-06-25T15:04:12Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "jblang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13244,
        "title": "Resuming to watch file",
        "bodyText": "how to fix the problem that resuming to watch the file.   \nI hope he can scan every time regardless of whether the file is repeated or not. Which parameter can I set and how?",
        "url": "https://github.com/vectordotdev/vector/discussions/13244",
        "createdAt": "2022-06-21T03:18:52Z",
        "updatedAt": "2022-08-16T10:31:55Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "summerance"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12990,
        "title": "GCP PubSub Source Flow Control",
        "bodyText": "Hey @bruceg,\nThanks for building the gcp pubsub source. Appreciate !!!\nI have a question regarding the flow control of messages pulled in by this PubSub Source.\nI am running a GKE Vector Deployment which pulls in messages from a GCP PubSub subscription. As part of the Deployment I am running a bunch of pods each with 2.5-3 vCPUs request-limits. But I see the Vector Pods are only using 0.05-0.07 vCPU on an average however there is significant and constant backlog of messages in the GCP Subscription.\nI am trying to figure out the reason for this backlog and could come up with 1 yet to be tested hypothesis : The subscription messages are itself being pulled not fast enough causing the backlog to build in the subscription. Vector receives messages at a very low rate thereby not requiring it to vertically scale up the CPU needs to what is being requested.\nTo it, my question : is there a way to identify where the bottleneck is via vector internal metrics or some debugging magic ? Any pro-tip would be super helpful.\n@jszwedko",
        "url": "https://github.com/vectordotdev/vector/discussions/12990",
        "createdAt": "2022-06-06T09:41:00Z",
        "updatedAt": "2022-06-21T15:59:02Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "atibdialpad"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 13
        },
        "upvoteCount": 1
    },
    {
        "number": 13235,
        "title": "search_dirs override when running tests",
        "bodyText": "Hi,\nWe have a transform that uses some lua code. We are mounting this code to the vector pods using the helm chart successfully.\nHowever when we are running tests locally that involves loading lua libraries, the tests fail to find the lua libs.\nIf I additionally add the local lua config directory to the search_dirs of the the transform then the tests pass and the lua libs are loaded successfully.\nIs there a way to override/mock the search_dirs when running tests?\nKind regards,\nNas",
        "url": "https://github.com/vectordotdev/vector/discussions/13235",
        "createdAt": "2022-06-20T09:34:47Z",
        "updatedAt": "2022-06-20T23:57:02Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "NasAmin"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12387,
        "title": "How to ingest gzip log files and use remap to parse it in json format?",
        "bodyText": "I want to use Vector to ingest some gzip files then use remap to parse them. We put some log files with json format into a gzip file to do the test.\nHere is my config:\ndata_dir = \"/home/elk/log/\"\n[sources.gz_logs]\ntype = \"file\"\ninclude = [ \"/home/elk/log/*.gz\" ]\n\n[transforms.parse_logs]\ntype = \"remap\"\ninputs = [\"gz_logs\"]\nsource = '''\n. = parse_json!(.message)\n'''\n\n[sinks.print]\ntype = \"console\"\ninputs = [\"parse_logs\"]\nencoding.codec = \"json\"\n\nFinally, I got error messages:\n2022-04-24T11:08:10.581077Z ERROR transform{component_kind=\"transform\" component_id=parse_logs component_type=remap component_name=parse_logs}: vector::internal_events::remap: Mapping failed with event. error=\"function call error for \\\"parse_json\\\" at (4:25): unable to parse json: expected ident at line 1 column 2\" error_type=\"conversion_failed\" stage=\"processing\" internal_log_rate_secs=10\n2022-04-24T11:08:10.582872Z ERROR transform{component_kind=\"transform\" component_id=parse_logs component_type=remap component_name=parse_logs}: vector::internal_events::remap: Internal log [Mapping failed with event.] is being rate limited.\n\n{\"file\":\"/home/elk/log/test-requests.log.tar.gz\",\"\"message\":\"test-requests.log.3\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u00000000644\\u0000\ufffd\\u0000\\u0000\\u0000\\u0001Jh*0000144\\u000000000137445\\u000014214062002\\u0000014131\\u0000 ......\n\nCould you help me to solve it please? Thank you so much for your help.",
        "url": "https://github.com/vectordotdev/vector/discussions/12387",
        "createdAt": "2022-04-24T11:14:24Z",
        "updatedAt": "2022-06-20T02:12:06Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "syzcch"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13192,
        "title": "expected a string for key Windows OS",
        "bodyText": "Hello, this might be me just not understanding what is going on, but I'm getting this error log below. I'm just trying to see how the evtx logs would look locally on the host before moving on with pushing it to an aggregator.\nERROR vector::cli: Configuration error. error=invalid type: sequence, expected a string for key sinks.vector_windows\nConfig file is here\n#gets powershell logs\n[sources.vector_windows]\ntype = \"file\"\ninclude = [\"C:\\Windows\\System32\\winevt\\Logs\\Windows PowerShell.evtx\"]\nread_from = \"beginning\"\n#prints out the powershell logs to the path below\n[sinks.vector_windows]\ntype = \"file\"\ninputs =[\"vector_windows\"]\ncompression= \"none\"\npath = [\"C:\\Users\\Administrator\\Documents\\vector.log\"]",
        "url": "https://github.com/vectordotdev/vector/discussions/13192",
        "createdAt": "2022-06-16T17:23:35Z",
        "updatedAt": "2022-06-16T20:09:24Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "umpa385"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13136,
        "title": "use clickhouse handler but got 404 Not Found",
        "bodyText": "Hi there,\nI run vector -c vector.toml to trans nginx log into clickhouse, but got the next error:\n2022-06-14T06:51:57.391239Z ERROR vector::topology::builder: msg=\"Healthcheck: Failed Reason.\" error=Unexpected status: 404 Not Found component_kind=\"sink\" component_type=\"clickhouse\" component_id=nginx_access_log_to_databend component_name=nginx_access_log_to_databend",
        "url": "https://github.com/vectordotdev/vector/discussions/13136",
        "createdAt": "2022-06-14T06:55:01Z",
        "updatedAt": "2022-06-16T04:20:41Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "hantmac"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 13080,
        "title": "Multi-line with kubernetes source?",
        "bodyText": "Things like Go, when panicking, will print multiple lines out.\nIs there any way to add multi-line support to kubernetes? I see that it is available with the file source, but I don't see any settings within the k8s source that would allow multi-line",
        "url": "https://github.com/vectordotdev/vector/discussions/13080",
        "createdAt": "2022-06-09T23:59:36Z",
        "updatedAt": "2022-06-15T19:47:02Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "danthegoodman1"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 13053,
        "title": "S3_sink region",
        "bodyText": "Hi,\nI going to use my local Minio s3 server\nwhat endpoint should I use for my vector s3 sink region?\ncan I use \"default\" or my s3 URI for the region?",
        "url": "https://github.com/vectordotdev/vector/discussions/13053",
        "createdAt": "2022-06-09T08:16:38Z",
        "updatedAt": "2022-07-14T07:45:07Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12991,
        "title": "Acknowledgement of S3 Source when using Lua transform",
        "bodyText": "Hi,\nI have the following setup:\nAWS_S3 Source (SQS) --> Transform (Lua) --> AWS_S3 Sink\nEven though I defined acknowledgements.enabled on the Sink and the source supports it - when there is an error in the Lua code, the message is deleted from the Queue and does not reach the DLQ.\nIs this a bug? or am I mis-configuring?\nThis is a shortened version of my toml:\n[sources.my_files]\ntype = \"aws_s3\"\nregion = \"<THE REGION>\"\nstrategy = \"sqs\"\ncompression = \"gzip\"\nsqs.queue_url = \"<THE QUEUE URL>\"\n\n[transforms.lua]\ninputs = [\"my_files\"]\ntype = \"lua\"\nversion = \"2\"\nhooks.process = \"\"\"\n  function (event, emit)\n    if (event.thisMethodDoesNotExistAndShouldFail())\n    then\n      event.log.message = \"Can't belive this worked!\"\n      emit(event) -- emit the transformed event\n    end\n  end\n  \"\"\"\n\n[sinks.write_output]\ntype = \"aws_s3\"\ninputs = [ \"lua\" ]\nbucket = \"<THE OUTPUT BUCKET>\"\nkey_prefix = \"date=%F/\" \ncompression = \"gzip\"\nregion = \"<THE REGION>\"\nencoding.codec = \"text\"\nacknowledgements.enabled = true\n\nI'm using this vector release:\n\u276f vector --version\nvector 0.22.0 (x86_64-unknown-linux-gnu 5e937e3 2022-06-01)",
        "url": "https://github.com/vectordotdev/vector/discussions/12991",
        "createdAt": "2022-06-06T11:10:59Z",
        "updatedAt": "2022-06-07T09:18:14Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ronkitay"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 10620,
        "title": "Parse Grok after Multiline event capture truncates the multiline event.",
        "bodyText": "I have a Python Stacktrace that I am capturing from the log file using the 'file ' source.\nThe trace looks like:\n2021-12-28 07:21:30,793 ERROR\nTraceback (most recent call last):\n  File \"/usr/local/rackman/lib/python2.7/site-packages/flask/app.py\", line 1950, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/rackman/lib/python2.7/site-packages/flask/app.py\", line 1926, in dispatch_request\n    self.raise_routing_exception(req)                                                                                                                      \n  File \"/usr/local/rackman/lib/python2.7/site-packages/flask/app.py\", line 1908, in raise_routing_exception\n    raise request.routing_exception\nNotFound: 404 Not Found: The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.\n\nThe multi-line config looks like this. Basically says \"to group all lines that don't start with a TIMESTAMP with the last line that did in a single event\"\nmultiline.start_pattern = '.*'\nmultiline.condition_pattern = '^(?P<year>[0-9]{4})-(?P<month>[0-9]{2})-(?P<day>[0-9]{2})'\nmultiline.mode = \"halt_before\"\n\nUsing this, I am able to capture the event successfully, the event looks like this :\n{\"file\":\"/Users/atib/src/VECTOR/test/rackman.log\",\"host\":\"mdatib.local\",\"message\":\"2021-12-28 07:21:30,793 ERROR\\nTraceback (most recent call last):\\n  File \\\"/usr/local/rackman/lib/python2.7/site-packages/flask/app.py\\\", line 1950, in full_dispatch_request\\n    rv = self.dispatch_request()\\n  File \\\"/usr/local/rackman/lib/python2.7/site-packages/flask/app.py\\\", line 1926, in dispatch_request\\n    self.raise_routing_exception(req)\\n  File \\\"/usr/local/rackman/lib/python2.7/site-packages/flask/app.py\\\", line 1908, in raise_routing_exception\\n    raise request.routing_exception\\nNotFound: 404 Not Found: The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.\",\"service\":{\"name\":\"rackman\"},\"source_type\":\"file\",\"timestamp\":\"2021-12-29T04:07:36.814126Z\"}\n\nHowever when I pass it through another GROK layer (to extract to loglevel, date, exception type, etc..) I loose the date after the newline.\npattern = s'%{TIMESTAMP_ISO8601:timestamp}%{SPACE}%{LOGLEVEL:loglevel}%{SPACE}%{GREEDYDATA:message}'\nmatch, err = parse_grok(.message, pattern)\n\npost which my event looks like\n{\"file\":\"/Users/atib/src/VECTOR/test/rackman.log\",\"host\":\"mdatib.local\",\"loglevel\":\"ERROR\",\"message\":\"Traceback (most recent call last):\",\"service\":{\"name\":\"rackman\"},\"source_type\":\"file\",\"timestamp\":\"2021-12-28 07:21:30,793\"}\n\nAs you can see, the message filed is reduced to only the first line of the multi-line event. I guess this is something to do with escaping \\n  in the msg field but since the msg is generated internally in the pipeline, Vector should handle this ??\n@jszwedko",
        "url": "https://github.com/vectordotdev/vector/discussions/10620",
        "createdAt": "2021-12-29T06:03:50Z",
        "updatedAt": "2022-06-06T15:52:07Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "atibdialpad"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 12988,
        "title": "would be nice for \"vector top\" to allow selecting specific components to display",
        "bodyText": "When there are lots of components, they may not fit in a terminal. For that reason, I wish there was a way to limit what gets displayed.",
        "url": "https://github.com/vectordotdev/vector/discussions/12988",
        "createdAt": "2022-06-06T04:45:22Z",
        "updatedAt": "2022-06-06T14:46:04Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tshepang"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 12969,
        "title": "How to filter metrics by type",
        "bodyText": "In my metrics environment, histograms and counters/gauges need to be handled differently. I'm trying to set up a filter to Separate out the histograms, but I can't seem to figure out how to match the type.\nI sent the internal metrics to a stdout sink as json, and got this event.\n{\n  \"name\": \"internal_metrics_cardinality_total\",\n  \"namespace\": \"vector-agent\",\n  \"tags\":\n    {\n      \"az\": \"local\",\n      \"clusterGroupName\": \"local\",\n      \"clusterName\": \"local\",\n      \"project_id\": \"undefined\",\n      \"region\": \"local\",\n      \"serviceName\": \"service-agent\",\n    },\n  \"timestamp\": \"2022-06-03T18:19:35.076353386Z\",\n  \"kind\": \"absolute\",\n  \"counter\": { \"value\": 263.0 },\n}\n\nI added this transform to filter out histogram metrics,\n  \"agent_metrics_filter\":\n    \"condition\": \"exists(.counter) || exists(.gauge)\"\n    \"inputs\":\n      - \"agent_metrics\"\n    \"type\": \"filter\"\n\nBut I get no output from it. According to the schema, there should be a .counter or .gauge field on metrics events. Am I approaching this wrong?",
        "url": "https://github.com/vectordotdev/vector/discussions/12969",
        "createdAt": "2022-06-03T18:28:41Z",
        "updatedAt": "2022-06-03T19:20:08Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Quyzi"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12943,
        "title": "[geo][location] (Geo Point Field) how ?",
        "bodyText": "Hi there,\nhow do i get created with GeoIP transforms location (Geo Point Field) extracted / generate.\nlocation* | [geo][location] | {\"lat\": 47.6062, \"lon\": -122.3321}\"\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/geo-point.html\nexample:\n    \"GeoLocation\": {\n      \"country_name\": \"Germany\",\n      \"location\": {\n        \"lon\": 9.491,\n        \"lat\": 51.2993\n      }\n    },",
        "url": "https://github.com/vectordotdev/vector/discussions/12943",
        "createdAt": "2022-06-02T12:52:23Z",
        "updatedAt": "2022-06-03T13:58:23Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "StefanSa"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 12883,
        "title": "sink elasticsearch,  contains an unknown parameter [_type] status 400",
        "bodyText": "Hi there,\nafter upgrading to last vector version, i get this error message.\n2022-05-27T12:47:21.007441Z ERROR sink{component_kind=\"sink\" component_id=out_telegraf component_type=elasticsearch component_name=out_telegraf}:request{request_id=8}: vector::sinks::util::retries: Not retriable; dropping the request. reason=\"client-side error, 400 Bad Request: {\\\"error\\\":{\\\"root_cause\\\":[{\\\"type\\\":\\\"illegal_argument_exception\\\",\\\"reason\\\":\\\"Action/metadata line [1] contains an unknown parameter [_type]\\\"}],\\\"type\\\":\\\"illegal_argument_exception\\\",\\\"reason\\\":\\\"Action/metadata line [1] contains an unknown parameter [_type]\\\"},\\\"status\\\":400}\"\n\nbakend is opensearch v2, but with opensearch v1.3, we did not see this error.\n{\n  \"name\" : \"os-client02\",\n  \"cluster_name\" : \"development-cluster\",\n  \"cluster_uuid\" : \"i_yvShiNS1ScuLlB9uWVgg\",\n  \"version\" : {\n    \"distribution\" : \"opensearch\",\n    \"number\" : \"2.0.0\",\n    \"build_type\" : \"tar\",\n    \"build_hash\" : \"bae3b4e4178c20ac24fece8e82099abe3b2630d0\",\n    \"build_date\" : \"2022-05-19T00:26:04.115016552Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"9.1.0\",\n    \"minimum_wire_compatibility_version\" : \"7.10.0\",\n    \"minimum_index_compatibility_version\" : \"7.0.0\"\n  },\n  \"tagline\" : \"The OpenSearch Project: https://opensearch.org/\"\n}",
        "url": "https://github.com/vectordotdev/vector/discussions/12883",
        "createdAt": "2022-05-27T12:55:38Z",
        "updatedAt": "2022-05-31T12:36:47Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "StefanSa"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 12884,
        "title": "Problems with wildcards in config path",
        "bodyText": "Hi there,\nsince the upgrade to the last version of vector, i see this wildcards problem.\nvector -c /etc/vector/vector.toml -c /etc/vector/modules/sophos-xg/*.toml\nerror: Found argument '/etc/vector/modules/sophos-xg/sophos-xg-aspam.toml' which wasn't expected, or isn't valid in this context\n\nvector -c /etc/vector/vector.toml -c '/etc/vector/modules/sophos-xg/*.toml'\n2022-05-27T13:14:43.445709Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=trace,rdkafka=info,buffers=info,kube=info\"\n2022-05-27T13:14:43.446028Z  INFO vector::app: Loading configs. paths=[\"/etc/vector/modules/sophos-xg/all.toml\", \"/etc/vector/modules/sophos-xg/sophos-xg-aspam.toml\", \"/etc/vector/modules/sophos-xg/sophos-xg-atp.toml\", \"/etc/vector/modules/sophos-xg/sophos-xg-av.toml\", \"/etc/vector/modules/sophos-xg/sophos-xg-cfilter.toml\", \"/etc/vector/modules/sophos-xg/sophos-xg-event.toml\", \"/etc/vector/modules/sophos-xg/sophos-xg-firewall.toml\", \"/etc/vector/modules/sophos-xg/sophos-xg-idp.toml\", \"/etc/vector/modules/sophos-xg/sophos-xg-pipeline.toml\", \"/etc/vector/modules/sophos-xg/sophos-xg-sandbox.toml\", \"/etc/vector/modules/sophos-xg/sophos-xg-sink.toml\", \"/etc/vector/modules/sophos-xg/sophos-xg-source.toml\", \"/etc/vector/modules/sophos-xg/sophos-xg-waf.toml\", \"/etc/vector/vector.toml\"]\n2022-05-27T13:14:43.578646Z  WARN vector::config::loading: Transform \"route_ip_waf._unmatched\" has no consumers\n2022-05-27T13:14:43.578683Z  WARN vector::config::loading: Transform \"route_sophos_xg._unmatched\" has no consumers\n2022-05-27T13:14:43.578700Z  WARN vector::config::loading: Transform \"route_ip_idp._unmatched\" has no consumers\n2022-05-27T13:14:43.578711Z  WARN vector::config::loading: Transform \"route_ip_fw._unmatched\" has no consumers\n2022-05-27T13:14:43.578721Z  WARN vector::config::loading: Transform \"route_ip_event._unmatched\" has no consumers\n2022-05-27T13:14:43.578732Z  WARN vector::config::loading: Transform \"route_ip_cfl._unmatched\" has no consumers\n2022-05-27T13:14:43.578745Z  WARN vector::config::loading: Transform \"route_ip_av._unmatched\" has no consumers\n2022-05-27T13:14:43.578757Z  WARN vector::config::loading: Transform \"route_ip_atp._unmatched\" has no consumers\n2022-05-27T13:14:43.578768Z  WARN vector::config::loading: Transform \"route_ip_aspam._unmatched\" has no consumers\n2022-05-27T13:14:44.737208Z  WARN vector::tls::settings: The `verify_hostname` option is DISABLED, this may lead to security vulnerabilities.\n2022-05-27T13:14:44.761214Z  WARN vector::tls::settings: The `verify_hostname` option is DISABLED, this may lead to security vulnerabilities.\n2022-05-27T13:14:44.811173Z ERROR vector::topology: Configuration error. error=Transform \"ecs_mapping\":\n\nWe had not seen this behavior in the previous version.",
        "url": "https://github.com/vectordotdev/vector/discussions/12884",
        "createdAt": "2022-05-27T13:20:37Z",
        "updatedAt": "2022-06-01T17:21:15Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "StefanSa"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 4
        },
        "upvoteCount": 1
    },
    {
        "number": 12869,
        "title": "Question on buffer.max_size when using \"vector\" as sink",
        "bodyText": "Hello,\nI needed to understand how the buffering works. Correct me if my understanding is wrong, but I assumed that when buffering is turned on using buffer.max_size, the events will be written to the location pointed out by data_dir and once the batch age meets batch.timeout_secs, it will be flushed out. When this is done, the events will no longer be available in the local buffer. Is this correct?\nThis is the configuration that I have for the sink\n[sinks.forward]\ntype = \"vector\"\ninputs = [\"validator\"]\naddress = \"is1:50335\"\nbuffer.type = \"disk\"\nbuffer.max_size = 50000\n#batch.max_bytes = 50000\nbuffer.when_full = \"block\"\nbatch.timeout_secs = 10\nversion = \"2\"\n\nWhen an event is generated, I notice that the file size in the forward_id directory increases but once the timeout is met, the size does not decrease\n[root@is1 forward_id]# ls -lrth\ntotal 20K\n-rw-r--r-- 1 root root    0 May 26 13:02 LOCK\n-rw-r--r-- 1 root root   50 May 26 13:02 MANIFEST-000004\n-rw-r--r-- 1 root root   60 May 26 13:02 LOG.old\n-rw-r--r-- 1 root root   16 May 26 13:02 CURRENT\n-rw-r--r-- 1 root root  181 May 26 13:02 LOG\n-rw-r--r-- 1 root root 1.2K May 26 13:03 000005.log\n[root@is1 forward_id]# date\nThu May 26 13:14:55 UTC 2022\n(reverse-i-search)`dat': ^Cte\n[root@is1 forward_id]# curl -XPOST -d '{\"severity_text\": \"Error\",\"severity_number\": \"5\",\"name\": \"NetError\",\"body\": \"ABC\",\"flags\":\"A\",\"trace_id\": \"xxxnd-548\",\"span_id\": \"\",\"resource\": { \"attributes\":{\"os.type\":\"linux\",\"os.name\":\"fedora\",\"device.type\":\"372\",\"device.sub_type\":\"001\"}}}' '127.0.0.1:50332/ot/log'\n[root@is1 forward_id]# ls -lrth\ntotal 20K\n-rw-r--r-- 1 root root    0 May 26 13:02 LOCK\n-rw-r--r-- 1 root root   50 May 26 13:02 MANIFEST-000004\n-rw-r--r-- 1 root root   60 May 26 13:02 LOG.old\n-rw-r--r-- 1 root root   16 May 26 13:02 CURRENT\n-rw-r--r-- 1 root root  181 May 26 13:02 LOG\n-rw-r--r-- 1 root root 1.5K May 26 13:15 000005.log\n[root@is1 forward_id]# date\nThu May 26 13:15:16 UTC 2022 <<==At this point the event should have been moved out? ==\n[root@is1 forward_id]# ls -lrth\ntotal 20K\n-rw-r--r-- 1 root root    0 May 26 13:02 LOCK\n-rw-r--r-- 1 root root   50 May 26 13:02 MANIFEST-000004\n-rw-r--r-- 1 root root   60 May 26 13:02 LOG.old\n-rw-r--r-- 1 root root   16 May 26 13:02 CURRENT\n-rw-r--r-- 1 root root  181 May 26 13:02 LOG\n-rw-r--r-- 1 root root 1.5K May 26 13:15 000005.log\n[root@is1 forward_id]#\n\nI would like to ideally want to be using batch.max_size (with batch.max_size = buffer.max_size), so that we can send all the events together and then clear out the buffer\nI understand that the event size in buffer might not be same as the one used in batch due to serialization. Is there a known overhead that we could possibly account for? say buffer.max_size  = batch.max_size + X",
        "url": "https://github.com/vectordotdev/vector/discussions/12869",
        "createdAt": "2022-05-26T13:18:29Z",
        "updatedAt": "2022-06-01T17:21:28Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "bmallya"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12751,
        "title": "cargo cross compile on v21.2 gives \"can't find crate for `pest`\" error",
        "bodyText": "Hello,\nI tried to cross compile vector v21.2 with target as armv7-unknown-linux-gnueabihf, but ended up getting the error on pest. Has anyone come across this?\n`make cross-build-armv7-unknown-linux-gnueabihf\nfatal: not a git repository (or any of the parent directories): .git\nfatal: not a git repository (or any of the parent directories): .git\nfatal: not a git repository (or any of the parent directories): .git\nfatal: not a git repository (or any of the parent directories): .git\nfatal: not a git repository (or any of the parent directories): .git\nfatal: not a git repository (or any of the parent directories): .git\nfatal: not a git repository (or any of the parent directories): .git\nfatal: not a git repository (or any of the parent directories): .git\nfatal: not a git repository (or any of the parent directories): .git\nfatal: not a git repository (or any of the parent directories): .git\nmake -k cross-image-armv7-unknown-linux-gnueabihf\nfatal: not a git repository (or any of the parent directories): .git\nmake[1]: Entering directory '/home/bharath/vector/vector'\npodman build \n--tag vector-cross-env:armv7-unknown-linux-gnueabihf \n--file scripts/cross/armv7-unknown-linux-gnueabihf.dockerfile \nscripts/cross\nSTEP 1/3: FROM rustembedded/cross:armv7-unknown-linux-gnueabihf\nSTEP 2/3: COPY bootstrap-ubuntu.sh .\n--> Using cache 458a3ed0509f1d8d49a9da8dafe6035128b53bcd9bb1ed51fda969cbfdbe825f\n--> 458a3ed0509\nSTEP 3/3: RUN ./bootstrap-ubuntu.sh\n--> Using cache df9a5bd98d213653ba8b99be665b5fa9f84a1d148e0d7a917cb747ac2c3c649d\nCOMMIT vector-cross-env:armv7-unknown-linux-gnueabihf\n--> df9a5bd98d2\nSuccessfully tagged localhost/vector-cross-env:armv7-unknown-linux-gnueabihf\ndf9a5bd98d213653ba8b99be665b5fa9f84a1d148e0d7a917cb747ac2c3c649d\nmake[1]: Leaving directory '/home/bharath/vector/vector'\ncross build \n--release \n--target armv7-unknown-linux-gnueabihf \n--no-default-features \n--features target-armv7-unknown-linux-gnueabihf\ninfo: syncing channel updates for '1.58.1-x86_64-unknown-linux-gnu'\n1.58.1-x86_64-unknown-linux-gnu unchanged - rustc 1.58.1 (db9d1b20b 2022-01-20)\ninfo: checking for self-updates\nCompiling pest_meta v2.1.3\nerror[E0463]: can't find crate for pest\n--> /cargo/registry/src/github.com-1ecc6299db9ec823/pest_meta-2.1.3/src/lib.rs:17:1\n|\n17 | extern crate pest;\n| ^^^^^^^^^^^^^^^^^^ can't find crate\nFor more information about this error, try rustc --explain E0463.\nerror: could not compile pest_meta due to previous error\nmake: *** [Makefile:243: cross-build-armv7-unknown-linux-gnueabihf] Error 101\n`",
        "url": "https://github.com/vectordotdev/vector/discussions/12751",
        "createdAt": "2022-05-17T14:27:14Z",
        "updatedAt": "2022-09-19T20:05:13Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bmallya"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12746,
        "title": "buffer type configuration issue",
        "bodyText": "Hello Team,\nWe have configured the vector sink instance to buffer on the disk and set the buffer.max_size value and also the batch.max_bytes. For some reason we are facing the following error on the arm platform of vector and not on the x86_64 platform :\n\n** error=unknown variant disk, expected memory**\nthe version of the vector arm variant we are using is 18.0. Can you help us understand the issue!",
        "url": "https://github.com/vectordotdev/vector/discussions/12746",
        "createdAt": "2022-05-17T08:35:37Z",
        "updatedAt": "2022-06-16T17:53:21Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "hemanthofficial3009"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12815,
        "title": "Can't find some metrics from internal_metrics",
        "bodyText": "Hi,\nI can't able to find some http metrics like http_requests_total or http_request_errors_total",
        "url": "https://github.com/vectordotdev/vector/discussions/12815",
        "createdAt": "2022-05-22T16:27:30Z",
        "updatedAt": "2022-05-23T14:35:38Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12759,
        "title": "Need help on configuring AWS Elastic Beanstalk logs and cfn init logs",
        "bodyText": "Hi,\nCan someone help me on configuring AWS Elastic Beanstalk logs and Cloud init logs.\nSample log:\neb-engine.log:\n2022/02/25 05:44:25.151010 [INFO] Executing instruction: SyncClock\n2022/02/25 05:44:25.151015 [INFO] Starting SyncClock\n2022/02/25 05:44:25.151025 [INFO] Running command /bin/sh -c /usr/bin/chronyc tracking\n2022/02/25 05:44:25.153016 [INFO] Reference ID    : 7F7F0101 ()\nStratum         : 10\nRef time (UTC)  : Fri Feb 25 05:44:24 2022\nSystem time     : 0.000000000 seconds fast of NTP time\nLast offset     : +0.000000000 seconds\nRMS offset      : 0.000000000 seconds\nFrequency       : 0.000 ppm slow\nResidual freq   : +0.000 ppm\nSkew            : 0.000 ppm\nRoot delay      : 0.000000000 seconds\nRoot dispersion : 0.000000000 seconds\nUpdate interval : 0.0 seconds\nLeap status     : Normal\n2022/02/25 05:44:34.918864 [INFO] Running command /bin/sh -c /usr/bin/yum update -y\n2022/02/25 05:44:45.144027 [INFO] Loaded plugins: extras_suggestions, langpacks, priorities, update-motd\nResolving Dependencies\n--> Running transaction check\n---> Package vector.x86_64 0:0.19.0-1 will be updated\n---> Package vector.x86_64 0:0.20.0-1 will be an update\n--> Finished Dependency Resolution\nDependencies Resolved\n================================================================================\nPackage         Arch            Version           Repository              Size\nUpdating:\nvector          x86_64          0.20.0-1          timber-vector           45 M\nTransaction Summary\nUpgrade  1 Package\ncfn-init.log:\n2022-02-25 05:46:29,331 [INFO] -----------------------Starting build-----------------------\n2022-02-25 05:46:29,337 [INFO] Running configSets: Infra-EmbeddedPreBuild\n2022-02-25 05:46:29,339 [INFO] Running configSet Infra-EmbeddedPreBuild\n2022-02-25 05:46:29,342 [INFO] Running config prebuild_0_mo\n2022-02-25 05:46:43,363 [INFO] Yum installed ['amazon-efs-utils']\n2022-02-25 05:46:43,365 [INFO] ConfigSets completed\n2022-02-25 05:46:43,365 [INFO] -----------------------Build complete-----------------------\nI have tried remap with parse_regex function but no luck with that.\nThanks in advance.\nThanks,\nNaresh Y",
        "url": "https://github.com/vectordotdev/vector/discussions/12759",
        "createdAt": "2022-05-18T00:32:16Z",
        "updatedAt": "2022-06-15T16:49:12Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "NareshYakkati"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12773,
        "title": "Telegraf -> Kafka -> Vector -> Prometheus",
        "bodyText": "I'm trying to achieve the flow from telegraf -> kafka -> vector -> prometheus\nIn telegraf we have configured kafka output plugin and sending data in json format.\n{\n    \"fields\": {\n        \"field_1\": 30,\n        \"field_2\": 4,\n        \"field_N\": 59,\n        \"n_images\": 660\n    },\n    \"name\": \"docker\",\n    \"tags\": {\n        \"host\": \"raynor\"\n    },\n    \"timestamp\": 1458229140\n}\n\nNow in vector side. We have configured kafka source and able to read it in vector as verified by using console sink.\nHowever my goal is to remote_write the kafka logs (which are essentially metrics) into prometheus using remote_write sink.\nI read through documentation and there isn't a parser available for it. Is there anyone who has tried this scenario?",
        "url": "https://github.com/vectordotdev/vector/discussions/12773",
        "createdAt": "2022-05-18T19:07:57Z",
        "updatedAt": "2022-05-31T14:41:34Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "sushovan23"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12714,
        "title": "Two and more sources",
        "bodyText": "Hi,\nCould you advice, how to configure 'vector' to process several sources?\nI have two (or maybe three) files on a disk. There are csv and json. They are quite large.\nAnd I'd like to get data from them, mix data got from all and sink to database.\nIt's quite easy to do it in 'awk' or 'python' but I'd like to have more performance and 'vector' looks very very exciting. But I can't understand how to configure several inputs .. :(",
        "url": "https://github.com/vectordotdev/vector/discussions/12714",
        "createdAt": "2022-05-12T17:33:01Z",
        "updatedAt": "2022-06-10T14:29:12Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "kirill-ratkin"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12762,
        "title": "Any contribution tutorial?",
        "bodyText": "I want to extend vector to support source like pulsar and sinks that suitable for our infrustructures. But I don't know how to get started.\nI found devloping docs, but that just tell me how to setup dev env, and the arch doc is just a very simple description of how components are connected. And nothing more!\nIt'd be better if there's a documentation that defines what a (source/sink) plugin is (the interface definition in java or go, or trait in rust), if people impl that trait how to tell the core to identify it, and this can help people make sense.\nfurther more, the doc may contain an simple example of how to develop your own sources\\sinks (maybe a simple http source with very few configs and ignore some complicated err handlings)\n* where to add the code\n* where to define the config strcuture\n* how to setup a server listen on configured port\n* how to set decoder to decode data received from that connection,\n* how to send data to the next pipeline",
        "url": "https://github.com/vectordotdev/vector/discussions/12762",
        "createdAt": "2022-05-18T07:21:36Z",
        "updatedAt": "2022-06-03T04:35:29Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "caibirdme"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 9334,
        "title": "The difference between logs and metric events",
        "bodyText": "Something I'm confused about, which relates to issue #9306.\nI am led to understand that logs and metric events are treated differently.  I see examples with {\"log\": {...}} and {\"metric\": {...}} [^1].  The data model documentation makes this explicit:\n\nA metric event in Vector represents a numerical operation performed on a time series. In Vector, unlike in other tools, metrics are first-class citizens. They are not represented as logs.\n\nHowever, by the time I get to a sink - let's say the console sink for simplicity - the \"log\" or \"metric\" wrapper has been removed.  All I see is a JSON object:\n{\"name\":\"process_cpu_seconds_total\",\"timestamp\":\"2021-09-24T08:26:55.450211329Z\",\"kind\":\"absolute\",\"counter\":{\"value\":4853.94}}\n\n{\"appname\":\"non\",\"exampleSDID@32473\":{\"eventID\":\"1011\",\"eventSource\":\"Application\",\"iut\":\"3\"},\"facility\":\"user\",\"host\":\"dynamicwireless.name\",\"hostname\":\"dynamicwireless.name\",\"message\":\"Try to override the THX port, maybe it will reboot the neural interface!\",\"msgid\":\"ID931\",\"procid\":2426,\"severity\":\"notice\",\"source_ip\":\"127.0.0.1\",\"source_type\":\"syslog\",\"version\":1}\n\nEven in an intermediate transform I don't see the outer wrapper.  For example, I would write\n  . = parse_json!(.message)\n\nand not\n  . = parse_json!(.log.message)\n\nSo my question is: how does the distinction between logs and metric events make a difference in practice to the end user?  Is it possible to distinguish between them in a transform or sink?  If not, what purpose does the separation serve?\nEDIT: if I receive data from a general-purpose source - kafka say - what determines if this is a stream of logs or metrics?\n\n[^1] #9306 says these are from \"the cue data\".  In the repo I found website/cue/reference.cue although I can't see where it's linked in the website.",
        "url": "https://github.com/vectordotdev/vector/discussions/9334",
        "createdAt": "2021-09-24T08:55:20Z",
        "updatedAt": "2022-06-10T14:30:20Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "candlerb"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12734,
        "title": "ServiceAccount token expiration",
        "bodyText": "After the upgrade of the kubernetes cluster to 1.21 we've got a notification from AWS that some of our applications do not refresh the service account tokens. It's related to BoundServiceAccountTokenVolume feature that is enabled by default starting from 1.21.\nIt looks like that there is a need to upgrade the client sdk. Or maybe there is another workaround that you can suggest.",
        "url": "https://github.com/vectordotdev/vector/discussions/12734",
        "createdAt": "2022-05-16T13:34:56Z",
        "updatedAt": "2022-08-10T18:09:30Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "akunafin"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12720,
        "title": "docker_source logs not going to elasticsearch",
        "bodyText": "Hi\nI have 2 vector instances - first one is collecting docker logs and sends them to another vector, which then modifies them and pushes to ES.\nFirst vector config looks like this:\n[sources.docker_logs]\ntype = \"docker_logs\"\ndocker_host = \"unix:///var/run/docker.sock\"\nexclude_containers = [ \"node-exporter\" ]\ninclude_containers = [ \"node\", \"jibri\" ]\n\n[transforms.docker_transform]\ntype = \"remap\"\ninputs = [ \"docker_logs\" ]\nsource = \"\"\"\n.host = \"example_hostname\"\n\"\"\"\n\n[sinks.vector_server]\ntype = \"vector\"\ninputs = [ \"docker_transform\" ]\naddress = \"vector-server.example.com:6000\"\nversion = \"2\"\ntls.ca_file    = \"/etc/vector/ca.pem\"\ntls.crt_file   = \"/etc/vector/client-cert.pem\"\ntls.enabled    = true\ntls.key_file   = \"/etc/vector/client-key.pem\"\ntls.verify_certificate = true\n\n\nAnd second vector config looks like this (it collects janus logs from another vector instance as well) :\n[sources.janus_logs]\ntype           = \"vector\"\naddress        = \"0.0.0.0:6000\"\ntls.ca_file    = \"/etc/vector/ca.pem\"\ntls.crt_file   = \"/etc/vector/server-cert.pem\"\ntls.enabled    = true\ntls.key_file   = \"/etc/vector/server-key.pem\"\ntls.verify_certificate = true\n\n[transforms.janus_transform]\ntype = \"remap\"\ninputs = [ \"janus_logs\" ]\nsource = \"\"\"\n.@timestamp = del(.timestamp)\"\"\"\n\n[sinks.es_cluster]\ninputs         = [\"janus_transform\"]\ntype           = \"elasticsearch\"\nbulk.index     = \"vector-{{host}}-%Y-%m-%d\"\nendpoint       = \"http://elasticsearch:9200\"\nauth.user      = \"user\"\nauth.password  = \"password\"\nauth.strategy  = \"basic\"\n\nProblem is that nothing appears in ES index. I see it gets created, but it's empty:\ngreen open vector-example_hostname-2022-05-13 xulUKCDARGqgkjVL-Jo_zg 1 0      0      0    226b    226b\n\nI checked with netcat on second vector - it receives messages from first vector with docker logs and it looks like this:\nsource_type\ufffd\n\ufffddocker\n\ufffd\n\ufffdstream\ufffd\n\ufffdstdout\n\ufffd\n\ttimestamp\ufffd\ufffd\n                  \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd^C\n\nSo, what am I missing here? Maybe docker logs should be normalized some way? Because at first I had single index for both janus logs and vector logs and ES throw error that can't merge a non object mapping [label.com.docker.compose.project] with an object mapping. So I created separate index for docker logs based on host, but it seems that it didn't help/there is another reason why ES can't receive logs from vector.\nThanks in advance for any help.",
        "url": "https://github.com/vectordotdev/vector/discussions/12720",
        "createdAt": "2022-05-13T13:05:27Z",
        "updatedAt": "2022-06-30T12:03:48Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "agorkiy"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12724,
        "title": "Batching and Buffering - inter dependencies",
        "bodyText": "hello team,\nWe have implemented the buffering and batching concepts on the vector sink instance.\nour sample configuration:\n[sinks.forward]\ntype = \"vector\"\ninputs = [\"validator\"]\naddress = \"ABC:1234\"\nbuffer.type = \"disk\"\nbuffer.max_size = 50000\nversion = \"2\"\nbatch.max_bytes = 50000\nbatch.timeout_secs = 60\nin this case we have tried to test by exceeding the batch.max_bytes before the batch.timeout_secs. we expected the sink to flush the data to the downstream vector source instance which did not happen.\ncan you please elaborate on these parameters for buffering and batching\n\nhow are related internally?\nhow do they behave when the max limits are met each of them?\ndata type of buffer.max_size is it in bytes ?\nWhen does the vector sink instance flush the data to downstream ?",
        "url": "https://github.com/vectordotdev/vector/discussions/12724",
        "createdAt": "2022-05-13T17:17:18Z",
        "updatedAt": "2022-08-24T06:43:41Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "hemanthofficial3009"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10558,
        "title": "Vector Sink - Batch & Buffer",
        "bodyText": "Hello,\nWe are using vector as an agent and collector. In the agent the sink is vector and the source on the collector is vector.\nAs part of our use case, we would like to have the buffering and batching of log events. It would be really helpful if someone could explain in details few things related to Batching and Buffering of the log event provided by Vector.\nBuffer:\nRef: Buffer\n\nis buffer.max_size /bytes confined to each sink or does it apply for all the sinks we have in configuration?\n\nBatch\nRef: Batch\nWhat would be ceiling point for the-\n\nmaximum size per batch?\nmaximum batch.timeout_secs ?\n\nCan we use Buffer and Batching together in the same sink? If yes please refer to the following config file :\n[sinks.forward]\ntype = \"vector\"\ninputs = [\"validator\" ]\naddress = \"xyz\"\nbatch.timeout_secs=10\nbuffer.type=\"disk\"\nbuffer.max_size=104900000\nbuffer.when_full=\"drop_newest\"\nWhen tried to implement, we are facing the following error :\n\nAny references to topics related to batching and buffering for vector sinks would be of great help.\nThanks alot,\nHemanth.",
        "url": "https://github.com/vectordotdev/vector/discussions/10558",
        "createdAt": "2021-12-22T18:02:46Z",
        "updatedAt": "2022-06-10T14:30:57Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "hemanthofficial3009"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12386,
        "title": "Does `kubernetes_logs` cause tokio task leaks after config reloading?",
        "bodyText": "I found current codes of source kubernetes_logs spawn tokio task directly but not respect the shutdown signal:\n(https://github.com/vectordotdev/vector/blob/master/src/sources/kubernetes_logs/mod.rs#L310-L331)\n        let pods = Api::<Pod>::all(client.clone());\n        let pod_watcher = watcher(\n            pods,\n            ListParams {\n                field_selector: Some(field_selector),\n                label_selector: Some(label_selector),\n                ..Default::default()\n            },\n        );\n        let pod_store_w = reflector::store::Writer::default();\n        let pod_state = pod_store_w.as_reader();\n\n        tokio::spawn(custom_reflector(pod_store_w, pod_watcher, delay_deletion));\n\n        // -----------------------------------------------------------------\n\n        let namespaces = Api::<Namespace>::all(client);\n        let ns_watcher = watcher(namespaces, ListParams::default());\n        let ns_store_w = reflector::store::Writer::default();\n        let ns_state = ns_store_w.as_reader();\n\n        tokio::spawn(custom_reflector(ns_store_w, ns_watcher, delay_deletion));\nAnd inside custom_reflector is an infinite loop, I wonder if it will cause tokio task leaks after kubernetes_logs config modified & reloads in-flight? - Source will destruct and rebuild, but previous reflector task doesn't respect the shutdown signal.",
        "url": "https://github.com/vectordotdev/vector/discussions/12386",
        "createdAt": "2022-04-24T10:05:55Z",
        "updatedAt": "2022-06-02T11:06:44Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "xdatcloud"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12537,
        "title": "what is the best way to encrypt sensitive data?",
        "bodyText": "Hi Guys!\nAssume that there is a field with some sensitive data in your log message and using redact is not an option because you need to read it later.\nWhat is the best way to encrypt data with Vector? Are there any solutions out of the box?\nThank you",
        "url": "https://github.com/vectordotdev/vector/discussions/12537",
        "createdAt": "2022-05-03T00:07:22Z",
        "updatedAt": "2022-06-15T17:24:50Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ruslandanilin"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12521,
        "title": "Vector Nightly (2022-05) version LUA is not calling libraries",
        "bodyText": "Dear Community\nI tested the Nightly version (2022-05) related to encrypt function and it is working fine.\nI added two \"transforms\" with \"type = \"remap\"\". and I have the following warning. I don't have more information about the warning.\n2022-05-02T16:37:11.831091Z  WARN vector::transforms::remap: VRL compilation warning. warnings=\n2022-05-02T16:37:11.831151Z  WARN vector::transforms::remap: VRL compilation warning. warnings=\n\nHowever, when I tried to call LUA Libraries like httpclient, the result is:\n2022-05-02T16:38:11.442467Z ERROR transform{component_kind=\"transform\" component_id=t_lua component_type=lua\n component_name=t_lua}: vector::internal_events::lua: Error in lua script; discarding event. error=RuntimeErrorHooksProcess \n{ source: RuntimeError(\"[string \\\"src/transforms/lua/v2/mod.rs:162:17\\\"]:17: attempt to index a nil value (global 'http_request')\n\\nstack traceback:\\n\\t[C]: in metamethod 'index'\\n\\t[string \\\"src/transforms/lua/v2/mod.rs:162:17\\\"]:17: in function 'process'\") }\n error_type=\"script_failed\" error_code=\"runtime_error_hook_process\" stage=\"processing\" internal_log_rate_secs=30\n\n, but it is not the same result on the Vector 0.21.1.\nIn order to reproduce the scenario, you can install the following packages:\nBASIC CONFIGURATION OF THE VM\n\nInstall CentOs7 (CentOS-7-x86_64-Minimal-1810)\nInstall the following libraries in CentOs7\n\n$ yum install -y epel-release\n$ yum check-update\n$ yum search epel\n$ yum install -y epel-rpm-macros\n$ yum check-update\n$ yum install -y git gcc g++ make readline-devel unzip\n$ yum install -y wget cmake yum-utils vim\n$ yum install -y pcre2 pcre2-devel\n\n\nInstall LUA 5.4, as part of the OS.\n\n$ curl -R -O http://www.lua.org/ftp/lua-5.4.3.tar.gz\n$ tar -zxf lua-5.4.3.tar.gz\n$ cd lua-5.4.3\n$ make linux test\n$ sudo make install\n$ cd /usr/bin/\n$ mv lua lua5.1\n$ ln -s /usr/local/bin/lua\n\n\nInstall LUAROCK and configure it to user version 5.4\n\n$ wget https://luarocks.org/releases/luarocks-3.8.0.tar.gz --no-check-certificate\n$ tar zxpf luarocks-3.8.0.tar.gz\n$ cd luarocks-3.8.0\n$ ./configure --lua-version=5.4\n$ make\n$ make install\n$ cd /usr/share/lua\n$ ln -s /usr/local/share/lua/5.4/\n\n\nInstall LUA libraries, and fix it.\n\n$ luarocks install lunajson\n$ luarocks install luasocket\n$ luarocks install inspect\n$ luarocks install httpclient\n$ sed -i -e 's,receive(),receive(\"*l\"),g' /usr/local/share/lua/5.4/socket/tp.lua\n$ sed -i -e 's,receive(),receive(\"*l\"),g' /usr/local/share/lua/5.4/socket/http.lua\n\nINSTALL AND TEST ON VECTOR 0.21.1\n\nInstall Vector 0.21.1\n\n$ curl -1sLf 'https://repositories.timber.io/public/vector/cfg/setup/bash.rpm.sh' | sudo -E bash\n$ sudo yum install vector\n\n\nTest the following configuration\n\n[sources.input_file]\ntype = \"file\"\ninclude = [\"/root/log\"]\nread_from = \"end\"\n\n[transforms.t_json]\ntype = \"remap\"\ninputs = [ \"input_file\" ]\nsource = \"\"\"\n. = parse_json!(.message)\n\"\"\"\n\n[transforms.t_lua]\ntype = \"lua\"\ninputs = [ \"t_json\" ]\nversion = \"2\"\nsource = '''\n-- LIBRARIES\nljson = require(\"lunajson\")\nkey = \"1a2b3c4d5e6f7g8h\"\niv  = \"1234ABCD1234ABCD\"\n\nfunction process(event, emit)\n        -- HTTP Request\n        json = require(\"lunajson\")\n        http_request = require(\"httpclient\").new()\n        MY_URL='http://www.google.com'\n        KEY_PATH=\"/\"\n        headers={[\"User-Agent\"]=\"Lua\"}\n        http_response = http_request:get(MY_URL .. KEY_PATH, headers)\n        event.log.key = key\n        event.log.iv = iv\n        event.log.response = http_response\n        emit(event)\nend\n'''\nhooks.process = \"process\"\n\n[transforms.t_encrypt]\ntype = \"remap\"\ninputs = [\"t_lua\"]\nsource = '''\n. = .\n'''\n\n[sinks.serverout]\ntype = \"console\"\ninputs = [\"t_encrypt\"]\ntarget = \"stdout\"\nencoding.codec = \"json\"\n\n\nValidating the configuration\n\n$ vector validate vector.toml\n\u221a Loaded [\"vector.toml\"]\n\u221a Component configuration\n\u221a Health check \"serverout\"\n--------------------------\n                 Validated\n\n\nTest file, content of the test file.\n\n{\"context\":{\"correlationId\":\"00MYSECRETPASSWORD99\"}}\n\n\nSend the request\n\n$ cd /root\n$ cat test >> log\n\n\nResult on Vector.\n\n$ vector --config vector.toml\n2022-05-02T17:03:39.779859Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=trace,rdkafka=info,buffers=info,kube=info\"\n2022-05-02T17:03:39.779916Z  INFO vector::app: Loading configs. paths=[\"vector.toml\"]\n2022-05-02T17:03:39.783051Z  INFO vector::topology::running: Running healthchecks.\n2022-05-02T17:03:39.783177Z  INFO vector: Vector has started. debug=\"false\" version=\"0.21.1\" arch=\"x86_64\" build_id=\"18787c0 2022-04-22\"\n2022-05-02T17:03:39.783194Z  INFO vector::app: API is disabled, enable by setting `api.enabled` to `true` and use commands like `vector top`.\n2022-05-02T17:03:39.783301Z  INFO vector::topology::builder: Healthcheck: Passed.\n2022-05-02T17:03:39.783367Z  INFO source{component_kind=\"source\" component_id=input_file component_type=file component_name=input_file}: vector::sources::file: Starting file server. include=[\"/root/log\"] exclude=[]\n2022-05-02T17:03:39.783909Z  INFO source{component_kind=\"source\" component_id=input_file component_type=file component_name=input_file}:file_server: file_source::checkpointer: Loaded checkpoint data.\n2022-05-02T17:03:39.784163Z  INFO source{component_kind=\"source\" component_id=input_file component_type=file component_name=input_file}:file_server: vector::internal_events::file::source: Resuming to watch file. file=/root/log file_position=583\n{\"context\":{\"correlationId\":\"00MYSECRETPASSWORD99\"},\"iv\":\"1234ABCD1234ABCD\",\"key\":\"1a2b3c4d5e6f7g8h\",\"response\":{\"body\":\"<!doctype html>... \"status_line\":\"HTTP/1.1 200 OK\"}}\n\n\nThe request worked as expected.\n\nINSTALL AND TEST ON VECTOR NIGHTLY (2022-05)\n\nInstall Vector Nightly\n\n$ cd /root/\n$ mkdir -p vector && curl -sSfL --proto '=https' --tlsv1.2 https://packages.timber.io/vector/nightly/latest/vector-nightly-x86_64-unknown-linux-musl.tar.gz | tar xzf - -C vector --strip-components=2\n\nAdd the vector path to PATH. Edit the file ~/.bash_profile and\n$ vim ~/.bash_profile\nPATH=$PATH:$HOME/bin:/root/vector/bin\n$ source ~/.bash_profile\n# Add new Folder\n$ cd /root/vector\n$ mkdir lib\n\n\nTest the following configuration\n\ndata_dir = \"/root/vector/lib\"\n\n[sources.input_file]\ntype = \"file\"\ninclude = [\"/root/log\"]\nread_from = \"end\"\n\n[transforms.t_json]\ntype = \"remap\"\ninputs = [ \"input_file\" ]\nsource = \"\"\"\n. = parse_json!(.message)\n\"\"\"\n\n[transforms.t_lua]\ntype = \"lua\"\ninputs = [ \"t_json\" ]\nversion = \"2\"\nsource = '''\n-- LIBRARIES\nljson = require(\"lunajson\")\nkey = \"1a2b3c4d5e6f7g8h\"\niv  = \"1234ABCD1234ABCD\"\n\nfunction process(event, emit)\n        -- HTTP Request\n        json = require(\"lunajson\")\n        http_request = require(\"httpclient\").new()\n        MY_URL='http://www.google.com'\n        KEY_PATH=\"/\"\n        headers={[\"User-Agent\"]=\"Lua\"}\n        http_response = http_request:get(MY_URL .. KEY_PATH, headers)\n        event.log.key = key\n        event.log.iv = iv\n        event.log.response = http_response\n        emit(event)\nend\n'''\nhooks.process = \"process\"\n\n[transforms.t_encrypt]\ntype = \"remap\"\ninputs = [\"t_lua\"]\nsource = '''\n. = .\n'''\n\n[sinks.serverout]\ntype = \"console\"\ninputs = [\"t_encrypt\"]\ntarget = \"stdout\"\nencoding.codec = \"json\"\n\n\nValidating the configuration\n\n$ vector validate vector.toml\n\u221a Loaded [\"vector.toml\"]\n2022-05-02T17:07:55.922143Z  WARN vector::transforms::remap: VRL compilation warning. warnings=\n\n2022-05-02T17:07:55.922215Z  WARN vector::transforms::remap: VRL compilation warning. warnings=\n\n\u221a Component configuration\n\u221a Health check \"serverout\"\n--------------------------\n                 Validated\n\n\nTest file, content of the test file.\n\n{\"context\":{\"correlationId\":\"00MYSECRETPASSWORD99\"}}\n\n\nSend the request\n\n$ cd /root\n$ cat test >> log\n\n\nResult on Vector.\n\n$ vector --config vector.toml\n2022-05-02T17:09:05.691292Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=trace,rdkafka=info,buffers=info,kube=info\"\n2022-05-02T17:09:05.691349Z  INFO vector::app: Loading configs. paths=[\"vector.toml\"]\n2022-05-02T17:09:05.693939Z  WARN vector::transforms::remap: VRL compilation warning. warnings=\n\n2022-05-02T17:09:05.694016Z  WARN vector::transforms::remap: VRL compilation warning. warnings=\n\n2022-05-02T17:09:05.694090Z  INFO vector::topology::running: Running healthchecks.\n2022-05-02T17:09:05.694124Z  INFO vector::topology::builder: Healthcheck: Passed.\n2022-05-02T17:09:05.694196Z  INFO vector: Vector has started. debug=\"false\" version=\"0.22.0\" arch=\"x86_64\" build_id=\"5099040 2022-04-29\"\n2022-05-02T17:09:05.694190Z  INFO source{component_kind=\"source\" component_id=input_file component_type=file component_name=input_file}: vector::sources::file: Starting file server. include=[\"/root/log\"] exclude=[]\n2022-05-02T17:09:05.694213Z  INFO vector::app: API is disabled, enable by setting `api.enabled` to `true` and use commands like `vector top`.\n2022-05-02T17:09:05.694837Z  INFO source{component_kind=\"source\" component_id=input_file component_type=file component_name=input_file}:file_server: file_source::checkpointer: Loaded checkpoint data.\n2022-05-02T17:09:05.695114Z  INFO source{component_kind=\"source\" component_id=input_file component_type=file component_name=input_file}:file_server: vector::internal_events::file::source: Resuming to watch file. file=/root/log file_position=477\n2022-05-02T17:09:15.982165Z ERROR transform{component_kind=\"transform\" component_id=t_lua component_type=lua component_name=t_lua}: vector::internal_events::lua: Error in lua script; discarding event. error=RuntimeErrorHooksProcess { source: RuntimeError(\"[string \\\"src/transforms/lua/v2/mod.rs:162:17\\\"]:13: attempt to index a nil value (global 'http_request')\\nstack traceback:\\n\\t[C]: in metamethod 'index'\\n\\t[string \\\"src/transforms/lua/v2/mod.rs:162:17\\\"]:13: in function 'process'\") } error_type=\"script_failed\" error_code=\"runtime_error_hook_process\" stage=\"processing\" internal_log_rate_secs=30\n\n\nLUA is not working on Vector Nightly, but in Vector 0.21.1 is working fine.\n\nYou could help us to validate, why LUA is working on Vector 0.21.1 but it is not the same case on Vector Nightly (2022-05) or maybe I have a bad configuration in Vector Nightly.\nBest Regards.",
        "url": "https://github.com/vectordotdev/vector/discussions/12521",
        "createdAt": "2022-05-02T17:16:12Z",
        "updatedAt": "2022-06-13T10:07:30Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "mindhack03d"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12456,
        "title": "How to use the pb to impl a grpc client",
        "bodyText": "vector/lib/vector-core/proto/event.proto\n    \n    \n         Line 35\n      in\n      f1f1be7\n    \n  \n  \n    \n\n        \n          \n           map<string, Value> fields = 1; \n        \n    \n  \n\n\nI want to implement a grpc client to send logs directly to vector aggregator, because the performance of grpc is much better than http.\nI found protobuf definitions here, but don't know how to use it. Especially how to set the Key of the map, what does the key mean? Are there any required keys or any conventions? Could you please give some Pseudo code to explain that?",
        "url": "https://github.com/vectordotdev/vector/discussions/12456",
        "createdAt": "2022-04-28T09:05:55Z",
        "updatedAt": "2022-08-06T00:41:10Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "caibirdme"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 7494,
        "title": "Add pod_annotations to kubernetes_logs source output log events",
        "bodyText": "Is it possible to get annotations from the pod in kubernetes_logs and the log events similar to pod_labels?\nIn some cases it is convenient to use annotations.\nExample:\n{\n  \"file\": \"/var/log/pods/kube-system_storage-provisioner_93bde4d0-9731-4785-a80e-cd27ba8ad7c2/storage-provisioner/1.log\",\n  ...  \n  \"kubernetes.pod_labels\": {\n    \"addonmanager.kubernetes.io/mode\": \"Reconcile\",\n    \"gcp-auth-skip-secret\": \"true\",\n    \"integration-test\": \"storage-provisioner\"\n  },\n  \"kubernetes.pod_annotations\": {\n    \"annotation1\": \"sample text\",\n    \"annotation2\": \"sample text\",\n  },\n  \"kubernetes.pod_name\": \"storage-provisioner\",\n  ...\n  \"source_type\": \"kubernetes_logs\",\n  \"stream\": \"stderr\",\n  \"timestamp\": \"2020-10-15T11:01:46.499555308Z\"\n}\nExample using in VRL\n  if .kubernetes.pod_annotations.annotation1 != null {\n    ...\n  }",
        "url": "https://github.com/vectordotdev/vector/discussions/7494",
        "createdAt": "2021-05-18T09:45:37Z",
        "updatedAt": "2022-06-24T06:49:31Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "aivanov-citc"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 12418,
        "title": "Event missing the message key; dropping event",
        "bodyText": "Hey,\nWe are trying to coordinate 3 vectors. Flow goes as follow:\n(socket_source)Vector 1  -> (http_source) Vector2 --> HTTP source Vector3\n\nVector1 Source and sink look like this:\nsinks:\n...\nhttp_out:\n    auth:\n      password: ${VECTOR_REMOTE_PASSWORD}\n      strategy: basic\n      user: username\n    batch:\n      max_bytes: 1049000\n      timeout_secs: 5\n    buffer:\n      max_events: 10000\n      type: memory\n      when_full: drop_newest\n    compression: gzip\n    encoding:\n      codec: text\n    healthcheck: true\n    inputs:\n    - socket_in\n    type: http\n    uri: https://our_external_url_for_vector2/data\n...\nsources:\n  socket_in:\n    address: 0.0.0.0:8001\n    max_length: 262144\n    mode: tcp\n    type: socket\n\nOur Vector 2 configuration looks like this:\nsources:\n  http_in:\n    address: 0.0.0.0:8001\n    auth:\n      password: password\n      username:username\n    encoding: json\n    path: /data\n    type: http\nsinks:\n http_out:\n    auth:\n      password: ${PASSWORD}\n      strategy: basic\n      user: superuser\n    batch:\n      max_bytes: 1049000\n      timeout_secs: 5\n    buffer:\n      max_events: 10000\n      type: memory\n      when_full: drop_newest\n    compression: gzip\n    encoding:\n      codec: text\n    healthcheck: true\n    inputs:\n    - http_in\n    type: http\n    uri: https://our_external_url_for_vector3/data\n\n\nUnfortunately, between vector2 and vector3 we get an error:  Event missing the message key; dropping event\nAnd indeed, if I stdout Vector1 message, it has keys message, timestamp etc...\nBut, stdout of Vector2 message are simply keys that were previously inside message key. Do you know what might be the issue here?\nWe have some transforms of http_in source in Vector2, but used for other sinks. I am not sure it is the issue, because http_out uses http_in as input directly.\nOne suspicion I have is that Vector1 is Agent and Vector2 and Vector3 are Aggregators. May it be that aggregator is dropping this structure?\nAny ideas would be greatly appreciated!",
        "url": "https://github.com/vectordotdev/vector/discussions/12418",
        "createdAt": "2022-04-26T18:25:48Z",
        "updatedAt": "2022-07-01T17:06:39Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "adam-mrozik"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12373,
        "title": "nats: cannot connect to ipv6 host",
        "bodyText": "Hello, after the auth fixes in 0.21.1 I'm attempting to connect to a ipv6 host, but vector doesn't seem to like my config. Something like:\n[sources.my_nats]\n  type = \"nats\"\n  url = \"nats://[::1]:4222\"\n  connection_name = \"logs\"\nGives:\n2022-04-22T18:56:08.769003Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=trace,rdkafka=info,buffers=info,kube=info\"\n2022-04-22T18:56:08.769115Z  INFO vector::app: Loading configs. paths=[\"/etc/vector/vector.toml\"]\n2022-04-22T18:54:44.030456Z ERROR vector::topology: Configuration error. error=Source \"my_nats\": NATS Connect Error: failed to lookup address information: Name does not resolve\nI've tried variations of the host but are running out of options. Any input much appreciated!",
        "url": "https://github.com/vectordotdev/vector/discussions/12373",
        "createdAt": "2022-04-22T19:00:13Z",
        "updatedAt": "2023-01-19T12:41:40Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "jbergstroem"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12274,
        "title": "Filter Logs Before Remap",
        "bodyText": "I am trying to eliminate some logs even before remap in the Transforms section.\ni am using File as my source.\ni tried datadog_search but i am able to do conditioning only once.\nhow can i do multi conditioning in filters?\nBelow example is for Datadog_search but i am happy to use VRL too. Suggest some ideas\n[transforms.reduce_logs]\ntype = \"filter\"\ninputs = [ \"logs\" ]\ncondition = { type = \"datadog_search\", source = \"PASSIVE SERVICE CHECK\" }\n#condition = '.message != \"PROCESS_SERVICE_CHECK_RESULT\"'\n[transforms.modify]\ntype = \"remap\"\ninputs = [\"reduce_logs\"]\nsource = '''\n. = parse_groks!(\n.message,\npatterns: [\n\"\\[%{NUMBER:nagios_epoch}\\] Warning:%{SPACE}%{GREEDYDATA:nagios_message}\",\n\"\\[%{NUMBER:nagios_epoch}\\] %{NAGIOS_TYPE_CURRENT_SERVICE_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}\n'''\n)\nTIA",
        "url": "https://github.com/vectordotdev/vector/discussions/12274",
        "createdAt": "2022-04-19T10:28:31Z",
        "updatedAt": "2022-06-12T04:54:28Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "jamalrahaman"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 5
        },
        "upvoteCount": 1
    },
    {
        "number": 12254,
        "title": "How to receive data in deffirent pods on the same node?",
        "bodyText": "If I install vector as daemonset(not sidecar) in k8s cluster, how could vector receive log data in pods on the same node? I don't find any documentation about this. Or if I try to send logs to vector aggregator, what's the protocol should be, what's the endpoint? I cannot find document about that either? Can anyone help?",
        "url": "https://github.com/vectordotdev/vector/discussions/12254",
        "createdAt": "2022-04-16T04:37:55Z",
        "updatedAt": "2022-07-19T12:27:06Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "caibirdme"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12196,
        "title": "Elasticsearch index name based on kubernetes.pod_namespace value",
        "bodyText": "I use kubernetes_logs source elasticsearch sink and can't find the way how to generate index name based on kubernetes.pod_namespace value. For examle:\n  sources:\n    monitoring_grafana:\n      type: kubernetes_logs\n      extra_label_selector: \"app.kubernetes.io/instance=prom,app.kubernetes.io/name=grafana\"\n  sinks:\n    es:\n      type: elasticsearch\n      inputs:\n        - monitoring_grafana\n      bulk:\n        index: \"vector-{{kubernetes.pod_namespace}}-%Y-%m-%d\"\n\nWhat should i use instead of {{kubernetes.pod_namespace}}? Please, add an example in documentation.",
        "url": "https://github.com/vectordotdev/vector/discussions/12196",
        "createdAt": "2022-04-13T08:39:23Z",
        "updatedAt": "2022-07-07T18:39:07Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "andy812"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 12244,
        "title": "VRL parse_json depth",
        "bodyText": "Hello, community! I have a question regarding some VRL features.\nThere is a problem with complicated nested JSON objects using the Elasticsearch sink. I use a simple remap rule to parse messages into objects - .parsed_message = parse_json!(.message); del(.message). However, users tend to put anything in their JSON logs. Vector then parses these nested documents, and Elasticsearch indexes ALL the fields.\nThere is no way to change the users' behavior because log collecting is provided as a service. I'd like to protect a message parsing depth somehow to prevent Vector/Elasticsearch overloading.\nThe example:\n{\n  \"1\": {\n    \"2\": {\n      \"3\": {\n        \"4\": {\n          \"5\": {\n            \"6\": \"finish\"\n          }\n        }\n      }\n    }\n  }\n}\nI want have a message converted into something like this (docs no more profound than five levels deep are allowed):\n{\n  \"1\": {\n    \"2\": {\n      \"3\": {\n        \"4\": {\n          \"5\": \"{\\\"6\\\": \\\"finish\\\"}\"\n        }\n      }\n    }\n  }\n}\n\nMy initial idea was to implement a function for VRL to control the depth like parse_json_shallow(5, .message).\nI thought about writing this remap in Lua, but Lua transforms are ~60% slower and seem to utilize only a single core.\n\nAre there any workarounds for that case? What would you suggest?",
        "url": "https://github.com/vectordotdev/vector/discussions/12244",
        "createdAt": "2022-04-15T13:06:23Z",
        "updatedAt": "2022-06-17T14:31:10Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "nabokihms"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12085,
        "title": "How to increase through put for Vector consuming from kafka",
        "bodyText": "Suspect two issues\n\nLua transform is reducing the consumption rate\nwith multiple instance of vector, the source is not getting load shared across replicas\n\nConfiguration:\nsources:\n  kafka_metric_read:\n    type: kafka\n    bootstrap_servers: \u201cxxx.svc.cluster.local:9092\"\n    group_id: \"vector-group\"\n    topics:\n      - \u201ct_metric\"\n    auto_offset_reset: \"latest\"\n    acknowledgements:\n      enabled: true\n\ntransforms:\n  aggregator_metric_processing:\n    type: remap\n    inputs: [\"kafka_metric_read\"]\n    source: |-\n      .metric = parse_json!(.message)\n      #.metric.timestamp = to_timestamp!(.metric.timestamp)\n\n## refer https://github.com/vectordotdev/vector/discussions/7689#discussioncomment-885366\n  metric_process:\n    type: lua\n    inputs:\n      - aggregator_metric_processing\n    version: '2'\n    hooks:\n      process: |\n        function (event, emit)\n          event.metric = event.log.metric\n          event.log = nil\n          emit(event)\n        end\n\nsinks:\n  prom_metric:\n    type: prometheus_remote_write\n    inputs: [\"metric_process\"]\n    endpoint: http://thanos-receive.svc.cluster.local:9598/api/v1/receive\n    healthcheck: true\n    request:\n      retry_attempts: 0\n      timeout_secs: 120\n\nIn kafka, the ingress rate is ~21k where as the egress is around 17k  , so i suspect the bottle neck is not at transforms but at the vector's consuming from kafka ..\nLooking at utilisation metrics , looks like there is some lag at lua transform.\nsum(vector_utilization) by (component_type)\n\n{component_type=\"remap\"} | 0.0946328675065908\n{component_type=\"lua\"} | 0.9684126307364543\n{component_type=\"prometheus_remote_write\"} | 0.08025006493975331\n{component_type=\"prometheus_exporter\"} | 0.000161718097033596\n\nAny suggestion on how to improve through put and how to enable multiple instance of vector to consume will help.",
        "url": "https://github.com/vectordotdev/vector/discussions/12085",
        "createdAt": "2022-04-05T06:26:16Z",
        "updatedAt": "2022-06-07T16:20:02Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "pkirubak"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12200,
        "title": "Transfer rules from fluent to vector",
        "bodyText": "Hello,\nI have some rules from Fluetn taking from docker swarm dotnet apps and its look like:\nhttps://pastebin.com/JZgSL4gE\nCan some one help change this rewrite rules to vector?",
        "url": "https://github.com/vectordotdev/vector/discussions/12200",
        "createdAt": "2022-04-13T15:02:41Z",
        "updatedAt": "2022-06-17T14:26:42Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "KaMaToZzz"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12114,
        "title": "Internal Metrics - Emit only required parameters",
        "bodyText": "Hello Team,\nIs there a way to configure the Internal Metrics (for vector instance acting as client which is configured to received the events from http component), so that it would emit only the required parameters such as component_sent_events_total , component_received_events_total etc., rather than having all the parameter flushed out to the sink file.?",
        "url": "https://github.com/vectordotdev/vector/discussions/12114",
        "createdAt": "2022-04-07T05:15:58Z",
        "updatedAt": "2022-07-15T05:58:39Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "hemanthofficial3009"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12120,
        "title": "What is the `field` attribute for `log_to_metric` transform ?",
        "bodyText": "From example. I am not sure I understand what it does\n[transforms.my_transform_id]\ntype = \"log_to_metric\"\ninputs = [ \"my-source-or-transform-id\" ]\n\n  [[transforms.my_transform_id.metrics]]\n  type = \"counter\"\n  field = \"status\" # I don't understand how it maps to the output \n  name = \"response_total\"\n  namespace = \"service\"\n\n    [transforms.my_transform_id.metrics.tags]\n    status = \"{{status}}\"\n    host = \"{{host}}\"",
        "url": "https://github.com/vectordotdev/vector/discussions/12120",
        "createdAt": "2022-04-07T13:23:04Z",
        "updatedAt": "2022-06-20T09:56:40Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "midnightexigent"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12116,
        "title": "rename_fields explanation please",
        "bodyText": "Hi\nI need to rename my field, timestamp to @timestamp. How this can me done?\nI see that vectore added rename_fields function, but I don't see any examples of how to use it.\nThanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/12116",
        "createdAt": "2022-04-07T08:33:39Z",
        "updatedAt": "2022-09-01T04:57:18Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "agorkiy"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12088,
        "title": "wildcards is not observed in the config path.",
        "bodyText": "Hi there,\nI start vector in the console this way:\nvector -c /etc/vector/vector.toml -c /etc/vector/modules/*/*.toml\nAll toml files were loaded.\nThe same configuration in systemd does not work.\n/etc/vector/modules\n                 \u2514\u2500\u2500\u2500sophos-xg/*.toml\n                 \u2514\u2500\u2500\u2500telegraf/*.toml\n\nHere only the first found toml is loaded, in the case in the folder sophos-xg.\nAny solution to this problem ?",
        "url": "https://github.com/vectordotdev/vector/discussions/12088",
        "createdAt": "2022-04-05T12:49:26Z",
        "updatedAt": "2023-02-09T10:21:32Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "StefanSa"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12087,
        "title": "Socket Data Raw Encoding (Vector 0.20.0)",
        "bodyText": "Hi everyone :)\nOur company just recently started using Vector and we love it so far and it vast functionality. One of our main use cases is to have Vector put multicast UDP data onto a Kafka topic (see below for the basic configuration). The issue I am running into is that the message put onto Kafka isn't the raw packet data and looks to be UTF8 encoded.  See below for the expected data vs received data. Is there any way to fix this? Is there a configuration parameter I am missing?\nvector 0.20.0 (x86_64-unknown-linux-gnu 2a706a3 2022-02-11)\nVector Configuration File\n[sources.in]\ntype = \"socket\" # required\naddress = \"224.3.3.100:61000\" # required, required when mode = \"tcp\" or mode = \"udp\"\nmode = \"udp\" # required\ndecoding.codec = \"bytes\"\n# framing = { method = \"bytes\" }\n\n[sinks.out]\n# General\ntype = \"kafka\" # required\ninputs = [\"in\"] # required\nbootstrap_servers = \"kafka:9094\" # required\nkey_field = \"user_id\" # required\ntopic = \"live_data\" # required\nencoding.codec = \"json\" # required\n\nExpected Data\n00 17 0e 02 55 55 aa aa  00 ff 00 ac 03 a3 49 c9\n00 00 00 00 0b 00 48 df  df 01 01 02 00 06 01 0b\neb a4 0f db 1f 74 50 fe  6c ea f9 22 fb 12 00 00\n00 3f fc 00 00 0b ce 41  01 00 0a e9 78 0a 58 1d\n01 20 00 01 41 00 00 00  00 00 00 02 00 00 00 00 \n00 00 d8 b7 7c 00 00 00  00 00 50 ca     \n\nReceived Data\nHere is the data on Kafka:\n{\"host\":\"192.148.154.5:53016\",\"message\":\"\\u0000\\u0017\\u000e\\u0002UU\ufffd\ufffd\\u0000\ufffd\\u0000\ufffd\\u0003\ufffdI\ufffd\\u0000\\u0000\\u0000\\u0000\\u000b\\u0000H\ufffd\ufffd\\u0001\\u0001\\u0002\\u0000\\u0006\\u0001\\u000b\ufffd\\u000f\ufffd\\u001ftP\ufffdl\ufffd\ufffd\\\"\ufffd\\u0012\\u0000\\u0000\\u0000?\ufffd\\u0000\\u0000\\u000b\ufffdA\\u0001\\u0000\\n\ufffdx\\nX\\u001d\\u0001 \\u0000\\u0001A\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0002\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\u0637|\\u0000\\u0000\\u0000\\u0000\\u0000P\ufffd\",\"source_type\":\"socket\",\"timestamp\":\"2022-04-05T12:00:16.843574010Z\"}\n\nWhich translates to:\n00 17 0E 02 55 55 EF BF BD EF BF BD 00 EF BF BD 00 EF BF BD 03 EF BF BD 49 EF BF BD 00 00 00 00 0B 00 48 EF BF BD EF BF BD 01 01 02 00 06 01 0B EF BF BD 0F EF BF BD 1F 74 50 EF BF BD 6C EF BF BD EF BF BD 22 EF BF BD 12 00 00 00 3F EF BF BD 00 00 0B EF BF BD 41 01 00 0A EF BF BD 78 0A 58 1D 01 20 00 01 41 00 00 00 00 00 00 02 00 00 00 00 00 00 D8 B7 7C 00 00 00 00 00 50 EF BF BD\n\nThanks for any help in advance! We are looking forward to deploying Vector across our products :)",
        "url": "https://github.com/vectordotdev/vector/discussions/12087",
        "createdAt": "2022-04-05T12:26:13Z",
        "updatedAt": "2022-09-05T17:00:09Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "musenotdead"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 12086,
        "title": "Vector Unit Testing - Cant send or parse array/json elements directly",
        "bodyText": "Config yaml:\nsources:\n  stdin:\n    type: stdin\n    decoding:\n      codec: json\n      \ntransforms:\n  filter-empty:\n    type: filter\n    inputs:\n      - stdin\n    condition: >-\n      .message != \"\"\n  \n  parse-common-stuff:\n    type: remap\n    inputs:\n      - filter-empty\n    drop_on_error: true\n    source: |-\n      .name = string!(.actors[0].name)\n      .src = string!(.source)\n      .eId = string!(.id)\n\n  route-logs:\n    type: route\n    inputs:\n      - parse-common-stuff\n    route:\n      result-log: '.result != null'\n      empty-result-log: '.result == null'\n  \n  parse-result-log:\n    type: remap\n    inputs:\n      - route-logs.result-log\n    drop_on_error: true\n    source: |-\n      .message = .result.message\n      .status = string!(.result.status)\n      .host = string!(.host)\n      .session = string!(.resources[0].session)\n  \n  parse-empty-result-log:\n    type: remap\n    inputs:\n      - route-logs.empty-result-log\n    drop_on_error: true\n    source: |-\n      if is_array(.resources) {\n        if length!(.resources) >= 1 {\n          web = get!(value: .resources[0], path: [\"session\"])\n          .session = if web == null { null } else { string!(web) }\n        }\n      }\n      . = {\n        \"session\" : .session\n      }\n\nsinks:\n  stdout:\n    type: console\n    inputs:\n      - stdin\n    target: stdout\n    encoding:\n      codec: ndjson\n\nTest file:\ntests:\n  - name: filter-empty-logs\n    inputs:\n      - insert_at: filter-empty\n        type: log\n        log_fields:\n          host: 0.0\n          message: \"\"\n    no_outputs_from:\n      - filter-empty\n\n  - name: validate-result-log-block\n    inputs:\n      - insert_at: filter-empty\n        type: log\n        log_fields:\n          host: 0.0\n          message: \"{\\\"source\\\": \\\"test\\\", \\\"id\\\": \\\"testid\\\", \\\"recorded\\\": \\\"date\\\", \\\"actors\\\": [{\\\"type\\\": \\\"user\\\", \\\"name\\\": \\\"somename\\\", \\\"id\\\": \\\"someid\\\"}], \\\"resources\\\": [{\\\"id\\\": null, \\\"type\\\": null, \\\"name\\\": null, \\\"session\\\": \\\"test\\\"}], \\\"result\\\": {\\\"status\\\": \\\"test\\\", \\\"message\\\": \\\"somemessage\\\"}}\"\n    outputs:\n      - extract_from: parse-result-log\n        conditions:\n          - type: vrl\n            source: |-\n              assert!(.host == \"0.0\")\n              assert!(.session == \"test\")\n   \n\nCommand:\nvector -v test --config-yaml test.yaml test2-test.yaml\nError:\nWARN transform{component_kind=\"transform\" component_id=parse-common-stuff component_type=remap component_name=parse-common-stuff}: vector::internal_events::remap: Mapping failed with event; discarding event. error=\"function call error for \\\"string\\\" at (8:32): expected \\\"string\\\", got \\\"null\\\"\" internal_log_rate_secs=30\nINFO vector::topology::builder: Healthcheck: Passed.\ntest validate-result-log-block ... \ufffd[31mfailed\ufffd[0m\n\nfailures:\n\ntest validate-result-log-block:\n\nchecks for transform \"parse-result-log\" failed: no events received. Topology may be disconnected or transform is missing inputs.\n\nBut when i run the vector file alone with below message input, it does parse it. but how can i test the use case by just sending message json alone.\n\ni think message json not being parsed as an input with the type of json.\nor how can we send array under log_fields:",
        "url": "https://github.com/vectordotdev/vector/discussions/12086",
        "createdAt": "2022-04-05T12:05:47Z",
        "updatedAt": "2022-06-06T14:05:59Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "satscreate"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12097,
        "title": "Splunk source health check URL.",
        "bodyText": "We're configuring https://vector.dev/docs/reference/configuration/sources/splunk_hec source running in a Docker (timberio/vector) container.\nWe need to identify a health check URL.\nOur infrastructure has a limitation that the Docker container (AWS ECS) running vector.dev can only expose one port.\nDoes https://vector.dev/docs/reference/configuration/sources/splunk_hec provide a no-auth http GET based health check URL?\nDoes vector.dev running in the Docker container (timberio/vector) provide a no-auth http GET based health check URL accessible via the same port (as Splunk source)?",
        "url": "https://github.com/vectordotdev/vector/discussions/12097",
        "createdAt": "2022-04-05T17:58:39Z",
        "updatedAt": "2022-07-19T22:51:25Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "hlbraddock"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 12067,
        "title": "rename a metric tag, how ?",
        "bodyText": "Hi there,\ni get these metrics from the telegraf client.\n{\n  \"fields\": {\n    \"usage_guest\": 0,\n    \"usage_guest_nice\": 0,\n    \"usage_idle\": 99.82499999983702,\n    \"usage_iowait\": 0,\n    \"usage_irq\": 0,\n    \"usage_nice\": 0,\n    \"usage_softirq\": 0,\n    \"usage_steal\": 0,\n    \"usage_system\": 0.024999999999977263,\n    \"usage_user\": 0.14999999999986358\n  },\n  \"name\": \"cpu\",\n  \"tags\": {\n    \"cpu\": \"cpu-total\",\n    \"host\": \"os-dash01\"\n  },\n  \"timestamp\": 1649056860\n}\n\nThe array name fields would have to be renamed with the contents of the field name (cpu). What is the best way to do this ?\nThat would be the goal:\n{\n  \"cpu\": {\n    \"usage_guest\": 0,\n    \"usage_guest_nice\": 0,\n    \"usage_idle\": 99.82499999983702,\n    \"usage_iowait\": 0,\n    \"usage_irq\": 0,\n    \"usage_nice\": 0,\n    \"usage_softirq\": 0,\n    \"usage_steal\": 0,\n    \"usage_system\": 0.024999999999977263,\n    \"usage_user\": 0.14999999999986358\n  },\n  \"name\": \"cpu\",\n  \"tags\": {\n    \"cpu\": \"cpu-total\",\n    \"host\": \"os-dash01\"\n  },\n  \"timestamp\": 1649056860\n}",
        "url": "https://github.com/vectordotdev/vector/discussions/12067",
        "createdAt": "2022-04-04T07:34:46Z",
        "updatedAt": "2022-06-18T16:02:34Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "StefanSa"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 11418,
        "title": "Performance test to detect transforms code regressions",
        "bodyText": "Hi \ud83d\udc4b,\nI want to build performance tests for the VRL code we write for our logging pipelines.\nI looked into the Soak tests, but they require too much infra/setup to have them running. Also I am looking for something that I can run as a GitHub Workflow.\nThe setup I came up with looks like this :\n\nreplace all Vector sources with demo_logs\nmerge all sinks into a sink blackhole sink\nrun vector with that config and capture it's logs. Example log:\n\n{\"timestamp\":\"2022-02-16T17:10:15.290685Z\",\"level\":\"INFO\",\"message\":\"Total events collected\",\"events\":4000,\"raw_bytes_collected\":10058173,\"target\":\"vector::sinks::blackhole::sink\"}\n\nSince the demo_logs source is \"non-blocking\" Vector will simply terminate gracefully once all events have been sent to the transforms. Now I can measure how long it took Vector to finish and I know how many bytes and events the blackhole sink used (from the last Vector log event).\nI would appreciate it to get feedback about this setup or maybe share ideas about a better (yet simple) way of doing it.",
        "url": "https://github.com/vectordotdev/vector/discussions/11418",
        "createdAt": "2022-02-16T17:13:27Z",
        "updatedAt": "2022-06-20T15:17:41Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "anas-aso"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 3
    },
    {
        "number": 11527,
        "title": "How to call API and consume the output using exec",
        "bodyText": "Any example script for exec source, which calls an API every N minute and collects the response and transform.?\nTried,\nsources:\n  my_source_id:\n    type: exec\n    command: [\"curl\",\"-k\",\"https://gorest.co.in/public/v2/users\"]\nsinks:\n  stdout:\n    type: console\n    inputs:\n      - my_source_id\n    target: stdout\n    encoding:\n      codec: json\n\nAny examples?",
        "url": "https://github.com/vectordotdev/vector/discussions/11527",
        "createdAt": "2022-02-23T10:54:59Z",
        "updatedAt": "2022-06-27T05:44:38Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "satscreate"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11968,
        "title": "Splunk log sink http error messages",
        "bodyText": "We infrequently see some of our requests fail with a 400 status code e.g.:\n2022-03-23T16:20:20.895212Z ERROR sink{component_kind=\"sink\" component_id=splunk component_type=splunk_hec_logs component_name=splunk}:request{request_id=21}: vector::sinks::util::retries: Not retriable; dropping the request. reason=\"response status: 400 Bad Request\"\n\nbut unlike the generic http sink the response body containing the reason why it's a Bad Request isn't visible.\ne.g.\n2022-03-23T16:45:20.809535Z ERROR sink{component_kind=\"sink\" component_id=scalyr component_type=http component_name=scalyr}:request{request_id=17}: vector::sinks::util::retries: Not retriable; dropping the request. reason=\"response status: 400 Bad Request\"\n2022-03-23T16:45:20.809652Z ERROR sink{component_kind=\"sink\" component_id=scalyr component_type=http component_name=scalyr}:request{request_id=17}: vector::sinks::util::sink: Response failed. response=Response { status: 400, version: HTTP/1.1, headers: {\"server\": \"nginx\", \"date\": \"Wed, 23 Mar 2022 16:45:20 GMT\", \"content-type\": \"application/json;charset=UTF-8\", \"content-length\": \"83\", \"connection\": \"keep-alive\", \"access-control-allow-credentials\": \"true\", \"access-control-allow-methods\": \"GET, POST, PUT, DELETE, OPTIONS\", \"access-control-allow-headers\": \"Accept,Authorization,Cache-Control,Content-Type,DNT,If-Modified-Since,Keep-Alive,Origin,User-Agent,X-Requested-With\"}, body: b\"{\\n  \\\"message\\\": \\\"request is not valid JSON\\\",\\n  \\\"status\\\": \\\"error/client/badRequest\\\"\\n}\" }\n\nIs there any way of configuring just the Splunk sink to be more verbose or so that the error message is visible?\nsetting the global verbose flag emits more details however the body is still omitted\n2022-03-24T13:06:59.181727Z DEBUG sink{component_kind=\"sink\" component_id=splunk component_type=splunk_hec_logs component_name=splunk}:request{request_id=2630}:http: vector::internal_events::http_client: HTTP response. status=400 Bad Request version=HTTP/1.1 headers={\"date\": \"Thu, 24 Mar 2022 13:06:57 GMT\", \"content-type\": \"application/json; charset=UTF-8\", \"x-content-type-options\": \"nosniff\", \"content-length\": \"60\", \"vary\": \"Authorization\", \"connection\": \"Keep-Alive\", \"x-frame-options\": \"SAMEORIGIN\", \"server\": \"Splunkd\"} body=[60 bytes]",
        "url": "https://github.com/vectordotdev/vector/discussions/11968",
        "createdAt": "2022-03-24T12:00:33Z",
        "updatedAt": "2022-07-14T11:21:14Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "matt-simons"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 12003,
        "title": "Add additional tags to all vector  mertics",
        "bodyText": "Hi, how can I add specific tags to all components metrics in the vector pipeline? is there any way?\nin the transform section, I add some tags to my metrics but I want to add these specific tags to all vector metrics",
        "url": "https://github.com/vectordotdev/vector/discussions/12003",
        "createdAt": "2022-03-28T07:42:28Z",
        "updatedAt": "2022-06-22T13:19:34Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "LinTechSo"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11958,
        "title": "LSB shared object for arm 32-bit",
        "bodyText": "Hello team,\nWhen i tried to download the vector binary for arm 32-bit architecture (version 0.18.1), im running in to the following issue: \"vector: ELF 32-bit LSB shared object, ARM, EABI5 version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-armhf.so.3, for GNU/Linux 3.2.0, BuildID[sha1]=414d8e9563cf09861560c79fa80a033d95741805, with debug_info, not stripped\".\nCan you please help us understand what was the issue in terms of LSB Shared object and not the executable?",
        "url": "https://github.com/vectordotdev/vector/discussions/11958",
        "createdAt": "2022-03-23T17:44:58Z",
        "updatedAt": "2022-06-10T05:41:16Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "hemanthofficial3009"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11920,
        "title": "Cumulative Histogram in log2metrics",
        "bodyText": "Is there a way or new feature is needed to add histogram with buckets ranges in tags annotation just like implemented here - also with cumulative option ?? https://github.com/influxdata/telegraf/blob/master/plugins/aggregators/histogram/README.md",
        "url": "https://github.com/vectordotdev/vector/discussions/11920",
        "createdAt": "2022-03-21T10:42:17Z",
        "updatedAt": "2022-06-26T01:41:12Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "szibis"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11956,
        "title": "VRL doesn't have the ability read some filed from a certern file",
        "bodyText": "When i transforms my old log to a standard format log, I need to add many new filed that not exist in the old log file. These new fileds come from other config files. Is the any way to read them?",
        "url": "https://github.com/vectordotdev/vector/discussions/11956",
        "createdAt": "2022-03-23T07:39:35Z",
        "updatedAt": "2023-12-12T23:02:34Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "lusvan"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 11934,
        "title": "How to use the name of the input in VRL?",
        "bodyText": "How can I write and if statement using the name of the input to make a decision?",
        "url": "https://github.com/vectordotdev/vector/discussions/11934",
        "createdAt": "2022-03-22T12:27:32Z",
        "updatedAt": "2022-07-28T01:37:43Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "smitsr72"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11834,
        "title": "how to change map to clickhouse Nested structure?",
        "bodyText": "I use vector to parse_json from json file to clickhouse,  because the json  like below\n{\"res\":\"5e60bbc17f068d0ba4f156e5\",\"_id\":\"622e150183d43d02dca5139b\",\"properties\":{\"key1\":\"value1\", \"key2\":2, \"key3\": 3.0}}\nand the clickhouse columns have a Nested structure like below:\nres String,\n_id String,\nproperties.key Array(Nullable(String)),\nproperties.value Array(Nullable(String)),\n\nso I need change the above json to the clickhouse format:\n{\"res\":\"5e60bbc17f068d0ba4f156e5\",\"_id\":\"622e150183d43d02dca5139b\",\"properties.key\": [\"key1\",\"key2\",\"key3\"],\"properties.value\": [\"value1\",\"2\",\"3.0\"]\n@binarylogic  how to transform it use the vector vrl?",
        "url": "https://github.com/vectordotdev/vector/discussions/11834",
        "createdAt": "2022-03-14T22:37:22Z",
        "updatedAt": "2022-06-15T18:59:59Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "dengc367"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11877,
        "title": "elastic sinks not work",
        "bodyText": "good day\ni up vector on kubernetes. i already got there an elasticsearch\ni want to push the kubernetes logs to elastic. and i got error when add sinks of elastic to config map of vector\n2022-03-17T11:16:52.083433Z INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=trace,rdkafka=info,buffers=info\"\nThu, Mar 17 2022 2:16:52 pm | 2022-03-17T11:16:52.083647Z INFO vector::app: Loading configs. paths=[\"/etc/vector\"]\nThu, Mar 17 2022 2:16:52 pm | 2022-03-17T11:16:52.084116Z ERROR vector::cli: Configuration error. error=while parsing a block mapping, did not find expected key at line 30 column 3\nmy config map with sinks look like this:\nThis file has been generated by helm template vector vector/vector from vector/templates/configmap.yaml. Please re-run make generate-kubernetes-manifests rather than modifying this file manually.\napiVersion: v1\nkind: ConfigMap\nmetadata:\nname: vector\nnamespace: vector\nlabels:\napp.kubernetes.io/name: vector\napp.kubernetes.io/instance: vector\napp.kubernetes.io/component: Agent\napp.kubernetes.io/version: \"0.20.0-distroless-libc\"\ndata:\nagent.yaml: |\ndata_dir: /vector-data-dir\napi:\nenabled: true\naddress: 127.0.0.1:8686\nplayground: false\nsources:\nkubernetes_logs:\ntype: kubernetes_logs\nhost_metrics:\nfilesystem:\ndevices:\nexcludes: [binfmt_misc]\nfilesystems:\nexcludes: [binfmt_misc]\nmountPoints:\nexcludes: [\"*/proc/sys/fs/binfmt_misc\"]\ntype: host_metrics\ninternal_metrics:\ntype: internal_metrics\nsinks:\nprom_exporter:\ntype: prometheus_exporter\ninputs: [host_metrics, internal_metrics]\naddress: 0.0.0.0:9090\nstdout:\ntype: console\ninputs: [kubernetes_logs]\nencoding:\ncodec: json\nelastic:\ninputs = [kubernetes_logs]            # only take sampled data\ntype = \"elasticsearch\"\nauth.user: \"elastic\"\nauth.password: \"password\"\nhost = \"https://elastic-host:9200\"   # local or external host\nindex = \"vector-%Y-%m-%d\"             # daily indices#",
        "url": "https://github.com/vectordotdev/vector/discussions/11877",
        "createdAt": "2022-03-17T11:21:59Z",
        "updatedAt": "2022-06-03T21:57:38Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "therus000"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11869,
        "title": "moving from logstash to vector",
        "bodyText": "Hi\nI'm want to move from logstash to vector and now trying to transform existing logstash config into vector one.\nHere is lsome part of it:\ninput {\n  udp {\n    port => 5042\n    type => \"rsyslog\"\n    tags => [\"container\"]\n  }\n}\nfilter {\n  mutate {\n    remove_field => [\"host\"]\n    add_field => {\"host\" => \"localhost\"}\n  }\n  if \"container\" in [tags] {\n    grok {\n      match => { \"message\" => \"(?!\\<%{NONNEGINT}\\>)%{SYSLOGTIMESTAMP:timestamp}%{SPACE}%{USERNAME:container}/%{WORD:hash}\\[%{NONNEGINT:syslog_version}]:%{SPACE}%{GREEDYDATA:parsed_message}\" }\n    }\n    if ![container] {\n      mutate {\n        add_field => {\"container\" => \"unknown\"}\n      }\n    }\n   if \"_grokparsefailure\" not in [tags] {      mutate {         remove_field => [\"message\"]       }       mutate {         rename => [\"parsed_message\", \"message\"]       }     }     if \"traefik\" in [container] {       json {         source => \"message\"       }       json {         source => \"Request\"       }.....      if [ClientHost] {         geoip {           source => \"ClientHost\"         }       }     }   } }`\noutput {\n  elasticsearch {\n    hosts => [\"elasticsearch:9200\"]\n....\n    index => \"logstash-container-%{container}-%{+YYYY.MM.dd}\"\n  }\n}\nThe question is - can I add tags with vector and perform operations based on that tags? Does anyone have such config example that can be shared? I saw somewhere that tags can be added only as key-value pairs, is that true?",
        "url": "https://github.com/vectordotdev/vector/discussions/11869",
        "createdAt": "2022-03-16T16:21:37Z",
        "updatedAt": "2022-06-10T14:36:00Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "agorkiy"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11794,
        "title": "Getting error from docker vector instance for exec command",
        "bodyText": "Getting error from docker vector instance,\nVector.yaml:\nsources:\n  my_source_id:\n    type: exec\n    command: [\"curl\",\"-k\",\"https://gorest.co.in/public/v2/users\"]\n    mode: scheduled\n    scheduled:\n      exec_interval_secs: 600 # five minutes\nsinks:\n  stdout:\n    type: console\n    inputs:\n      - my_source_id\n    target: stdout\n    encoding:\n      codec: json\n\n\nLog:\nERROR source{component_kind=\"source\" component_id=stdin component_type=exec component_name=stdin}: vector::internal_events::exec: Unable to exec. command=curl -k https://gorest.co.in/public/v2/users error=Os { code: 2, kind: NotFound, message: \"No such file or directory\" }\nAny Idea?",
        "url": "https://github.com/vectordotdev/vector/discussions/11794",
        "createdAt": "2022-03-11T07:03:14Z",
        "updatedAt": "2022-06-22T10:41:53Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "satscreate"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11796,
        "title": "Is there a way to split one event into multiple events according to custom logic?",
        "bodyText": "Hi there\uff0c\nI want to split one event into multiple events according to custom logic, like Record Split in Fluent-bit.\nfor example\uff1a\ninput:\n{\n\"a\":\"11\",\n\"b\":\"11\",\n\"c\":\"11\",\n\"d\":\"11\",\n\"extra\":[\n{\"aa\":\"1\"},\n{\"bb\":\"2}\n]\n}\n\ntarget output:\n{\"a\":\"11\",\"b\":\"11\",\"c\":\"11\",\"d\":\"11\",\"aa\":\"1\"}\n{\"a\":\"11\",\"b\":\"11\",\"c\":\"11\",\"d\":\"11\",\"bb\":\"2\"}\n\nI tried parse_json but it seems can only handle arrays",
        "url": "https://github.com/vectordotdev/vector/discussions/11796",
        "createdAt": "2022-03-11T07:08:47Z",
        "updatedAt": "2022-06-26T15:47:18Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "zmingz123"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11789,
        "title": "what does \"foo, err |= ...\" mean",
        "bodyText": "I was surprised to see the following result in all data removed (log(.) results in empty object) if function fails:\n., err |= parse_syslog(.no_exist)\n\nIs this due to not understanding |=?",
        "url": "https://github.com/vectordotdev/vector/discussions/11789",
        "createdAt": "2022-03-10T23:43:50Z",
        "updatedAt": "2022-06-15T19:50:50Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tshepang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11763,
        "title": "datadog_archive sink?",
        "bodyText": "curious if this sink is intended to be used? it doesn't appear to have any documentation.\n#8929",
        "url": "https://github.com/vectordotdev/vector/discussions/11763",
        "createdAt": "2022-03-10T01:01:43Z",
        "updatedAt": "2022-07-25T07:47:28Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "cspargo"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11683,
        "title": "Vector's Built-in-Redundancy Mechanism",
        "bodyText": "Hello,\nWe run vector instances on different machines which acts as Agents and Collector.\nhttp end-point -> Agent (source type=\"http\") (sink type =\"vector\") -> Transform (type=\"remap\") -> Collector (source type=\"vector\") (sink type=\"file\")\n\nVector acting as an agent would received data from an Http end-point.\nThe data received would be enriched (transform) and passed to a down stream vector object which acts as a collector running on another machine.\n\nCan you let us know if there is an option for retries on the sink of the agent which can help to identify that the collector instance is not responding",
        "url": "https://github.com/vectordotdev/vector/discussions/11683",
        "createdAt": "2022-03-04T05:57:19Z",
        "updatedAt": "2022-08-07T13:42:29Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "hemanthofficial3009"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11705,
        "title": "An error occurred after the data source was changed to Kafka using the Vector Agent service installed at helm",
        "bodyText": "I found a very strange problem and need your help.\nVector installed using helm\nhelm install vector vector/vector   --namespace vector   --create-namespace\nAfter the installation is complete, change the data source to Kafka, sink to Loki, and configMap as follows:\ndata:\n   vector.yaml: |\n     data_dir: /vector-data-dir\n     api:\n       enabled: true\n       address: 127.0.0.1:8686\n       playground: false\n     sources:\n       kafka:\n         type: kafka\n         acknowledgements: null\n         bootstrap_servers: kafka-0.kafka-svc.default.svc.cluster.local:9092,kafka-1.kafka-svc.default.svc.cluster.local:9092,kafka-2.kafka-svc.default.svc.cluster.local:9092\n         group_id: consumer-group-name-ventor\n         topics:\n          - ^(kube)-.+\n     sinks:\n       loki:\n         type: loki\n         inputs: [kafka]\n         endpoint: loki.loki-stack.svc.cluster.local:3100\nkind: ConfigMap\nmetadata:\n   annotations:\n     meta.helm.sh/release-name: vector\n     meta.helm.sh/release-namespace: vector\n   labels:\n     app.kubernetes.io/component: Agent\n     app.kubernetes.io/instance: vector\n     app.kubernetes.io/managed-by: Helm\n     app.kubernetes.io/name: vector\n     app.kubernetes.io/version: 0.20.0-distroless-libc\n     helm.sh/chart: vector-0.6.0\n   name: vector\n   namespace: vector\n\nVector-0 POD startup failed and error logs are as follows:\n2022-03-07T02:15:42.131211Z  INFO vector::app: Loading configs. paths=[\"/etc/vector\"]\n2022-03-07T02:15:42.133246Z ERROR vector::cli: Configuration error. error=sources.kafka: invalid type: unit value, expected bool or map at line 8 column 9",
        "url": "https://github.com/vectordotdev/vector/discussions/11705",
        "createdAt": "2022-03-07T02:17:27Z",
        "updatedAt": "2022-07-22T08:55:08Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "king5211"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11679,
        "title": "is there a way to drop events without `abort`",
        "bodyText": "I want to be able to use reroute_dropped, but don't want certain events reaching re-route destination.",
        "url": "https://github.com/vectordotdev/vector/discussions/11679",
        "createdAt": "2022-03-03T21:47:01Z",
        "updatedAt": "2022-09-27T23:25:53Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tshepang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 3
    },
    {
        "number": 11676,
        "title": "is there a way to search values in an array",
        "bodyText": "This would be equivalent of .contains in Rust libstd, or contains (for string type) in VRL.",
        "url": "https://github.com/vectordotdev/vector/discussions/11676",
        "createdAt": "2022-03-03T21:33:08Z",
        "updatedAt": "2022-06-25T20:52:23Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tshepang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11643,
        "title": "is there a way to change case of field names",
        "bodyText": "Am looking to translate a logstash filter which has a kv filter using transform_key.",
        "url": "https://github.com/vectordotdev/vector/discussions/11643",
        "createdAt": "2022-03-02T13:55:46Z",
        "updatedAt": "2022-06-25T20:52:34Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tshepang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11646,
        "title": "Sink console not printing output all the time. why?",
        "bodyText": "I am running vector file like this,\n head some.jsonl | vector -c vector.yaml | jq\nNot always my sinks print console output. Only like 1 time or twice it prints the output in linux debian.\nI have 3 json objects which is very small & i am sure vector config also works because i am passing same input all the time & prints output sometimes. any help?\nVersion:\n2022-03-02T14:07:38.930885Z  INFO vector: Vector has started. debug=\"false\" version=\"0.19.0\" arch=\"x86_64\" build_id=\"da60b55 2021-12-28\"\nError:\n2022-03-02T14:07:45.815417Z ERROR sink{component_kind=\"sink\" component_id=stdout component_type=console component_name=stdout}: vector::topology: An error occurred that vector couldn't handle.\nSometimes: when i press ctrl+c\n2022-03-02T14:16:46.653847Z  INFO vector::shutdown: All sources have finished.\n2022-03-02T14:16:46.654136Z  INFO vector: Vector has stopped.\n2022-03-02T14:16:51.337853Z  INFO vector: Vector has quit.\nthread 'vector-worker' panicked at 'internal error: entered unreachable code: join error or bad poll', src/topology/builder.rs:581:30\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n2022-03-02T14:16:51.341908Z ERROR transform{component_kind=\"transform\" component_id=parse-failure-log component_type=remap component_name=parse-failure-log}: vector::topology: An error occurred that vector couldn't handle.",
        "url": "https://github.com/vectordotdev/vector/discussions/11646",
        "createdAt": "2022-03-02T14:17:52Z",
        "updatedAt": "2022-06-10T23:54:32Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "satscreate"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11551,
        "title": "Redact feature",
        "bodyText": "Is there any timeline by when the credit card filter for redact would be released?",
        "url": "https://github.com/vectordotdev/vector/discussions/11551",
        "createdAt": "2022-02-24T07:31:10Z",
        "updatedAt": "2022-09-08T14:00:43Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Sg-23"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11496,
        "title": "Redact functionality",
        "bodyText": "I am working on an implementation to redact credit card numbers, but with regex, I am getting false positives. I need to implement luhn checker for the same. Do we have any inbuilt functionality in vector or can the same be achieved with VRL? Will it require lua for the same?",
        "url": "https://github.com/vectordotdev/vector/discussions/11496",
        "createdAt": "2022-02-22T03:47:38Z",
        "updatedAt": "2022-06-28T04:15:09Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Sg-23"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11525,
        "title": "Anonymize data, how ?",
        "bodyText": "Hi there,\nWe have the requirement to anonymize incoming syslog data due to DSGVO / GDPR regulations.\nQuestion, can Vector help here, if so how ?",
        "url": "https://github.com/vectordotdev/vector/discussions/11525",
        "createdAt": "2022-02-23T07:30:05Z",
        "updatedAt": "2022-07-20T13:46:36Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "StefanSa"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10881,
        "title": "Vector config for better collecting metrics perfomance",
        "bodyText": "Hello. I'm testing vector as a replacement for promtail and experienced some performance problems with straightforward configuration. Logs are collected and sent to loki from nginx in a JSON format with ~1500 lines per second rate.\nHere is part of the example message:\n\u200b\n{\n  \"msec\": \"1642071602.041\",\n  \"bytes_sent\": \"14372\",\n  \"request_uri\": \"/some-url\",\n  \"http_host\": \"some.host.example\",\n  \"server_name\": \"some.server\",\n  \"server_port\": \"443\",\n  \"request_time\": \"0.001\",\n  \"request_method\": \"GET\",\n  \"cdn_upstream\": \"someupstream\",  \n  \"upstream_addr\": \"127.0.0.1:8081\",\n  \"upstream_status\": \"200\",\n  \"upstream_cache_status\": \"EXPIRED\"\n}\n\u200b\nAgent should export metrics from logs to prometheus with labels cdn_upstream, status, upstream_addr, upstream_cache_status, upstream_status:\n\nrequest counter\nbytes_sent counter\nrequest_time histogram\n\u200b\nHere is promtail config:\n\u200b\n\nserver:\n  http_listen_port: 30200\n  grpc_listen_address: 127.0.0.1\n  grpc_listen_port: 30201\n\u200b\npositions:\n  filename: /tmp/positions.yaml\n\u200b\nclients:\n  - url: http://loki:3100/loki/api/v1/push\n\u200b\n\u200b\nscrape_configs:\n    - job_name: var_log\n      pipeline_stages:\n      - json:\n          expressions:\n            timestamp: msec\n            request_time: request_time\n            bytes_sent: bytes_sent\n            status: status\n            upstream_status: upstream_status\n            upstream_addr: upstream_addr\n            cdn_upstream: cdn_upstream\n            upstream_cache_status: upstream_cache_status\n      - regex:\n          source: timestamp\n          expression: \"(?P<timestamp>[0-9]+)\"\n      - timestamp:\n          source: timestamp\n          format: Unix\n          action_on_failure: fudge\n      - labels:\n          cdn_upstream:\n          status:\n          upstream_addr:\n          upstream_status:\n          upstream_cache_status:\n      - metrics:\n          nginx_bytes_sent:\n            type: Counter\n            description: \"total bytes sent\"\n            prefix: promtail_custom_\n            max_idle_duration: 10m\n            source: bytes_sent\n            config:\n              action: add\n          nginx_request_time:\n            type: Histogram\n            description: \"request time ms\"\n            prefix: promtail_custom_\n            max_idle_duration: 10m\n            source: request_time\n            config:\n              buckets: [0.050,0.100,0.500,0.800,1.0,1.5,2.0,5.0]\n          nginx_request:\n            type: Counter\n            description: \"request\"\n            prefix: promtail_custom_\n            max_idle_duration: 10m\n            config:\n              match_all: true\n              action: inc\n      static_configs:\n      - targets:\n        - localhost\n        labels:\n          job: nginx_access_log\n          host: myhost\n          agent: promtail\n          __path__: /var/log/nginx/*_json.log\n\u200b\nHere is a straightforward vector config:\n\u200b\n[sources.var_json_log]\ntype = \"file\"\nread_from = \"end\"\ninclude = [ \"/var/log/nginx/*_json.log\" ]\n\u200b\n\u200b\n[transforms.parser]\ntype = \"remap\"\ninputs = [ \"var_json_log\", \"data_json_log\" ]\nsource = \"\"\"\n. |= parse_json!(string!(.message))\ndel(.message)\n\"\"\"\n\u200b\n\u200b\n[transforms.meter]\ntype = \"log_to_metric\"\ninputs = [ \"parser\" ]\n\u200b\n  [[transforms.meter.metrics]]\n  field = \"bytes_sent\"\n  name = \"bytes_sent\"\n  type = \"counter\"\n  increment_by_value = true\n  namespace = \"vector_custom_nginx\"\n\u200b\n    [transforms.meter.metrics.tags]\n    cdn_upstream = \"{{ cdn_upstream }}\"\n    status = \"{{ status }}\"\n    upstream_addr = \"{{ upstream_addr }}\"\n    upstream_cache_status = \"{{ upstream_cache_status }}\"\n    upstream_status = \"{{ upstream_status }}\"\n\u200b\n  [[transforms.meter.metrics]]\n  field = \"request_time\"\n  name = \"request_time\"\n  type = \"histogram\"\n  increment_by_value = true\n  namespace = \"vector_custom_nginx\"\n\u200b\n    [transforms.meter.metrics.tags]\n    cdn_upstream = \"{{ cdn_upstream }}\"\n    status = \"{{ status }}\"\n    upstream_addr = \"{{ upstream_addr }}\"\n    upstream_cache_status = \"{{ upstream_cache_status }}\"\n    upstream_status = \"{{ upstream_status }}\"\n\u200b\n  [[transforms.meter.metrics]]\n  field = \"msec\"\n  name = \"request\"\n  type = \"counter\"\n  namespace = \"vector_custom_nginx\"\n\u200b\n    [transforms.meter.metrics.tags]\n    cdn_upstream = \"{{ cdn_upstream }}\"\n    status = \"{{ status }}\"\n    upstream_addr = \"{{ upstream_addr }}\"\n    upstream_cache_status = \"{{ upstream_cache_status }}\"\n    upstream_status = \"{{ upstream_status }}\"\n\u200b\n\u200b\n[sinks.loki]\ntype = \"loki\"\ninputs = [ \"parser\" ]\nendpoint = \"http://loki:3100\"\nencoding = \"json\"\n\u200b\n  [sinks.loki.labels]\n  cdn_upstream = \"{{ cdn_upstream }}\"\n  status = \"{{ status }}\"\n  upstream_addr = \"{{ upstream_addr }}\"\n  upstream_cache_status = \"{{ upstream_cache_status }}\"\n  upstream_status = \"{{ upstream_status }}\"\n\u200b\n\u200b\n[sinks.prometheus]\ntype = \"prometheus_exporter\"\ninputs = [ \"meter\" ]\naddress = \"0.0.0.0:30201\"\nbuckets = [0.050,0.100,0.500,0.800,1.0,1.5,2.0,5.0]\n\u200b\nWith this config, vector is consuming ~1.5 - 2 times more cpu than promtail. In my opinion, it happens because vector generates 3 metric events for one log event. So I've added reduce transformer for aggregating parameters and lua transformer for generating proper histogram metric event.\n\u200b\n\u200b\n[transforms.parser]\ntype = \"remap\"\ninputs = [ \"var_json_log\", \"data_json_log\" ]\ndrop_on_abort = true\ndrop_on_error = true\nreroute_dropped = true\nsource = \"\"\"\n._let_me_flush = floor(to_float(to_unix_timestamp!(.timestamp)) / 10 ?? 0)\n. |= object!(parse_json!(string!(.message)))\ndel(.message)\n._i = 1\n._bytes_sent = to_int(.bytes_sent) ?? 0\n._request_time = to_float(.request_time) ?? 0.0\n\"\"\"\n\u200b\n\u200b\n[transforms.reducer]\ntype = \"reduce\"\ninputs = [\"parser\"]\nexpire_after_ms = 8000\ngroup_by = [\n  \"_let_me_flush\",\n  \"cdn_upstream\",\n  \"file\",\n  \"status\",\n  \"upstream_addr\",\n  \"upstream_cache_status\",\n  \"upstream_status\"\n]\n\u200b\n  [transforms.reducer.merge_strategies]\n  _request_time = \"array\"\n\u200b\n\u200b\n[transforms.histogram]\ntype = \"lua\"\nversion = \"2\"\ninputs = [ \"reducer\" ]\n\u200b\n  [transforms.histogram.hooks]\n  process = \"\"\"\n  function (event, emit)\n    local freq = {}\n\u200b\n    for _, v in ipairs(event.log._request_time) do\n      freq[v] = (freq[v] or 0) + 1\n    end\n\u200b\n    local sample_rates = {}\n    local values = {}\n    for k, v in  pairs(freq) do\n      table.insert(sample_rates, v)\n      table.insert(values, k + 0.0)\n    end\n\u200b\n    emit({ \n      metric = {\n        name = \"request_time\",\n        namespace = \"vector_custom_nginx\",\n        tags = {\n          cdn_upstream = event.log.cdn_upstream,\n          file = event.log.file,\n          status = event.log.status,\n          upstream_addr = event.log.upstream_addr,\n          upstream_cache_status = event.log.upstream_cache_status,\n          upstream_status =  event.log.upstream_status\n        },\n        timestamp = event.log.timestamp,\n        kind = \"incremental\",\n        distribution = {\n          sample_rates = sample_rates,\n          values = values,\n          statistic = \"histogram\"\n        }\n      }\n    })\n  end\n  \"\"\"\n\u200b\n\u200b\n[transforms.meter]\ntype = \"log_to_metric\"\ninputs = [ \"reducer\" ]\n\u200b\n  [[transforms.meter.metrics]]\n  field = \"_bytes_sent\"\n  name = \"bytes_sent\"\n  type = \"counter\"\n  increment_by_value = true\n  namespace = \"vector_custom_nginx\"\n\u200b\n    [transforms.meter.metrics.tags]\n    cdn_upstream = \"{{ cdn_upstream }}\"\n    file = \"{{ file }}\"\n    status = \"{{ status }}\"\n    upstream_addr = \"{{ upstream_addr }}\"\n    upstream_cache_status = \"{{ upstream_cache_status }}\"\n    upstream_status = \"{{ upstream_status }}\"\n\u200b\n  [[transforms.meter.metrics]]\n  field = \"_i\"\n  name = \"request\"\n  type = \"counter\"\n  increment_by_value = true\n  namespace = \"vector_custom_nginx\"\n\u200b\n    [transforms.meter.metrics.tags]\n    cdn_upstream = \"{{ cdn_upstream }}\"\n    file = \"{{ file }}\"\n    status = \"{{ status }}\"\n    upstream_addr = \"{{ upstream_addr }}\"\n    upstream_cache_status = \"{{ upstream_cache_status }}\"\n    upstream_status = \"{{ upstream_status }}\"\n\u200b\nWith this config, vector cpu consumption decreased to promtail level.\n\u200b\nI have a couple of questions:\n\u200b\n\nIs there a better way to force reducer to emit event than generating a custom field that changes with a given interval\n\u200b\nIs there a way to generate histogram metric event from reducer without lua transformer",
        "url": "https://github.com/vectordotdev/vector/discussions/10881",
        "createdAt": "2022-01-17T12:35:45Z",
        "updatedAt": "2022-06-08T08:10:15Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "EvgeniiAl"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 4
    },
    {
        "number": 11494,
        "title": "In reduce transforms, is there a limit on array concat merge strategy?",
        "bodyText": "Currently, when you use an array concat merge strategy, I don't see a limit on the number of items you concatenate. The expire_after_ms is the only way I could find to indirectly control the the size of array, IE flush out with shorter expiry.\nI would like to check if there is any setting to explicitly set a limit on concatenated array and expire the state when we hit the array limit?",
        "url": "https://github.com/vectordotdev/vector/discussions/11494",
        "createdAt": "2022-02-21T20:54:41Z",
        "updatedAt": "2022-06-12T04:42:20Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ddayal219"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11381,
        "title": "Accessing fields that contain a special character",
        "bodyText": "json like blow, how to get the value of \"b/c\"?\n{\n  \"a\": {\n    \"b/c\": 2\n  }\n}\nI try some transforms config blow but failed:\n\n.a.b/c\n.a.'\"b/c\"'\n.a'.\"b/c\"'\n.a[\"b/c\"]\n\nmost error like unexpected end of query path\nwhen I use jq to accessing fields I succeed\n$ echo '{\"a\": {\"b/c\": 2}}' | jq .a.'\"b/c\"'\n$ 2\n$ echo '{\"a\": {\"b/c\": 2}}' | jq .a'.\"b/c\"'\n$ 2\nIn kubernetes, there are some lables with \"/\", such as \"app_kubernetes_io/name\" and \"app_kubernetes_io/component\", I do not find any method to get those values with \"/\" in key.",
        "url": "https://github.com/vectordotdev/vector/discussions/11381",
        "createdAt": "2022-02-15T09:56:36Z",
        "updatedAt": "2024-04-03T12:42:46Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "yxyx520"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11399,
        "title": "looks like version 1 of Vector source/sink is still supported",
        "bodyText": "Looking at https://vector.dev/highlights/2021-08-24-vector-source-sink, it is said there would be no need to set version on Vector source/sink by now, but I see Vector still supports such. Is it an oversight?",
        "url": "https://github.com/vectordotdev/vector/discussions/11399",
        "createdAt": "2022-02-15T20:24:52Z",
        "updatedAt": "2022-02-15T21:11:58Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tshepang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11390,
        "title": "should non-existent /var/lib/vector/ be created automatically",
        "bodyText": "I get this when I run vector on fresh manual install:\n\ndata_dir \"/var/lib/vector/\" does not exist\n\nI expected this to be created automatically. I suspect the other installs (like yum and apt) do create such?",
        "url": "https://github.com/vectordotdev/vector/discussions/11390",
        "createdAt": "2022-02-15T16:42:21Z",
        "updatedAt": "2022-06-19T15:40:26Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tshepang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 11333,
        "title": "Route Tranform Behavior change in v20",
        "bodyText": "I am noticing a behavior change in the \"route\" transform between v19.1 and v20.\nConfig:\n[transforms.common_log_processor]\n  type = \"remap\"\n  inputs = [\"log_source\"]\n  source = '''\n    <some common GROK'ing>\n  '''\n\n[transforms.log_event_router]\n  type = \"route\"\n  inputs = [\"common_log_processor\"]\n  [transforms.log_event_router.route]\n    foo = \".program == 'foo'\" # route 1\n    bar = \".program == 'bar'\" # route 2\n\n[transforms.foo_log_processor]\n  inputs = [\"log_event_router.foo\"]\n  type = \"remap\"\n  source = '''\n    <some stuff>\n  '''\n\n[transforms.bar_log_processor]\n  inputs = [\"log_event_router.bar\"]\n  type = \"remap\"\n  source = '''\n    <some stuff>\n  '''\n\nNow when I have a unit test which tests only route1 i.e insert_at=common_log_processor  and extract_from=foo_log_processor,\n(i.e a logevent that follows this route : common_log_processor->log_event_router.foo->foo_log_processor)\nthe test generates WARNINGS like:\n2022-02-11T08:02:49.100665Z  WARN vector::config::builder: Transform \"log_event_router.bar\" has no consumers. (even though bar_log_processor is a consumer of log_event_router.bar, its just that this test log event doesn't take that route)\n@jszwedko",
        "url": "https://github.com/vectordotdev/vector/discussions/11333",
        "createdAt": "2022-02-11T08:45:37Z",
        "updatedAt": "2022-06-27T16:12:13Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "atibdialpad"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 5
        },
        "upvoteCount": 2
    },
    {
        "number": 11276,
        "title": "Syslog parsing not getting fields in message",
        "bodyText": "Hi, this is probably something really obvious, but I'm not seeing it.  Hopefully it's not just a stupid typo like my last question.  ;)\nI've got a configuration going where I have syslogd running on a machine, outputting to both a file and to localhost port 8514.  Relevant config snippets follow:\n[sources.local_syslog]\n  type = \"syslog\"\n  max_length = 102_400\n  address = \"127.0.0.1:8514\"\n  mode = \"udp\"\n\n[transforms.parsed_syslog]\n  inputs = [\"local_syslog\"]\n  type = \"remap\"\n  source = '''\n    ., err = parse_syslog(.message)\n    .source = \"syslog-logging\"\n  '''\n\n[sinks.print]\n  type = \"console\"\n  inputs = [\n    \"parsed_syslog\"\n  ]\n  encoding.codec = \"json\"\n\nNow here's where things get a little weird.\nWhen I get a message directly from syslog, vector produces only the following output (tcpdump for completeness):\nFeb  9 19:30:00 logging cron.info crond[12372]: USER root pid 4650 cmd run-parts /etc/periodic/15min\n\n\n{\"source\":\"syslog-logging\"}\n\n\n19:30:00.477085 IP (tos 0x0, ttl 64, id 63620, offset 0, flags [DF], proto UDP (17), length 115)\n    xxx.aurora.dev.53203 > xxx.aurora.dev.8514: [bad udp cksum 0xfe72 -> 0x53c8!] UDP, length 87\n\t0x0000:  4500 0073 f884 4000 4011 43f3 7f00 0001  E..s..@.@.C.....\n\t0x0010:  7f00 0001 cfd3 2142 005f fe72 3c37 383e  ......!B._.r<78>\n\t0x0020:  4665 6220 2039 2031 393a 3330 3a30 3020  Feb..9.19:30:00.\n\t0x0030:  6372 6f6e 645b 3132 3337 325d 3a20 5553  crond[12372]:.US\n\t0x0040:  4552 2072 6f6f 7420 7069 6420 3436 3530  ER.root.pid.4650\n\t0x0050:  2063 6d64 2072 756e 2d70 6172 7473 202f  .cmd.run-parts./\n\t0x0060:  6574 632f 7065 7269 6f64 6963 2f31 356d  etc/periodic/15m\n\t0x0070:  696e 0a                                  in.\n\nWhen I then paste the very same line back into the syslog with the 'logger' util:\nFeb  9 19:30:31 logging user.notice root: Feb  9 19:30:00 logging cron.info crond[12372]: USER root pid 4650 cmd run-parts /etc/periodic/15min\n\n\n{\"appname\":\"cron.info\",\"hostname\":\"logging\",\"message\":\"crond[12372]: USER root pid 4650 cmd run-parts /etc/periodic/15min\",\"source\":\"syslog-logging\",\"timestamp\":\"2022-02-09T19:30:00Z\"}\n\n\n19:30:31.779986 IP (tos 0x0, ttl 64, id 1886, offset 0, flags [DF], proto UDP (17), length 155)\n    xxx.aurora.dev.53203 > xxx.aurora.dev.8514: [bad udp cksum 0xfe9a -> 0xf895!] UDP, length 127\n\t0x0000:  4500 009b 075e 4000 4011 34f2 7f00 0001  E....^@.@.4.....\n\t0x0010:  7f00 0001 cfd3 2142 0087 fe9a 3c31 333e  ......!B....<13>\n\t0x0020:  4665 6220 2039 2031 393a 3330 3a33 3120  Feb..9.19:30:31.\n\t0x0030:  726f 6f74 3a20 4665 6220 2039 2031 393a  root:.Feb..9.19:\n\t0x0040:  3330 3a30 3020 6c6f 6767 696e 6720 6372  30:00.logging.cr\n\t0x0050:  6f6e 2e69 6e66 6f20 6372 6f6e 645b 3132  on.info.crond[12\n\t0x0060:  3337 325d 3a20 5553 4552 2072 6f6f 7420  372]:.USER.root.\n\t0x0070:  7069 6420 3436 3530 2063 6d64 2072 756e  pid.4650.cmd.run\n\t0x0080:  2d70 6172 7473 202f 6574 632f 7065 7269  -parts./etc/peri\n\t0x0090:  6f64 6963 2f31 356d 696e 0a              odic/15min.\n\nIs there something about the way the parse_syslog() function works that the format of my log file isn't playing nicely with?\nWhat's the best way to solve this?  Write my own regex in VRL?",
        "url": "https://github.com/vectordotdev/vector/discussions/11276",
        "createdAt": "2022-02-09T19:51:47Z",
        "updatedAt": "2022-08-07T13:42:53Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ilcylic"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11270,
        "title": "Elasticsearch source",
        "bodyText": "Does Vector have an Elasticsearch source option like the Logstash Elastic input plugin (https://www.elastic.co/guide/en/logstash/current/plugins-inputs-elasticsearch.html)? If not, is there a plan to support this?",
        "url": "https://github.com/vectordotdev/vector/discussions/11270",
        "createdAt": "2022-02-09T14:36:04Z",
        "updatedAt": "2022-07-06T09:54:44Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "sundara20"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11248,
        "title": "How to handle a json array log",
        "bodyText": "Input:\nsample.jsonl\n[{\"id\":\"1\",\"val\":\"1.0\"},{\"id\":\"2\",\"val\":\"2.0\"}]\nvector.yaml:\nsources:\nstdin:\ntype: stdin\ndecoding:\ncodec: json\ntransforms:\npatch:\ntype: remap\ninputs:\n- stdin\ndrop_on_error: true\nsource: |-\nid= string!(.id)  <---------- This reads only ID. How to read whole object & each json object one by one and store like id\nlog(\"json message \" + id , level: \"warn\")   <----------- This prints ID 1 but How to get the whole JSON first line Object?\nsinks:\nstdout:\ntype: console\ninputs:\n- patch\ntarget: stdout\nencoding:\ncodec: json\nRunning :\njq -c '.[0]' sample.jsonl | vector -c vector.yaml\nHow to deal with base source file as JSON array input and how to read the whole json object (1st object) and keep it in memory?",
        "url": "https://github.com/vectordotdev/vector/discussions/11248",
        "createdAt": "2022-02-08T12:36:21Z",
        "updatedAt": "2022-07-14T03:53:35Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "satscreate"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11149,
        "title": "getting 403 for kubernetes_logs from kube apiserver | AWS EKS",
        "bodyText": "Hello,\nI am trying to deploy vector helm chart in AWS EKS.\nInstalling via the command from Readme:\nhelm install vector vector/vector -n vector -f values.yaml\nThere is a custom config in the values.yaml:\ncustomConfig:\n   data_dir: /vector-data-dir\n   sources:\n     kubernetes_logs:\n       type: kubernetes_logs\n       self_node_name: ${VECTOR_SELF_NODE_NAME:-unspecified}\n     internal_metrics:\n       type: internal_metrics\n   sinks:\n     s3_archives:\n       inputs:\n         - kubernetes_logs\n       type: aws_s3\n       auth:\n         access_key_id: \"ACCESS_KEY_ID\"\n         secret_access_key: \"SECRET_ACCESS_KEY\"\n       region: us-east-1\n       bucket: vector-k8s-logs\n       key_prefix: date=%Y-%m-%d\n       compression: gzip\n       encoding: ndjson\n\nS3 connection is OK. But I got the 403 error for kubernetes_logs component from kube apiserver:\n\n2022-02-02T15:10:32.392470Z DEBUG source{component_kind=\u201csource\u201d component_id=kubernetes_logs component_type=kubernetes_logs component_name=kubernetes_logs}:http: vector::internal_events::http_client: Sending HTTP request. uri=https://10.100.0.1:443/api/v1/namespaces?&allowWatchBookmarks=true&resourceVersion=0&timeoutSeconds=290&watch=true method=GET version=HTTP/1.1 headers={\u201cauthorization\u201d: Sensitive, \u201cuser-agent\u201d: \u201cVector/0.19.0 (x86_64-unknown-linux-gnu da60b55 2021-12-28)\u201c, \u201caccept-encoding\u201d: \u201cidentity\u201d} body=[empty]\n2022-02-02T15:10:32.398595Z DEBUG source{component_kind=\u201csource\u201d component_id=kubernetes_logs component_type=kubernetes_logs component_name=kubernetes_logs}:http: vector::internal_events::http_client: HTTP response. status=403 Forbidden version=HTTP/1.1 headers={\u201caudit-id\u201d: \u201ca1aef5f6-0f7c-4324-98f6-93714f318598\u201d, \u201ccache-control\u201d: \u201cno-cache, private\u201d, \u201ccontent-type\u201d: \u201capplication/json\u201d, \u201cx-content-type-options\u201d: \u201cnosniff\u201d, \u201cdate\u201d: \u201cWed, 02 Feb 2022 15:10:32 GMT\u201d, \u201ccontent-length\u201d: \u201c291\u201d} body=[291 bytes]\n\nWhat am I missing in the configuration and how to resolve the 403 error?\nThe k8s cluster consists of 6 nodes, maybe the ${VECTOR_SELF_NODE_NAME} variable is wrong? Please advise.\nThank you in advance.",
        "url": "https://github.com/vectordotdev/vector/discussions/11149",
        "createdAt": "2022-02-02T16:55:23Z",
        "updatedAt": "2022-06-16T06:33:38Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "burlesque13"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11155,
        "title": "Rolling file logs ?",
        "bodyText": "I see there's a file sink\nWhat's the recommended approach for rolling logs ? Is there anything built into vector or best to use logrotate ?",
        "url": "https://github.com/vectordotdev/vector/discussions/11155",
        "createdAt": "2022-02-02T18:13:54Z",
        "updatedAt": "2022-08-24T10:32:42Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "jknight"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11099,
        "title": "VECTOR_WATCH_CONFIG does not work with VECTOR_CONFIG_DIR",
        "bodyText": "Is this intentional, or an oversight (or am I missing something)?\n\u276f vector --version\nvector 0.19.1 (x86_64-unknown-linux-gnu 3cf70cf 2022-01-25)",
        "url": "https://github.com/vectordotdev/vector/discussions/11099",
        "createdAt": "2022-01-31T07:00:25Z",
        "updatedAt": "2022-06-15T07:32:27Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tshepang"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 0
    },
    {
        "number": 11058,
        "title": "Have event as a json string?",
        "bodyText": "Is there a method where I can take a journald event and encode into a string formatted as json so I can save the whole event into a single clickhouse string field?",
        "url": "https://github.com/vectordotdev/vector/discussions/11058",
        "createdAt": "2022-01-27T08:21:35Z",
        "updatedAt": "2022-09-02T14:20:22Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "daledude"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 11062,
        "title": "Json Parser - Parsing multiple values into Nested JSON",
        "bodyText": "I'm trying to parse Envoy logs from a Docker container, however, after parsing the fields that start with the same name, I do not get nested JSON as I'd expect.\nvector.yaml\nsources:\n  docker:\n    type: docker_logs\n    host_key: host\n    include_containers:\n      - envoy\ntransforms:\n  json_parser:\n    inputs:\n      - docker\n    type: json_parser\n    field: message\nsinks:\n  console:\n    type: console\n    inputs:\n      - geo\n    target: stdout\n    encoding: json\n\nconsole output\n{\n    \"container_created_at\": \"2022-01-27T14:38:47.063289449Z\",\n    \"container_id\": \"83f1a2376b9a5b475xxxxxxxxxxxxxxxxxxx\",\n    \"container_name\": \"envoy\",\n    \"envoy.downstream_address\": \"172.xx.xxx.xxx\",\n    \"envoy.request_duration\": 0,\n    \"envoy.response_duration\": 2,\n    \"envoy.upstream_address\": \"127.0.0.1:9901\",\n    \"envoy.upstream_failure_reason\": null,\n    \"host\": \"prelive-envoy-f2\",\n    \"http.method\": \"GET\",\n    \"http.path\": \"/metrics\",\n    \"http.protocol\": \"HTTP/1.1\",\n    \"http.request_duration\": 0,\n    \"http.request_length\": 0,\n    \"http.request_start_time\": \"2022-01-27T16:26:07.170Z\",\n    \"http.response_code\": 200,\n    \"http.response_code_details\": \"via_upstream\",\n    \"http.response_duration\": 359,\n    \"http.response_flags\": \"-\",\n    \"http.response_length\": 3152815,\n    \"http.total_duration\": 361,\n    \"image\": \"envoyproxy/envoy:v1.20.1\",\n    \"query_user_agent\": \"Prometheus/2.28.1\",\n    \"source_type\": \"docker\",\n    \"stream\": \"stdout\",\n    \"timestamp\": \"2022-01-27T16:26:10.960218575Z\"\n}\n\nI was expecting this output:\n{\n    \"container_created_at\": \"2022-01-27T14:38:47.063289449Z\",\n    \"container_id\": \"83f1a2376b9a5b475xxxxxxxxxxxxxxxxxxx\",\n    \"container_name\": \"envoy\",\n    \"envoy\": {\n      \"downstream_address\": \"172.xx.xxx.xxx\",\n      \"request_duration\": 0,\n      ...\n    },\n    \"host\": \"prelive-envoy-f2\",\n    \"http\": {\n      \"method\": \"GET\",\n      \"path\": \"/metrics\",\n      ...\n    },\n    \"image\": \"envoyproxy/envoy:v1.20.1\",\n    \"query_user_agent\": \"Prometheus/2.28.1\",\n    \"source_type\": \"docker\",\n    \"stream\": \"stdout\",\n    \"timestamp\": \"2022-01-27T16:26:10.960218575Z\"\n}\n\nAny suggestions on how I'd go about nesting those fields?\nMany thanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/11062",
        "createdAt": "2022-01-27T16:32:12Z",
        "updatedAt": "2022-06-13T22:48:48Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "JoeAshworth"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10892,
        "title": "Performance issues, comparison between vector and filebeat",
        "bodyText": "Hi there,\ni am currently migrating some filebeat modules to vector.\nI have noticed that vector causes more than 100% system load than filebeat with the same module, filebeat about 15%  and vector 190%.\nI would like to understand why this is so.\nWhat can be the cause, too many remap or where is the bottleneck here ?\nIs there a debug mode similar to vector tap where you can see what system load each module of vector is causing ?\nsystemload:\n\nvector top:\n\nEdit:\nOk, i made a mistake and was a bit too quick with the question here. Because the ingester nodes ultimately do the work and not Filebeat (pipelines). Nevertheless, 190% system load seems a bit too high for two firewalls that deliver data via syslog.",
        "url": "https://github.com/vectordotdev/vector/discussions/10892",
        "createdAt": "2022-01-18T14:33:45Z",
        "updatedAt": "2022-08-09T21:35:47Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "StefanSa"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 2
    },
    {
        "number": 10928,
        "title": "opensearch error ink{component_kind=\"sink\" component_id=es_cluster component_type=elasticsearch component_name=es_cluste",
        "bodyText": "[sources.my_docker_logs_source]\ntype = \"docker_logs\"\ndocker_host =\"/var/run/docker.sock\"\ninclude_containers = [ \"include_\",\"xxxx\"]\nParse Syslog logs\nSee the Vector Remap Language reference for more info: https://vrl.dev\nPrint parsed logs to stdouti\n#[sinks.out]\n#inputs = [\"json_parser\"]\n#type = \"console\"\n#encoding.codec = \"json\"\n[transforms.json_parser]\ntype   = \"json_parser\"\ninputs = [\"my_docker_logs_source\"]\ndrop_field = true\n[sinks.es_cluster]\ntype = \"elasticsearch\"\ninputs = [ \"my_docker_logs_source\"]\nendpoint = \"https://sxxxxx\"\nhealthcheck = false\nauth.strategy = \"basic\"\nauth.user = \"admin\"\nauth.password = \"xxxxxxx\"\nVector's GraphQL API (disabled by default)\nUncomment to try it out with the vector top command or\nin your browser at http://localhost:8686\n#[api]\n---- ERROR --\nJan 20 11:01:04 ip-10-82-53-212.ap-southeast-1.compute.internal vector[26413]: 2022-01-20T11:01:04.352245Z ERROR sink{component_kind=\"sink\" component_id=es_cluster component_type=elasticsearch component_name=es_cluster}:request{request_id=330}: vector::sinks::elasticsearch::service: Response contained errors. response=Response { status: 200, version: HTTP/1.1, headers: {\"date\": \"Thu, 20 Jan 2022 11:01:04 GMT\", \"content-type\": \"application/json; charset=UTF-8\", \"content-length\": \"1018\", \"connection\": \"keep-alive\", \"access-control-allow-origin\": \"*\"}, body: b\"{\"took\":6,\"errors\":true,\"items\":[{\"index\":{\"_index\":\"vector-2022.01.20\",\"_type\":\"_doc\",\"_id\":\"77cld34BScMf1AAkqgHY\",\"status\":400,\"error\":{\"type\":\"illegal_argument_exception\",\"reason\":\"can't merge a non object mapping [label.com.docker.compose.project] with an object mapping\"}}},{\"index\":{\"_index\":\"vector-2022.01.20\",\"_type\":\"_doc\",\"_id\":\"8Lcld34BScMf1AAkqgHY\",\"status\":400,\"error\":{\"type\":\"illegal_argument_exception\",\"reason\":\"can't merge a non object mapping [label.com.docker.compose.project] with an object mapping\"}}},{\"index\":{\"_index\":\"vector-2022.01.20\",\"_type\":\"_doc\",\"_id\":\"8bcld34BScMf1AAkqgHY\",\"status\":400,\"error\":{\"type\":\"illegal_argument_exception\",\"reason\":\"can't merge a non object mapping [label.com.docker.compose.project] with an object mapping\"}}},{\"index\":{\"_index\":\"vector-2022.01.20\",\"_type\":\"_doc\",\"_id\":\"8rcld34BScMf1AAkqgHY\",\"status\":400,\"error\":{\"type\":\"illegal_argument_exception\",\"reason\":\"can't merge a non object mapping [label.com.docker.compose.project] with an object mapping\"}}}]}\" }",
        "url": "https://github.com/vectordotdev/vector/discussions/10928",
        "createdAt": "2022-01-20T11:03:06Z",
        "updatedAt": "2022-05-31T22:43:12Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "9xalex"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 10873,
        "title": "generel config, an \"include file option\" would be helpful",
        "bodyText": "hi there,\nhave this structure under /etc/vector\n/etc/vector/modules\n               \u2514\u2500\u2500\u2500sophos-xg/*.toml\n               \u2514\u2500\u2500\u2500sophos-utm/*.toml\n               \u2514\u2500\u2500\u2500cisco-asa/*.toml\n               \u2514\u2500\u2500\u2500cisco-ios/*.toml\n               \u2514\u2500\u2500\u2500juniper/*.toml\n\nand start vector as follows:\nvector -c /etc/vector/vector.toml -c /etc/vector/modules/*/*.toml\nit would be helpful if this could be simplified, with this option Include e.g. in the vector.toml.\n# Change this to use a non-default directory for Vector data storage:\n# data_dir = \"/var/lib/vector\"\n\n[api]\nenabled = true\n\nInclude /etc/vector/modules/*/*.toml\n\nWith this option you don't need to customize the services of Linux / Windows after the installation,\nbecause you can pass the config paths via Include in the *.toml file.\nWould something like this be possible ?",
        "url": "https://github.com/vectordotdev/vector/discussions/10873",
        "createdAt": "2022-01-15T09:37:15Z",
        "updatedAt": "2023-01-19T03:31:54Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "StefanSa"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10886,
        "title": "Issue with \"Exec\" source",
        "bodyText": "I'm trying to read some NATS data, but the system that's generating it wants authentication, so I'm using the \"Exec\" source to run a program that writes NATS records to STDOUT.\nMy relevant Vector config sections are:\n[sources.exec_relayer_nats]\n  type = \"exec\"\n  mode = \"streaming\"\n  framing.method = \"newline_delimited\"\n  framing.newline_delimited.max_length = 102400\n  include_stderr = true\n  decoding.codec = \"json\"\n  command = [ \"/usr/bin/nats\", \"sub\", \"--creds\", \"/etc/vector/relayer_nats.creds\", \"-s\", \"nats://westcoast.nats.backend.aurora.dev:4222i\", \"-r\", \"writelog\" ]\n\n[transforms.parsed_relayer_nats]\n  inputs = [\"exec_relayer_nats\"]\n  type = \"remap\"\n  source = '''\n  ., err = parse_json(.message)\n  if !exists(.source) {\n      .source = \"nats\"\n  }\n  '''\n\nAnd the log messages I'm getting that describe the error are:\n2022-01-17T21:39:07.313653Z DEBUG source{component_kind=\"source\" component_id=exec_relayer_nats component_type=exec component_name=exec_relayer_nats}: vector::sources::exec: Restarting streaming process.\n2022-01-17T21:39:07.313676Z DEBUG source{component_kind=\"source\" component_id=exec_relayer_nats component_type=exec component_name=exec_relayer_nats}: vector::sources::exec: Starting command run.\n2022-01-17T21:39:07.313843Z DEBUG vector::sources::exec: Start capturing stdout command output.\n2022-01-17T21:39:07.313854Z DEBUG vector::sources::exec: Start capturing stderr command output.\n2022-01-17T21:39:07.329609Z TRACE vector::codecs::framing::character_delimited: Decoding the frame. bytes_proccesed=113\n2022-01-17T21:39:07.329627Z  WARN vector::internal_events::decoder: Internal log [Failed deserializing frame.] is being rate limited.\n2022-01-17T21:39:07.329633Z DEBUG vector::sources::exec: Finished capturing stderr command output.\n2022-01-17T21:39:07.330183Z DEBUG vector::sources::exec: Finished capturing stdout command output.\n2022-01-17T21:39:07.330204Z TRACE source{component_kind=\"source\" component_id=exec_relayer_nats component_type=exec component_name=exec_relayer_nats}: vector::internal_events::exec: Executed command. command=/usr/bin/nats sub --creds /etc/vector/relayer_nats.creds -s nats://westcoast.nats.backend.aurora.dev:4222i -r writelog exit_status=1 elapsed_millis=16\n2022-01-17T21:39:07.330220Z DEBUG source{component_kind=\"source\" component_id=exec_relayer_nats component_type=exec component_name=exec_relayer_nats}: vector::sources::exec: Finished command run.\n2022-01-17T21:39:07.330228Z  WARN source{component_kind=\"source\" component_id=exec_relayer_nats component_type=exec component_name=exec_relayer_nats}: vector::sources::exec: Streaming process ended before shutdown.\n\nThe command runs on the CLI, and I can run its output through awk and the like.  I'm not sure what I'm doing worng, unless I should be using the \"Stdin\" source type, but I wasn't sure how to configure that.\nAlternatively, if there's a way to use the NATS source module directly with authentication, I'd be happy to use that instead.\nThanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/10886",
        "createdAt": "2022-01-17T22:53:38Z",
        "updatedAt": "2022-06-10T23:14:49Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ilcylic"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10822,
        "title": "Need to parse a yaml file using remap",
        "bodyText": "I am relatively new to vector. I have a use case where I want to parse a yaml file and create a table out of it. I am able to do it using lua transform but did not find a way to do the same using remap. Want to know if it is possible. Thanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/10822",
        "createdAt": "2022-01-11T09:46:29Z",
        "updatedAt": "2022-06-22T12:40:51Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "amanmahajan26"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10791,
        "title": "In the array / variable search, how most effective ?",
        "bodyText": "I have a syslog device that does not send the hostname, but the serial number. Currently i create the hostname in number of devices over several \"if\" queries.\nI would like to have this simpler if possible, but do not want to solve this via the enrichment option.\nI imagine for example a variable / array like this:\n.hosts =[\"SN01\": \"host01\", \"SN02\": \"host02\"]\nMy question now how to search in this variable / array for the SN and then assign it to the matching host ?",
        "url": "https://github.com/vectordotdev/vector/discussions/10791",
        "createdAt": "2022-01-11T07:29:33Z",
        "updatedAt": "2022-06-25T20:52:56Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "StefanSa"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10820,
        "title": "How to transform canal message? Which is flag map Array data and add field",
        "bodyText": "I have a canal message in the following format:\n{\n  \"data\": [{\n    \"create_time\": \"2022-01-09 00:00:19\",\n    \"id\": \"9204245\",\n    \"reason\": \"4\",\n    \"user_id\": \"134048732\"\n  },{\n    \"create_time\": \"2022-01-09 00:00:19\",\n    \"id\": \"9204246\",\n    \"reason\": \"4\",\n    \"user_id\": \"134048733\"\n  }],\n  \"database\": \"rc_live_chat_statistics\",\n  \"es\": 1641686418000,\n  \"id\": 8892351,\n  \"isDdl\": false,\n  \"old\": null,\n  \"pkNames\": [\"id\"],\n  \"sql\": \"\",\n  \"table\": \"rc_delete_reason\",\n  \"ts\": 1641686418914,\n  \"type\": \"INSERT\"\n}\nAnd I want to convert the message to the following format:\n[{\n    \"id\": \"9204245\",\n    \"create_time\": \"2022-01-09 00:00:19\",\n    \"feedback\": \"\",\n    \"gender\": \"1\",\n    \"reason\": \"4\",\n    \"score\": \"0\",\n    \"user_id\": \"134048732\",\n    \"_meta_data\":{\n      \"database\": \"rc_live_chat_statistics\",\n      \"table\": \"rc_delete_reason\",\n      \"ts\": 1641686418914,\n      \"type\": \"INSERT\"\n    }\n  },{\n    \"id\": \"9204246\",\n    \"create_time\": \"2022-01-09 00:00:19\",\n    \"feedback\": \"\",\n    \"gender\": \"2\",\n    \"reason\": \"4\",\n    \"score\": \"0\",\n    \"user_id\": \"134048733\"\n    \"_meta_data\":{\n      \"database\": \"rc_live_chat_statistics\",\n      \"table\": \"rc_delete_reason\",\n      \"ts\": 1641686418914,\n      \"type\": \"INSERT\"\n    }\n  }\n]\nWhat shall I do?",
        "url": "https://github.com/vectordotdev/vector/discussions/10820",
        "createdAt": "2022-01-10T13:08:59Z",
        "updatedAt": "2022-12-13T10:05:29Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "yaohuacheng"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10699,
        "title": "if expression problem, how best to solve ?",
        "bodyText": "hi there,\nhave this if expression\nif (is_string(.observer.egress.zone) && is_string(.observer.ingress.zone)) {\n     if (match_any(.observer.egress.zone, [r'^LAN', r'^DMZ', r'^VPN', r'^WiFi'])) && match_any(.observer.ingress.zone, [r'^WAN']) {\n      .network.direction = \"inbound\"\n     }\n}\n\nbut sporadically get this error message.\n2022-01-05T09:47:48.048737Z  WARN vector::internal_events::remap: Mapping failed with event. error=\"function call error for \\\"match_any\\\" at (943:987): expected \\\"string\\\", got \\\"null\\\"\" internal_log_rate_secs=30\nthread 'vector-worker' panicked at 'event will be set', src/transforms/remap.rs:234:48\n\nThe observer zone values were assigned as follows.\nif exists(.sophos.xg.src_zone_type) {\n.observer.ingress.zone = .sophos.xg.src_zone_type\n}\nif exists(.sophos.xg.dst_zone_type) {\n.observer.egress.zone = .sophos.xg.dst_zone_type\n}\n\nThere is already no \"null\" values from the syslogserver and is also backed up with this.\n. = compact(., string: true, array: true, null: true, nullish: true)\n\nWhat am i doing wrong here ?",
        "url": "https://github.com/vectordotdev/vector/discussions/10699",
        "createdAt": "2022-01-05T09:57:44Z",
        "updatedAt": "2022-06-15T09:18:48Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "StefanSa"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 10739,
        "title": "path problem, how can i solve this ?",
        "bodyText": "Hi there,\nhave this:\n[transforms.ecs_url_mapping_atp]\ninputs = [\"ecs_src_mapping_atp\"]\ntype   = \"remap\"\nsource = '''\nif exists(.sophos.xg.url) {\n   .url.original = del(.sophos.xg.url)\n}\n. = parse_url!(.url.original)\n.url.domain = del(.host)\n.url.path = del(.path)\n.url.scheme = del(.scheme)\n.url.fragment = del(.fragment)\n.url.password = del(.password)\n.url.username = del(.username)\n.url.port = del(.xg.port)\n.url.query = del(.query)\n\n[transforms.geo_mapping_src_atp]\ntype = \"geoip\"\ninputs = [\"ecs_url_mapping_atp\"]\ndatabase = \"/etc/vector/GeoIP/GeoLite2-City.mmdb\"\nsource = \"source.ip\"\ntarget = \"source.geo\"\n\nWith this construct, vector aborts with the error message that it cannot find the field for .source.ip (geoip)\nand that the values of parce_url are make complete the json value defect.\ni have understood that this is a path problem, for example, if i set the path as in the master pipeline,\n. = set!(., path: [\"sophos\", \"xg\"], data: parse_url!(.url.original))\nthe previous events with this path will be deleted / missing.\nHowever the value of the parse_url is correct and also the json result.\nThe question is, what is the correct value of the path for parce_url function?",
        "url": "https://github.com/vectordotdev/vector/discussions/10739",
        "createdAt": "2022-01-06T15:37:24Z",
        "updatedAt": "2022-06-01T08:48:26Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "StefanSa"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 1
    },
    {
        "number": 10726,
        "title": "How do i handle an array ?",
        "bodyText": "Hi there,\nhave this here currently:\nif exists(.source.ip) {\n .related.ip = [.source.ip]\n}\nif exists(.destination.ip) {\n  .related.ip = [.destination.ip]\n}\n\n.related.ipcontains the array to be filled with .source.ipand .destination.ip.\nHow do i do that ?",
        "url": "https://github.com/vectordotdev/vector/discussions/10726",
        "createdAt": "2022-01-05T13:23:03Z",
        "updatedAt": "2023-03-07T07:20:22Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "StefanSa"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10682,
        "title": "converting bytes to UTF-8 string. error=invalid utf-8",
        "bodyText": "Hi there,\nsporadically get this error message for source \"syslog\":\n022-01-04T14:10:32.047522Z  INFO vector::app: API is disabled, enable by setting `api.enabled` to `true` and use commands like `vector top`.\n^@^@2022-01-04T14:13:36.684758Z ERROR source{component_kind=\"source\" component_id=syslog component_type=syslog component_name=syslog}: vector::internal_events::syslog: Error converting bytes to UTF-8 string. error=invalid utf-8 sequence of 1 bytes from index 1657 internal_log_rate_secs=10\n2022-01-04T14:13:36.684827Z  WARN source{component_kind=\"source\" component_id=syslog component_type=syslog component_name=syslog}: vector::internal_events::decoder: Failed deserializing frame. error=invalid utf-8 sequence of 1 bytes from index 1657 internal_log_rate_secs=10\n2022-01-04T14:13:36.684849Z ERROR source{component_kind=\"source\" component_id=syslog component_type=syslog component_name=syslog}: vector::internal_events::syslog: Error reading datagram. error=ParsingError(Utf8Error { valid_up_to: 1657, error_len: Some(1) }) internal_log_rate_secs=10\n^@^@2022-01-04T14:15:26.539675Z ERROR source{component_kind=\"source\" component_id=syslog component_type=syslog component_name=syslog}: vector::internal_events::syslog: Error converting bytes to UTF-8 string. error=invalid utf-8 sequence of 1 bytes from index 1642 internal_log_rate_secs=10\n2022-01-04T14:15:26.539719Z  WARN source{component_kind=\"source\" component_id=syslog component_type=syslog component_name=syslog}: vector::internal_events::decoder: Failed d\n\nIs this a general network problem ?",
        "url": "https://github.com/vectordotdev/vector/discussions/10682",
        "createdAt": "2022-01-04T14:33:56Z",
        "updatedAt": "2022-06-08T11:11:41Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "StefanSa"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10631,
        "title": "prefix for path in a transform object, how ?",
        "bodyText": "Hi there,\nIs it possible to give a prefix for the path during logfile parsing, e.g. parse_syslog.\nA prefix of .sophos.xg. will then become .sophos.xg.hostname during parsing.\nThe general question is, is it possible to dynamically generate the path with a path prefix ?\nExample kv filter:\n# split Sophos-XG fields\n- kv:\n    field: log.original\n    field_split: \" (?=[a-zA-Z0-9\\\\_\\\\-]+=)\"\n    value_split: \"=\"\n    prefix: \"sophos.xg.\"\n    ignore_missing: true\n    ignore_failure: false\n    trim_value: \"\\\"",
        "url": "https://github.com/vectordotdev/vector/discussions/10631",
        "createdAt": "2021-12-30T08:00:15Z",
        "updatedAt": "2022-11-01T10:12:12Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "StefanSa"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10654,
        "title": "parse_key_value sends at the beginning of the result \"(0x0800)\":true or  \"(0x0000)\":true, why ?",
        "bodyText": "hi there,\nparse_key_value sends at the beginning of the result \"(0x0800)\":true or  \"(0x0000)\":true, why ?\n{\"(0x0800)\":true,or {\"(0x0800)\":true,\nQuestion, why is this information sent along ?",
        "url": "https://github.com/vectordotdev/vector/discussions/10654",
        "createdAt": "2022-01-03T08:30:04Z",
        "updatedAt": "2022-07-02T08:31:35Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "StefanSa"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 10634,
        "title": "to_unix_timestamp, get error \"invalid timestamp format: input contains invalid characters\"",
        "bodyText": "hi there,\ni have this here:\n.event.start = join!([.date, .time], separator: \" \")\n.event.start = parse_timestamp!(.event.start, format: \"%Y-%m-%d %H:%M:%S\")\n.nano_start = to_unix_timestamp(t'.event.start', unit: \"milliseconds\")\n\nevent.start a correct timestamp, 2021-12-30T15:59:34Zbut get this error message.\nerror[E601]: invalid timestamp\n  \u250c\u2500 :6:33\n  \u2502\n6 \u2502 .nano_start = to_unix_timestamp(t'.event.start', unit: \"milliseconds\")\n  \u2502                                 ^^^^^^^^^^^^^^^ invalid timestamp format: input contains invalid characters\n  \u2502\n  = see documentation about timestamps at https://vrl.dev/expressions/#timestamp\n  = see language documentation at https://vrl.dev",
        "url": "https://github.com/vectordotdev/vector/discussions/10634",
        "createdAt": "2021-12-30T16:06:50Z",
        "updatedAt": "2022-08-16T03:50:02Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "StefanSa"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 10482,
        "title": "If buffer event in memory or disk will write to sink before  k8s pod shutdown?",
        "bodyText": "If buffer event in memory or disk will write to sink before  k8s pod shutdown?",
        "url": "https://github.com/vectordotdev/vector/discussions/10482",
        "createdAt": "2021-12-16T08:44:36Z",
        "updatedAt": "2022-06-15T19:14:58Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "linnaname"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10552,
        "title": "Working with key_value logs when one of the keys does not have a value?",
        "bodyText": "I'm working on extracting data from an IP Tables log using VRL, but because the fields aren't always populated, VRL seems to have issues when using the parse_key_value function.\nAs an example, the line IN=eth1 OUT= MAC=fc:ff:da:47:af:13:b4:2e:99:19:12:00:00:00 SRC=10.x.x.x DST=10.x.x.x LEN=60 TOS=0x00 PREC=0x00 TTL=63 ID=62910 DF PROTO=TCP SPT=47468 DPT=9100 WINDOW=64240 RES=0x00 SYN URGP=0 with the rule\n    source: >-\n      . |= parse_key_value!(.message)\n\n      .dest_ip = .DST\n\n      .dest_port = .DPT\n\n      .src_ip = .SRC\n\n      .src_port = .SPT\n\nproduces OUT=MAC=fc:ff:da:47:af:13:b4:2e:99:19:12:00:00:00 when it should be two distinct fields, OUT with a null value and MAC with the MAC Address.  Is there anyway to tell Vector to \"skip\" a field if it's in a key-value pair but doesn't have a value?",
        "url": "https://github.com/vectordotdev/vector/discussions/10552",
        "createdAt": "2021-12-22T08:19:22Z",
        "updatedAt": "2022-06-16T15:56:52Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "proffalken"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10024,
        "title": "Using Vector as a proxy for Datadog agents",
        "bodyText": "Can Vector be used as an endpoint for filtering/proxying metrics from Datadog agents? I have customers that don't have proper proxies but want to utilize Datadog agents to still collect metrics from systems with no outbound internet access. Can Vector be used as a metric proxy for systems with Datadog agents that have no outbound internet access?",
        "url": "https://github.com/vectordotdev/vector/discussions/10024",
        "createdAt": "2021-11-14T19:14:54Z",
        "updatedAt": "2022-06-15T17:12:49Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "lognarly"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 10489,
        "title": "K8s Vector Agent w/ Sink Vector - What happens if buffer is full?",
        "bodyText": "Hi,\nWe are using a centralized deployment topology as explained here in our Kubernetes clusters.\nI would like to better understand what happens exactly on the Vector agent side (collecting container logs) when the sink type Vector is reaching the maximum allowed number of events in the buffer.\nDefault Settings:\n\nbuffer.max_events: 500\nbuffer.type: memory\nbuffer.when_full: block\n\nThe documentation says:\n\nApplies back pressure when the buffer is full. This prevents data loss, but will cause data to pile up on the edge.\n\nCan someone please explain in detail what does this mean for the K8s Vector agent? Will it stop reading events from container log files completely?\nThx a lot,\nSven",
        "url": "https://github.com/vectordotdev/vector/discussions/10489",
        "createdAt": "2021-12-16T17:17:11Z",
        "updatedAt": "2022-06-07T07:51:08Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "svenmueller"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 10164,
        "title": "Vector agent adds/unwatches the same container log file multiple times",
        "bodyText": "Hi everyone! Is it expected for a Vector agent (running in K8s) to find the same Kubernetes container log file and stop watching it over and over (we are running latest version 0.18.0)? We currently have a lot of missing Kubernetes container log events and therefore wonder if this behavior is normal.",
        "url": "https://github.com/vectordotdev/vector/discussions/10164",
        "createdAt": "2021-11-23T23:46:59Z",
        "updatedAt": "2022-08-16T09:48:11Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "svenmueller"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 10439,
        "title": "Reload configuration",
        "bodyText": "Hello,\nAs mentioned in the usage of vector, it is only looking at the file passed with the -w command to check for updates in configuration to reload, but no mention of checking other files (say in the same directory).\nIn my configuration, I have a vrl file associated with a remap transform and wanted to update some of the fields at run time. In a test that I ran, the change  I made in the vrl file were not reloaded.\nWould you guys think that this is possible?\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/10439",
        "createdAt": "2021-12-14T06:49:25Z",
        "updatedAt": "2022-08-09T04:39:34Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bmallya"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10411,
        "title": "SYSLOG with Unix-Epoch plus nano-seconds",
        "bodyText": "Hi,\nI am new to Vector and have a problem to forward syslog messages to datadog.\nWhen looking in datadog the packets have the timestamp from my vector-system, not from the device where the syslog is send from.\nThe syslog-messages look like in this format:\n<134>1 1639328927.519695901 AAA_BBB  flows src=10.221.11.141 dst=12.225.102.103 mac=00:1D:12:23:45:86 protocol=icmp type=8 pattern: 0 all\nIn my opinion the syslog-source of the vector-system has a problem to transform the \"1639328927.519695901\" into a valid timestamp.\nSo it takes the local time.\nAs you see, I tried to remap the timestamp. But for me it still seems, that the \"1639328927.519695901\" is not used for the timestamp...\nI tried this here:\n[sources.syslog]\ntype = \"syslog\"\naddress = \"0.0.0.0:514\"\nmode = \"udp\"\npath = \"/path/to/socket\"\n[transforms.remap]\ntype = \"remap\"\ninputs = [\"syslog\"]\nsource = '''\n.ddsource = \"meraki-syslog\"\n'''\n[transforms.timestamp]\ntype = \"remap\"\ninputs = [\"remap\"]\nsource = '''\n.timestamp, err = format_timestamp(.timestamp, \"%s.%f\")\n'''\n[sinks.out]\ntype = \"console\" # required\ninputs = [\"timestamp\"] # required\ntarget = \"stdout\" # optional, default\nencoding.codec = \"json\" # required\n[sinks.datadog]\nregion = \"eu\"\ntype = \"datadog_logs\" # required\ninputs = [\"timestamp\"] # required\napi_key = \"${DD_API_KEY}\" # required\ncompression = \"gzip\" # optional, default\nencoding.codec = \"json\" # required\ntls.enabled = true\ntls.verify_certificate = true # optional, default\ntls.verify_hostname = true # optional, default\nMany thanks fou your help\nJuergen",
        "url": "https://github.com/vectordotdev/vector/discussions/10411",
        "createdAt": "2021-12-13T05:52:12Z",
        "updatedAt": "2022-06-26T04:41:53Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "JM-2019"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 10197,
        "title": "Vector Routing and GEOIP",
        "bodyText": "Hey Guys,\nTrying to convert logstash to vector having a few issues with the routing transform and geoip logic.   Basically the transform below is not matching the check statements even though the fields are there i.  Yes we have a parsing map above that i have tested by deleting and adding fields. I have tested each below with vector vrl and it works fine.\nt_kfk_parsed_route_geoip:\ntype: route\ninputs: [t_kfk_parsed_route.parsesuccess]\nroute:\n\"geoipsrc\": exists(.source.ip) && !match!(.source.ip,r'(^127.0.0.1)|(^10.)|(^172.1[6-9].)|(^172.2[0-9].)|(^172.3[0-1].)|(^192.168.)|(^169.254.)|(^22[4-9]{1}.)|(^23[0-9]{1}.)|(^255.255.255.255)')\n\"geoipdst\": exists(.destination.ip) && !match!(.destination.ip,r'(^127.0.0.1)|(^10.)|(^172.1[6-9].)|(^172.2[0-9].)|(^172.3[0-1].)|(^192.168.)|(^169.254.)|(^22[4-9]{1}.)|(^23[0-9]{1}.)|(^255.255.255.255)')\n\"geoipsrcnat\": exists(.source.ip_nat) && !match!(.source.ip_nat,r'(^127.0.0.1)|(^10.)|(^172.1[6-9].)|(^172.2[0-9].)|(^172.3[0-1].)|(^192.168.)|(^169.254.)|(^22[4-9]{1}.)|(^23[0-9]{1}.)|(^255.255.255.255)')\n\"geoipdstnat\": exists(.destination.ip_nat) && !match!(.destination.ip_nat,r'(^127.0.0.1)|(^10.)|(^172.1[6-9].)|(^172.2[0-9].)|(^172.3[0-1].)|(^192.168.)|(^169.254.)|(^22[4-9]{1}.)|(^23[0-9]{1}.)|(^255.255.255.255)')\nAdditionally because of the way the geoip transform works we have to run it back through multiple geoip transforms as some logs have multiples of the fields above, ie .destination.ip and .source.ip exist in the same log message.  It would be great in geoip could be incorporated into the vector vrl remap implementation.",
        "url": "https://github.com/vectordotdev/vector/discussions/10197",
        "createdAt": "2021-11-30T10:31:01Z",
        "updatedAt": "2022-07-19T16:19:15Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tastyfrankfurt"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 9560,
        "title": "Vector cross-compiled for ppc",
        "bodyText": "Can someone guide me on cross-compiling vector for ppc? Here is what I have\n  sh-4.4# uname -a\n  Linux a8041bffb0d9 5.10.16.3-microsoft-standard-WSL2 #1 SMP Fri Apr 2 22:23:49 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\n  sh-4.4# rustup show\n  Default host: x86_64-unknown-linux-gnu\n  rustup home:  /root/.rustup\n  \n  installed toolchains\n  --------------------\n  \n  stable-powerpc-unknown-linux-gnu\n  stable-x86_64-unknown-linux-gnu (default)\n  1.53.0-x86_64-unknown-linux-gnu\n  \n  installed targets for active toolchain\n  --------------------------------------\n  \n  powerpc-unknown-linux-gnu\n  x86_64-unknown-linux-gnu\n  \n  active toolchain\n  ----------------\n  \n  1.53.0-x86_64-unknown-linux-gnu (overridden by '/root/ppc/vector/rust-toolchain')\n  rustc 1.53.0 (53cb7b09b 2021-06-17)\n  \n  sh-4.4#        \n\n\nAnd with this when I try to build vector providing a target, I run into an error.\nsh-4.4# cargo build --target=powerpc-unknown-linux-gnu\nCompiling proc-macro2 v1.0.26\nCompiling libc v0.2.98\nCompiling syn v1.0.72\nCompiling serde_derive v1.0.126\nerror: linking with cc failed: exit status: 1\n|\n= note: \"cc\" \"-m64\" \"-Wl,--eh-frame-hdr\" \"-Wl,-znoexecstack\" \"-Wl,--as-needed\" \"-L\" \"/root/.rustup/toolchains/1.53.0-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib\" \"/root/ppc/vector/target/debug/build/serde_derive-2a6d36fc55b2b4fa/build_script_build-2a6d36fc55b2b4fa.build_script_build.9gdh8nj4-cgu.0.rcgu.o\" \"/root/ppc/vector/target/debug/build/serde_derive-2a6d36fc55b2b4fa/build_script_build-2a6d36fc55b2b4fa.build_script_build.9gdh8nj4-cgu.1.rcgu.o\" \"/root/ppc/vector/target/debug/build/serde_derive-2a6d36fc55b2b4fa/build_script_build-2a6d36fc55b2b4fa.build_script_build.9gdh8nj4-cgu.10.rcgu.o\" \"/root/ppc/vector/target/debug/build/serde_derive-2a6d36fc55b2b4fa/build_script_build-2a6d36fc55b2b4fa.build_script_build.9gdh8nj4-cgu.11.rcgu.o\" \"/root/ppc/vector/target/debug/build/serde_derive-2a6d36fc55b2b4fa/build_script_build-2a6d36fc55b2b4fa.build_script_build.9gdh8nj4-cgu.12.rcgu.o\" \"/root/ppc/vector/target/debug/build/serde_derive-2a6d36fc55b2b4fa/build_script_build-2a6d36fc55b2b4fa.build_script_build.9gdh8nj4-cgu.13.rcgu.o\" \"/root/ppc/vector/target/debug/build/serde_derive-2a6d36fc55b2b4fa/build_script_build-2a6d36fc55b2b4fa.build_script_build.9gdh8nj4-cgu.14.rcgu.o\" \"/root/ppc/vector/target/debug/build/serde_derive-2a6d36fc55b2b4fa/build_script_build-2a6d36fc55b2b4fa.build_script_build.9gdh8nj4-cgu.15.rcgu.o\" \"/root/ppc/vector/target/debug/build/serde_derive-2a6d36fc55b2b4fa/build_script_build-2a6d36fc55b2b4fa.build_script_build.9gdh8nj4-cgu.2.rcgu.o\" \"/root/ppc/vector/target/debug/build/serde_derive-2a6d36fc55b2b4fa/build_script_build-2a6d36fc55b2b4fa.build_script_build.9gdh8nj4-cgu.3.rcgu.o\" \"/root/ppc/vector/target/debug/build/serde_derive-2a6d36fc55b2b4fa/build_script_build-2a6d36fc55b2b4fa.build_script_build.9gdh8nj4-cgu.4.rcgu.o\" \"/root/ppc/vector/target/debug/build/serde_derive-2a6d36fc55b2b4fa/build_script_build-2a6d36fc55b2b4fa.build_script_build.9gdh8nj4-cgu.5.rcgu.o\" \"/root/ppc/vector/target/debug/build/serde_derive-2a6d36fc55b2b4fa/build_script_build-2a6d36fc55b2b4fa.build_script_build.9gdh8nj4-cgu.6.rcgu.o\" \"/root/ppc/vector/target/debug/build/serde_derive-2a6d36fc55b2b4fa/build_script_build-2a6d36fc55b2b4fa.build_script_build.9gdh8nj4-cgu.7.rcgu.o\" \"/root/ppc/vector/target/debug/build/serde_derive-2a6d36fc55b2b4fa/build_script_build-2a6d36fc55b2b4fa.build_script_build.9gdh8nj4-cgu.8.rcgu.o\" \"/root/ppc/vector/target/debug/build/serde_derive-2a6d36fc55b2b4fa/build_script_build-2a6d36fc55b2b4fa.build_script_build.9gdh8nj4-cgu.9.rcgu.o\" \"-o\" \"/root/ppc/vector/target/debug/build/serde_derive-2a6d36fc55b2b4fa/build_script_build-2a6d36fc55b2b4fa\" \"/root/ppc/vector/target/debug/build/serde_derive-2a6d36fc55b2b4fa/build_script_build-2a6d36fc55b2b4fa.188xjvth382161et.rcgu.o\" \"-Wl,--gc-sections\" \"-pie\" \"-Wl,-zrelro\" \"-Wl,-znow\" \"-nodefaultlibs\" \"-L\" \"/root/ppc/vector/target/debug/deps\" \"-L\" \"/root/.rustup/toolchains/1.53.0-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib\" \"-Wl,--start-group\" \"-Wl,-Bstatic\" \"/root/.rustup/toolchains/1.53.0-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/libstd-b6b48477bfa8c673.rlib\" \"/root/.rustup/toolchains/1.53.0-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/libpanic_unwind-f560ec02638f7ffe.rlib\" \"/root/.rustup/toolchains/1.53.0-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/libminiz_oxide-9c8eadb7013c9e0b.rlib\" \"/root/.rustup/toolchains/1.53.0-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/libadler-8b0ec8dbdb85d0bf.rlib\" \"/root/.rustup/toolchains/1.53.0-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/libobject-ba5d5ee707c805d2.rlib\" \"/root/.rustup/toolchains/1.53.0-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/libaddr2line-55166126dbdd5e46.rlib\" \"/root/.rustup/toolchains/1.53.0-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/libgimli-c327b365eae3b2f3.rlib\" \"/root/.rustup/toolchains/1.53.0-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/libstd_detect-416439b546a0d033.rlib\" \"/root/.rustup/toolchains/1.53.0-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/librustc_demangle-2581188d29552e15.rlib\" \"/root/.rustup/toolchains/1.53.0-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/libhashbrown-da7b2635bfcce6ef.rlib\" \"/root/.rustup/toolchains/1.53.0-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/librustc_std_workspace_alloc-09200ed1945e7b2b.rlib\" \"/root/.rustup/toolchains/1.53.0-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/libunwind-223ac369b29f5000.rlib\" \"/root/.rustup/toolchains/1.53.0-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/libcfg_if-39562fe6600dd936.rlib\" \"/root/.rustup/toolchains/1.53.0-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/liblibc-9b411bb7a19f81b3.rlib\" \"/root/.rustup/toolchains/1.53.0-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/liballoc-64ea0581d80339f7.rlib\" \"/root/.rustup/toolchains/1.53.0-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/librustc_std_workspace_core-b2dbda88b377d685.rlib\" \"/root/.rustup/toolchains/1.53.0-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/libcore-2a8415a96ed1d7dc.rlib\" \"-Wl,--end-group\" \"/root/.rustup/toolchains/1.53.0-x86_64-unknown-linux-gnu/lib/rustlib/x86_64-unknown-linux-gnu/lib/libcompiler_builtins-c4d9a5b072ee3191.rlib\" \"-Wl,-Bdynamic\" \"-lgcc_s\" \"-lutil\" \"-lrt\" \"-lpthread\" \"-lm\" \"-ldl\" \"-lc\"\n= note: /root/ppc/vector/target/debug/build/serde_derive-2a6d36fc55b2b4fa/build_script_build-2a6d36fc55b2b4fa.build_script_build.9gdh8nj4-cgu.0.rcgu.o: file not recognized: File format not recognized\ncollect2: error: ld returned 1 exit status\nerror: aborting due to previous error",
        "url": "https://github.com/vectordotdev/vector/discussions/9560",
        "createdAt": "2021-10-11T13:21:57Z",
        "updatedAt": "2022-06-28T11:09:49Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "bmallya"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10391,
        "title": "Exploding the array of JSON objects",
        "bodyText": "How can we explode/unnest the array of JSON objects, for example:\n{ \"message\": \"[ { \"file\": \"/var/log/nginx/API.access.log\", \"host\": \"platform-testing-api2\", \"json\": { \"agent\": \"Go-http-client/1.1\", \"body_sent\": { \"bytes\": 35 }, \"http_version\": \"HTTP/1.1\", \"method\": \"GET\", \"referrer\": \"\", \"remote_ip\": \"172.17.0.3\", \"request_time\": 0.052, \"response_code\": 200, \"time\": \"2021-12-10T15:27:09+02:00\", \"upstream_cache_status\": \"MISS\", \"url\": \"/api/health/dino\", \"user_name\": \"\" }, \"source_type\": \"file\", \"timestamp\": \"2021-12-10T13:27:10.456924207Z\" }, { \"file\": \"/var/log/nginx/API.access.log\", \"host\": \"platform-testing-api2\", \"json\": { \"agent\": \"\", \"body_sent\": { \"bytes\": 0 }, \"http_version\": \"HTTP/1.0\", \"method\": \"GET\", \"referrer\": \"\", \"remote_ip\": \"172.18.201.21\", \"request_time\": 0.015, \"response_code\": 200, \"time\": \"2021-12-10T15:27:10+02:00\", \"upstream_cache_status\": \"\", \"url\": \"/api/health\", \"user_name\": \"\" }, \"source_type\": \"file\", \"timestamp\": \"2021-12-10T13:27:10.975833700Z\" } ]\" }\nWhen we use:\n. = parse_json!(.message)\nwe get the following:\n[{ \"file\": \"/var/log/nginx/API.access.log\", \"host\": \"platform-testing-api2\", \"json\": { \"agent\": \"Go-http-client/1.1\", \"body_sent\": { \"bytes\": 35 }, \"http_version\": \"HTTP/1.1\", \"method\": \"GET\", \"referrer\": \"\", \"remote_ip\": \"172.17.0.3\", \"request_time\": 0.052, \"response_code\": 200, \"time\": \"2021-12-10T15:27:09+02:00\", \"upstream_cache_status\": \"MISS\", \"url\": \"/api/health/dino\", \"user_name\": \"\" }, \"source_type\": \"file\", \"timestamp\": \"2021-12-10T13:27:10.456924207Z\" }, { \"file\": \"/var/log/nginx/API.access.log\", \"host\": \"platform-testing-api2\", \"json\": { \"agent\": \"\", \"body_sent\": { \"bytes\": 0 }, \"http_version\": \"HTTP/1.0\", \"method\": \"GET\", \"referrer\": \"\", \"remote_ip\": \"172.18.201.21\", \"request_time\": 0.015, \"response_code\": 200, \"time\": \"2021-12-10T15:27:10+02:00\", \"upstream_cache_status\": \"\", \"url\": \"/api/health\", \"user_name\": \"\" }, \"source_type\": \"file\", \"timestamp\": \"2021-12-10T13:27:10.975833700Z\" }]\nHow can we explode this dynamically? as we can get 1..N JSON messages inside an array.",
        "url": "https://github.com/vectordotdev/vector/discussions/10391",
        "createdAt": "2021-12-10T16:00:10Z",
        "updatedAt": "2022-06-24T17:34:50Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "omurov"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 3
        },
        "upvoteCount": 2
    },
    {
        "number": 10214,
        "title": "Datadog automatic integration",
        "bodyText": "It would be great to have Datadog integration - via an agent or via vector direct into Datadog (I think Enterprise Vector option will have that I suppose), but how. about OSS ??\nI think tighter integration with Datadog would help for example in understanding and automatic building experience over metrics generated by Vector.",
        "url": "https://github.com/vectordotdev/vector/discussions/10214",
        "createdAt": "2021-12-01T13:35:23Z",
        "updatedAt": "2022-09-22T13:59:17Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "szibis"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10321,
        "title": "Problems with nested fields",
        "bodyText": "Hello,\nThanks for the awesome project!\nI have a slight problem with using VRL with nested fields (mainly renaming).\nConfiguration\n  sources:\n    var_log_kubelet:\n      type: file\n      include:\n        - /var/log/kubelet/kubelet.ERROR\n        - /var/log/kubelet/kubelet.WARNING\n  transforms:\n    kubelet_parse:\n      type: remap\n      inputs:\n        - var_log_kubelet\n      source: |\n        . = parse_klog!(.message)\n        .@timestamp = del(.timestamp)\n    aws_ec2_metadata:\n      type: aws_ec2_metadata\n      inputs:\n        - kubelet_parse\n      fields:\n        - ami-id\n        - availability-zone\n        - instance-id\n        - local-hostname\n        - local-ipv4\n        - public-hostname\n        - public-ipv4\n        - region\n    ecs_transform:\n      type: remap\n      inputs:\n        - aws_ec2_metadata\n      source: |\n        .cloud.region = del(.region)\n        .cloud.availability_zone = del(.availability-zone)\n  sinks:\n    console:\n      type: console\n      inputs:\n        - ecs_transform\n      target: stdout\n      encoding: json\n\nPlease note the transforms.ecs_transform.source configuration.\nWhen the source contains only cloud.region = del(.region) then everything works and the field is renamed successfully.\nProblems start when I start adding other renames like .cloud.availability_zone = del(.availability-zone), which starts producing errors:\n{\"timestamp\":\"2021-12-07T15:11:30.771645Z\",\"level\":\"INFO\",\"message\":\"Log level is enabled.\",\"level\":\"\\\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=trace,rdkafka=info\\\"\",\"target\":\"vector::app\"}\n{\"timestamp\":\"2021-12-07T15:11:30.771754Z\",\"level\":\"INFO\",\"message\":\"Loading configs.\",\"paths\":\"[\\\"/etc/vector\\\"]\",\"target\":\"vector::app\"}\n{\"timestamp\":\"2021-12-07T15:11:30.797327Z\",\"level\":\"ERROR\",\"message\":\"Configuration error.\",\"error\":\"Transform \\\"ecs_transform\\\": \\n\\u001b[0m\\u001b[1m\\u001b[38;5;9merror[E701]\\u001b[0m\\u001b[1m: call to undefined variable\\u001b[0m\\n  \\u001b[0m\\u001b[34m\u250c\u2500\\u001b[0m :2:46\\n  \\u001b[0m\\u001b[34m\u2502\\u001b[0m\\n\\u001b[0m\\u001b[34m2\\u001b[0m \\u001b[0m\\u001b[34m\u2502\\u001b[0m .cloud.availability_zone = del(.availability-\\u001b[0m\\u001b[31mzone\\u001b[0m)\\n  \\u001b[0m\\u001b[34m\u2502\\u001b[0m                                              \\u001b[0m\\u001b[31m^^^^\\u001b[0m\\n  \\u001b[0m\\u001b[34m\u2502\\u001b[0m                                              \\u001b[0m\\u001b[31m\u2502\\u001b[0m\\n  \\u001b[0m\\u001b[34m\u2502\\u001b[0m                                              \\u001b[0m\\u001b[31mundefined variable\\u001b[0m\\n  \\u001b[0m\\u001b[34m\u2502\\u001b[0m                                              \\u001b[0m\\u001b[34mdid you mean \\\"true\\\"?\\u001b[0m\\n  \\u001b[0m\\u001b[34m\u2502\\u001b[0m\\n  \\u001b[0m\\u001b[34m=\\u001b[0m see language documentation at https://vrl.dev\\n\\n\\u001b[0m\\u001b[1m\\u001b[38;5;9merror[E630]\\u001b[0m\\u001b[1m: fallible argument\\u001b[0m\\n  \\u001b[0m\\u001b[34m\u250c\u2500\\u001b[0m :2:32\\n  \\u001b[0m\\u001b[34m\u2502\\u001b[0m\\n\\u001b[0m\\u001b[34m2\\u001b[0m \\u001b[0m\\u001b[34m\u2502\\u001b[0m .cloud.availability_zone = del(\\u001b[0m\\u001b[31m.availability-zone\\u001b[0m)\\n  \\u001b[0m\\u001b[34m\u2502\\u001b[0m                                \\u001b[0m\\u001b[31m^^^^^^^^^^^^^^^^^^\\u001b[0m\\n  \\u001b[0m\\u001b[34m\u2502\\u001b[0m                                \\u001b[0m\\u001b[31m\u2502\\u001b[0m\\n  \\u001b[0m\\u001b[34m\u2502\\u001b[0m                                \\u001b[0m\\u001b[31mthis expression can fail\\u001b[0m\\n  \\u001b[0m\\u001b[34m\u2502\\u001b[0m                                \\u001b[0m\\u001b[34mhandle the error before passing it in as an argument\\u001b[0m\\n  \\u001b[0m\\u001b[34m\u2502\\u001b[0m\\n  \\u001b[0m\\u001b[34m=\\u001b[0m see documentation about error handling at https://errors.vrl.dev/#handling\\n  \\u001b[0m\\u001b[34m=\\u001b[0m see language documentation at https://vrl.dev\\n\",\"target\":\"vector::topology\"}\n\nAlso can confirm that if the source features only .cloud.availability_zone = del(.availability-zone) I get the same exact error.\nNot sure where to go from here, any help and/or guidance will be greatly appreciated.\nThank you!",
        "url": "https://github.com/vectordotdev/vector/discussions/10321",
        "createdAt": "2021-12-07T15:17:57Z",
        "updatedAt": "2022-07-02T04:58:05Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ottramst"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10286,
        "title": "Expanding a log Message to 2",
        "bodyText": "Hey Team,\nWe have a use case currently in logstash that we wish vector to solve, currently we have OpenSearch sending alerts.  When opensearch sends an alert it send 1 alert with all the instances where the alert occured in 1 message.  We would like vector to solve this by being able to split this one alert into multiple alerts.  The below is a source log example going to a target log example.\nSOURCE LOG\n{\n\"timestamp: \"yyyymmdd hh:mm:ss\",\n\"field1\": \"blah\",\n\"field2\": \"blad\",\n\"alerts\": {\n\"alert1\": \"some strange string\",\n\"alert2\": \"some other strange string\"\n}\n}\nTARGET LOGS\n{\n\"timestamp: \"yyyymmdd hh:mm:ss\",\n\"field1\": \"blah\",\n\"field2\": \"blad\",\n\"alert\": \"some strange string\"\n}\n{\n\"timestamp: \"yyyymmdd hh:mm:ss\",\n\"field1\": \"blah\",\n\"field2\": \"blad\",\n\"alert\": \"some other strange string\"\n}",
        "url": "https://github.com/vectordotdev/vector/discussions/10286",
        "createdAt": "2021-12-06T09:36:37Z",
        "updatedAt": "2022-06-10T14:36:42Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tastyfrankfurt"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10269,
        "title": "What's the behavior for one source with multiple sinks",
        "bodyText": "Hi I have a question about the scenario below:\nThere is one source, and two sinks A and B consume the data from this source. But the sink A cannot work correctly. Then, what's the behavior for Vector?\n\nwill the data forwarded to sink B successfully?\nwhen the external client post data to the source, will it always get error response because sink A failure?\nThanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/10269",
        "createdAt": "2021-12-04T17:51:26Z",
        "updatedAt": "2022-06-12T23:23:01Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "marcolan018"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10259,
        "title": "Will the http headers in a source be passed through the transforms/sinks, or discarded?",
        "bodyText": "We have a scenario to use prometheus_remote_write source, then through some transforms, finally go to a prometheus_remote_write sink.\nWhen post data to Vector source, we will set a tenant id inside a http header and hope that header can be pass through to the sink, so that when Vector send request to the sink endpoint, the header is still there.\nIs this supported?",
        "url": "https://github.com/vectordotdev/vector/discussions/10259",
        "createdAt": "2021-12-03T20:50:55Z",
        "updatedAt": "2022-06-26T01:38:58Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "marcolan018"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10220,
        "title": "vector support on x86 32 bit",
        "bodyText": "Hello,\nIn the downloads section, I did not see any off the shelf binaries available for 32 bit systems. Then I tried to look if there is any docker image in the rust embedded section which can be used to compile using cross, but did not find any.\nIs this supported?",
        "url": "https://github.com/vectordotdev/vector/discussions/10220",
        "createdAt": "2021-12-01T16:27:24Z",
        "updatedAt": "2022-06-24T15:50:11Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bmallya"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 9143,
        "title": "Metrics changes after 0.15.0 -> 0.16.0 upgrade",
        "bodyText": "Looking for pointers to explain new behaviour\u2026\nVector Version\nvector --version\nvector 0.16.0 (x86_64-unknown-linux-gnu 9e4174d 2021-08-25)\n\nExpected Behavior\nSimilar metrics behaviour\nActual Behavior\nMetrics changed signifcantly\nExample Data\n\nAdditional Context\nWe had no changes other than the version bump from 0.15.0 to 0.16.0.\nIt looks like the new behavior is the correct one but I'm wondering what have changed to cause it.\nCoincidentally number of watched files dropped 10fold.",
        "url": "https://github.com/vectordotdev/vector/discussions/9143",
        "createdAt": "2021-09-14T20:09:31Z",
        "updatedAt": "2022-07-12T13:59:33Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mhratson"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 0
    },
    {
        "number": 10127,
        "title": "Issues encountered upgrading v0.17.3 -> v0.18.0",
        "bodyText": "Issues encountered\nThey weren't mentioned in upgrade guide so  documenting it here for there record.\nmax_bytes limit 4250000\nTests and validation haven't picked up\nError\n{\"timestamp\":\"2021-11-19T20:32:53.271657Z\",\"level\":\"ERROR\",\"message\":\"Configuration error.\",\"error\":\"Sink \\\"datadog\\\": provided `max_bytes` exceeds the maximum limit of 4250000\",\"target\":\"vector::topology\"}\n\nConfig\nIs no longer valid\n  # max DD allows https://docs.datadoghq.com/api/latest/logs/#send-logs\n  batch.max_bytes = 5242880\n\nworked around by changing to batch.max_bytes = 4250000 instead\nImprovements Encountered\nLower CPU usage\n\nLower Memory usage",
        "url": "https://github.com/vectordotdev/vector/discussions/10127",
        "createdAt": "2021-11-19T21:05:30Z",
        "updatedAt": "2022-11-15T12:04:55Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mhratson"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10102,
        "title": "Configuring vector connection to kafka",
        "bodyText": "Hello everyone!\nI have a Kafka cluster in my company that runs on the kerberos keytab, I want to use a vector agent in order to pick up microservices logs from Kafka,   in the documentation https://vector.dev/docs/reference/configuration/sources/kafka/#:~:text=message_key-,librdkafka_options,-optional this case is hardly touched upon and hence there is a misunderstanding of how to implement it. Tried to configure sources like this but nothing happens:\n[sources.kafka-logs.librdkafka_options]\n\"sasl.kerberos.keytab\" = \"/opt/file.keytab\"\nalso tried https://component-pages--vector-project.netlify.app/docs/reference/sources/kafka/#:~:text=%23-,Advanced,-librdkafka_options.%22client.id\nlibrdkafka_options. \"sasl.kerberos.keytab\" = \"/opt/file.keytab\"\nbut also unsuccessfully.",
        "url": "https://github.com/vectordotdev/vector/discussions/10102",
        "createdAt": "2021-11-18T10:13:15Z",
        "updatedAt": "2022-06-07T15:32:43Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "k8s-comandante"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10054,
        "title": "Vector Job Management",
        "bodyText": "Hi, I'm a fresh player of Vector. After some docs reading and experiments on local machine, I do have some question about the Vector's job management(I know there's no \"job\" concept in Vector, here I use \"job\" to present a complete pipeline for specific source and sink).\nIn my case, I'm using Vector as a batch processing tool. e.g. I have a log file(or a directory of log files) and I want to collect these logs and sink them to a kafka instance. It's easy to configure a file source and a kafka sink in toml file to finish that job. But my question is: is there a concept of \"batch processing\" in Vector? since Vector has collected all logs from the source, it would still be running and waiting for new appending file contents. If my source files are immutable(no new file contents will be added), how could I know if Vector has finished the processing of all logs? Can I rely on the Graphql API's processedEventsTotal field in sinks : metrics to judge that if all logs processed? Or there are better API for checking this?\nFurthermore, In my usage, when I know all logs are processed, I want to stop the Vector instance and start it again if there are new processing job(seems not a good practice?), or remove the finished job's config segment and then reload so that the finished processing job can be \"removed\". I see that Vector supports hot-reloading when watched config changes so I can do \"job add/remove\" by changing config files, but seems that's not a graceful way. Do we support changing config by Graphql API? If so, that'll be more convenient for me since I want to use Vector as a micro service and other service can operate it by APIs.\nPlease forgive my ignorance and correct me if there are misunderstanding of Vector features. Thanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/10054",
        "createdAt": "2021-11-17T03:48:16Z",
        "updatedAt": "2022-07-06T08:58:57Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ProBrian"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 10101,
        "title": "Use vector as a library and access vector via programmable API",
        "bodyText": "Hi there,\nI am working on a C++ data project, and is evaluating if I can use vector to connect to multiple different sinks. In my case, users may submit data export jobs and I would like to feed the exported data entries into vector and send them to sinks like kafka/clickhouse/etc.\nSince the sinks information are submitted by users during runtime, I need to add sink entry dynamically during runtime. This makes using toml configuration inconvenient. It seems currently the GraphQL API is read only, and I am not able to use it to add new sink during runtime.\nI wonder if there is any possibility that I can use vector as a library, so that I may use tools like cbingen to create a C++ binding for Rust and use it in my project. Additionally, I think if vector can be accessed via API, more granular control/response is possible, for example, I would like to know if a data entry is really successfully delivered to the sink.\nI read the documentation but find no mention about this kind of usage, is there any one who can shed some light on this and how I can give it a shot if it can be achieved (maybe requiring some hacking for the vector codebase)? Thanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/10101",
        "createdAt": "2021-11-18T11:22:34Z",
        "updatedAt": "2022-06-08T10:23:53Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "niyue"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 9987,
        "title": "Log event data model validation at source",
        "bodyText": "I have started using some of the Vector's components and while I am still new to it, I think that they are really great and easy to setup (for the initial configuration).\nWhile I continue to read the documentation and examples provided, I wanted to check/understand if there is a way that we can enforce a data model/schema on the events that are coming in. Basically in the log event coming in from a particular source(s), I want to ensure that the required/mandatory fields are present. If not we drop them (with a log  if possible?), and if present we continue to work on the log event either by transforming it or sinking it.\nThanks in Advance.",
        "url": "https://github.com/vectordotdev/vector/discussions/9987",
        "createdAt": "2021-11-10T16:55:25Z",
        "updatedAt": "2022-06-07T10:21:08Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bmallya"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 9311,
        "title": "Event Specific logs/Metrics",
        "bodyText": "Hello,\nI would like to know if anyone had tried generating or collecting the event specific information as part of the logs/metrics for an vector instance.\nScenario:\nWe have a vector instance that generates events (sources-Http post messages) and forwards them to another vector instance over specific network port. The messages that are received at the vector instance are then redirected/written to a specific file using the sinks plugin.\nI was wondering if we can have the background information such as timestamps, received messages length and some more information that is specific to a single event at the receiver vector instance.",
        "url": "https://github.com/vectordotdev/vector/discussions/9311",
        "createdAt": "2021-09-23T03:17:37Z",
        "updatedAt": "2022-09-09T15:16:28Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "hemanthofficial3009"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 9905,
        "title": "Mapping hardware hostnames from syslogs to Datadog \"hostname\" attribute",
        "bodyText": "I am looking to send syslogs from hardware to Vector, and transform this to JSON and remap the hostname attribute from the syslogs to \"hostname\" so logs in Datadog say they are from that host, not the system doing the relay like syslog-ng does. Is this possible?",
        "url": "https://github.com/vectordotdev/vector/discussions/9905",
        "createdAt": "2021-11-04T16:42:53Z",
        "updatedAt": "2022-06-25T20:49:07Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "lognarly"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 9902,
        "title": "I used the official example, but the error was reported",
        "bodyText": "version\nvector 0.15.0 (x86_64-pc-windows-msvc 994d812 2021-07-16)\ntransforms\n`\n[transforms.del_other_field]\ntype = \"remap\"\ninputs = [\"inputs\"]\nsource = \"\"\"\ndosomething\n.test = find!(\"foobar\", r'b.r')\n\"\"\"\n`\nerror\nNov 04 23:23:26.774 ERROR vector::topology: Configuration error. error=Transform \"del_other_field\":\n\ufffd[0m\ufffd[1m\ufffd[38;5;9merror[E105]\ufffd[0m\ufffd[1m: call to undefined function\ufffd[0m\n  \ufffd[0m\ufffd[36m\u250c\u2500\ufffd[0m :2:9\n  \ufffd[0m\ufffd[36m\u2502\ufffd[0m\n\ufffd[0m\ufffd[36m2\ufffd[0m \ufffd[0m\ufffd[36m\u2502\ufffd[0m .test = \ufffd[0m\ufffd[31mfind\ufffd[0m!(\"foobar\", r'b.r')\n  \ufffd[0m\ufffd[36m\u2502\ufffd[0m         \ufffd[0m\ufffd[31m^^^^\ufffd[0m\n  \ufffd[0m\ufffd[36m\u2502\ufffd[0m         \ufffd[0m\ufffd[31m\u2502\ufffd[0m\n  \ufffd[0m\ufffd[36m\u2502\ufffd[0m         \ufffd[0m\ufffd[31mundefined function\ufffd[0m\n  \ufffd[0m\ufffd[36m\u2502\ufffd[0m         \ufffd[0m\ufffd[36mdid you mean \"int\"?\ufffd[0m\n  \ufffd[0m\ufffd[36m\u2502\ufffd[0m\n  \ufffd[0m\ufffd[36m=\ufffd[0m learn more about error code 105 at https://errors.vrl.dev/105\n  \ufffd[0m\ufffd[36m=\ufffd[0m see language documentation at https://vrl.dev",
        "url": "https://github.com/vectordotdev/vector/discussions/9902",
        "createdAt": "2021-11-04T15:25:41Z",
        "updatedAt": "2022-11-15T08:38:44Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "whyiyu"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 9793,
        "title": "Issues upgrading from 0.16.1 to 0.17.3",
        "bodyText": "Issues encountered\nThey weren't mentioned in upgrade guide so i'm documenting it here for there record.\n1 Undocumented Rename\nBefore we had rtt_threshold_ratio in our sinks.datadog-metrics sink. After we're getting error:\n   Running tests\n   Oct 25 23:58:28.508 ERROR vector::unit_test: Failed to execute tests:\n   unknown field `rtt_threshold_ratio`, expected one of `decrease_ratio`, `ewma_alpha`, `rtt_deviation_scale` for key `sinks.datadog-metrics` at line 45 column 1.\n\nSolution: to use rtt_deviation_scale instead.\n2 nil is no longer accepted as VRL\n error[E701]: call to undefined variable\n     \u250c\u2500 :10:14\n     \u2502\n  10 \u2502    if err == nil {\n     \u2502              ^^^\n     \u2502              \u2502\n     \u2502              undefined variable\n     \u2502              did you mean \"err\"?\n     \u2502\n     = see language documentation at https://vrl.dev\n\nSolution: use null instead.",
        "url": "https://github.com/vectordotdev/vector/discussions/9793",
        "createdAt": "2021-10-26T00:28:32Z",
        "updatedAt": "2022-07-16T14:52:55Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mhratson"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 9651,
        "title": "Vector cross-compilation for AIX POWER8",
        "bodyText": "Is it possible to cross-compile for the AIX OS?\nThere is very little information on the web about compiling rust for AIX (7.1).\nWhat do you think about this?\n# uname -a\nAIX\n\n# lsconf | grep \"Processor Type\"\nProcessor Type: PowerPC_POWER8",
        "url": "https://github.com/vectordotdev/vector/discussions/9651",
        "createdAt": "2021-10-16T18:15:40Z",
        "updatedAt": "2022-10-27T03:01:58Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "mrgolangv"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 9574,
        "title": "How to test abort in remap?",
        "bodyText": "Hi guys, I have something like that in my transformer.\nif smth == \"1\" { some_var = \"One\" } else if smth == \"2\" { some_var = \"Two\" } else { abort }\nHow to write unit test for else branch? I need to test abort case",
        "url": "https://github.com/vectordotdev/vector/discussions/9574",
        "createdAt": "2021-10-12T17:59:46Z",
        "updatedAt": "2022-06-20T15:28:55Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "kozliuk"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 9352,
        "title": "AggregatedSummary not supported on GCP Monitoring",
        "bodyText": "I am trying to prometheus_scrape -> GCP monitoring.\nPrometheus source is directly parsed to Summary metric, but the field is called AggregatedSummary and this metric is not recognized by GCP monitoring\napp_request_duration_seconds_sum xxx\napp_request_duration_seconds_count yyy\n\nvector::sinks::gcp::stackdriver_metrics: Unsupported metric type: AggregatedSummary { quantiles: [], count: yyy, sum: xxx }",
        "url": "https://github.com/vectordotdev/vector/discussions/9352",
        "createdAt": "2021-09-27T13:01:13Z",
        "updatedAt": "2022-11-24T22:32:06Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "trickster"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 9321,
        "title": "Adding an \"instance\" tag for the prometheus scraper",
        "bodyText": "I've been playing with the prometheus scraper, and it's pretty awesome:\n[sources.node_exporter]\ntype = \"prometheus_scrape\"\nendpoints = [\n  \"http://host1:9100/metrics\",\n  \"http://host2:9100/metrics\",\n]\nscrape_interval_secs = 60\n\n# Print scraped metrics to stdout\n[sinks.print]\ntype = \"console\"\ninputs = [\"node_exporter\"]\nencoding.codec = \"json\"\n\nScraping works instantly.  The problem is that I can't see how to distinguish the metrics originating from different hosts. e.g.\n$ vector --config vector-scrape.toml  | grep node_memory_Active_bytes\n{\"name\":\"node_memory_Active_bytes\",\"timestamp\":\"2021-09-23T17:14:30.775368953Z\",\"kind\":\"absolute\",\"gauge\":{\"value\":1648369664.0}}\n{\"name\":\"node_memory_Active_bytes\",\"timestamp\":\"2021-09-23T17:14:30.817781032Z\",\"kind\":\"absolute\",\"gauge\":{\"value\":305164288.0}}\n\nWhich host does each metric belong to??\nWith a \"real\" prometheus server, it would add a label instance=\"...\", which defaults to the __address__ which was scraped.\nWhat's the standard way in Vector of distinguish the same metric from multiple sources: a tag? A namespace?  Does that association already exist for prometheus scrapes, but is hidden in the \"console\" sink output?\nI can see that internal metrics have tags.host_key and tags.pid_key, but I don't see anything equivalent for prometheus scraping.\nI'm only just getting to grips with the Vector data model, so I apologise if I've missed something obvious.",
        "url": "https://github.com/vectordotdev/vector/discussions/9321",
        "createdAt": "2021-09-23T17:29:27Z",
        "updatedAt": "2022-08-25T08:28:33Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "candlerb"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 9153,
        "title": "Vector retry function failed ??",
        "bodyText": "Hi All\nI am running vector01 (sink) -> Vector02 (source, port 1111) on a system.\nNo healthcheck on the sink.\nBoth running version 0.16.1.\nThe other day the vector02 had en networking error, i am still not sure what happend, where the node did not response on TCP 1111. The vector01 logged a lot of errors in the log regarding this (obviously). Vector02 server was fixed again after 1 hour and the port was up and responding on a netcat test.\nBut the vector01 never connected and started sending logs again through the pipeline.\nDoes anyone know how the retry function works in a vector sink ?\nIs heatlthcheck mandatory for retry to function correct ?\nThanks in advance\n\\Peter",
        "url": "https://github.com/vectordotdev/vector/discussions/9153",
        "createdAt": "2021-09-15T08:33:08Z",
        "updatedAt": "2022-07-02T04:59:34Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "PejeDK"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 9010,
        "title": "Vector lite",
        "bodyText": "Hello,\nI was wondering if anyone has attempted to build a lite version for vector given that only a few of sources/sinks/transforms will be used.\nFor example,  Given a case where the sources needed could be http, socket, files\nwhile sinks are  vector , file along with the transforms, how do we\n\nIdentify the lib dependencies that are not required\nTest the finally compiled binary to ensure that it is good.\n\nI have tried to remove the sources/sinks/transforms that are not required along with any dependencies that are mentioned in Cargo.toml file. This I then verified by compiling vector and then generating dummy data and forwarding from one instance to another instance.\nEx: I removed the following source and also rusto, rusto_s3 from Cargo.toml\nsources-aws_s3 = [\"rusoto\", \"rusoto_s3\", \"rusoto_sqs\", \"semver\", \"uuid\"]\nBut I do not know if there are any other indirect dependencies.\nAttached is a cargo.toml file that I had used (with some sources/sinks/transforms and dependencies\nvectorCargo.txt\ncommented). With this file, I was able to get vector of size 77 MB from the default ~145 MB for v0.15.1",
        "url": "https://github.com/vectordotdev/vector/discussions/9010",
        "createdAt": "2021-09-02T18:30:47Z",
        "updatedAt": "2022-08-22T07:47:59Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "bmallya"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 9023,
        "title": "Extraction specific JSON keys in vector",
        "bodyText": "Can we extract specific fields of a JSON inside the \"transform\" block of vector ? Also we need to combine that fields in to yet another JSON.",
        "url": "https://github.com/vectordotdev/vector/discussions/9023",
        "createdAt": "2021-09-03T06:00:06Z",
        "updatedAt": "2022-08-01T10:55:23Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "jintomv"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 8950,
        "title": "Vector Panic",
        "bodyText": "Vector will not start, have deleted the disk queues from /var/lib/vector but still get the following error with FULL trace enabled.\nAug 29 23:50:00 test.example.local vector[5933]: thread 'vector-worker' panicked at 'Tried to ack beyond read offset', /project/lib/vector-core/buffers/src/disk/leveldb_buffer/reader.rs:184:13\nAug 29 23:50:00 test.example.local vector[5933]: stack backtrace:\nAug 29 23:50:00 test.example.local vector[5933]:    0:     0x55ef46fd0340 - std::backtrace_rs::backtrace::libunwind::trace::h34055254b57d8e79\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/../../backtrace/src/backtrace/libunwind.rs:90:5\nAug 29 23:50:00 test.example.local vector[5933]:    1:     0x55ef46fd0340 - std::backtrace_rs::backtrace::trace_unsynchronized::h8f1e3fbd9afff6ec\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/../../backtrace/src/backtrace/mod.rs:66:5\nAug 29 23:50:00 test.example.local vector[5933]:    2:     0x55ef46fd0340 - std::sys_common::backtrace::_print_fmt::h3a99a796b770c360\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/sys_common/backtrace.rs:67:5\nAug 29 23:50:00 test.example.local vector[5933]:    3:     0x55ef46fd0340 - <std::sys_common::backtrace::_print::DisplayBacktrace as core::fmt::Display>::fmt::h32d1f94a80615d18\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/sys_common/backtrace.rs:46:22\nAug 29 23:50:00 test.example.local vector[5933]:    4:     0x55ef4703cb3c - core::fmt::write::h306731c068f7162c\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/core/src/fmt/mod.rs:1110:17\nAug 29 23:50:00 test.example.local vector[5933]:    5:     0x55ef46fc1d05 - std::io::Write::write_fmt::hd2fa90334eee2a21\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/io/mod.rs:1588:15\nAug 29 23:50:00 test.example.local vector[5933]:    6:     0x55ef46fd41cb - std::sys_common::backtrace::_print::h5abaa2601a852287\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/sys_common/backtrace.rs:49:5\nAug 29 23:50:00 test.example.local vector[5933]:    7:     0x55ef46fd41cb - std::sys_common::backtrace::print::h8d81445442bb638f\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/sys_common/backtrace.rs:36:9\nAug 29 23:50:00 test.example.local vector[5933]:    8:     0x55ef46fd41cb - std::panicking::default_hook::{{closure}}::hcfe804496a9fa747\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/panicking.rs:208:50\nAug 29 23:50:00 test.example.local vector[5933]:    9:     0x55ef46fd3ca1 - std::panicking::default_hook::hbea8e3ccf2ba8901\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/panicking.rs:225:9\nAug 29 23:50:00 test.example.local vector[5933]:   10:     0x55ef46fd49d4 - std::panicking::rust_panic_with_hook::h7ee9e1a2d0f8975a\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/panicking.rs:622:17\nAug 29 23:50:00 test.example.local vector[5933]:   11:     0x55ef436205e4 - std::panicking::begin_panic::{{closure}}::hc4f15b317d0889cb\nAug 29 23:50:00 test.example.local vector[5933]:   12:     0x55ef435fc80c - std::sys_common::backtrace::__rust_end_short_backtrace::h8123e621a7044df9\nAug 29 23:50:00 test.example.local vector[5933]:   13:     0x55ef431b37cc - std::panicking::begin_panic::hda4466d9cb99be3f\nAug 29 23:50:00 test.example.local vector[5933]:   14:     0x55ef43b9a509 - buffers::disk::leveldb_buffer::reader::Reader::flush::hd8550b8fbb50016b\nAug 29 23:50:00 test.example.local vector[5933]:   15:     0x55ef433256c9 - <buffers::disk::leveldb_buffer::reader::Reader as futures_core::stream::Stream>::poll_next::h7223edf30de07c65\nAug 29 23:50:00 test.example.local vector[5933]:   16:     0x55ef44719598 - <vector::utilization::Utilization as futures_core::stream::Stream>::poll_next::ha94dfacc6a4b8e39\nAug 29 23:50:00 test.example.local vector[5933]:   17:     0x55ef432cdf63 - <stream_cancel::combinator::TakeUntilIf<S,F> as futures_core::stream::Stream>::poll_next::he18f0ed82d4d75b2\nAug 29 23:50:00 test.example.local vector[5933]:   18:     0x55ef44425c19 - <futures_util::stream::stream::fuse::Fuse as futures_core::stream::Stream>::poll_next::h64dee8bfda165581\nAug 29 23:50:00 test.example.local vector[5933]:   19:     0x55ef44425a6b - <futures_util::stream::stream::fuse::Fuse as futures_core::stream::Stream>::poll_next::h29aef6395e30c98d\nAug 29 23:50:00 test.example.local vector[5933]:   20:     0x55ef4431a8c3 - <core::future::from_generator::GenFuture as core::future::future::Future>::poll::hd5493fb48a04400d\nAug 29 23:50:00 test.example.local vector[5933]:   21:     0x55ef442a6d9c - <core::future::from_generator::GenFuture as core::future::future::Future>::poll::ha493d247a22d2675\nAug 29 23:50:00 test.example.local vector[5933]:   22:     0x55ef43a1b378 - tokio::runtime::task::harness::poll_future::h8d174f5c8dbaebad\nAug 29 23:50:00 test.example.local vector[5933]:   23:     0x55ef439d4aa4 - tokio::runtime::task::raw::poll::h95648d99a4de7c69\nAug 29 23:50:00 test.example.local vector[5933]:   24:     0x55ef46782f4f - tokio::runtime::thread_pool::worker::Context::run_task::h5aeb6dc5f7237987\nAug 29 23:50:00 test.example.local vector[5933]:   25:     0x55ef46782502 - tokio::runtime::thread_pool::worker::run::hb1eb5812bff9a631\nAug 29 23:50:00 test.example.local vector[5933]:   26:     0x55ef439d42f8 - tokio::runtime::task::raw::poll::h8eb845e9130b4063\nAug 29 23:50:00 test.example.local vector[5933]:   27:     0x55ef4674558a - std::sys_common::backtrace::__rust_begin_short_backtrace::h6374674f497847c1\nAug 29 23:50:00 test.example.local vector[5933]:   28:     0x55ef4674cb7e - core::ops::function::FnOnce::call_once{{vtable.shim}}::h42364c10fed52b6a\nAug 29 23:50:00 test.example.local vector[5933]:   29:     0x55ef46fe0ed7 - <alloc::boxed::Box<F,A> as core::ops::function::FnOnce>::call_once::h7ece6cfefaff1005\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/alloc/src/boxed.rs:1575:9\nAug 29 23:50:00 test.example.local vector[5933]:   30:     0x55ef46fe0ed7 - <alloc::boxed::Box<F,A> as core::ops::function::FnOnce>::call_once::hb8b48e55c21f193e\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/alloc/src/boxed.rs:1575:9\nAug 29 23:50:00 test.example.local vector[5933]:   31:     0x55ef46fe0ed7 - std::sys::unix::thread::Thread::new::thread_start::h8c7c4450dba62914\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/sys/unix/thread.rs:71:17\nAug 29 23:50:00 test.example.local vector[5933]:   32:     0x7f09cd537609 - start_thread\nAug 29 23:50:00 test.example.local vector[5933]:   33:     0x7f09cd307293 - clone\nAug 29 23:50:00 test.example.local vector[5933]:   34:                0x0 - \nAug 29 23:50:00 test.example.local vector[5933]: thread 'vector-worker' panicked at 'Tried to ack beyond read offset', /project/lib/vector-core/buffers/src/disk/leveldb_buffer/reader.rs:184:13\nAug 29 23:50:00 test.example.local vector[5933]: stack backtrace:\nAug 29 23:50:00 test.example.local vector[5933]:    0:     0x55ef46fd0340 - std::backtrace_rs::backtrace::libunwind::trace::h34055254b57d8e79\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/../../backtrace/src/backtrace/libunwind.rs:90:5\nAug 29 23:50:00 test.example.local vector[5933]:    1:     0x55ef46fd0340 - std::backtrace_rs::backtrace::trace_unsynchronized::h8f1e3fbd9afff6ec\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/../../backtrace/src/backtrace/mod.rs:66:5\nAug 29 23:50:00 test.example.local vector[5933]:    2:     0x55ef46fd0340 - std::sys_common::backtrace::_print_fmt::h3a99a796b770c360\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/sys_common/backtrace.rs:67:5\nAug 29 23:50:00 test.example.local vector[5933]:    3:     0x55ef46fd0340 - <std::sys_common::backtrace::_print::DisplayBacktrace as core::fmt::Display>::fmt::h32d1f94a80615d18\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/sys_common/backtrace.rs:46:22\nAug 29 23:50:00 test.example.local vector[5933]:    4:     0x55ef4703cb3c - core::fmt::write::h306731c068f7162c\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/core/src/fmt/mod.rs:1110:17\nAug 29 23:50:00 test.example.local vector[5933]:    5:     0x55ef46fc1d05 - std::io::Write::write_fmt::hd2fa90334eee2a21\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/io/mod.rs:1588:15\nAug 29 23:50:00 test.example.local vector[5933]:    6:     0x55ef46fd41cb - std::sys_common::backtrace::_print::h5abaa2601a852287\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/sys_common/backtrace.rs:49:5\nAug 29 23:50:00 test.example.local vector[5933]:    7:     0x55ef46fd41cb - std::sys_common::backtrace::print::h8d81445442bb638f\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/sys_common/backtrace.rs:36:9\nAug 29 23:50:00 test.example.local vector[5933]:    8:     0x55ef46fd41cb - std::panicking::default_hook::{{closure}}::hcfe804496a9fa747\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/panicking.rs:208:50\nAug 29 23:50:00 test.example.local vector[5933]:    9:     0x55ef46fd3ca1 - std::panicking::default_hook::hbea8e3ccf2ba8901\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/panicking.rs:225:9\nAug 29 23:50:00 test.example.local vector[5933]:   10:     0x55ef46fd49d4 - std::panicking::rust_panic_with_hook::h7ee9e1a2d0f8975a\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/panicking.rs:622:17\nAug 29 23:50:00 test.example.local vector[5933]:   11:     0x55ef436205e4 - std::panicking::begin_panic::{{closure}}::hc4f15b317d0889cb\nAug 29 23:50:00 test.example.local vector[5933]:   12:     0x55ef435fc80c - std::sys_common::backtrace::__rust_end_short_backtrace::h8123e621a7044df9\nAug 29 23:50:00 test.example.local vector[5933]:   13:     0x55ef431b37cc - std::panicking::begin_panic::hda4466d9cb99be3f\nAug 29 23:50:00 test.example.local vector[5933]:   14:     0x55ef43b9a509 - buffers::disk::leveldb_buffer::reader::Reader::flush::hd8550b8fbb50016b\nAug 29 23:50:00 test.example.local vector[5933]:   15:     0x55ef437d6d47 - core::ptr::drop_in_place<buffers::disk::leveldb_buffer::reader::Reader<vector_core::event::Event>>::h25a4f4383fb30338\nAug 29 23:50:00 test.example.local vector[5933]:   16:     0x55ef437858ea - core::ptr::drop_in_placevector::utilization::Utilization::he26fff44ab499616\nAug 29 23:50:00 test.example.local vector[5933]:   17:     0x55ef43738bd4 - core::ptr::drop_in_place<core::pin::Pin<alloc::boxed::Box<core::pin::Pin<alloc::boxed::Box<dyn futures_core::stream::Stream+Item = vect>\nAug 29 23:50:00 test.example.local vector[5933]:   18:     0x55ef442a7673 - <core::future::from_generator::GenFuture as core::future::future::Future>::poll::ha493d247a22d2675\nAug 29 23:50:00 test.example.local vector[5933]:   19:     0x55ef43a1b378 - tokio::runtime::task::harness::poll_future::h8d174f5c8dbaebad\nAug 29 23:50:00 test.example.local vector[5933]:   20:     0x55ef439d4aa4 - tokio::runtime::task::raw::poll::h95648d99a4de7c69\nAug 29 23:50:00 test.example.local vector[5933]:   21:     0x55ef46782f4f - tokio::runtime::thread_pool::worker::Context::run_task::h5aeb6dc5f7237987\nAug 29 23:50:00 test.example.local vector[5933]:   22:     0x55ef46782502 - tokio::runtime::thread_pool::worker::run::hb1eb5812bff9a631\nAug 29 23:50:00 test.example.local vector[5933]:   23:     0x55ef439d42f8 - tokio::runtime::task::raw::poll::h8eb845e9130b4063\nAug 29 23:50:00 test.example.local vector[5933]:   24:     0x55ef4674558a - std::sys_common::backtrace::__rust_begin_short_backtrace::h6374674f497847c1\nAug 29 23:50:00 test.example.local vector[5933]:   25:     0x55ef4674cb7e - core::ops::function::FnOnce::call_once{{vtable.shim}}::h42364c10fed52b6a\nAug 29 23:50:00 test.example.local vector[5933]:   26:     0x55ef46fe0ed7 - <alloc::boxed::Box<F,A> as core::ops::function::FnOnce>::call_once::h7ece6cfefaff1005\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/alloc/src/boxed.rs:1575:9\nAug 29 23:50:00 test.example.local vector[5933]:   27:     0x55ef46fe0ed7 - <alloc::boxed::Box<F,A> as core::ops::function::FnOnce>::call_once::hb8b48e55c21f193e\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/alloc/src/boxed.rs:1575:9\nAug 29 23:50:00 test.example.local vector[5933]:   28:     0x55ef46fe0ed7 - std::sys::unix::thread::Thread::new::thread_start::h8c7c4450dba62914\nAug 29 23:50:00 test.example.local vector[5933]:                                at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633/library/std/src/sys/unix/thread.rs:71:17\nAug 29 23:50:00 test.example.local vector[5933]:   29:     0x7f09cd537609 - start_thread\nAug 29 23:50:00 test.example.local vector[5933]:   30:     0x7f09cd307293 - clone\nAug 29 23:50:00 test.example.local vector[5933]:   31:                0x0 - \nAug 29 23:50:00 test.example.local vector[5933]: thread panicked while panicking. aborting.\nAug 29 23:50:00 test.example.local systemd-coredump[5956]: Process 5933 (vector) of user 1001 dumped core.\nAug 29 23:50:00 test.example.local systemd[1]: vector.service: Main process exited, code=killed, status=4/ILL\nAug 29 23:50:00 test.example.local systemd[1]: vector.service: Failed with result 'signal'.",
        "url": "https://github.com/vectordotdev/vector/discussions/8950",
        "createdAt": "2021-08-30T00:05:48Z",
        "updatedAt": "2022-07-20T09:20:22Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tastyfrankfurt"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 8866,
        "title": "Clarification on journald source vs syslog",
        "bodyText": "Journald can forward logs to a syslog datagram listener, and vector is capable of being one as well as pulling from journald directly. The latter has the downside of requiring an extra binary, so you can't use the alpine docker image for example. Is there guidance on which approach is generally better?",
        "url": "https://github.com/vectordotdev/vector/discussions/8866",
        "createdAt": "2021-08-24T23:11:45Z",
        "updatedAt": "2022-06-17T05:24:28Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "nivekuil"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 8835,
        "title": "Proxy Support for Sink Adaptor",
        "bodyText": "Do you support posting logs through API over proxy ?",
        "url": "https://github.com/vectordotdev/vector/discussions/8835",
        "createdAt": "2021-08-23T13:29:28Z",
        "updatedAt": "2021-11-09T23:34:17Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "ambujjain"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 8746,
        "title": "Using a field value as a field name",
        "bodyText": "Hi,\nI would like to transform a json in this way (I simplified a little the data for the example) :\nBefore : {\"gauge\":{\"value\":973.0},\"kind\":\"absolute\",\"name\":\"scrape_samples_scraped\",\"tags\":{\"job\":\"node_exporter\"}}\nAfter: {\"fields\":{\"job\":\"node_exporter\",\"metric_name:scrape_samples_scraped\":973.0}}\nI tried to do it with a remap but I don't manage to get the part with the metric_name inside the field name : \"metric_name:scrape_samples_scraped\":973.0\nSo, is there a way with vector to use a field value as, or inside a field name ?\nI tried to get around that by using the parse_key_value() function, but then the metric value field becomes a String, and I can't convert it to an Int because the field name will not always be the same...\nWhat I tried : .fields = merge!(del(.tags),parse_key_value!(\"metric_name:\" + to_string!(.name) + \"=\" + to_string!(.gauge.value),\"=\"))\nWhat I got : {\"fields\": { \"job\": \"node_exporter\", \"metric_name:scrape_samples_scraped\": \"973\" }}\nPlease tell me if you have any idea,\nThanks",
        "url": "https://github.com/vectordotdev/vector/discussions/8746",
        "createdAt": "2021-08-16T16:48:08Z",
        "updatedAt": "2022-06-12T13:31:37Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "AdAndre"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 8732,
        "title": "Does vector support automatic deletion of files after reading?",
        "bodyText": "I am writing logs to a file by hour via Nginx lua, and then reading the file via vector to write the contents to kafka, but when the reading is done, the file needs to be deleted manually, which bothers me a lot.\nDoes vector support automatic deletion of individual files after they are read?\n[sources.nginx_file_sources]\n  type = \"file\"\n  include = [\"/openresty/nginx/conf/waf/log/*.log\"]\n  data_dir = \"/vector\"",
        "url": "https://github.com/vectordotdev/vector/discussions/8732",
        "createdAt": "2021-08-16T01:35:07Z",
        "updatedAt": "2023-02-16T06:36:23Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "PanHywel"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 8708,
        "title": "Looking for guidance on contributions",
        "bodyText": "Hello, I'm just looking for a good place to have a conversation around whether it makes sense for us to try to contribute? I wanted to post a discussion before I dived into your discord.\nFor context, I'm from a team in an MSSP that helps customers get various sources of data generally into Google Cloud Chronicle and we were actually looking at completely redoing our internal Golang library to something that resembles Vector with some minor differences (we just now discovered Vector so good to know it seems we were on the right track). We haven't started that work beyond a PoC of sorts and we are wanting to evaluate options and just looking to get whether some of the features we would like align with your aspirations for Vector or not as that would help us make a decision.\nSide note: Man are titles hard",
        "url": "https://github.com/vectordotdev/vector/discussions/8708",
        "createdAt": "2021-08-13T16:20:57Z",
        "updatedAt": "2024-12-24T22:08:36Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "TrevinTeacutter"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 8663,
        "title": "Kafka messages sink to Clickhouse",
        "bodyText": "Hello all,\nWe are trying to get JSON messages from Kafka into Clickhouse, following the Source and Sink connectors we are getting some residual JSON fields from Kafka that we do not need in Clickhouse, we just need the JSON payload.\nHow can we extract this field only?\nExample message:\n{\n\"session_id\": null,\n\"headers\": {},\n\"message\": {\n\"CLIENT_ID\": \"someid\",\n\"SESSION_ID\": \"7017558492\",\n\"SESSION_NUMBER\": \"1\",\n\"LANGUAGE\": \"en\",\n\"EVENT_NAME\": \"session\",\n\"TIMESTAMP\": \"1628691008\",\n\"JSON_VALUE\": {\n\"application_action\": \"open\",\n\"application_app_state\": \"starting\",\n\"application_device_id\": \"somevalue\"\n}\n},\n\"offset\": 7,\n\"partition\": 4,\n\"source_type\": \"kafka\",\n\"timestamp\": \"2021-08-11T14:10:08.144Z\",\n\"topic\": \"EVENTS\"\n}\nWe need only what is inside message:\n{\n\"CLIENT_ID\": \"someid\",\n\"SESSION_ID\": \"7017558492\",\n\"SESSION_NUMBER\": \"1\",\n\"LANGUAGE\": \"en\",\n\"EVENT_NAME\": \"session\",\n\"TIMESTAMP\": \"1628691008\",\n\"JSON_VALUE\": {\n\"application_action\": \"open\",\n\"application_app_state\": \"starting\",\n\"application_device_id\": \"somevalue\"\n}\nWe cannot find any kind of extract or transform to satisfy this.\nAny help is appreciated,\nAlex",
        "url": "https://github.com/vectordotdev/vector/discussions/8663",
        "createdAt": "2021-08-11T15:41:04Z",
        "updatedAt": "2022-06-07T15:44:22Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "omurov"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 8644,
        "title": "How do I add fixed attributes to the expanded array?",
        "bodyText": "How do I add fixed attributes to the expanded array\nVector Version\nvector 0.15.0\n[sources.httpSources]\ntype = \"http\"\naddress = \"172.16.1.41:7001\"\npath = \"/client-lua\"\nencoding = \"json\"\nheaders = [\"x-forwarded-for\"]\n\n\n[transforms.parse_logs]\ntype = \"remap\"\ninputs = [\"httpSources\"]\nsource = '''\n. = .msg\n.clientIp = .x-forwarded-for\n'''\n\n\n[sinks.print]\n type = \"console\"\n inputs = [\"parse_logs\"]\n encoding.codec = \"json\"\n{\n   \"x-forwarded-for\":\"192.168.1.1\",\n   \"msg\":[\n     {\n       \"name\":\"tom\"\n     },\n     {\n       \"name\":\"mimi\"\n     }\n  ]\n}\n\nI hope to get\uff1a\n{\n  \"clientIp\":\"192.168.1.1\",\n  \"name\":\"tom\"\n},\n{\n  \"clientIp\":\"192.168.1.1\",\n  \"name\":\"mimi\"\n}",
        "url": "https://github.com/vectordotdev/vector/discussions/8644",
        "createdAt": "2021-08-10T12:38:32Z",
        "updatedAt": "2022-07-06T16:34:17Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "PanHywel"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 8634,
        "title": "Filter transform getting rate limited interally",
        "bodyText": "I've got the following values.yml file:\ntransforms:\n  logFilter:\n    type: filter\n    inputs:\n      - \"kubernetes_logs\"\n    condition: \"length(parse_regex!(.kubernetes.pod_name, r'.*deployment.*')) > 0\"\n  # jsonFlatten:\n  #   type: remap\n  #   inputs:\n  #     - \"logFilter\"\n  #   source: \" .message = parse_json!(.message)\\n\"\n\nsinks:\n  # Adjust as necessary. By default we use the console sink\n  # to print all data. This allows you to see Vector working.\n  # /docs/reference/sinks/\n  stdout:\n    type: console\n    inputs: [\"logFilter\"]\n    target: \"stdout\"\n    encoding: \"json\"\n\n  lokiSink:\n    type: loki\n    inputs:\n      - logFilter\n    endpoint: http://loki.default.svc.cluster.local:3100\n    encoding:\n      codec: json\n    healthcheck:\n      enabled: true\n    labels:\n      forwarder: vector\n      podname: \"{{kubernetes.pod_name}}\"\n      nodename: \"{{kubernetes.pod_node_name}}\"\n      messagetext: \"{{message.message}}\"\nAnd I am getting errors:\nAug 09 16:54:18.635  WARN transform{component_kind=\"transform\" component_name=logFilter component_type=filter}: vector::internal_events::conditions: VRL condition execution failed. internal_log_rate_secs=120\nAug 09 16:54:18.635  WARN transform{component_kind=\"transform\" component_name=logFilter component_type=filter}: vector::internal_events::conditions: Internal log [VRL condition execution failed.] is being rate limited.\n\nHow do I remove this rate limit?",
        "url": "https://github.com/vectordotdev/vector/discussions/8634",
        "createdAt": "2021-08-09T16:55:47Z",
        "updatedAt": "2022-06-13T17:29:58Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "danthegoodman1"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 8563,
        "title": "Buffering and data loss prevention on k8s",
        "bodyText": "Hi, I'm looking to configure buffering for my sink, with the intent of having 0 data loss. I saw in the docs that you suggest not using aws gp2 (what we are using) for disk buffer, are there any other suggestions for what I should use?\nAlso the disk buffer doesn't specify what volume it is loaded on, is there a way I can use a persistent volume and not the node volume for this?\nAlso is there any info on how hard is the performance hit for using disk buffer? I know it will be dependent on a bunch of things like type of disk + cpu etc. But is this on a scale of milliseconds, seconds or minutes difference?\nThanks.",
        "url": "https://github.com/vectordotdev/vector/discussions/8563",
        "createdAt": "2021-08-03T08:26:59Z",
        "updatedAt": "2022-06-10T16:05:12Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "asafdl"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 8603,
        "title": "Multiline events from a file using Vector FILE source",
        "bodyText": "I am using the \"file\" source to read log lines and I need to use the multi-line feature to group >1 lines together. In the most general case, I want something like: \"every new log line starts with a DATE and all lines that follows and doesn't starts with a DATE is part of the same event\"\nThe config that I am using is :\nstart_pattern = \"^(?P<year>[0-9]{4})-(?P<month>[0-9]{2})-(?P<day>[0-9]{2})\"\ncondition_pattern = \"^(?P<year>[0-9]{4})-(?P<month>[0-9]{2})-(?P<day>[0-9]{2})\"\nmode = \"halt_before\"\n\nwith this context I have couple of questions :\n\nDoes this introduce a ( significant ) performance hit with having to check start and condition pattern on every line even before the event goes to the transform stage (which btw has its own GROKing)\nIs their any better way to capture such multi-line events.\n\nSample Multi-line event :\nevent 1 starts here--->2021-08-04 06:06:37.877096 [INFO] switch_core.c:2515\nFreeSWITCH Version 1.10.5-release+git~20210729T051935Z~90077fa1af~64bit (git 90077fa 2021-07-29 05:19:35Z 64bit)\nFreeSWITCH Started\nMax Sessions [4000]\nSession Rate [100]\nSQL [Enabled]\nevent2 starts here --->2021-08-04 06:38:27.336864 [NOTICE] switch_channel.c:1118 New Channel sofia/internal/+13334440016@23.253.254.125:5064 [prober-bfea31a7-75d4-4c45-a706-3833a4bb14e9]",
        "url": "https://github.com/vectordotdev/vector/discussions/8603",
        "createdAt": "2021-08-05T14:31:01Z",
        "updatedAt": "2022-06-20T06:18:57Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "atibdialpad"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 8334,
        "title": "Opensearch Dashboard Issues",
        "bodyText": "Hey Team,\nWe are using OpenSearch to monitor our servers and using the host metrics but we are unable to correlate the metric events because events per devices always resolve to gauge.value or counter.value, below is an example of the current log format,\n{\n\"gauge\": {\n\"value\": 741896192.0\n},\n\"kind\": \"absolute\",\n\"name\": \"filesystem_free_bytes\",\n\"namespace\": \"host\",\n\"tags\": {\n\"collector\": \"filesystem\",\n\"device\": \"/dev/sda2\",\n\"filesystem\": \"ext4\",\n\"host\": \"demo.example.local\",\n\"mountpoint\": \"/boot\"\n},\n\"timestamp\": \"2021-07-16T03:26:40.545460831Z\"\n}\nWhat we would like to do is have it as follows to work within the constraints of Kibana/Opensearch Dashboard limitations,\n{\n\"kind\": \"absolute\",\n\"filesystem_free_bytes\": 741896192.0\n\"namespace\": \"host\",\n\"tags\": {\n\"collector\": \"filesystem\",\n\"device\": \"/dev/sda2\",\n\"filesystem\": \"ext4\",\n\"host\": \"demo01.example.local\",\n\"mountpoint\": \"/boot\"\n},\n\"timestamp\": \"2021-07-16T03:26:40.545460831Z\"\n}\nIs there a way to represent this data this way in VRL  (we have tried but have not had any success) or is this a feature request we would need to submit.\nThanks\nTasty",
        "url": "https://github.com/vectordotdev/vector/discussions/8334",
        "createdAt": "2021-07-16T03:40:57Z",
        "updatedAt": "2022-06-13T13:42:15Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tastyfrankfurt"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 8306,
        "title": "Is there any \"length limitation\" in parse_json function?",
        "bodyText": "Hello, I have a question regarding the message size and parse_json function.\nFirst, I am trying to send log message as a json format, with exception field which contains whole stacktraces emitted by spring boot project.\nI found strange things when I worked with transform with rvl.\nThis my configurations:\n[sources.generate_syslog]\n  type = \"stdin\"\n\n[transforms.remap_syslog]\n  inputs = [ \"generate_syslog\"]\n  type = \"remap\"\n  source = '''\n  \u00a6 structured = parse_json!(.message)\n  \u00a6 ., err = merge(., structured)\n  '''\n\n[sinks.emit_syslog]\n  inputs = [\"remap_syslog\"]\n  type = \"console\"\n  encoding.codec = \"json\"\nI think there's no special things to check.\nThe weird part is this.\nIf the message size is larger than \"4095\" bytes, then vector says like this:\nJul 15 03:22:20.806  WARN transform{component_kind=\"transform\" component_name=remap_syslog component_type=remap}: vector::internal_events::remap: Mapping failed with event. error=\"function call error for \\\"parse_json\\\" at (17:38): unable to parse json: EOF while parsing an object at line 1 column 4095\" internal_log_rate_secs=30\n\nI tried with this message:\n{\"exception\":{\"exception_class\":\"org.springframework.data.redis.RedisConnectionFailureException\",\"exception_message\":\"Unable to connect to Redis; nested exception is io.lettuce.core.RedisConnectionException: Unable to connect to engine2-dev-01001-rd-jp2v-dev.lineinfra-dev.com/<unresolved>:7000\",\"stacktrace\":\"1--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\"},\"@version\":1,\"source_host\":\"AL01922436\",\"message\":{\"formattedMessage\":\"Redis health check failed\"},\"thread_name\":\"boundedElastic-2\",\"@timestamp\":\"2021-07-14T21:01:39.865+09:00\",\"level\":\"WARN\",\"logger_name\":\"org.springframework.boot.actuate.redis.RedisReactiveHealthIndicator\"}\n\nIf I delete 1 character in any string, then I is accepted with vector.\nIt seems that there may be some limit with size of input message, right?\nMy question is:\n\nI want to know whether this is a intended spec or not.\nIs there any way to workaround this? (because the size of stacktrace string is easily larger than 4095 bytes..)\n\nI tried to search and find regarding this issue but no result. \ud83d\ude22",
        "url": "https://github.com/vectordotdev/vector/discussions/8306",
        "createdAt": "2021-07-15T03:27:37Z",
        "updatedAt": "2022-06-14T17:13:19Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "iamdobi"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 8309,
        "title": "Vector Host Statisitics",
        "bodyText": "Hey Team,\nWe want to be able to monitor disk usage percentage for our systems on OpenSearch, wondering if the best path is feature request into the host metrics to calculate this, or if there is another way, as having trouble aggregating 2 events together in OpenSearch.\nThanks\nTasty",
        "url": "https://github.com/vectordotdev/vector/discussions/8309",
        "createdAt": "2021-07-15T05:10:49Z",
        "updatedAt": "2022-07-14T22:16:24Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tastyfrankfurt"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 8155,
        "title": "Syslog Replacement",
        "bodyText": "Hey Guys,\nWe mainly deal with centos/ubuntu/redhat/rocky.  These come with a default installation of rsyslog.  Currently trying to work out a suitable configuration to replace the defaults outputs of these.  So far i have struggled to get my head around the vrl implementation.  Is someone able to help?  Filtering the log is not an issue its the outputing in a format that is usable to the log files, ie data host processname proc id then the message.  We are using journald as the source.\nThanks in advanced, i have also attached the outputs i am looking at.\n.info;mail.none;authpriv.none;cron.none                /var/log/messages\nauthpriv.                                              /var/log/secure\nmail.*                                                  -/var/log/maillog\ncron.*                                                  /var/log/cron\n.emerg                                                 :omusrmsg:\nuucp,news.crit                                          /var/log/spooler\nlocal7.*                                                /var/log/boot.log",
        "url": "https://github.com/vectordotdev/vector/discussions/8155",
        "createdAt": "2021-07-07T14:20:09Z",
        "updatedAt": "2024-12-24T21:58:45Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "tastyfrankfurt"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 0
    },
    {
        "number": 8183,
        "title": "Kafka Sink Header Support",
        "bodyText": "Does the Kafka Sink support writing headers? Can't seem to find the docs on it if it does...",
        "url": "https://github.com/vectordotdev/vector/discussions/8183",
        "createdAt": "2021-07-08T19:45:15Z",
        "updatedAt": "2022-09-15T04:11:04Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "DKaos"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 8134,
        "title": "Using Caching in LUA transform",
        "bodyText": "I need to add fields to log_events by GET'ing info via a http call. I am planning to use LUA hooks.init to maintain the cache. Will something like this work ?\nhooks.init = '''\n  cache = {}\n  function get(key)\n    if not cache[key] then\n      cache[key] = <make a http call here to populate the cache>\n    end\n    return cache[key]\n  end\n'''\nhooks.process = '''\n  function(event, emit)\n    if event.log.interface_name then\n      event.log.interface_description = get(interface_name)\n    end\n  end\n'''",
        "url": "https://github.com/vectordotdev/vector/discussions/8134",
        "createdAt": "2021-07-06T04:49:25Z",
        "updatedAt": "2022-06-23T11:48:21Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "atibdialpad"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 8133,
        "title": "Vector Lua Transform Question",
        "bodyText": "I was trying some of the examples from here and getting errors. Are the configs outdated or am I doing something wrong ? @jszwedko\n[sources.junos_log_source]\n  type = \"file\"\n  include = [\"/Users/atib/src/VECTOR/example9/junos.log\"]\n  data_dir = \"/Users/atib/src/VECTOR/example9\"\n  file_key = \"file\"\n\n\n[transforms.junos_log_processor]\n  type = \"remap\"\n  inputs = [\"junos_log_source\"]\n  source = '''\n    .timestamp_string = \"2018-04-07 06:26:02.643\"\n  '''\n\n[transforms.junos_lua_transform]\n  type = \"lua\"\n  inputs = [\"junos_log_processor\"]\n\n  hooks.init = '''\n  timestamp_pattern = \"(%d%d%d%d)[-](%d%d)[-](%d%d) (%d%d):(%d%d):(%d%d).?(%d*)\"\n  function parse_timestamp(str)\n    local year, month, day, hour, min, sec, millis = string.match(str, timestamp_pattern)\n    local ms = 0\n    if millis and millis ~= \"\" then\n        ms = tonumber(millis)\n    end\n    return {\n        year    = tonumber(year),\n        month   = tonumber(month),\n        day     = tonumber(day),\n        hour    = tonumber(hour),\n        min     = tonumber(min),\n        sec     = tonumber(sec),\n        nanosec = ms * 1000000\n    }\n  end\n  '''\n\n  hooks.process = \"\"\"\n  function (event, emit)\n    event.log.timestamp = parse_timestamp(event.log.timestamp_string)\n    emit(event)\n  end\n  \"\"\"\n\n[sinks.console]\n  type = \"console\"\n  inputs = [\"junos_lua_transform\"]\n  target = \"stdout\"\n  encoding.codec = \"json\"\n\n\nI get the following error :\nJul 06 10:10:57.460 ERROR vector::cli: Configuration error. error=data did not match any variant of untagged enum LuaConfig for key `transforms.junos_lua_transform` at line 46 column 1",
        "url": "https://github.com/vectordotdev/vector/discussions/8133",
        "createdAt": "2021-07-06T04:41:11Z",
        "updatedAt": "2022-07-05T14:53:20Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "atibdialpad"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 8100,
        "title": "Unclear about timestamps",
        "bodyText": "I am trying to use file sink, where do these values come from? I tried with timezone=\"America/Los_Angeles at top in my pipeline. But it doesn't seem to honor it, default to UTC I think. At the time of writing, LA time is 11:30PM Jun 30, but the file is created with July 1 using the below\n[sinks.local_file]\n  type = \"file\"\n  path = \"/data/Vector-%Y-%m-%d.json\"\n  compression = \"none\"\n  encoding.codec = \"ndjson\"\n\nMinimal reproducible example\n# cat vector_sample.toml\n[sources.in]\n  type = \"stdin\"\n\n[sinks.out]\n  inputs = [\"in\"]\n  type = \"console\"\n  encoding.codec = \"json\"\n\n\n# date  # LA time\nWed Jun 30 23:44:35 PDT 2021\n#\n# vector --config ./vector_sample.toml\nJun 30 23:44:38.940  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=trace,rdkafka=info\"\nJun 30 23:44:38.940  INFO vector::sources::host_metrics: PROCFS_ROOT is unset. Using default '/proc' for procfs root.\nJun 30 23:44:38.940  INFO vector::sources::host_metrics: SYSFS_ROOT is unset. Using default '/sys' for sysfs root.\nJun 30 23:44:38.941  INFO vector::app: Loading configs. path=[(\"vector_sample.toml\", None)]\nJun 30 23:44:38.947  INFO vector::topology: Running healthchecks.\nJun 30 23:44:38.948  INFO vector::sources::stdin: Capturing STDIN.\nJun 30 23:44:38.948  INFO vector::topology: Starting source. name=\"in\"\nJun 30 23:44:38.948  INFO vector::topology::builder: Healthcheck: Passed.\nJun 30 23:44:38.948  INFO vector::topology: Starting sink. name=\"out\"\nJun 30 23:44:38.948  INFO vector: Vector has started. version=\"0.14.0\" arch=\"x86_64\" build_id=\"5f3a319 2021-06-03\"\nJun 30 23:44:38.948  INFO vector::app: API is disabled, enable by setting `api.enabled` to `true` and use commands like `vector top`.\n123\n{\"host\":\"xxx\",\"message\":\"123\",\"source_type\":\"stdin\",\"timestamp\":\"2021-07-01T06:44:40.795367048Z\"}\n^CJun 30 23:44:42.021  INFO vector: Vector has stopped.\nJun 30 23:44:42.021  INFO source{component_kind=\"source\" component_name=in component_type=stdin}: vector::sources::stdin: Finished sending.",
        "url": "https://github.com/vectordotdev/vector/discussions/8100",
        "createdAt": "2021-07-01T06:20:53Z",
        "updatedAt": "2022-06-01T17:23:42Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "trickster"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 6096,
        "title": "Kafka source with a Kerberos Keytab authentication",
        "bodyText": "Hello everyone,\nI have a Kafka server with Kerberos GSSAPI authentication using a keytab file.\nI was looking about how to set up vector to use the Keytab file to authenticate with Kafka, but i couldn't find anything in the documentation.\nCan anyone help me with this?",
        "url": "https://github.com/vectordotdev/vector/discussions/6096",
        "createdAt": "2021-01-16T05:06:46Z",
        "updatedAt": "2022-06-14T11:59:10Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Shogobg"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 7503,
        "title": "When does adding metadata to logs get to be too much?",
        "bodyText": "Given the recent set of questions being asked about enriching logs with metadata from different sources (primarily kubernetes related metadata sources ({node, pod, namespace} labels and annotations), how can we help folks understand the impact of adding all this metadata to logs?\nPerhaps we could offer a way to track the statistical distribution of message sizes being processed through vector independent of all the enrichment, and then track separately what each category of enrichment is adding to the data being processed?\nIf those numbers were available as observable behaviors of vector, then SREs operating vector can make informed decisions on its behavior and how to trade off the impact of a particular enrichment.\nFurther, it might be worth considering how to separately forward changes to the metadata sources used by the enrichment processes instead of enriching each piece of data.  Then, vector can efficiently forward off host the core data sets with only the required metadata needed to perform enrichment later, while still efficiently providing that metadata stream for some other system (providing for a late binding, if you will, of the metadata association).",
        "url": "https://github.com/vectordotdev/vector/discussions/7503",
        "createdAt": "2021-05-19T01:10:20Z",
        "updatedAt": "2022-08-02T05:18:14Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "portante"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 7879,
        "title": "sink downtime , buffering",
        "bodyText": "Hi,\nWe are interested how vector behave in case of sink downtime, does it support buffering .\nAre there any documentation on this topic?\nThanks,\nTomer",
        "url": "https://github.com/vectordotdev/vector/discussions/7879",
        "createdAt": "2021-06-16T11:12:46Z",
        "updatedAt": "2022-07-11T12:37:49Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "tomer-epstein"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 7772,
        "title": "Parsing apache access and error logs and inserting values in clickhouse",
        "bodyText": "Hi,\nsince two weeks now I am struggling with the following task:\nI have to receive apache access and error log as syslog messages and insert the separate values into clickhouse tables. With apache access logs I managed to do this using vector and a regex-parser transforms. With error logs I could not find out how to parse them correctly so that a clickhouse sink was able to insert the values into clickhouse.\nThen I discovered the \"parse_apache_log()\" function and I thought it might be easier to use this instead of the regex-parser. But with this function I do have the problem that I do not understand how to get a result which can be handled by the clickhouse sink correctly.\nIn whole I'd like to do the following:\n\nDefine an input which receives the syslog messages. That's no problem. I did it like this:\n\n[sources.in]\n  address = \"0.0.0.0:514\" # required, required when mode = \"tcp\" or mode = \"udp\"\n  mode = \"tcp\" # required\n  type = \"syslog\" # required\n\n\nI added a transforms section to modify some fields:\n\n[transforms.field_modifier]\n  type = \"remap\"\n  inputs = [\"in\"]\n  source = '''\n  .uuid = uuid_v4()\n  del(.facility)\n  del(.severity)\n  del(.source_type)\n  del(.hostname)\n  '''\n\n\nI think I need to have a filter which separates access and error log message to be able to parse them differently (cause they do have different format):\n\n[transforms.access_log_filter]\n  type = \"filter\"\n  inputs = [\"field_modifier\"]\n  condition = '.appname == \"apache_access\"'\n\n[transforms.error_log_filter]\n  type = \"filter\"\n  inputs = [\"in\"]\n  condition = '.appname == \"apache_error\"'\n\n\nI want to transform the syslog event and I want the following to be done:\n\n\nparse message field so that all parts (ip, status, path, length, user-agent ...) can be inserted by a sink into separate columns of a clickhouse table.\n\n[transforms.access_log_parser]\n  type = \"remap\"\n  inputs = [\"access_log_filter\"]\n  source = '''\n  .message = parse_apache_log!(.message, \"combined\")\n  '''\n\n\nLast I have a sink to insert data into clickhouse:\n\n[sinks.access_log]\n  host = \"http://localhost:8123\" # required\n  inputs = [\"access_log_parser\"] # required\n  table = \"access_log\"\n  type = \"clickhouse\" # required\n  auth.password = \"*****************\"\n  auth.strategy = \"basic\"\n  auth.user = \"default\"\n  encoding.timestamp_format = \"unix\"\n\nI also will need a second sink for error logs - but as the sink for access logs does not work yet I left this out yet.\nAll this does not work. I do get a sink error:\nJun 07 11:05:17.847 ERROR sink{component_kind=\"sink\" component_name=access_log component_type=clickhouse}:request{request_id=0}: vector::sinks::util::retries: Not retriable; dropping the request. reason=\"response status: 400 Bad Request\"\nJun 07 11:05:17.847 ERROR sink{component_kind=\"sink\" component_name=access_log component_type=clickhouse}:request{request_id=0}: vector::sinks::util::sink: Response wasn't successful. response=Response { status: 400, version: HTTP/1.1, headers: {\"date\": \"Mon, 07 Jun 2021 09:05:17 GMT\", \"connection\": \"Keep-Alive\", \"content-type\": \"text/tab-separated-values; charset=UTF-8\", \"x-clickhouse-server-display-name\": \"logexplorer.az-logexplorer.scalecommerce\", \"transfer-encoding\": \"chunked\", \"x-clickhouse-query-id\": \"aa5965b2-3980-4ab3-82cb-2626492d4c10\", \"x-clickhouse-format\": \"TabSeparated\", \"x-clickhouse-timezone\": \"Europe/Berlin\", \"x-clickhouse-exception-code\": \"26\", \"keep-alive\": \"timeout=3\", \"x-clickhouse-summary\": \"{\\\"read_rows\\\":\\\"0\\\",\\\"read_bytes\\\":\\\"0\\\",\\\"written_rows\\\":\\\"0\\\",\\\"written_bytes\\\":\\\"0\\\",\\\"total_rows_to_read\\\":\\\"0\\\"}\"}, body: b\"Code: 26, e.displayText() = DB::ParsingException: Cannot parse JSON string: expected opening quote: (while reading the value of key message): (at row 1)\\n (version 21.5.5.12 (official build))\\n\" }\n\nI assume that step 4 is wrong but I have no idea how to do it the right way. For the parsing part I also tried this one:\n[transforms.access_log_parser]\n  type = \"remap\"\n  inputs = [\"access_log_filter\"]\n  source = '''\n  . = parse_apache_log!(.message, \"combined\")\n  '''\n\nThis does work so far that values are inserted in clickhouse - but only the parsed parts of the message field not the contents of other fields (uuid, appname, source_ip).\nSo if anybody could help me to get the message parsed without loosing the other fields - and also how to do it with error logs I'd very much appreciate some support!",
        "url": "https://github.com/vectordotdev/vector/discussions/7772",
        "createdAt": "2021-06-07T09:40:43Z",
        "updatedAt": "2022-06-18T14:59:43Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "SirUrban"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 7800,
        "title": "How quickly are new kubernetes pod labels detected?",
        "bodyText": "Hi folks -- thanks for such a wonderful piece of open source software?\nI am using the kubernetes_logs source and processing the log entries just fine, but I have some fancy stuff running in kubernetes that applies new pod labels to existing pods, long after they started. Vector picks up these new labels in the autodiscovered data it sticks in each event eventually, but I am struggling to discern when or if/when there is a delay. How fast should we expect new pod labels to be reflected in emitted log events? Is there a way to control this timing?\nThanks for any insight you can share!",
        "url": "https://github.com/vectordotdev/vector/discussions/7800",
        "createdAt": "2021-06-09T00:36:20Z",
        "updatedAt": "2023-03-25T09:09:27Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "airhorns"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 7708,
        "title": "scrape http target with json?",
        "bodyText": "Didn't find a way to scrape a remote target over http with json support.\nDoes it exist ?",
        "url": "https://github.com/vectordotdev/vector/discussions/7708",
        "createdAt": "2021-06-02T13:15:49Z",
        "updatedAt": "2022-06-25T19:48:48Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "alpiua"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 7698,
        "title": "Routing based on simple regex",
        "bodyText": "Is there a simple way to route message based on a regex max outside of using \"parse_regex\".  Seems using that function requires to check if the match is not nil nor error",
        "url": "https://github.com/vectordotdev/vector/discussions/7698",
        "createdAt": "2021-06-01T21:09:59Z",
        "updatedAt": "2022-07-14T08:26:15Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "jcantrill"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 7512,
        "title": "Can vector sink data to HDFS/Hadoop ?",
        "bodyText": "Hi,\nNow we use Flume to consume data from kafka and send it to HDFS. But we want use Vector for that task, because we do that in other cases and Vector is better than Flume.\nIs it possible to do that?",
        "url": "https://github.com/vectordotdev/vector/discussions/7512",
        "createdAt": "2021-05-19T12:03:59Z",
        "updatedAt": "2022-06-21T13:21:30Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "Rattnik"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 7505,
        "title": "gcp_stackdriver sink api_key?",
        "bodyText": "I see some references to api_key in the stackdriver docs here https://vector.dev/docs/reference/configuration/sinks/gcp_stackdriver_metrics/#gcp-authentication\nbut it seems like this functionality is really only implemented for the pubsub sink?",
        "url": "https://github.com/vectordotdev/vector/discussions/7505",
        "createdAt": "2021-05-19T03:43:17Z",
        "updatedAt": "2022-06-23T15:59:18Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "phact"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 7480,
        "title": "k8s log source questions",
        "bodyText": "@jszwedko  I had couple of questions around the usage of vector's k8s source :\n\nDo I need to mount my container logs to a shared volume (/var/logs/pod_logs) on the kube-node to enable Vector to consume container logs ?\nDatadog Agent daemon set allows to add additional tags to log events via setting  environment variable in the daemon set declaration, examples  a) common tags from env here, b) append pod labels as tags to log events here .Is something like that possible to do in Vector ?",
        "url": "https://github.com/vectordotdev/vector/discussions/7480",
        "createdAt": "2021-05-17T12:42:26Z",
        "updatedAt": "2022-06-13T14:10:19Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "atibdialpad"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 2
        },
        "upvoteCount": 1
    },
    {
        "number": 7486,
        "title": "Exposing collector prometheus metrics",
        "bodyText": "Is there any documentation on how to expose prometheus metrics for the collector itself, not as an intermediary to pull/push metrics from source to destination?",
        "url": "https://github.com/vectordotdev/vector/discussions/7486",
        "createdAt": "2021-05-17T17:18:12Z",
        "updatedAt": "2024-12-24T21:57:30Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "jcantrill"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 7491,
        "title": "Nginx and node.js logs, new Elasticsearch alternative",
        "bodyText": "There is new Elasticsearch + Kibana alternative (from Elasticsearch 7.10.2 & Kibana 7.10.2). Amazon will develop this project because of changes in Elasticsearch license.\nhttps://opensearch.org/\nhttps://docs-beta.opensearch.org/\nI think I can pass logs to OpenSearch as for Elasticsearch.\n\n\nI'm newbie and I can't understand how to pass Nginx access and error logs using vector agent. Should I change access logs format to json in Nginx or leave it as it is by default and parse using vector config? Can you give an example how to parse Nginx access and error logs? Can I filter access logs to collect everything except HTTP 200 messages (I'd like to collect messages with anomalistic HTTP codes).\n\n\nI've read that Timber.io can collect data from node.js application? Is it possible to collect data to Elasticsearch/Opensearch using on premise vector installation (agent)? We use pm2 and  keep logs in .pm2/logsdirectory. Example:\n\n\n{\"level\":50,\"time\":1621270199982,\"pid\":18971,\"hostname\":\"main-infra-backup1\",\"name\":\"GraphQL Error\",\"requestId\":\"ad73eb20-f803-4e99-986b-05ac82a7a2bc\",\"msg\":\"Headers:\",\"message\":\"ER_NO_REFERENCED_ROW_2: Cannot add or update a child row: a foreign key constraint fails (cimp_staging.schedule_plan_stages_has_stages, CONSTRAINT FK_d54ff796a859acb62dd1f4d88b0 FOREIGN KEY (childrenId) REFERENCES schedule_plan_stages (schedulePlanStageId) ON DELETE CAS)\",\"locations\":[{\"line\":2,\"column\":3}],\"path\":[\"updateSchedulePlanStage\"], ...\nwhere can I find example how to collect nginx and node.js logs and sent them to Elasticsearch?\n\nWould vector collect data unless i press Ctrl+C using command in manual:\nvector --config /etc/vector/name_of_file.toml\n\nShould I use systemctl start vector to make it work permanently?\n\nCan I use the same name in config files on different servers? For example [sources.nginx_metrics]",
        "url": "https://github.com/vectordotdev/vector/discussions/7491",
        "createdAt": "2021-05-18T00:53:13Z",
        "updatedAt": "2022-11-13T06:26:40Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "anutator"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 2
    },
    {
        "number": 7433,
        "title": "How is vector able to keep up with multiple container logs?",
        "bodyText": "Hi folks,\nHow does vector handle scaling the number of log files for the various container use cases?\nIf I have an 40 CPU box (2 socket, 10 cores per socket, HT enabled), and perhaps have a ratio of 2:1 for containers to CPUs, where the containers are actively logging, how does vector handle the scale of reading from all those files?\nDoes it employ multiple CPUs to keep up?\nThanks!",
        "url": "https://github.com/vectordotdev/vector/discussions/7433",
        "createdAt": "2021-05-12T19:23:52Z",
        "updatedAt": "2022-09-07T17:04:32Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "portante"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 6983,
        "title": "Allow empty sources and sinks?",
        "bodyText": "I have a scene is that often generate config vector.json automatically and reload vector, but there are no sources and sinks during initialization. Is it a good idea that allow empty sources and sinks?",
        "url": "https://github.com/vectordotdev/vector/discussions/6983",
        "createdAt": "2021-04-02T10:32:05Z",
        "updatedAt": "2022-11-17T03:10:04Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "jmjoy"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 6862,
        "title": "Support syslog sink?",
        "bodyText": "I found that syslog only support for source, not sink, but like logstash, syslog is also support for output.\nAdd support?",
        "url": "https://github.com/vectordotdev/vector/discussions/6862",
        "createdAt": "2021-03-23T09:41:48Z",
        "updatedAt": "2022-06-13T10:14:26Z",
        "isAnswered": false,
        "locked": false,
        "author": {
            "login": "jmjoy"
        },
        "category": {
            "name": "General"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 5
    },
    {
        "number": 6259,
        "title": "Single line log file",
        "bodyText": "How to handle in vector log files written as one line? As now it stops on max_line_bytes and ignores the rest of the file...",
        "url": "https://github.com/vectordotdev/vector/discussions/6259",
        "createdAt": "2021-01-27T14:04:21Z",
        "updatedAt": "2022-08-10T18:26:27Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "igor-teresco"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    },
    {
        "number": 6068,
        "title": "API Endpoint not active when running as Windows Service",
        "bodyText": "vector 0.11.1 (v0.11.1 x86_64-pc-windows-msvc 2020-12-17)\nHello!\nI have specified\n[api]\n  enabled = true\n\nin the configuration.\nWhen i start vector from a Command Prompt, the Endpoint is accessible.\nWhen started as a Windows Service, the Endpoint is not available.\nIs this a problem with the configuration, or a problem?\nCheers\nS\u00f6nke",
        "url": "https://github.com/vectordotdev/vector/discussions/6068",
        "createdAt": "2021-01-15T10:38:43Z",
        "updatedAt": "2022-08-15T07:04:44Z",
        "isAnswered": true,
        "locked": false,
        "author": {
            "login": "xgcssch"
        },
        "category": {
            "name": "Q&A"
        },
        "comments": {
            "totalCount": 1
        },
        "upvoteCount": 1
    }
]